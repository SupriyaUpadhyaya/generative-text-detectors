{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/supriyaupadhyaya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from statistics import stdev, mean\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_sentences(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def count_sentences_per_paragraph(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    sentences_per_paragraph = []\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through each paragraph and count the sentences\n",
    "    for paragraph in paragraphs:\n",
    "        num_sentences = count_sentences(paragraph)\n",
    "        sentences_per_paragraph.append(num_sentences)\n",
    "        total += num_sentences\n",
    "\n",
    "    return total/len(paragraphs)\n",
    "\n",
    "def count_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "def count_words_per_paragraph(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    words_per_paragraph = []\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through each paragraph and count the words\n",
    "    for paragraph in paragraphs:\n",
    "        num_words = count_words(paragraph)\n",
    "        words_per_paragraph.append(num_words)\n",
    "        total += num_words\n",
    "\n",
    "    return total/len(paragraphs)\n",
    "\n",
    "def check_character_presence(text, character):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    character_presence = 0\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each paragraph and check if the character is present\n",
    "    for paragraph in paragraphs:\n",
    "        if character in paragraph:\n",
    "            character_presence = 1\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def paragraph_sentence_length_std_dev(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    paragraph_std_devs = []\n",
    "    total = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # Tokenize the paragraph into sentences\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "        # Calculate the length of each sentence\n",
    "        sentence_lengths = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "        if len(sentence_lengths) > 1:\n",
    "            # Calculate the mean length of sentences\n",
    "            mean_length = np.mean(sentence_lengths)\n",
    "\n",
    "            # Calculate the squared differences between each sentence length and the mean\n",
    "            squared_diffs = [(length - mean_length) ** 2 for length in sentence_lengths]\n",
    "\n",
    "            # Calculate the variance\n",
    "            variance = np.mean(squared_diffs)\n",
    "\n",
    "            # Calculate the standard deviation\n",
    "            std_dev = np.sqrt(variance)\n",
    "        else:\n",
    "            # If there's only one sentence in the paragraph, standard deviation is 0\n",
    "            std_dev = 0\n",
    "        \n",
    "        paragraph_std_devs.append(std_dev)\n",
    "        total += std_dev\n",
    "\n",
    "    return total/len(paragraphs)\n",
    "\n",
    "def max_length_difference_paragraph(paragraph):\n",
    "    # Tokenize the paragraph into sentences\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    \n",
    "    max_diff = 0\n",
    "\n",
    "    # Iterate over each pair of consecutive sentences\n",
    "    for i in range(len(sentences) - 1):\n",
    "        # Calculate the length difference between consecutive sentences\n",
    "        diff = abs(len(nltk.word_tokenize(sentences[i])) - len(nltk.word_tokenize(sentences[i+1])))\n",
    "\n",
    "        # Update max_diff if the current difference is greater\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "\n",
    "    return max_diff\n",
    "\n",
    "def count_short_sentences_in_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    short_sentence_counts = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # Tokenize the paragraph into sentences\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "        # Count the number of sentences with less than 11 words\n",
    "        count = sum(1 for sentence in sentences if len(nltk.word_tokenize(sentence)) < 11)\n",
    "\n",
    "        short_sentence_counts += count\n",
    "\n",
    "    return short_sentence_counts\n",
    "\n",
    "def count_long_sentences_in_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    long_sentence_counts = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # Tokenize the paragraph into sentences\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "        # Count the number of sentences with less than 11 words\n",
    "        count = sum(1 for sentence in sentences if len(nltk.word_tokenize(sentence)) > 34)\n",
    "\n",
    "        long_sentence_counts += count\n",
    "\n",
    "    return long_sentence_counts\n",
    "\n",
    "def check_words_in_paragraphs(text, words_to_check):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "\n",
    "    presence = 0\n",
    "    count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Check if any of the words are present in the paragraph\n",
    "        if any(word in paragraph for word in words_to_check):\n",
    "            presence = 1\n",
    "            count += 1\n",
    "\n",
    "    return presence\n",
    "\n",
    "def check_numbers_in_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    presence_per_paragraph = []\n",
    "    count = 0\n",
    "    check = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Check if any numbers are present in the paragraph using regular expression\n",
    "        if re.search(r'\\d+', paragraph):\n",
    "            presence_per_paragraph.append(1)\n",
    "            check = 1\n",
    "            count += 1\n",
    "        else:\n",
    "            presence_per_paragraph.append(0)\n",
    "\n",
    "    return count\n",
    "\n",
    "def check_capitals_to_periods_ratio(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    presence_per_paragraph = []\n",
    "    check = 0\n",
    "    count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Count the number of capital letters and periods in the paragraph\n",
    "        capital_count = sum(1 for char in paragraph if char.isupper())\n",
    "        period_count = paragraph.count('.')\n",
    "\n",
    "        # Check if the paragraph contains twice as many capitals as periods\n",
    "        if capital_count >= 2 * period_count:\n",
    "            presence_per_paragraph.append(1)\n",
    "            check = 1\n",
    "            count += 1\n",
    "        else:\n",
    "            presence_per_paragraph.append(0)\n",
    "\n",
    "    return count\n",
    "\n",
    "def check_et_in_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    presence_per_paragraph = []\n",
    "    check = 0\n",
    "    count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Check if the paragraph contains the substring \"et\"\n",
    "        if 'et' in paragraph:\n",
    "            presence_per_paragraph.append(1)\n",
    "            check = 1\n",
    "            count += 1\n",
    "        else:\n",
    "            presence_per_paragraph.append(0)\n",
    "\n",
    "    return check\n",
    "\n",
    "def normalize_column(column):\n",
    "    min_val = column.min()\n",
    "    max_val = column.max()\n",
    "    normalized_column = (column - min_val) / (max_val - min_val)\n",
    "    return normalized_column\n",
    "\n",
    "def delete_csv_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "\n",
    "def contains_word(string, word):\n",
    "    return word in string\n",
    "\n",
    "def extract_features(filepath, params):\n",
    "    header = True\n",
    "    path = '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/feature-extractor/features/m4/'\n",
    "    outputfilename =  path + filepath.split('.')[0].split('/')[-1] + '_feature.csv'\n",
    "    print(outputfilename)\n",
    "\n",
    "    delete_csv_if_exists(outputfilename)\n",
    "\n",
    "    dfs = pd.read_json(filepath, lines=True)\n",
    "    characters = [\")\", \"-\", \";\", \":\", \"?\", \"'\"]\n",
    "    count = 0\n",
    "    for df in dfs.itertuples():\n",
    "        df_for_loop = pd.DataFrame(df)\n",
    "\n",
    "        if contains_word(filepath, 'bloomz'):\n",
    "            abstract = df.abstract\n",
    "            machine_abstract = df.machine_abstract\n",
    "        else:\n",
    "            abstract = df.human_text\n",
    "            machine_abstract = df.machine_text\n",
    "\n",
    "        num_sentence_human = count_sentences_per_paragraph(abstract)\n",
    "        num_sentence_machine = count_sentences_per_paragraph(machine_abstract)\n",
    "\n",
    "        num_words_human = count_words_per_paragraph(abstract)\n",
    "        num_words_machine = count_words_per_paragraph(machine_abstract)\n",
    "\n",
    "        character0_human = check_character_presence(abstract, characters[0])\n",
    "        character1_human = check_character_presence(abstract, characters[1])\n",
    "        character2_human = check_character_presence(abstract, characters[2])\n",
    "        character3_human = check_character_presence(abstract, characters[3])\n",
    "        character4_human = check_character_presence(abstract, characters[4])\n",
    "        character5_human = check_character_presence(abstract, characters[5])\n",
    "\n",
    "        character2_3_human = 0\n",
    "\n",
    "        if character2_human == 1 or character3_human == 1:\n",
    "            character2_3_human = 1\n",
    "\n",
    "        character0_machine = check_character_presence(machine_abstract, characters[0])\n",
    "        character1_machine = check_character_presence(machine_abstract, characters[1])\n",
    "        character2_machine = check_character_presence(machine_abstract, characters[2])\n",
    "        character3_machine = check_character_presence(machine_abstract, characters[3])\n",
    "        character4_machine = check_character_presence(machine_abstract, characters[4])\n",
    "        character5_machine = check_character_presence(machine_abstract, characters[5])\n",
    "\n",
    "        character2_3_machine = 0\n",
    "\n",
    "        if character2_machine == 1 or character3_machine == 1:\n",
    "            character2_3_machine = 1\n",
    "        \n",
    "        std_dev_human = paragraph_sentence_length_std_dev(abstract)\n",
    "        std_dev_machine = paragraph_sentence_length_std_dev(machine_abstract)\n",
    "\n",
    "        sent_len_diff_human = max_length_difference_paragraph(abstract)\n",
    "        sent_len_diff_machine = max_length_difference_paragraph(machine_abstract)\n",
    "\n",
    "        count_short_sentences_in_paragraphs_human = count_short_sentences_in_paragraphs(abstract)\n",
    "        count_short_sentences_in_paragraphs_machine = count_short_sentences_in_paragraphs(machine_abstract)\n",
    "\n",
    "        count_long_sentences_in_paragraphs_human = count_long_sentences_in_paragraphs(abstract)\n",
    "        count_long_sentences_in_paragraphs_machine = count_long_sentences_in_paragraphs(machine_abstract)\n",
    "\n",
    "\n",
    "        words = [\"although\", \"However\", \"but\", \"because\", \"this\", \"others\", \"researchers\"]\n",
    "\n",
    "        check_word0_human = check_words_in_paragraphs(abstract, words[0])\n",
    "        check_word0_machine = check_words_in_paragraphs(machine_abstract, words[0])\n",
    "\n",
    "        check_word1_human = check_words_in_paragraphs(abstract, words[1])\n",
    "        check_word1_machine = check_words_in_paragraphs(machine_abstract, words[1])\n",
    "\n",
    "        check_word2_human = check_words_in_paragraphs(abstract, words[2])\n",
    "        check_word2_machine = check_words_in_paragraphs(machine_abstract, words[2])\n",
    "\n",
    "        check_word3_human = check_words_in_paragraphs(abstract, words[3])\n",
    "        check_word3_machine = check_words_in_paragraphs(machine_abstract, words[3])\n",
    "\n",
    "        check_word4_human = check_words_in_paragraphs(abstract, words[4])\n",
    "        check_word4_machine = check_words_in_paragraphs(machine_abstract, words[4])\n",
    "\n",
    "        check_word5_human = check_words_in_paragraphs(abstract, words[5])\n",
    "        check_word5_machine = check_words_in_paragraphs(machine_abstract, words[5])\n",
    "\n",
    "        check_word6_human = check_words_in_paragraphs(abstract, words[6])\n",
    "        check_word6_machine = check_words_in_paragraphs(machine_abstract, words[6])\n",
    "\n",
    "        check_word2_3_machine = 0\n",
    "\n",
    "        if check_word2_machine == 1 or check_word3_machine == 1:\n",
    "            check_word2_3_machine = 1\n",
    "\n",
    "        check_word2_3_human = 0\n",
    "\n",
    "        if check_word2_human == 1 or check_word3_human == 1:\n",
    "            check_word2_3_human = 1\n",
    "\n",
    "        check_num_human = check_numbers_in_paragraphs(abstract)\n",
    "        check_num_machine = check_numbers_in_paragraphs(machine_abstract)\n",
    "\n",
    "        check_capitals_human = check_capitals_to_periods_ratio(abstract)\n",
    "        check_capitals_machine = check_capitals_to_periods_ratio(machine_abstract)\n",
    "\n",
    "        check_et_human = check_et_in_paragraphs(abstract)\n",
    "        check_et_machine = check_et_in_paragraphs(machine_abstract)\n",
    "        #print(\"num_words_human \", num_words_human)\n",
    "        # print(\"character0_human \", character0_human)\n",
    "        # print(\"character1_human \", character1_human)\n",
    "        # print(\"character2_3_human \", character2_3_human)\n",
    "        # print(\"character4_human \", character4_human)\n",
    "        # print(\"character5_human \", character5_human)\n",
    "\n",
    "        # print(\"character0_machine \", character0_machine)\n",
    "        # print(\"character1_machine \", character1_machine)\n",
    "        # print(\"character2_3_machine \", character2_3_machine)\n",
    "        # print(\"character4_machine \", character4_machine)\n",
    "        # print(\"character5_machine \", character5_machine)\n",
    "        #print(\"count_long_sentences_in_paragraphs_human \", count_long_sentences_in_paragraphs_human)\n",
    "        #print(\"count_long_sentences_in_paragraphs_machine \", count_long_sentences_in_paragraphs_machine)\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for param in params:\n",
    "            data[param] = dfs.loc[count, param]\n",
    "\n",
    "        data['no_sentence_human'] = [num_sentence_human]\n",
    "        data['no_sentence_machine'] = [num_sentence_machine]\n",
    "        data['num_words_human'] = [num_words_human]\n",
    "        data['num_words_machine'] = [num_words_machine]\n",
    "        data['character0_human'] = [character0_human]\n",
    "        data['character1_human'] = [character1_human]\n",
    "        data['character2_3_human'] = [character2_3_human]\n",
    "        data['character4_human'] = [character4_human]\n",
    "        data['character5_human'] = [character5_human]\n",
    "        data['character0_machine'] = [character0_machine]\n",
    "        data['character1_machine'] = [character1_machine]\n",
    "        data['character2_3_machine'] = [character2_3_machine]\n",
    "        data['character4_machine'] = [character4_machine]\n",
    "        data['character5_machine'] = [character5_machine]\n",
    "        data['std_dev_human'] = [std_dev_human]\n",
    "        data['std_dev_machine'] = [std_dev_machine]\n",
    "        data['sent_len_diff_human'] = [sent_len_diff_human]\n",
    "        data['sent_len_diff_machine'] = [sent_len_diff_machine]\n",
    "        data['count_short_sentences_in_paragraphs_human'] = [count_short_sentences_in_paragraphs_human]\n",
    "        data['count_short_sentences_in_paragraphs_machine'] = [count_short_sentences_in_paragraphs_machine]\n",
    "        data['count_long_sentences_in_paragraphs_human'] = [count_long_sentences_in_paragraphs_human]\n",
    "        data['count_long_sentences_in_paragraphs_machine'] = [count_long_sentences_in_paragraphs_machine]\n",
    "        data['check_word0_human'] = [check_word0_human]\n",
    "        data['check_word1_human'] = [check_word1_human]\n",
    "        data['check_word2_3_human'] = [check_word2_3_human]\n",
    "        data['check_word3_human'] = [check_word3_human]\n",
    "        data['check_word4_human'] = [check_word4_human]\n",
    "        data['check_word5_human'] = [check_word5_human]\n",
    "        data['check_word0_machine'] = [check_word0_machine]\n",
    "        data['check_word1_machine'] = [check_word1_machine]\n",
    "        data['check_word2_3_machine'] = [check_word2_3_machine]\n",
    "        data['check_word3_machine'] = [check_word3_machine]\n",
    "        data['check_word4_machine'] = [check_word4_machine]\n",
    "        data['check_word5_machine'] = [check_word5_machine]\n",
    "        data['check_num_human'] = [check_num_human]\n",
    "        data['check_num_machine'] = [check_num_machine]\n",
    "        data['check_capitals_human'] = [check_capitals_human]\n",
    "        data['check_capitals_machine'] = [check_capitals_machine]\n",
    "        data['check_et_human'] = [check_et_human]\n",
    "        data['check_et_machine'] = [check_et_machine]\n",
    "\n",
    "        count += 1\n",
    "        df1 = pd.DataFrame(data)\n",
    "        df1.to_csv(outputfilename, mode='a', index=False, header=header)\n",
    "        header = False\n",
    "    \n",
    "    df = pd.read_csv(outputfilename)\n",
    "    df['no_sentence_human'] = normalize_column(df['no_sentence_human'])\n",
    "    df['no_sentence_machine'] = normalize_column(df['no_sentence_machine'])\n",
    "    df['num_words_human'] = normalize_column(df['num_words_human'])\n",
    "    df['num_words_machine'] = normalize_column(df['num_words_machine'])\n",
    "    df['std_dev_human'] = normalize_column(df['std_dev_human'])\n",
    "    df['sent_len_diff_human'] = normalize_column(df['sent_len_diff_human'])\n",
    "    df['std_dev_machine'] = normalize_column(df['std_dev_machine'])\n",
    "    df['sent_len_diff_machine'] = normalize_column(df['sent_len_diff_machine'])\n",
    "    df.to_csv(outputfilename, index=False)\n",
    "    print(df.head())\n",
    "    print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_cohere.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_bloomz_validation.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_davinci_train.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_davinci_test.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_chatGPT_train.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_flant5_validation.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_cohere_train.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_davinci.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_davinci_validation.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_chatGPT_validation.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_flant5_train.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_flant5.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_chatGPT.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_bloomz_train.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_chatGPT_test.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_cohere_test.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_cohere_validation.jsonl', '/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_flant5_test.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_file_paths(directory):\n",
    "    file_paths = []\n",
    "    # List all files in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        # Join directory path with file name to get the full file path\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        # Append file path to the list\n",
    "        file_paths.append(file_path)\n",
    "    return file_paths\n",
    "\n",
    "# Example usage:\n",
    "directory = \"/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n\"\n",
    "file_paths = get_file_paths(directory)\n",
    "print(file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/m4/with_n/arxiv_cohere.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_ID']\n",
      "/Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/feature-extractor/features/m4/arxiv_cohere_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/Library/Mobile Documents/com~apple~CloudDocs/OVGU/project-generative-text-detector/generative-text-detectors/feature-extractor/features/m4/arxiv_cohere_feature.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'source_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'source_ID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     attributes \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhuman_text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmachine_text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msource_ID\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(attributes)\n\u001b[0;32m---> 12\u001b[0m extract_features(\u001b[39minput\u001b[39;49m, attributes)\n",
      "Cell \u001b[0;32mIn[176], line 354\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(filepath, params)\u001b[0m\n\u001b[1;32m    351\u001b[0m data \u001b[39m=\u001b[39m {}\n\u001b[1;32m    353\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m params:\n\u001b[0;32m--> 354\u001b[0m     data[param] \u001b[39m=\u001b[39m dfs\u001b[39m.\u001b[39;49mloc[count, param]\n\u001b[1;32m    356\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mno_sentence_human\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [num_sentence_human]\n\u001b[1;32m    357\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mno_sentence_machine\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [num_sentence_machine]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexing.py:1066\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[1;32m   1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3914\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   3911\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   3912\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[index]\n\u001b[0;32m-> 3914\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item_cache(col)\n\u001b[1;32m   3915\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_engine\n\u001b[1;32m   3917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   3918\u001b[0m     \u001b[39m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   3919\u001b[0m     \u001b[39m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   3920\u001b[0m     \u001b[39m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:4271\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4266\u001b[0m res \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39mget(item)\n\u001b[1;32m   4267\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4268\u001b[0m     \u001b[39m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[1;32m   4269\u001b[0m     \u001b[39m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[0;32m-> 4271\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(item)\n\u001b[1;32m   4272\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(loc, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   4274\u001b[0m     cache[item] \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'source_ID'"
     ]
    }
   ],
   "source": [
    "def contains_word(string, word):\n",
    "    return word in string\n",
    "\n",
    "attributes = []\n",
    "for input in file_paths:\n",
    "    print(input)\n",
    "    if contains_word(input, 'bloomz'):\n",
    "        attributes = ['title', 'abstract', 'machine_abstract', 'source', 'model', 'source_id']\n",
    "    elif contains_word(input, 'cohere'):\n",
    "        attributes = ['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
    "    elif contains_word(input, 'davinci'):\n",
    "        attributes = ['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_ID']\n",
    "    elif contains_word(input, 'chatGPT'):\n",
    "        attributes = ['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_ID']\n",
    "    elif contains_word(input, 'flant5'):\n",
    "        attributes = ['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_ID']\n",
    "    print(attributes)\n",
    "    extract_features(input, attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
