{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from statistics import stdev, mean\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_sentences(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def count_sentences_per_paragraph(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    sentences_per_paragraph = []\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through each paragraph and count the sentences\n",
    "    for paragraph in paragraphs:\n",
    "        num_sentences = count_sentences(paragraph)\n",
    "        sentences_per_paragraph.append(num_sentences)\n",
    "        total += num_sentences\n",
    "\n",
    "    return total/len(paragraphs)\n",
    "\n",
    "def count_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "def count_words_per_paragraph(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    words_per_paragraph = []\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through each paragraph and count the words\n",
    "    for paragraph in paragraphs:\n",
    "        num_words = count_words(paragraph)\n",
    "        words_per_paragraph.append(num_words)\n",
    "        total += num_words\n",
    "\n",
    "    return total/len(paragraphs)\n",
    "\n",
    "def check_character_presence(text, character):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    character_presence = 0\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each paragraph and check if the character is present\n",
    "    for paragraph in paragraphs:\n",
    "        if character in paragraph:\n",
    "            character_presence = 1\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def paragraph_sentence_length_std_dev(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    paragraph_std_devs = []\n",
    "    total = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # Tokenize the paragraph into sentences\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "        # Calculate the length of each sentence\n",
    "        sentence_lengths = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "        if len(sentence_lengths) > 1:\n",
    "            # Calculate the mean length of sentences\n",
    "            mean_length = np.mean(sentence_lengths)\n",
    "\n",
    "            # Calculate the squared differences between each sentence length and the mean\n",
    "            squared_diffs = [(length - mean_length) ** 2 for length in sentence_lengths]\n",
    "\n",
    "            # Calculate the variance\n",
    "            variance = np.mean(squared_diffs)\n",
    "\n",
    "            # Calculate the standard deviation\n",
    "            std_dev = np.sqrt(variance)\n",
    "        else:\n",
    "            # If there's only one sentence in the paragraph, standard deviation is 0\n",
    "            std_dev = 0\n",
    "        \n",
    "        paragraph_std_devs.append(std_dev)\n",
    "        total += std_dev\n",
    "\n",
    "    return total/len(paragraphs)\n",
    "\n",
    "def max_length_difference_paragraph(paragraph):\n",
    "    # Tokenize the paragraph into sentences\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    \n",
    "    max_diff = 0\n",
    "\n",
    "    # Iterate over each pair of consecutive sentences\n",
    "    for i in range(len(sentences) - 1):\n",
    "        # Calculate the length difference between consecutive sentences\n",
    "        diff = abs(len(nltk.word_tokenize(sentences[i])) - len(nltk.word_tokenize(sentences[i+1])))\n",
    "\n",
    "        # Update max_diff if the current difference is greater\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "\n",
    "    return max_diff\n",
    "\n",
    "def count_short_sentences_in_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    short_sentence_counts = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # Tokenize the paragraph into sentences\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "        # Count the number of sentences with less than 11 words\n",
    "        count = sum(1 for sentence in sentences if len(nltk.word_tokenize(sentence)) < 11)\n",
    "\n",
    "        short_sentence_counts += count\n",
    "\n",
    "    return short_sentence_counts\n",
    "\n",
    "def count_long_sentences_in_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    long_sentence_counts = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # Tokenize the paragraph into sentences\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "        # Count the number of sentences with less than 11 words\n",
    "        count = sum(1 for sentence in sentences if len(nltk.word_tokenize(sentence)) > 34)\n",
    "\n",
    "        long_sentence_counts += count\n",
    "\n",
    "    return long_sentence_counts\n",
    "\n",
    "def check_words_in_paragraphs(text, words_to_check):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "\n",
    "    presence = 0\n",
    "    count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Check if any of the words are present in the paragraph\n",
    "        if any(word in paragraph for word in words_to_check):\n",
    "            presence = 1\n",
    "            count += 1\n",
    "\n",
    "    return presence\n",
    "\n",
    "def check_numbers_in_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    presence_per_paragraph = []\n",
    "    count = 0\n",
    "    check = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Check if any numbers are present in the paragraph using regular expression\n",
    "        if re.search(r'\\d+', paragraph):\n",
    "            presence_per_paragraph.append(1)\n",
    "            check = 1\n",
    "            count += 1\n",
    "        else:\n",
    "            presence_per_paragraph.append(0)\n",
    "\n",
    "    return count\n",
    "\n",
    "def check_capitals_to_periods_ratio(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    presence_per_paragraph = []\n",
    "    check = 0\n",
    "    count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Count the number of capital letters and periods in the paragraph\n",
    "        capital_count = sum(1 for char in paragraph if char.isupper())\n",
    "        period_count = paragraph.count('.')\n",
    "\n",
    "        # Check if the paragraph contains twice as many capitals as periods\n",
    "        if capital_count >= 2 * period_count:\n",
    "            presence_per_paragraph.append(1)\n",
    "            check = 1\n",
    "            count += 1\n",
    "        else:\n",
    "            presence_per_paragraph.append(0)\n",
    "\n",
    "    return count\n",
    "\n",
    "def check_et_in_paragraphs(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newline characters\n",
    "    \n",
    "    presence_per_paragraph = []\n",
    "    check = 0\n",
    "    count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Check if the paragraph contains the substring \"et\"\n",
    "        if 'et' in paragraph:\n",
    "            presence_per_paragraph.append(1)\n",
    "            check = 1\n",
    "            count += 1\n",
    "        else:\n",
    "            presence_per_paragraph.append(0)\n",
    "\n",
    "    return check\n",
    "\n",
    "def normalize_column(column):\n",
    "    min_val = column.min()\n",
    "    max_val = column.max()\n",
    "    normalized_column = (column - min_val) / (max_val - min_val)\n",
    "    return normalized_column\n",
    "\n",
    "def delete_csv_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "\n",
    "def contains_word(string, word):\n",
    "    return word in string\n",
    "\n",
    "def extract_features(filepath, params):\n",
    "    header = True\n",
    "    path = '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/'\n",
    "    outputfilename =  path + filepath.split('.')[0].split('/')[-1] + '_feature.csv'\n",
    "    print(outputfilename)\n",
    "\n",
    "    delete_csv_if_exists(outputfilename)\n",
    "\n",
    "    dfs = pd.read_json(filepath, lines=True)\n",
    "    characters = [\")\", \"-\", \";\", \":\", \"?\", \"'\"]\n",
    "    count = 0\n",
    "    for df in dfs.itertuples():\n",
    "        df_for_loop = pd.DataFrame(df)\n",
    "\n",
    "        if contains_word(filepath, 'bloomz'):\n",
    "            abstract = df.text\n",
    "            machine_abstract = df.machine_abstract\n",
    "        elif contains_word(filepath, 'llama'):\n",
    "            abstract = df.Abstract\n",
    "            machine_abstract = df.machine_text\n",
    "        else:\n",
    "            abstract = df.human_text\n",
    "            machine_abstract = df.machine_text\n",
    "\n",
    "        num_sentence_human = count_sentences_per_paragraph(abstract)\n",
    "        num_sentence_machine = count_sentences_per_paragraph(machine_abstract)\n",
    "\n",
    "        num_words_human = count_words_per_paragraph(abstract)\n",
    "        num_words_machine = count_words_per_paragraph(machine_abstract)\n",
    "\n",
    "        character0_human = check_character_presence(abstract, characters[0])\n",
    "        character1_human = check_character_presence(abstract, characters[1])\n",
    "        character2_human = check_character_presence(abstract, characters[2])\n",
    "        character3_human = check_character_presence(abstract, characters[3])\n",
    "        character4_human = check_character_presence(abstract, characters[4])\n",
    "        character5_human = check_character_presence(abstract, characters[5])\n",
    "\n",
    "        character2_3_human = 0\n",
    "\n",
    "        if character2_human == 1 or character3_human == 1:\n",
    "            character2_3_human = 1\n",
    "\n",
    "        character0_machine = check_character_presence(machine_abstract, characters[0])\n",
    "        character1_machine = check_character_presence(machine_abstract, characters[1])\n",
    "        character2_machine = check_character_presence(machine_abstract, characters[2])\n",
    "        character3_machine = check_character_presence(machine_abstract, characters[3])\n",
    "        character4_machine = check_character_presence(machine_abstract, characters[4])\n",
    "        character5_machine = check_character_presence(machine_abstract, characters[5])\n",
    "\n",
    "        character2_3_machine = 0\n",
    "\n",
    "        if character2_machine == 1 or character3_machine == 1:\n",
    "            character2_3_machine = 1\n",
    "        \n",
    "        std_dev_human = paragraph_sentence_length_std_dev(abstract)\n",
    "        std_dev_machine = paragraph_sentence_length_std_dev(machine_abstract)\n",
    "\n",
    "        sent_len_diff_human = max_length_difference_paragraph(abstract)\n",
    "        sent_len_diff_machine = max_length_difference_paragraph(machine_abstract)\n",
    "\n",
    "        count_short_sentences_in_paragraphs_human = count_short_sentences_in_paragraphs(abstract)\n",
    "        count_short_sentences_in_paragraphs_machine = count_short_sentences_in_paragraphs(machine_abstract)\n",
    "\n",
    "        count_long_sentences_in_paragraphs_human = count_long_sentences_in_paragraphs(abstract)\n",
    "        count_long_sentences_in_paragraphs_machine = count_long_sentences_in_paragraphs(machine_abstract)\n",
    "\n",
    "\n",
    "        words = [\"although\", \"However\", \"but\", \"because\", \"this\", \"others\", \"researchers\"]\n",
    "\n",
    "        check_word0_human = check_words_in_paragraphs(abstract, words[0])\n",
    "        check_word0_machine = check_words_in_paragraphs(machine_abstract, words[0])\n",
    "\n",
    "        check_word1_human = check_words_in_paragraphs(abstract, words[1])\n",
    "        check_word1_machine = check_words_in_paragraphs(machine_abstract, words[1])\n",
    "\n",
    "        check_word2_human = check_words_in_paragraphs(abstract, words[2])\n",
    "        check_word2_machine = check_words_in_paragraphs(machine_abstract, words[2])\n",
    "\n",
    "        check_word3_human = check_words_in_paragraphs(abstract, words[3])\n",
    "        check_word3_machine = check_words_in_paragraphs(machine_abstract, words[3])\n",
    "\n",
    "        check_word4_human = check_words_in_paragraphs(abstract, words[4])\n",
    "        check_word4_machine = check_words_in_paragraphs(machine_abstract, words[4])\n",
    "\n",
    "        check_word5_human = check_words_in_paragraphs(abstract, words[5])\n",
    "        check_word5_machine = check_words_in_paragraphs(machine_abstract, words[5])\n",
    "\n",
    "        check_word6_human = check_words_in_paragraphs(abstract, words[6])\n",
    "        check_word6_machine = check_words_in_paragraphs(machine_abstract, words[6])\n",
    "\n",
    "        check_word2_3_machine = 0\n",
    "\n",
    "        if check_word2_machine == 1 or check_word3_machine == 1:\n",
    "            check_word2_3_machine = 1\n",
    "\n",
    "        check_word2_3_human = 0\n",
    "\n",
    "        if check_word2_human == 1 or check_word3_human == 1:\n",
    "            check_word2_3_human = 1\n",
    "\n",
    "        check_num_human = check_numbers_in_paragraphs(abstract)\n",
    "        check_num_machine = check_numbers_in_paragraphs(machine_abstract)\n",
    "\n",
    "        check_capitals_human = check_capitals_to_periods_ratio(abstract)\n",
    "        check_capitals_machine = check_capitals_to_periods_ratio(machine_abstract)\n",
    "\n",
    "        check_et_human = check_et_in_paragraphs(abstract)\n",
    "        check_et_machine = check_et_in_paragraphs(machine_abstract)\n",
    "        #print(\"num_words_human \", num_words_human)\n",
    "        # print(\"character0_human \", character0_human)\n",
    "        # print(\"character1_human \", character1_human)\n",
    "        # print(\"character2_3_human \", character2_3_human)\n",
    "        # print(\"character4_human \", character4_human)\n",
    "        # print(\"character5_human \", character5_human)\n",
    "\n",
    "        # print(\"character0_machine \", character0_machine)\n",
    "        # print(\"character1_machine \", character1_machine)\n",
    "        # print(\"character2_3_machine \", character2_3_machine)\n",
    "        # print(\"character4_machine \", character4_machine)\n",
    "        # print(\"character5_machine \", character5_machine)\n",
    "        #print(\"count_long_sentences_in_paragraphs_human \", count_long_sentences_in_paragraphs_human)\n",
    "        #print(\"count_long_sentences_in_paragraphs_machine \", count_long_sentences_in_paragraphs_machine)\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for param in params:\n",
    "            data[param] = dfs.loc[count, param]\n",
    "\n",
    "        data['no_sentence_human'] = [num_sentence_human]\n",
    "        data['no_sentence_machine'] = [num_sentence_machine]\n",
    "        data['num_words_human'] = [num_words_human]\n",
    "        data['num_words_machine'] = [num_words_machine]\n",
    "        data['character0_human'] = [character0_human]\n",
    "        data['character1_human'] = [character1_human]\n",
    "        data['character2_3_human'] = [character2_3_human]\n",
    "        data['character4_human'] = [character4_human]\n",
    "        data['character5_human'] = [character5_human]\n",
    "        data['character0_machine'] = [character0_machine]\n",
    "        data['character1_machine'] = [character1_machine]\n",
    "        data['character2_3_machine'] = [character2_3_machine]\n",
    "        data['character4_machine'] = [character4_machine]\n",
    "        data['character5_machine'] = [character5_machine]\n",
    "        data['std_dev_human'] = [std_dev_human]\n",
    "        data['std_dev_machine'] = [std_dev_machine]\n",
    "        data['sent_len_diff_human'] = [sent_len_diff_human]\n",
    "        data['sent_len_diff_machine'] = [sent_len_diff_machine]\n",
    "        data['count_short_sentences_in_paragraphs_human'] = [count_short_sentences_in_paragraphs_human]\n",
    "        data['count_short_sentences_in_paragraphs_machine'] = [count_short_sentences_in_paragraphs_machine]\n",
    "        data['count_long_sentences_in_paragraphs_human'] = [count_long_sentences_in_paragraphs_human]\n",
    "        data['count_long_sentences_in_paragraphs_machine'] = [count_long_sentences_in_paragraphs_machine]\n",
    "        data['check_word0_human'] = [check_word0_human]\n",
    "        data['check_word1_human'] = [check_word1_human]\n",
    "        data['check_word2_3_human'] = [check_word2_3_human]\n",
    "        data['check_word3_human'] = [check_word3_human]\n",
    "        data['check_word4_human'] = [check_word4_human]\n",
    "        data['check_word5_human'] = [check_word5_human]\n",
    "        data['check_word0_machine'] = [check_word0_machine]\n",
    "        data['check_word1_machine'] = [check_word1_machine]\n",
    "        data['check_word2_3_machine'] = [check_word2_3_machine]\n",
    "        data['check_word3_machine'] = [check_word3_machine]\n",
    "        data['check_word4_machine'] = [check_word4_machine]\n",
    "        data['check_word5_machine'] = [check_word5_machine]\n",
    "        data['check_num_human'] = [check_num_human]\n",
    "        data['check_num_machine'] = [check_num_machine]\n",
    "        data['check_capitals_human'] = [check_capitals_human]\n",
    "        data['check_capitals_machine'] = [check_capitals_machine]\n",
    "        data['check_et_human'] = [check_et_human]\n",
    "        data['check_et_machine'] = [check_et_machine]\n",
    "\n",
    "        count += 1\n",
    "        df1 = pd.DataFrame(data)\n",
    "        df1.to_csv(outputfilename, mode='a', index=False, header=header)\n",
    "        header = False\n",
    "    \n",
    "    df = pd.read_csv(outputfilename)\n",
    "    df['no_sentence_human'] = normalize_column(df['no_sentence_human'])\n",
    "    df['no_sentence_machine'] = normalize_column(df['no_sentence_machine'])\n",
    "    df['num_words_human'] = normalize_column(df['num_words_human'])\n",
    "    df['num_words_machine'] = normalize_column(df['num_words_machine'])\n",
    "    df['std_dev_human'] = normalize_column(df['std_dev_human'])\n",
    "    df['sent_len_diff_human'] = normalize_column(df['sent_len_diff_human'])\n",
    "    df['std_dev_machine'] = normalize_column(df['std_dev_machine'])\n",
    "    df['sent_len_diff_machine'] = normalize_column(df['sent_len_diff_machine'])\n",
    "    df.to_csv(outputfilename, index=False)\n",
    "    print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_chatgpt_train.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_cohere.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_davinci_train.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_chatgpt.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_cohere_test.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_chatgpt_validation.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_chatgpt_test.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_cohere_validation.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_bloomz_train.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_davinci_test.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_davinci_validation.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_bloomz_validation.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_davinci.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_bloomz.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_bloomz_test.jsonl', '/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_cohere_train.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_file_paths(directory):\n",
    "    file_paths = []\n",
    "    # List all files in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if not file_name.startswith('.'):\n",
    "            # Join directory path with file name to get the full file path\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            # Append file path to the list\n",
    "            file_paths.append(file_path)\n",
    "    return file_paths\n",
    "\n",
    "# Example usage:\n",
    "directory = \"/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/\"\n",
    "file_paths = get_file_paths(directory)\n",
    "print(file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_chatgpt_train.jsonl\n",
      "[]\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_chatgpt_train_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_chatgpt_train_feature.csv\n",
      "1916\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_cohere.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_cohere_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_cohere_feature.csv\n",
      "2336\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_davinci_train.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_davinci_train_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_davinci_train_feature.csv\n",
      "1920\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_chatgpt.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_chatgpt_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_chatgpt_feature.csv\n",
      "2995\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_cohere_test.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_cohere_test_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_cohere_test_feature.csv\n",
      "468\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_chatgpt_validation.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_chatgpt_validation_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_chatgpt_validation_feature.csv\n",
      "480\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_chatgpt_test.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_chatgpt_test_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_chatgpt_test_feature.csv\n",
      "599\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_cohere_validation.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_cohere_validation_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_cohere_validation_feature.csv\n",
      "374\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_bloomz_train.jsonl\n",
      "['title', 'text', 'machine_abstract', 'source', 'model', 'id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_bloomz_train_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_bloomz_train_feature.csv\n",
      "1920\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_davinci_test.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_davinci_test_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_davinci_test_feature.csv\n",
      "600\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_davinci_validation.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_davinci_validation_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_davinci_validation_feature.csv\n",
      "480\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_bloomz_validation.jsonl\n",
      "['title', 'text', 'machine_abstract', 'source', 'model', 'id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_bloomz_validation_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_bloomz_validation_feature.csv\n",
      "480\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_davinci.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_davinci_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_davinci_feature.csv\n",
      "3000\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_bloomz.jsonl\n",
      "['title', 'text', 'machine_abstract', 'source', 'model', 'id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_bloomz_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_bloomz_feature.csv\n",
      "3000\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_bloomz_test.jsonl\n",
      "['title', 'text', 'machine_abstract', 'source', 'model', 'id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_bloomz_test_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_bloomz_test_feature.csv\n",
      "600\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/m4/without_n_wiki/wikipedia_cohere_train.jsonl\n",
      "['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
      "/Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_cohere_train_feature.csv\n",
      "File does not exist: /Users/supriyaupadhyaya/ovgu/generative-text-detectors/feature-extractor/features/m4_withoutN/wikipedia_cohere_train_feature.csv\n",
      "1494\n"
     ]
    }
   ],
   "source": [
    "def contains_word(string, word):\n",
    "    return word in string\n",
    "\n",
    "attributes = []\n",
    "for input in file_paths:\n",
    "    print(input)\n",
    "    if contains_word(input, 'bloomz'):\n",
    "        attributes = ['title', 'text', 'machine_abstract', 'source', 'model', 'id']\n",
    "    elif contains_word(input, 'cohere'):\n",
    "        attributes = ['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
    "    elif contains_word(input, 'davinci'):\n",
    "        attributes = ['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_id']\n",
    "    elif contains_word(input, 'chatGPT'):\n",
    "        attributes = ['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_ID']\n",
    "    elif contains_word(input, 'flant5'):\n",
    "        attributes = ['prompt', 'human_text', 'machine_text', 'source', 'model', 'source_ID']\n",
    "    elif contains_word(input, 'llama'):\n",
    "        attributes = ['Abstract', 'machine_text', 'arxiv_id']\n",
    "    print(attributes)\n",
    "    extract_features(input, attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
