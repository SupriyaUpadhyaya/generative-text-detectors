{"source_id": 705.2166, "title": "Ultimate Fate of our Universe from Quantum Mechanics", "abstract": "  It is conjectured that time intervals of any kind are proportional to the age of the Universe taken at the time we are considering the interval. If this is the case then the speed of light, in fact any speed, must decrease inversely proportional to this age. The immediate consequence is that energy is not conserved: the hypothesis that time is a homogeneous property implies conservation of energy (the theorem of Noether). Nonconservation of energy follows from the condition that any time interval is proportional to the cosmological time, and therefore time can not be homogeneous. From the uncertainty principle, taking the constant of Planck as a real constant, time independent, it follows that any energy in the Universe decreases linearly with time. We then prove that Schroedinger equation does not change, except for the potential energy term. The future of the Universe gives for the wave functions a long sinusoidal spatial solution, so that everything becomes unlocalized. The relativistic absolute interval remains the same, even with a changing speed of light, and the Universe turns out to be nonexpanding. A Mass-Boom effect is confirmed. ", "machine_abstract": "The ultimate fate of the universe is one of the most important questions in physics and cosmology today, but it has been difficult to answer because quantum mechanics (QM) cannot be applied directly to macroscopic systems such as the whole universe.  In this talk I will present an approach that allows us to use QM to study the evolution of the universe on all scales by applying it only to small subsystems within the universe.   This method can also be used to calculate the probability distribution for the time at which the universe ends its existence. The results are consistent with current observations and provide new insights into how the universe may end up. For example, we find that there is a finite probability that the universe will expand forever without ever ending or collapsing. We also show that if the universe does collapse then it must do so before 10^(10^28) years have passed since the Big Bang. Finally, we discuss some possible implications of these results for the future of humanity.", "paraphrased_abstract": "The result is consistent with the current observation, and provides a new insight into how the universe may end. The results are consistent with current observations, and are a new insight into the nature of the universe. In this talk I will introduce a method to study the evolution of the universe on all scales by applying it only to the small subsystems of the universe. This method can also be applied to calculate the probability of the collapse of the universe before ten (10) years have passed since the Big Bang. We shall then discuss a few implications for humanity. The end of the universe is a very important question, but it has not been answered so far because quantum mechanics cannot be applied to large systems such as the universe. We will thus be able to study the development of the universe at all scales by applying it to all the subsystems of the universe. We shall be able to show that if the universe were to collapse, it would have to do so before 10 (28) years had passed since the Big Bang. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0115, "title": "Dynamical Computation on Coefficients of Electroweak Chiral Lagrangian from One-doublet and Topcolor-assisted Technicolor Models", "abstract": "  Based on previous studies deriving the chiral Lagrangian for pseudo scalar mesons from the first principle of QCD, we derive the electroweak chiral Lagrangian and build up a formulation for computing its coefficients from one-doublet technicolor model and a schematic topcolor-assisted technicolor model. We find that the coefficients of the electroweak chiral Lagrangian for the topcolor-assisted technicolor model are divided into three parts: direct TC2 interaction part, TC1 and TC2 induced effective Z' particle contribution part, and ordinary quarks contribution part. The first two parts are computed in this paper and we show that the direct TC2 interaction part is the same as that in the one-doublet technicolor model, while effective Z' contributions are at least proportional to the p^2 order parameter \\beta_1 in the electroweak chiral Lagrangian and typical features of topcolor-assisted technicolor model are that it only allows positive T and U parameters and the T parameter varies in the range 0\\sim 1/(25\\alpha), the upper bound of T parameter will decrease as long as Z' mass become large. The S parameter can be either positive or negative depending on whether the Z' mass is large or small. The Z' mass is also bounded above and the upper bound depend on value of T parameter. We obtain the values for all the coefficients of the electroweak chiral Lagrangian up to order of p^4. ", "machine_abstract": "We study the dynamical generation of electroweak symmetry breaking in one-doublet and top-color assisted technicolor models by computing the coefficients of the chiral lagrangian at next-to-leading order (NLO) using lattice QCD simulations with two flavors of light quarks, Nf=2. We find that the results are consistent with those obtained previously for the standard model within statistical errors.  The new results presented here include the first determination of the coefficient L5 which is related to the S parameter. In addition we present results for the coefficients L3 and L4 which were not determined before. These results provide further evidence for the viability of these theories as candidates for physics beyond the Standard Model. This work was supported by DOE grant DE-FG02-97ER4014. PACS numbers: 11.15.Ha, 12.38.Qk, 13.25.Hw", "paraphrased_abstract": "In this work we have been supported by DOE Grant DE-FG02-97-ER401. We present here the first results of the first determination of the coefficient L5 for the second parameter and the coefficient L4 for the third. This result confirms the existence of new theories in physics beyond the Standard Model. The results are consistent with those that have already been proved in the Standard Model. This work is supported by DOE grant DE-FG02-97-ER401. We have investigated the dynamical generation of electroweak symmetry breaking in the one-colour and top-color assisted model by computing the coefficients of the chiral lagrangian, NLO. We have calculated the coefficient L5 for the third parameter, which is related to the S parameter. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1656, "title": "Temperature-driven transition from the Wigner Crystal to the Bond-Charge-Density Wave in the Quasi-One-Dimensional Quarter-Filled band", "abstract": "  It is known that within the interacting electron model Hamiltonian for the one-dimensional 1/4-filled band, the singlet ground state is a Wigner crystal only if the nearest neighbor electron-electron repulsion is larger than a critical value. We show that this critical nearest neighbor Coulomb interaction is different for each spin subspace, with the critical value decreasing with increasing spin. As a consequence, with the lowering of temperature, there can occur a transition from a Wigner crystal charge-ordered state to a spin-Peierls state that is a Bond-Charge-Density Wave with charge occupancies different from the Wigner crystal. This transition is possible because spin excitations from the spin-Peierls state in the 1/4-filled band are necessarily accompanied by changes in site charge densities. We apply our theory to the 1/4-filled band quasi-one-dimensional organic charge-transfer solids in general and to 2:1 tetramethyltetrathiafulvalene (TMTTF) and tetramethyltetraselenafulvalene (TMTSF) cationic salts in particular. We believe that many recent experiments strongly indicate the Wigner crystal to Bond-Charge-Density Wave transition in several members of the TMTTF family. We explain the occurrence of two different antiferromagnetic phases but a single spin-Peierls state in the generic phase diagram for the 2:1 cationic solids. The antiferromagnetic phases can have either the Wigner crystal or the Bond-Charge-Spin-Density Wave charge occupancies. The spin-Peierls state is always a Bond-Charge-Density Wave. ", "machine_abstract": "We study temperature driven phase transitions between different charge ordered states in one-dimensional quarter-filled Hubbard model with nearest-neighbor hopping and on-site Coulomb repulsion using density matrix renormalization group (DMRG) method. We find that at low temperatures, the system is in the Wigner crystal state where electrons are localized by strong electron correlations. With increasing temperature, we observe a first-order phase transition into bond-charge-density wave state which has been predicted theoretically earlier but not observed experimentally so far. The nature of this phase transition can be understood as melting of the Wigner crystal due to thermal fluctuations. Finally, we also investigate the effect of next-nearest neighbor hopping t\u2032 on these phases. Our results show that for small values of t\u2032 , the ground state remains in the Wigner crystal phase while for large values of t\u2032 it becomes metallic. This shows that the presence of next-nearest neighbor hoping may destroy the charge ordering in 1D systems.", "paraphrased_abstract": "A small t-wave is formed in the ground, and a large t-wave is formed in the metallic. Moreover, we examine the effect of the nearest neighbor hopping t on the phase transitions of these phases. The main result is that the ground states remain in the Wigner crystal state, while the large t-wave is melted by thermal changes. This is a proof that the absence of neighbor hopping in a 1D system can destroy the equilibrium. The study shows that at low temperatures the system is a crystal-crystalline state where electrons are localized, with strong electron correlations, while at higher temperatures the system is shifted to the bond-charged wave, which was predicted theoretically earlier, but not experimentally, as the temperature increases. This is the phenomenon that is supposed to have occurred, due to the heating. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2603, "title": "Molecular line intensities as measures of cloud masses - II. Conversion factors for specific galaxy types", "abstract": "  We present theoretically-established values of the CO-to-H2 and C-to-H2 conversion factors that may be used to estimate the gas masses of external galaxies. We consider four distinct galaxy types, represented by M51, NGC 6946, M82 and SMC N27. The physical parameters that best represent the conditions within the molecular clouds in each of the galaxy types are estimated using a chi^2 analysis of several observed atomic fine structure and CO rotational lines. This analysis is explored over a wide range of density, radiation field, extinction, and other relevant parameters. Using these estimated physical conditions in methods that we have previously established, CO-to-H2 conversion factors are then computed for CO transitions up to J=9-8. For the conventional CO(1-0) transition, the computed conversion factor varies significantly below and above the canonical value for the Milky Way in the four galaxy types considered. Since atomic carbon emission is now frequently used as a probe of external galaxies, we also present, for the first time, the C-to-H2 conversion factor for this emission in the four galaxy types considered. ", "machine_abstract": "We present the results of our analysis of molecular gas mass estimates based on CO and HCN observations in nearby galaxies, using data obtained with the IRAM 30m telescope. We find that conversion factors between luminosity and mass are strongly dependent on the star formation rate (SFR) per unit area within each galaxy disk. The SFR surface density is found to be an important parameter controlling the conversion factor XCO = M(H2)/L(CO), which we derive by fitting the observed L(HCN) / L(CO) ratio versus metallicity relation. For low values of \u03a3SFR < 1M\u2299 yr-1 kpc-2 , corresponding to quiescent disks or nuclear regions dominated by old stellar populations, we obtain XCO \u2248 2 \u00d7 10 20 cm\u22122 K\u22121 km\u22121 s. This value increases up to XCO \u2248 5\u00d710 20 cm\u22122 K\u22121km\u22121s at high \u03a3SFR > 3M\u2299yr-1kpc-2 . These findings suggest that the physical conditions of the interstellar medium may change significantly depending on whether it is located in actively star-forming regions or not.", "paraphrased_abstract": "The density of the surface of the disk is the first parameter, and it is XCO  M(H2)/L(CO). XCO is an important factor in the determination of the conversion factor XCO  M(H2)/L(CO) and by means of the obtained L(HCN/L(CO) ratio. XCO  2102 K1 km1 s and 5102 K1 s are measured at low values of SFR  1myr-1 k1 s, and at high values of SFR  3myr-1 . We have derived a resemblance between radiance and mass to the state of star formation in the atmosphere, in which we have found that the radiance and mass of the universe depend strongly on the radiance of the gaseous substance. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3979, "title": "The Magnetic Sensitivity of the Ba II D1 and D2 Lines of the Fraunhofer Spectrum", "abstract": "  The physical interpretation of the spectral line polarization produced by the joint action of the Hanle and Zeeman effects offers a unique opportunity to obtain empirical information about hidden aspects of solar and stellar magnetism. To this end, it is important to achieve a complete understanding of the sensitivity of the emergent spectral line polarization to the presence of a magnetic field. Here we present a detailed theoretical investigation on the role of resonance scattering and magnetic fields on the polarization signals of the Ba II D1 and D2 lines of the Fraunhofer spectrum, respectively at 4934 \\AA\\ and 4554 \\AA. We adopt a three-level model of Ba II, and we take into account the hyperfine structure that is shown by the $^{135}$Ba and $^{137}$Ba isotopes. Despite of their relatively small abundance (18%), the contribution coming from these two isotopes is indeed fundamental for the interpretation of the polarization signals observed in these lines. We consider an optically thin slab model, through which we can investigate in a rigorous way the essential physical mechanisms involved (resonance polarization, Zeeman, Paschen-Back and Hanle effects), avoiding complications due to radiative transfer effects. We assume the slab to be illuminated from below by the photospheric solar continuum radiation field, and we investigate the radiation scattered at 90 degrees, both in the absence and in the presence of magnetic fields, deterministic and microturbulent. We show in particular the existence of a differential magnetic sensitivity of the three-peak Q/I profile that is observed in the D2 line in quiet regions close to the solar limb, which is of great interest for magnetic field diagnostics. ", "machine_abstract": "The magnetic sensitivity of the Ba II D1 (4554 \u00c5) and D2 lines in the solar spectrum is investigated by using high-resolution observations obtained with the Hinode Solar Optical Telescope Spectrograph (SOT-SP). The observed Stokes profiles are analyzed to determine the longitudinal Zeeman splitting, which is then used as an input for forward modeling of the Stokes profiles under various conditions. We find that the magnetic sensitivity of these two lines depends on their formation heights; while the D1 line forms at higher layers than the photosphere, the D2 line forms deeper down into the photosphere where the plasma density is much larger.  In addition, we show that the magnetic sensitivity of both lines decreases rapidly when they form above or below certain height levels within the photosphere. These results suggest that it may be possible to use the magnetic sensitivity of these lines to probe different atmospheric regions of the Sun. This work was supported by NASA's Living With a Star program through grant NNX10AC99G issued by the Office of Space Science.", "paraphrased_abstract": "The researchers investigated the magnetic sensitivity of Ba II, 4554 nm, and the two lines of Ba II in the solar spectrum. The two lines form higher than the surface of the photosphere, and the D2 line forms at the bottom of the photosphere, where the density of the plasma is much greater. These results indicate that it is possible to use the magnetic sensitivity of these lines in different atmospheric regions of the sun. This research was supported by NASA's Living With a Star program, funded by the Office of Space Science, with the grant NX10AC99G. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.388, "title": "Energy-Efficient Resource Allocation in Wireless Networks with Quality-of-Service Constraints", "abstract": "  A game-theoretic model is proposed to study the cross-layer problem of joint power and rate control with quality of service (QoS) constraints in multiple-access networks. In the proposed game, each user seeks to choose its transmit power and rate in a distributed manner in order to maximize its own utility while satisfying its QoS requirements. The user's QoS constraints are specified in terms of the average source rate and an upper bound on the average delay where the delay includes both transmission and queuing delays. The utility function considered here measures energy efficiency and is particularly suitable for wireless networks with energy constraints. The Nash equilibrium solution for the proposed non-cooperative game is derived and a closed-form expression for the utility achieved at equilibrium is obtained. It is shown that the QoS requirements of a user translate into a \"size\" for the user which is an indication of the amount of network resources consumed by the user. Using this competitive multiuser framework, the tradeoffs among throughput, delay, network capacity and energy efficiency are studied. In addition, analytical expressions are given for users' delay profiles and the delay performance of the users at Nash equilibrium is quantified. ", "machine_abstract": "We consider the problem of energy-efficient resource allocation for wireless networks under qualityof-service (QoS) constraints, where each user has an individual QoS requirement and is equipped with multiple radios to communicate simultaneously over different channels. We propose a novel framework that jointly optimizes power control, channel assignment, scheduling, and routing by formulating it as a mixed integer linear program (MILP). The proposed MILP formulation takes into account both intra-cell interference among users within one cell and inter-cell interference between neighboring cells. To solve this large-scale optimization problem efficiently, we develop two efficient algorithms based on Lagrangian relaxation techniques. Our simulation results show that our proposed algorithm can significantly improve network performance compared to existing schemes. In particular, when there are only a few active links per cell at any time instant, our scheme achieves up to 4 times higher throughput than the baseline scheme without increasing transmit powers or bandwidths.     Keywords: Energy efficiency, Power control, Channel assignment, Scheduling", "paraphrased_abstract": "The energy efficiency of wireless networks is a special problem of a certain quality of service, where each user has a different QoS and is provided with multiple radios to communicate over different channels. To address this great need, we have developed two efficient algorithms based on Lagrangian relaxation. The first method combines the energy efficiency of the two functions, determining the best channel, and the second method combines the best channel and the best channel. The second method can achieve four times more throughput than the previous method without reducing the transmission bandwidth or the bandwidth. Our study shows that the proposed method is able to improve the performance of the network substantially by reducing the number of active links per cell. This is because of the fact that when a few links are active at one time, the system can achieve four times more throughput without increasing the transmission power or the bandwidth. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0913, "title": "Domain wall switching: optimizing the energy landscape", "abstract": "  It has recently been suggested that exchange spring media offer a way to increase media density without causing thermal instability (superparamagnetism), by using a hard and a soft layer coupled by exchange. Victora has suggested a figure of merit xi = 2 E_b/mu_0 m_s H_sw, the ratio of the energy barrier to that of a Stoner-Wohlfarth system with the same switching field, which is 1 for a Stoner-Wohlfarth (coherently switching) particle and 2 for an optimal two-layer composite medium. A number of theoretical approaches have been used for this problem (e.g., various numbers of coupled Stoner-Wohlfarth layers and continuum micromagnetics). In this paper we show that many of these approaches can be regarded as special cases or approximations to a variational formulation of the problem, in which the energy is minimized for fixed magnetization. The results can be easily visualized in terms of a plot of the energy as a function of magnetic moment m_z, in which both the switching field [the maximum slope of E(m_z)] and the stability (determined by the energy barrier E_b) are geometrically visible. In this formulation we can prove a rigorous limit on the figure of merit xi, which can be no higher than 4. We also show that a quadratic anistropy suggested by Suess et al comes very close to this limit. ", "machine_abstract": "We propose an alternative switching mechanism for spintronic devices based on domain walls (DWs). The proposed device consists of two ferromagnetic layers separated by a non-magnetic spacer layer, where DWs can be driven between different positions in each magnetic layer using spin-orbit torques and electric fields. We show that this new type of device is able to operate at lower current densities than conventional spin valves with comparable magnetoresistance values. In addition we demonstrate how the energy barrier associated with the motion of the DWs can be tuned through changes in the thicknesses of both the ferromagnets and the non-magnetic spacer. This allows us to optimize the energy landscape such that the DWs are trapped in their equilibrium position when no external field or voltage bias is applied. Finally, we discuss possible applications of our proposal as well as its limitations. Spintronics has emerged over recent years as one of the most promising technologies for future information processing systems [1] . One of the main challenges faced by these devices is the development of efficient ways to control the flow of charge carriers without compromising their high mobility [2] . In order to overcome this problem several groups have recently investigated the possibility of controlling the direction of electron transport via the manipulation of magnetic textures [3] , which include vortex states [4] , skyrmions [5] and domain walls [6] . Domain walls are particularly interesting since they can be manipulated electrically [7, 8] and thermally [9] , making them ideal candidates for low-power consumption devices [10] . However, despite significant progress made towards understanding the physics behind the dynamics of domain walls [11] , there remains much uncertainty about the exact nature of the mechanisms responsible for driving their motion [12] .", "paraphrased_abstract": "In the last few years, spintronics have been considered a promising technology for the future processing of information. Spintronics has recently become one of the most promising technologies for the future computing of information. One of the main difficulties of these devices is the problem of achieving the right angle of charge to the carriers without compromising their mobility. One of the main problems for the emergence of these devices is to determine how to regulate the direction of electron flow without compromising their mobility. This problem is posed by the complication of manipulating the magnetic textures, namely vortex states, skyrmions, and domain walls. These are particularly interesting because they can be modulated electrically and thermally, which makes them suitable for low-power devices. Furthermore, we show that the spacer of the DWs can be adjusted by changing the thickness of both ferromagnetic layers and the non-metal layer. This allows the DWs to be stabilized in their equilibrium position when there is no external pressure or voltage. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1674, "title": "Evolutionary Optimisation Methods for Template Based Image Registration", "abstract": "  This paper investigates the use of evolutionary optimisation techniques to register a template with a scene image. An error function is created to measure the correspondence of the template to the image. The problem presented here is to optimise the horizontal, vertical and scaling parameters that register the template with the scene. The Genetic Algorithm, Simulated Annealing and Particle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation with starting points chosen in a pre-processing stage. The paper investigates the precision and accuracy of each method and shows that all four methods perform favourably for image registration. SA is the most precise, GA is the most accurate. PSO is a good mix of both and the Simplex method returns local minima the most. A pre-processing stage should be investigated for the evolutionary methods in order to improve performance. Discrete versions of the optimisation methods should be investigated to further improve computational performance. ", "machine_abstract": "Image registration is an important problem in medical imaging and computer vision, where the goal is to find a transformation that aligns two or more images taken at different times and/or by different sensors. In this work we present evolutionary optimisation methods for template based image registration problems. We consider both rigid and non-rigid transformations between images. The proposed algorithms are tested on synthetic data as well as real world datasets including brain MRI scans and CT angiography (CTA) volumes. Our results show that our approach outperforms state-of-the-art techniques in terms of accuracy while being computationally efficient. This research was supported by EPSRC grant EP/N014560/1. Keywords: Evolutionary Computation, Registration, Non-Rigid Transformation, Rigid Transformation, Brain Imaging, Computer Vision. 1 Introduction Image registration is one of the most fundamental tasks in many areas such as medical imaging [1] , remote sensing [2] , video processing [3] , etc., which aims to find a spatial transformation T that maps each point x \u2208 \u21261 = [0, 1]d into its corresponding location y = Tx \u2208 \u21262 = [0, 1]d in another image I(y). Here d denotes the dimension of the space. For example, if T1 and T2 denote two consecutive time points in a dynamic sequence of images then finding the optimal transformation T would allow us to track the movement of objects over time [4] . Similarly, if S1 and S2 represent two views of the same scene captured using cameras with slightly differing orientations then registering these images will help us fuse information across multiple viewpoints [5] . In recent years there has been significant interest in developing fast and accurate registration algorithms [6] - [8] . However, despite considerable progress made towards solving this challenging problem [9] - [11] , it remains unsolved due to several factors including large number of degrees of freedom involved [12] , presence of noise [13] , partial occlusions [14] , lack of feature correspondence [15] , etc..", "paraphrased_abstract": "It is a challenge that, although several attempts have been made to resolve this challenging problem, remain unsolved, owing to many other factors, including the large number of degrees of freedom, the presence of noise, partial occlusions, and lack of feature correspondence. We propose a method of evolutionary optimization, which is based on a simple transformation, which combines a point x  1 = 0, and a point y = 0, and the dimension of the space. This work is supported by the European Research Council grant EPSRC-036060. In many fields of science, especially medical imaging, remote sensing, video processing, etc., the aim is to find a spatial transformation T, which will orientate a point x  1 = 0 in its corresponding position y = x  2 = 0 in another image y, whereas here d is the dimension of the space. This problem is an important one in the field of medical imaging and computer vision. It is important to find a transformation T that is a perfect fit to two or more images taken at different times and by different sensors, and then to find a transformation T which enables to distinguish between the motion of the objects over time. In this paper we present an evolutionary approach for detecting the images, analyzing them and testing them", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0602, "title": "Risk Assessment Algorithms Based On Recursive Neural Networks", "abstract": "  The assessment of highly-risky situations at road intersections have been recently revealed as an important research topic within the context of the automotive industry. In this paper we shall introduce a novel approach to compute risk functions by using a combination of a highly non-linear processing model in conjunction with a powerful information encoding procedure. Specifically, the elements of information either static or dynamic that appear in a road intersection scene are encoded by using directed positional acyclic labeled graphs. The risk assessment problem is then reformulated in terms of an inductive learning task carried out by a recursive neural network. Recursive neural networks are connectionist models capable of solving supervised and non-supervised learning problems represented by directed ordered acyclic graphs. The potential of this novel approach is demonstrated through well predefined scenarios. The major difference of our approach compared to others is expressed by the fact of learning the structure of the risk. Furthermore, the combination of a rich information encoding procedure with a generalized model of dynamical recurrent networks permit us, as we shall demonstrate, a sophisticated processing of information that we believe as being a first step for building future advanced intersection safety systems ", "machine_abstract": "The risk assessment algorithms based on recursive neural networks are proposed in this paper, which can be used to assess the risks for different types of financial assets and make decisions accordingly. The algorithm is composed by three parts: input data preprocessing, network training and output results analysis. In order to improve the accuracy of prediction, we use genetic algorithm (GA) to optimize the parameters of RNNs. Finally, an example is given to show how our method works. Keywords: Risk assessment; Financial asset; Genetic algorithm; Recurrent neural networks; Optimization. 1 Introduction With the rapid development of information technology, more and more people have access to online trading platforms such as Alibaba Group's Taobao Marketplace and Tencent Holdings' WeChat Pay. As a result, there has been growing interest among researchers in developing intelligent systems that can help investors make better investment decisions [1] . However, it remains challenging to develop accurate models due to the complexity of real-world problems [2] . In recent years, artificial intelligence techniques have attracted increasing attention because they provide powerful tools for solving complex problems [3] , especially recurrent neural networks (RNN). Compared with traditional feed-forward neural networks [4] , RNNs have advantages over time series forecasting [5] - [8] . For instance, RNNs can learn long-term dependencies between inputs and outputs [9] . Therefore, RNNs are widely applied in many fields including stock market prediction [10] - [12] , traffic flow prediction [13] , energy consumption prediction [14] , etc..", "paraphrased_abstract": "It is the time to calculate the rate of the trade of the world, and the time to learn the ratio of the inputs and the outputs. To evaluate the market, the algorithms are based on recursive neural networks. They are capable of learning long-term relationships between the input and the output, which are in turn useful in the prediction of the price of the market. Among them, the analysis of the recurrence of the data is based on the knowledge of three parts: the input data is preprocessed, the training of the output, and the resulting predictions. In the recent years, there have been numerous applications for artificial intelligence, especially recurrent neural networks, which are particularly suitable for forecasting the volatility of the market. However, the results are incomparably poor. Besides, there are many other applications for this, and they are many, and all the above are incomparably more numerous than the one we have already described. The research is advancing, and there is a great interest in creating intelligent systems that are more efficient than other systems, but it is not easy to do it. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.035, "title": "Visible spectroscopic and photometric survey of Jupiter Trojans: final results on dynamical families", "abstract": "  We present the results of a visible spectroscopic and photometric survey of Jupiter Trojans belonging to different dynamical families carried out at the ESO-NTT telescope. We obtained data on 47 objects, 23 belonging to the L5 swarm and 24 to the L4 one. These data together with those already published by Fornasier et al. (2004a) and Dotto et al. (2006), constitute a total sample of visible spectra for 80 objects. The survey allows us to investigate six families (Aneas, Anchises, Misenus, Phereclos, Sarpedon, Panthoos) in the L5 cloud and four L4 families (Eurybates, Menelaus, 1986 WD and 1986 TS6). The sample that we measured is dominated by D--type asteroids, with the exception of the Eurybates family in the L4 swarm, where there is a dominance of C- and P-type asteroids. All the spectra that we obtained are featureless with the exception of some Eurybates members, where a drop--off of the reflectance is detected shortward of 5200 A. Similar features are seen in main belt C-type asteroids and commonly attributed to the intervalence charge transfer transition in oxidized iron. Our sample comprises fainter and smaller Trojans as compared to the literature's data and allows us to investigate the properties of objects with estimated diameter smaller than 40--50 km. The analysis of the spectral slopes and colors versus the estimated diameters shows that the blue and red objects have indistinguishable size distribution. We perform a statistical investigation of the Trojans's spectra property distributions as a function of their orbital and physical parameters, and in comparison with other classes of minor bodies in the outer Solar System. Trojans at lower inclination appear significantly bluer than those at higher inclination, but this effect is strongly driven by the Eurybates family. ", "machine_abstract": "We present the first comprehensive study of visible spectroscopy for all known Jupiter trojans (JTs). We obtained high-resolution spectra with Subaru/HDS, Gemini/GNIRS, and Keck/NIRC-2 in order to determine their surface compositions. The sample includes 49 JTs including two new discoveries by our team. Our analysis shows that most JTs are S-complex asteroids or C-type asteroids. However, we found four objects which show unusual spectral features; one is an E-type asteroid, while three others have featureless red-sloped continuum. These objects may be primitive bodies such as carbonaceous chondrites. In addition, we find that some JTs belong to dynamically cold families. This suggests that these families were formed recently through catastrophic collisions among parent bodies. Finally, we discuss possible formation scenarios for JTs based on our findings. Keywords: Asteroids", "paraphrased_abstract": "In addition, we found that some of the JTs were members of dynamically cold families, which presumably were formed recently by collisions among parent bodies. The results were in the form of five hundred JTs, all of which had been found to have been identified in the present study. The sample consisted of a total of fifty asteroids, including two new discoveries by our team. We have collected the spectral signatures of a total of fifty such asteroids and two new discoveries of ours. Besides, we have identified a number of new spectral features, one of which is a asteroid of the asteroid group of the E type, and three of these are of the formless red-lined continuum of the asteroid class, which may be primitive bodies, like carbonaceous chondrites. We present the first comprehensive study of visible spectroscopy for all known Jupiter-tues. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0928, "title": "Cosmology from String Theory", "abstract": "  We explore the cosmological content of Salam-Sezgin six dimensional supergravity, and find a solution to the field equations in qualitative agreement with observation of distant supernovae, primordial nucleosynthesis abundances, and recent measurements of the cosmic microwave background. The carrier of the acceleration in the present de Sitter epoch is a quintessence field slowly rolling down its exponential potential. Intrinsic to this model is a second modulus which is automatically stabilized and acts as a source of cold dark matter with a mass proportional to an exponential function of the quintessence field (hence realizing VAMP models within a String context). However, any attempt to saturate the present cold dark matter component in this manner leads to unacceptable deviations from cosmological data -- a numerical study reveals that this source can account for up to about 7% of the total cold dark matter budget. We also show that (1) the model will support a de Sitter energy in agreement with observation at the expense of a miniscule breaking of supersymmetry in the compact space; (2) variations in the fine structure constant are controlled by the stabilized modulus and are negligible; (3) ``fifth''forces are carried by the stabilized modulus and are short range; (4) the long time behavior of the model in four dimensions is that of a Robertson-Walker universe with a constant expansion rate (w = -1/3). Finally, we present a String theory background by lifting our six dimensional cosmological solution to ten dimensions. ", "machine_abstract": "The talk will be divided into three parts, each part corresponding to one of the following topics:  1) The Standard Model and Beyond; 2) Cosmological Constant Problem; 3) Inflationary Universe. In the first part we will discuss how string theory can provide an explanation for all known particles in nature as well as their interactions. We will also explain why it is important that there are extra dimensions beyond those which have been observed so far.     In the second part we will present some recent results on the cosmological constant problem using supersymmetric field theories with extra dimensions. Finally, in the third part we will review our work on inflationary universe models based on supergravity coupled to gauge fields. This includes both single-field slow-roll inflation and multi-field chaotic inflation scenarios. These models may lead to observable signatures such as primordial gravitational waves or non-Gaussianities in the cosmic microwave background radiation (CMB).", "paraphrased_abstract": "At the first stage we will describe the theory of string and how it is possible to explain all the particles of nature and their interactions. Then we will describe some recent results on the cosmological constant problem based on supersymmetric field theories with extra dimensions. At the second stage we will present a recent work on the cosmological constant problem using supersymmetric fields of extra dimensions. In the third stage we will consider the inflationary theories of the universe, based on supergravity and gauge fields. They will lead to the observation of signatures, such as primordial gravitational waves or non-Gaussian variations in the cosmic microwave background radiation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0603, "title": "Quasi Ordinary Singularities, Essential Divisors and Poincare Series", "abstract": "  We define Poincar\\'e series associated to a toric or analytically irreducible quasi-ordinary hypersurface singularity, (S,0), by a finite sequence of monomial valuations, such that at least one of them is centered at the origin 0. This involves the definition of a multi-graded ring associated to the analytic algebra of the singularity by the sequence of valuations. We prove that the Poincar\\'e series is a rational function with integer coefficients, which can be defined also as an integral with respect of the Euler characteristic, over the projectivization of the analytic algebra of the singularity, of a function defined by the valuations. In particular, the Poincar\\'e series associated to the set of divisorial valuations associated to the essential divisors, considered both over the singular locus and over the point 0, is an analytic invariant of the singularity. In the quasi-ordinary hypersurface case we prove that this Poincar\\'e series determines and it is determined by the normalized sequence of characteristic monomials. These monomials in the analytic case define a complete invariant of the embedded topological type of the hypersurface singularity. ", "machine_abstract": "We study the singularity structure of quasi ordinary differential equations (QODEs) in characteristic coordinates. We show that QODEs can be transformed into systems of partial differential equations with regular singular points at infinity by using an appropriate transformation. The essential divisors are introduced to describe the local behavior near these singular points. In particular we prove that if the coefficients of the system satisfy certain conditions then there exists only one essential divisor for each point on the line at infinity. Finally we give some examples which illustrate our results. Keywords: Quasi-ordinary Differential Equations; Regular Singular Points; Essential Divisor. 1 Introduction Let us consider the following quasi ordinary differential equation (QODE) where P(x), Q(x) \u2208 C[x] \\ {0} and f is analytic function defined around x = 0. It has been shown in [1] , [4] that any solution y(x) of this type of QODE satisfies the relation  for all n \u2265 2 where c i 's are constants depending upon initial values. This implies that the solutions have infinitely many poles along the line at infinity. Therefore it seems difficult to find out the exact form of the solutions. However, in [2] , [5] , [8] , [10] , [12] , [14] , [16] , [18] , [20] , [22] , [24] , [26] , [28] , [30] , [32] , [34] , [36] , [38] , [41] , [44] , [46] , [48] , [50] , [52] , [54] , [56] , [58] , [61] , [63] , [65] , [67] , [69] , [71] , [73] , [75] , [77] , [79] , [82] , [84] , [86] , [88] , [90] , [92] , [94] , [96] , [98] , [100] , [103] , [105] , [106] , [108] , [111] , [114] , [117] , [119] , [121] , [123] , [125] , [129] ,", "paraphrased_abstract": "It was seen in [1] and (4) that any QODE of such a form satisfied the relation of all n-squares, where c i-c is constants varying in initial values. Moreover, we found in [2] and 3 [4], that any QODE of such a form satisfied the relation for all n-squares, where c i-c is constants depending on initial values. In other words, solutions of this sort had infinitely many poles along the line at infinity. In the following sections we present the geometrical representation of the two types of QODEs: the semi-equilateral, the quadratic, and the quadratic; the corresponding divisors were introduced, which characterized the local behavior of the solution at the infinity point. We show how QODEs are transformed into systems of partial differential equations with regular symmetric points at infinity. We show that, for the coefficients of the system, there is only one quadratic point at infinity. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0653, "title": "The Kalman--Yakubovich--Popov inequality for passive discrete time-invariant systems", "abstract": "  We consider the Kalman - Yakubovich - Popov (KYP) inequality \\[ \\begin{pmatrix} X-A^* XA-C^*C & -A^*X B- C^*D\\cr -B^*X A-D^* C & I- B^*X B-D^*D \\end{pmatrix} \\ge 0 \\] for contractive operator matrices $ \\begin{pmatrix} A&B\\cr C &D \\end{pmatrix}:\\begin{pmatrix}\\mathfrak{H}\\cr\\mathfrak{M} \\end{pmatrix}\\to\\begin{pmatrix}\\mathfrak{H}\\cr\\mathfrak{N} \\end{pmatrix}, $ where $\\mathfrak{H},$ $\\mathfrak{M}$, and $\\mathfrak{N}$ are separable Hilbert spaces. We restrict ourselves to the solutions $X$ from the operator interval $[0, I_\\mathfrak{H}]$. Several equivalent forms of KYP are obtained. Using the parametrization of the blocks of contractive operator matrices, the Kre\\u{\\i}n shorted operator, and the M\\\"obius representation of the Schur class operator-valued function we find several equivalent forms of the KYP inequality. Properties of solutions are established and it is proved that the minimal solution of the KYP inequality satisfies the corresponding algebraic Riccati equation and can be obtained by the iterative procedure with the special choice of the initial point. In terms of the Kre\\u{\\i}n shorted operators a necessary condition and some sufficient conditions for uniqueness of the solution are established. ", "machine_abstract": "The main result is the following theorem.  THEOREM 1 Let  be an arbitrary real number and let  be any positive integer.  Then, there exists a constant , depending only on , , such that  for all matrices . The proof of this theorem will be given in Section 2 below. In particular, if we take  then (1) becomes  which was proved by Yakubovich [J. Math. Anal. Appl., vol. 16, pp. 719-726, (1964)]. If we choose  then (1) reduces to  which was established by Popov [Publ. Math. Inst. Steklov, vol. A, no. 3, p. 225-230, (1963) ]. We remark here that our results are not new but they can be obtained as corollaries of these two papers. However, it seems worthwhile to give proofs which do not depend upon those earlier works.", "paraphrased_abstract": "As a consequence, we have obtained a set of results which are not based on those two papers. In particular, let us assume that the result is the one given by Yakubovich, J. Math. Anal. Appl., vol. 16, pp. 719-726, 1964. We shall then present an argument in which the following theorem is applied. The argument is: if we take any arbitrary real number and if there is any positive integer, there is a constant of, such that, in all matrices, there exists a constant of. Then the result is the following theorem. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1191, "title": "Five Intermediate-Period Planets from the N2K Sample", "abstract": "  We report the detection of five Jovian mass planets orbiting high metallicity stars. Four of these stars were first observed as part of the N2K program and exhibited low RMS velocity scatter after three consecutive observations. However, follow-up observations over the last three years now reveal the presence of longer period planets with orbital periods ranging from 21 days to a few years. HD 11506 is a G0V star with a planet of \\msini = 4.74 \\mjup in a 3.85 year orbit. HD 17156 is a G0V star with a 3.12 \\mjup planet in a 21.2 day orbit. The eccentricity of this orbit is 0.67, one of the highest known for a planet with a relatively short period. The orbital period for this planet places it in a region of parameter space where relatively few planets have been detected. HD 125612 is a G3V star with a planet of \\msini = 3.5 \\mjup in a 1.4 year orbit. HD 170469 is a G5IV star with a planet of \\msini = 0.67 \\mjup in a 3.13 year orbit. HD 231701 is an F8V star with planet of 1.08 \\mjup in a 142 day orbit. All of these stars have supersolar metallicity. Three of the five stars were observed photometrically but showed no evidence of brightness variability. A transit search conducted for HD 17156 was negative but covered only 25% of the search space and so is not conclusive. ", "machine_abstract": "We report on five new planets discovered by the NASA K2 mission, which were found in the sample of targets observed during Campaigns 1 and 2 (C1/K2). The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 years. We present their discovery light curves as well as follow-up photometry obtained at several observatories around the world. All five objects have been confirmed as planetary-mass companions through radial velocity measurements using high-resolution spectroscopy or precision astrometry.     Keywords: Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby stars - TESS - PLATO - HARPS-N - SPECULOOS         Five intermediate-period planets from the N2K sample  The NASA Kepler space telescope has revolutionized our understanding of extrasolar planets over its primary mission that lasted for four years . However, due to technical difficulties, only about one third of the original target list was actually observed continuously throughout this period. In order to fill out the remaining two-thirds of the original target list, K2 is observing additional fields along the ecliptic plane since 2014 . In this work we report on five new planets detected by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1/K2 ) . These planet candidates are all located close to us , with distances less than 100 parsecs away , and they span orbital periods between three days up to sixteen years . Their masses range from 0 . 5 to 4 times Jupiter 's mass .      We present here the discovery light curves together with followup photometric observations performed at various observatories worldwide . All these objects have been confirmed as low-mass companions via precise radial-velocity measurements made either with high resolution spectroscopy or with precision astrometry .", "paraphrased_abstract": "The Kepler Space Telescope, the first to observe the extrasolar planets, has transformed our understanding of extrasolar planets for four years, and so far only a third of the original target list has been observed. In order to complete the remaining half of the original target list, K2 has been following the ecliptic from 2014 to present. Here we report on five new planets discovered by K2 and observed in the field of campaign 1 and 2 (C1 and K2). These new planets are all located within a hundred pc of earth, and spanned by periods of three days up to sixteen years. Their masses range from a thousand to four thousand times Jupiter's mass. We present the discovery light curves and the subsequent measurements of several different observatories. The discovery light curves are compared with the measurements obtained at several observatories throughout the world. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1536, "title": "Critical Current of Type-II Superconductors in a Broken Bose Glass State", "abstract": "  The tilt modulus of a defective Abrikosov vortex lattice pinned by material line defects is computed using the boson analogy. It tends to infinity at long wavelength, which yields a Bose glass state that is robust to the addition of weak point-pinning centers, and which implies a restoring force per vortex line for rigid translations about mechanical equilibrium that is independent of magnetic field. It also indicates that the Bose glass state breaks into pieces along the direction of the correlated pinning centers if the latter have finite length. The critical current is predicted to crossover from two dimensional to three dimensional behavior as a function of sample thickness along the correlated pinning centers in such case. That crossover notably can occur at a film thickness that is much larger than that expected from point pins of comparable strength. The above is compared to the dependence on thickness shown by the critical current in certain films of high-temperature superconductors currently being developed for wire technology. ", "machine_abstract": "We study the critical current density Jc(B) for type-II superconductors with broken translational symmetry, such as vortex glass and Bose glass states. We show that Jc(B) is determined by the superfluid stiffness $\\rho_s$ at zero magnetic field through the relation $$J_c = \\rho_s / \\mu_0$$ where $\\mu_0$ is the vacuum permeability. This result explains why Jc(B) decreases rapidly when the system enters into the vortex glass state or the Bose glass state. The present theory also predicts that Jc(B), which has been observed experimentally, should be proportional to the square root of the number of vortices per unit volume. Finally we discuss how our results can be tested experimentally. Introduction:-The critical current density Jc (the maximum supercurrent density allowed before the transition to normal state occurs) plays an important role in determining many physical properties of type-II superconductors [1] . For example, it determines the upper limit of the operating temperature Tmax of high-Tc cuprate superconductors [2] , and thus limits their applications [3] . In this Letter, we will show that Jc depends on the superfluid stiffness \u03c1s at zero magnetic field via the relation Jc=\u03c1s/\u03bc0 [4] , where \u03bc0 is the vacuum permeability. In particular, if there are no defects in the sample, then Jc increases linearly with decreasing temperature below Tc until it reaches its maximum value Jc0=\u03c1s/\u03bc0 at T=0K [5] . However, if there exist some defects in the sample, e.g., point-like impurities [6] , dislocations [7] , grain boundaries [8] , etc., then Jc decreases rapidly upon entering into the vortex-glass phase [9] or the Bose-glass phase [10] . These predictions have been confirmed by experiments [11] - [13] .  Model:-To calculate Jc, we consider a two-dimensional model described by the following Hamiltonian:-", "paraphrased_abstract": "The critical current density (Cad) is a crucial factor in the physical properties of type-II superconductor. It is also the upper limit of the operating temperature of high-Tc cuprate superconductor[7] and thus the limits of the application. In this paper, we will consider the calculation of Cd by the superfluid stiffness s at zero magnetic field, where s is the vacuum permeability, and in which Cd is the permeability of the atom. Moreover, we will prove that Cd is proportional to the square root of the number of vortices per unit volume. The present theory also predicts that Cd is proportional to the square root of the number of vortices per unit volume. Here, we will also study the critical current density Cd by the superfluid stiffness s at zero magnetic field, namely Cd = Cd = Cd = Cd = Cd -  s - 0 - 0 - and Cd =  - Cd -. In particular, Cd is given as Cd=Cd0 in the absence of defects, e.g., there are voids in the crystal, dislocations, grain boundaries, etc., and this explains why Cd", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0487, "title": "Natural Priors, CMSSM Fits and LHC Weather Forecasts", "abstract": "  Previous LHC forecasts for the constrained minimal supersymmetric standard model (CMSSM), based on current astrophysical and laboratory measurements, have used priors that are flat in the parameter tan beta, while being constrained to postdict the central experimental value of MZ. We construct a different, new and more natural prior with a measure in mu and B (the more fundamental MSSM parameters from which tan beta and MZ are actually derived). We find that as a consequence this choice leads to a well defined fine-tuning measure in the parameter space. We investigate the effect of such on global CMSSM fits to indirect constraints, providing posterior probability distributions for Large Hadron Collider (LHC) sparticle production cross sections. The change in priors has a significant effect, strongly suppressing the pseudoscalar Higgs boson dark matter annihilation region, and diminishing the probable values of sparticle masses. We also show how to interpret fit information from a Markov Chain Monte Carlo in a frequentist fashion; namely by using the profile likelihood. Bayesian and frequentist interpretations of CMSSM fits are compared and contrasted. ", "machine_abstract": "We present the results of fits to the constrained minimal supersymmetric standard model (CMSSM) using data on dark matter relic density, direct detection experiments, flavour physics observables, electroweak precision tests, Higgs boson mass measurements and cosmological observations. We find that the best-fit point is consistent with all experimental constraints at 1-sigma level except for the anomalous magnetic moment of muon which disagrees by 2-sigma.     The allowed region in the CMSSM parameter space has been determined by scanning over $m_0, m_1, A_0, tan \\beta$ parameters within their ranges as given below. In addition we have imposed the following theoretical constraints:                         0 < $A_0$ < 16000 GeV            0 < $m_0$ < 20000 GeV            0 < $tan\\beta$ < 50     We also impose the following experimental constraints:                        0.0941 < $\\Omega_{DM} h^2$ < 0.1258           0.0085 < $\\sigma_{SI}/\\sigma_{SD} < 0.1125           3.5 < $m_{Higgs}$ < 126 GeV           0.961 < $\\mathcal{R}_B^{Bbar}_{D0}/D_{SM}^{D0}\\mu< 1.039           0.99 < $\\mathcal{R}_B^{Bs}_{D0}/D_{MS}^{D0}\\mu< 0.999           0.98 < $\\mathcal{R}_B^{Ks}_{D0}/D_{MD}^{D0}\\mu< .995           0.95 < $\\mathcal{R}_B^{Ds}_{D0}/D_{MF}^{D0}\\mu<  .985           0.96 < $\\mathcal{R}_B^{Ds}_Ds0/ D_{MF}^{Ds1}\\mu<  .988           0.97 < $\\mathcal{R}_B^{J/psi}_{D0}/D_{MP}^{D0}\\mu<    .991           0.94 < $\\mathcal{R}_B^{psi}{'}_{D0}/D_{MP}^", "paraphrased_abstract": "The optimum is determined, for the first one sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma, a thousand sigma", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1241, "title": "Cooling and heating by adiabatic magnetization in the Ni$_{50}$Mn$_{34}$In$_{16}$ magnetic shape memory alloy", "abstract": "  We report on measurements of the adiabatic temperature change in the inverse magnetocaloric Ni$_{50}$Mn$_{34}$In$_{16}$ alloy. It is shown that this alloy heats up with the application of a magnetic field around the Curie point due to the conventional magnetocaloric effect. In contrast, the inverse magnetocaloric effect associated with the martensitic transition results in the unusual decrease of temperature by adiabatic magnetization. We also provide magnetization and specific heat data which enable to compare the measured temperature changes to the values indirectly computed from thermodynamic relationships. Good agreement is obtained for the conventional effect at the second-order paramagnetic-ferromagnetic phase transition. However, at the first order structural transition the measured values at high fields are lower than the computed ones. Irreversible thermodynamics arguments are given to show that such a discrepancy is due to the irreversibility of the first-order martensitic transition. ", "machine_abstract": "The effect of cooling rate on martensitic transformation temperature (Mf) was investigated for Ni$_{50}$ Mn$_{34}$ In$_{16}$ alloy using differential scanning calorimetry (DSC). The results show that Mf decreases with increasing cooling rates, which is attributed to the increase in nucleation sites at higher cooling rates. A comparison between DSC data obtained under different conditions shows that the presence of stress during cooling has no significant influence on the value of Mf. However, it does affect the microstructure of the material as revealed by transmission electron microscopy (TEM), where the formation of dislocations can be observed when samples are cooled down without applying any external pressure. It also affects the mechanical properties such as yield strength and ultimate tensile strength.     \u00a9 2014 Elsevier B.V.  Keywords: Magnetic shape memory alloys; Cooling rate; Martensitic transformation", "paraphrased_abstract": "Xuande, 2014 Elsevier B.V.  2014 Elsevier B.V., The effect of cooling on the martensitic transformation temperature (Mf) was examined by differential scanning calorimetry (DSC), and showed that the Mf was less with increasing cooling rates. This resulted from the increase in the nucleation of the nucleation sites with increasing cooling. The microstructure of the material was studied by TEM, which revealed dislocations, which were observable by TEM, and by the formation of dislocations in the sample without applying any external pressure. The influence of Mf was not attributed to any particular effect on the elasticity of the material. The influence of strain during cooling was minor. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0321, "title": "Fabrication of half metallicity in a ferromagnetic metal", "abstract": "  We investigate the growth of half metallic phase in a ferromagnetic material using state-of-the-art full potential linearized augmented plane wave method. To address the issue, we have substituted Ti at the Ru-sites in SrRuO3, where SrRuO3 is a ferromagnetic material. Calculated results establish Ti4+ valence states (similar to SrTiO3), which was predicted experimentally. Thus, Ti substitution dilutes the Ru-O-Ru connectivity, which is manifested in the calculated results in the form of significant band narrowing leading to finite gap between t2g and eg bands. At 75% substitution, a large gap (> 2 eV) appears at the Fermi level, e_F in the up spin density of states, while the down spin states contributes at e_F characterizing the system a half-metallic ferromagnet. The t2g - eg gap can be tailored judiciously by tuning Ti concentrations to minimize thermal effects, which is often the major bottleneck to achieve high spin polarization at elevated temperatures in other materials. This study, thus, provides a novel but simple way to fabricate half-metallicity in ferromagnetic materials, which are potential candidates for spin-based technology. ", "machine_abstract": "We report the fabrication and characterization of epitaxial Fe3O4 films grown on MgO(001) substrates by pulsed laser deposition (PLD). The structural properties were investigated using X-ray diffraction, transmission electron microscopy, and atomic force microscopy techniques. We found that the film is single phase with an in-plane lattice constant of 0.84 nm which agrees well with previous reports for bulk Fe3O4. The magnetic properties are studied as a function of temperature up to 300 K and applied field up to 5 T at room temperature. It was observed that the magnetization decreases gradually with increasing temperature due to thermal fluctuations. In addition, we have also shown that the coercivity increases with decreasing thickness indicating strong exchange coupling between adjacent layers.     Finally, it has been demonstrated that these films exhibit half metallic behavior when they are capped with Pt layer. This work demonstrates that PLD technique can be used successfully to grow high quality epitaxial Fe3O4/MgO thin films.", "paraphrased_abstract": "As a result, the material is of high quality and, as a result, is stable. This work is presented as an example of the high quality and the high quality of epitaxial Fe3O4-MgO4 films. The magnetic properties are investigated in the temperature range of 300 K and at room temperature up to 5 T. In this experiment, we have shown that the magnetic flux decreases gradually with temperature, indicating that the boundary between the layers is strengthened. The material is of a single phase, with an inorganic lattice of 0.84 nm, which is in agreement with our previous studies on bulk Fe3O4. X-ray diffraction, transmission electron microscopy and atomic force microscopy were examined. The magnetic properties of the film were analyzed from the temperature up to 300 K, the polarity up to 5 T, and the thickness twentied by five T, showed that the film is more metallic when it is covered with Pt. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1831, "title": "Continuous Magnetophoretic Separation of Blood Cells from Plasma at the Microscale", "abstract": "  We present a method for the direct and continuous separation of red and white blood cells from plasma at the microscale. The method is implemented in a microfluidic system with magnetic functionality. The fluidic structure within the microsystem consists of an inlet and a single microfluidic channel with multiple outlets. The magnetic functionality is provided by an array of integrated soft-magnetic elements that are embedded transverse and adjacent to the microchannel. The elements are magnetized using an external field, and once magnetized they produce a magnetic force on blood cells as they flow through the microchannel. In whole blood, white blood cells (WBCs) behave as diamagnetic microparticles, while red blood cells (RBCs) exhibit diamagnetic or paramagnetic behavior depending on the oxygenation of their hemoglobin. We study the motion of blood cells through the microchannel using a mathematical model that takes into account the magnetic, fluidic and gravitational forces on the cells. We use the model to study blood cell separation, and our analysis indicates that the microsystem is capable of separating WBC-rich plasma, deoxygenated RBC-rich plasma and cell-depleted plasma into respective outlets. ", "machine_abstract": "Magnetic separation is an important tool in biomedical research and clinical diagnostics, but it has been limited to macroscopic devices that are not suitable for point-of-care applications. Here we report on continuous magnetophoresis-based blood cell sorting using microfluidics. We demonstrate efficient separation of red blood cells (RBCs) from plasma by applying a magnetic field gradient across a microchannel containing RBCs suspended in buffer solution. The results show that our method can be used as a simple yet effective approach for separating different types of blood cells with high purity and efficiency. This work may have significant implications towards developing portable diagnostic tools based on microscale blood processing technologies. Magnetic separation techniques play an important role in many fields including medicine, biotechnology, environmental science, food industry etc., [1] . However, most existing methods require bulky equipment which makes them unsuitable for use outside laboratory settings [2] . Recently there has been growing interest in miniaturizing these systems into lab-on-a-chip platforms [3] , where various functionalities such as sample preparation [4] , chemical analysis [5] , drug delivery [6] , and bioassays [7] could be integrated onto one single chip. In particular, magnetic separators have attracted much attention due to their simplicity, low cost, portability, and compatibility with other microfabricated components [8] . For example, several groups have demonstrated magnetic separation of biological samples inside microchannels [9] - [11] or on planar surfaces [12] - [14] . Despite this progress, however, current approaches still suffer from some limitations. First, they typically rely on batch-wise operation mode [15] , which limits throughput and requires large volumes of input samples [16] . Second, the majority of reported designs only allow for separation between two distinct populations [17] , while more complex mixtures involving multiple species cannot be processed simultaneously [18] . Third, the fabrication process usually involves complicated multi-step procedures [19] , making it difficult to integrate additional functions [20] . Finally, most previous studies were performed under static conditions [21] , which limit the flexibility of device design [22] .", "paraphrased_abstract": "It is important to say that there are many kinds of instruments used for magnetic separation in medicine, biotechnology, environmental science, food industry, and so on. However, they are not portable, and they cannot be carried away in laboratory environments. In the last years, however, much has been done to reduce the equipment needed for this type of processing to the level of microfluidic devices. This new technology combines with the conventional methods of electrolysis, a powerful instrument for the separation of blood cells from the plasma, a device that has a low cost, a low cost, a small size, and it is also suitable for the manufacture of other devices. Among the many experiments, there are also those which have devoted their effort to the study of magnetically separated blood cells from the plasma, and the results show that this method is very useful for the extraction of blood cells, and may have important implications for the development of portable diagnostic instruments. Nevertheless, there are still some difficulties, the main disadvantages being that most of the current designs only permit the separation of two distinct populations, and the more complex mixtures of multiple species cannot be separated simultaneously. The manufacturing process is complicated and involves many steps, so that it is difficult to integrate the other functions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0479, "title": "The AMIGA sample of isolated galaxies. IV. A catalogue of neighbours around isolated galaxies", "abstract": "  Studies of the effects of environment on galaxy properties and evolution require well defined control samples. Such isolated galaxy samples have up to now been small or poorly defined. The AMIGA project (Analysis of the interstellar Medium of Isolated GAlaxies) represents an attempt to define a statistically useful sample of the most isolated galaxies in the local (z < 0.05) Universe. A suitable large sample for the AMIGA project already exists, the Catalogue of Isolated Galaxies (CIG, Karachentseva 1973; 1050 galaxies), and we use this sample as a starting point to refine and perform a better quantification of its isolation properties. Digitised POSS-I E images were analysed out to a minimum projected radius R > 0.5 Mpc around 950 CIG galaxies (those within Vr = 1500 km s-1 were excluded). We identified all galaxy candidates in each field brighter than B = 17.5 with a high degree of confidence using the LMORPHO software. We generated a catalogue of approximately 54 000 potential neighbours (redshifts exist for 30% of this sample). Six hundred sixty-six galaxies pass and two hundred eighty-four fail the original CIG isolation criterion. The available redshift data confirm that our catalogue involves a largely background population rather than physically associated neighbours. We find that the exclusion of neighbours within a factor of four in size around each CIG galaxy, employed in the original isolation criterion, corresponds to Delta Vr ~ 18000 km s-1 indicating that it was a conservative limit. Galaxies in the CIG have been found to show different degrees of isolation. We conclude that a quantitative measure of this is mandatory. It will be the subject of future work based on the catalogue of neighbours obtained here. ", "machine_abstract": "We present the results on galaxy pairs in the AMIGA sample, which is an optically selected sample of isolated galaxies with distances up to 100 Mpc (cz < 10000 km/s). The main goal was to study how close companions affect the evolution and properties of their host galaxies. We have found that about half of all isolated galaxies are surrounded by at least one companion within a projected distance of 1 h-1 Mpc or less. This fraction increases for smaller separations between the two galaxies. About 20% of our objects show signs of interaction such as tidal tails and/or bridges. These fractions depend only weakly on morphological type but strongly on luminosity class. In addition we find that the majority of interacting systems are dominated by early-type spirals. Finally, we discuss possible biases introduced by using different isolation criteria. For more information see also the following papers:  - Cortese et al. (2004) : http://arXiv.org/abs/astro-ph/0404520 - Verley et al. (2007a) : http://arXiv0709.1088v1 [astro-ph]", "paraphrased_abstract": "For more information, please see the following papers: - Cortese et al. (2004): - http://arxiv.org/abs/astro-ph/0404520 - Verley et al. (2007a) - [astro-ph] - In the AMIGA collection, a collection of isolated galaxies, with distances of up to 100 Mpc, with a doubling of the distance between the two galaxies, the doubling is a strong one and depends on the type of morphology and the type of luminosity, which is also a strong one, and the most interacting systems are early spirals, and there are almost 20% of these objects, and in addition, tidal crests and bridges, and some tidal peaks, which are only visible on the morphological level but on the chromosphere level, which is the highest, and which is dominated by the first type of spirals. In this way we have shown that half of the isolated galaxies are surrounded by a companion at a distance of about h-1 Mpc, and that this fraction increases only for the smaller distances between the two galaxies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1247, "title": "Stochastic interacting particle systems out of equilibrium", "abstract": "  This paper provides an introduction to some stochastic models of lattice gases out of equilibrium and a discussion of results of various kinds obtained in recent years. Although these models are different in their microscopic features, a unified picture is emerging at the macroscopic level, applicable, in our view, to real phenomena where diffusion is the dominating physical mechanism. We rely mainly on an approach developed by the authors based on the study of dynamical large fluctuations in stationary states of open systems. The outcome of this approach is a theory connecting the non equilibrium thermodynamics to the transport coefficients via a variational principle. This leads ultimately to a functional derivative equation of Hamilton-Jacobi type for the non equilibrium free energy in which local thermodynamic variables are the independent arguments. In the first part of the paper we give a detailed introduction to the microscopic dynamics considered, while the second part, devoted to the macroscopic properties, illustrates many consequences of the Hamilton-Jacobi equation. In both parts several novelties are included. ", "machine_abstract": "We consider the dynamics of stochastic interacting particle systems in which particles evolve according to an overdamped Langevin equation with multiplicative noise and interact via pair potentials that decay exponentially fast at large distances.  We prove that, under suitable conditions on the interaction potential, these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator. These results provide new insights into the statistical mechanics of such systems far away from thermal equilibrium. Stochastic interacting particle systems have been widely used as simple models for describing physical phenomena ranging from traffic flow [1] , granular media [2] , colloidal suspensions [3] , and biological transport [4] . In this work we focus our attention on one-dimensional models where each particle evolves according to an overdamped Brownian motion driven by white Gaussian noise and interacts with its neighbors through a pairwise potential that decays exponentially fast at infinity (see Figure 1 ). The resulting system is described by the following set of It\u00f4 SDE's:", "paraphrased_abstract": "But the result is that, under a suitable conditions for the interaction potential, the systems can exhibit unique stationary states, whose density is predicted by the solutions of the Laplacian operator. This research is conducted on one-dimensional models, in which each particle is driven by an overdamped Brownian motion, driven by white Gaussian noise, and in interaction with its neighbors by a pairwise potential that decays exponentially at infinity. This result is given by the following set of it\u00f4 SDEs: ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2678, "title": "Nobeyama CO Atlas of Nearby Spiral Galaxies: Distribution of Molecular Gas in Barred and Non-barred Spiral Galaxies", "abstract": "  The data from a CO(1 - 0) mapping survey of 40 nearby spiral galaxies performed with the Nobeyama 45-m telescope are presented. The criteria of the sample selection were (1) RC3 morphological type in the range Sa to Scd, (2) distance less than 25 Mpc, (3) inclination angle less than 79deg (RC3), (4) flux at 100 um higher than ~ 10 Jy, (5) spiral structure is not destroyed by interaction. The maps of CO cover most of the optical disk of the galaxies. We investigated the influence of bar on the distribution of molecular gas in spiral galaxies using these data. We confirmed that the degree of central concentration is higher in barred spirals than in non-barred spirals as shown by the previous works. Furthermore, we present an observational evidence that bars are efficient in driving molecular gas that lies within the bar length toward the center, while the role in bringing gas in from the outer parts of the disks is small. The transported gas accounts for about half of molecular gas within the central region in barred spiral galaxies. We found a correlation between the degree of central concentration and bar strength. Galaxies with stronger bars tend to have higher central concentration. The correlation implies that stronger bars accumulate molecular gas toward the center more efficiently. These results are consistent with long-lived bars. ", "machine_abstract": "We present the Nobeyama CO (J=1-0) atlas for nearby spiral galaxies, which is based on our previous work by Kuno et al. (1995) . The sample consists of 45 barred and nonbarred spirals with distances less than 30 Mpc. We have observed these galaxies at an angular resolution of about 3 arcsec using the NRO 45 m telescope. In this study we use only those data points that are detected above 5 sigma level to make maps of molecular gas distribution. These maps show clearly that there exists a difference between barred and unbarred spirals: while most of the barred spirals exhibit strong concentration of molecular gas along their bars, such concentrations do not appear in many cases of unbarred spirals. This result suggests that the presence or absence of a bar may be one of the important factors determining the structure of galactic disks. It also indicates that the formation mechanism of bars should be different from that of bulges.     The authors acknowledge support from Grants-in-Aid for Scientific Research (No. 08540349), Ministry of Education, Science and Culture, Japan.", "paraphrased_abstract": "The authors thank the Ministry of Education, Science and Culture for providing the support for the research. We present a data-recording project of the Nobeyama-Co (J=1) based on Kuno and et al. (2000). We have taken the data of 45 spherical spirals with distances less than 30 m and observed them in a sphere of a resolution of about three arcsec. We have made a detailed map of the gaseous distribution of the spirals of the surrounding galaxy at an angle of three arcsec. We found that the proportions of the gaseous spirals of the spherical and the spherical spirals of the spherical spirals were different. The data that was collected at the 5 sigma level clearly shows that the spirals of the spherical are different: while the spherical spirals contain much gaseous material, such concentrations are rare in the un-spherical spirals. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.15, "title": "X-ray Flares in Orion Low Mass Stars", "abstract": "  Context. X-ray flares are common phenomena in pre-main sequence stars. Their analysis gives insights into the physics at work in young stellar coronae. The Orion Nebula Cluster offers a unique opportunity to study large samples of young low mass stars. This work is part of the Chandra Orion Ultradeep project (COUP), an ~10 day long X-ray observation of the Orion Nebula Cluster (ONC). Aims. Our main goal is to statistically characterize the flare-like variability of 165 low mass (0.1-0.3 M_sun) ONC members in order to test and constrain the physical scenario in which flares explain all the observed emission. Methods. We adopt a maximum likelihood piece-wise representation of the observed X-ray light curves and detect flares by taking into account both the amplitude and time derivative of the count-rate. We then derive the frequency and energy distribution of the flares. Results. The high energy tail of the energy distribution of flares is well described by a power-law with index 2.2. We test the hypothesis that light curves are built entirely by overlapping flares with a single power law energy distribution. We constrain the parameters of this simple model for every single light curve. The analysis of synthetic light curves obtained from the model indicates a good agreement with the observed data. Comparing low mass stars with stars in the mass interval (0.9-1.2M_sun), we establish that, at ~1 Myr, low mass and solar mass stars of similar X-ray luminosity have very similar flare frequencies. Conclusions. Our observational results are consistent with the following model/scenario: the light curves are entirely built by over- lapping flares with a power-law intensity distribution; the intense flares are individually detected, while the weak ones merge and form a pseudo-quiescent level, which we indicate as the characteristic level. ", "machine_abstract": "We report on the detection of X-ray flares in low mass stars (0.5-0.8 M\u2299) with ages between 1 and 10 Myr, located within the Orion Nebula Cluster. The flare activity is found to be strongly dependent upon stellar age; we find that younger stars are more active than older ones by at least an order of magnitude. We also find evidence for a dependence of flaring rate on rotation period, such that faster rotating stars have higher rates of flaring.  These results suggest that magnetic fields play an important role in regulating the level of coronal activity in young solar-type stars. This work was supported under NASA contract NAS8-39073. We thank J. Townsley for providing us with his photometric data set prior to publication. We acknowledge useful discussions with A. Feigelson, D. Getman, C. Hillenbrand, R. Herbst, S. Preibisch, B. Reipurth, T. Stassun, E. Wolff, and W. Zuckermann.", "paraphrased_abstract": "The author acknowledges his valuable consultation with A. Feigelson, D. Getman, C. Hillenbrand, R. Herbst, S. Preibisch, B. Reipurth, T. Stassun, E. Wolff, and W. Zuckermann. We report that we detected X-ray flares in low-mass stars 0.5-0.8 M in the constellation Orion. We found that younger stars were more active than older ones by at least a factor of five. We found that magnetic fields are also important for regulating coronal activity in young solar-type stars. The brightness of the flare is strongly related to the age; the younger stars are more active than the older ones, by at least an order of magnitude. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2295, "title": "Using Image Attributes for Human Identification Protocols", "abstract": "  A secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. Recently, the authors proposed a human identification protocol in the RSA Conference 2007, which is loosely based on the ability of humans to efficiently process an image. The advantage being that an automated adversary is not effective in attacking the protocol without human assistance. This paper extends that work by trying to solve some of the open problems. First, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. Secondly, we propose a new construction based on textual CAPTCHAs (Reverse Turing Tests) in order to make the generation of automated challenges easier. We also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. Finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. Our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model. ", "machine_abstract": "In this work, we propose an approach to human identification based on the analysis of image attributes and their relationships with each other. We use a set of visual features that are extracted by applying state-of-the-art computer vision techniques over images in order to represent them as vectors of numerical values. These feature vectors can be used to train machine learning algorithms such as Support Vector Machines (SVMs) or Random Forests (RF). In addition, we also consider the relationship between these features using Graphical Models (GM), which allow us to learn how they interact with one another. The proposed method is evaluated against two different datasets containing face images captured under controlled conditions. Our results show that our system outperforms existing approaches when identifying individuals across multiple sessions. This research was supported by the National Science Foundation through awards IIS-1253153 and CNS-1527225. In this work, we propose a novel approach to identify humans based on the analysis of their facial appearance. To do so, we extract several visual features from faces using state-of-the-art computer vision methods. Then, we model the interactions among those features using graphical models. Finally, we evaluate the performance of our method against two publicly available databases.", "paraphrased_abstract": "This work was supported by the National Science Foundation under grant numbers IIS-1253153 and CNS-1527225. We have developed a new method to distinguish humans through facial features by the study of the face. We first obtain the visual features of the face, we then create graphical models and investigate the interactions between them. This graphical model is then used to train the following algorithms, which are called Support Vector Machines or Random Forests. Then we test our approach with two publicly available databases. The results show that our approach outperforms previous ones, detecting the individual through several sessions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1473, "title": "A Circulating Hydrogen Ultra-High Purification System for the MuCap Experiment", "abstract": "  The MuCap experiment is a high-precision measurement of the rate for the basic electroweak process of muon capture, mu- + p -> n + nu . The experimental approach is based on an active target consisting of a time projection chamber (TPC) operating with pure hydrogen gas. The hydrogen has to be kept extremely pure and at a stable pressure. A Circulating Hydrogen Ultrahigh Purification System was designed at the Petersburg Nuclear Physics Institute (PNPI) to continuously clean the hydrogen from impurities. The system is based on an adsorption cryopump to stimulate the hydrogen flow and on a cold adsorbent for the hydrogen cleaning. It was installed at the Paul Scherrer Institute (PSI) in 2004 and performed reliably during three experiment runs. During several months long operating periods the system maintained the hydrogen purity in the detector on the level of 20 ppb for moisture, which is the main contaminant, and of better than 7 ppb and 5 ppb for nitrogen and oxygen, respectively. The pressure inside the TPC was stabilized to within 0.024% of 10 bar at a hydrogen flow rate of 3 standard liters per minute. ", "machine_abstract": "Muon-catalyzed fusion (MCF) is an exotic nuclear reaction that can be used to produce energy in future reactors, but it requires extremely pure hydrogen gas as fuel.  The MuCap experiment at TRIUMF has developed and tested a novel system for producing ultra-pure hydrogen using liquid helium cryogenic distillation followed by two stages of molecular sieves.  This system produces up to 1 liter per minute with less than 10 parts-per-trillion impurities.  It will provide enough clean hydrogen gas to operate the MuCap experiment until 2020 when the next generation of experiments are expected to begin taking data. A circulating hydrogen ultra-high purification system was designed and built for the MuCap experiment at TRI-UMF. Liquid helium cryogenic distillation is combined with two stages of molecular sieve beds to achieve high purity levels required for MCF research. The system provides up to one litre of purified hydrogen per minute with less than ten parts-per-trillion impurity content.", "paraphrased_abstract": "The circulating hydrogen system was built and tested at the Triumf facility. The circulating hydrogen system combines the cryo-dielectric system with the molecular sieves to achieve the high purity required for the study of MCF. The circulating hydrogen is ultra-pure and has a concentration of less than ten parts per million. The MuCap system is a novel experiment in the fusion of liquid helium with two layers of sieves. It yields one liter of purified hydrogen per minute with less than ten parts per million. The system will be enough for the next generation of experiments to have been performed. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4066, "title": "Can a charged dust ball be sent through the Reissner--Nordstr\\\"{o}m wormhole?", "abstract": "  In a previous paper we formulated a set of necessary conditions for the spherically symmetric weakly charged dust to avoid Big Bang/Big Crunch, shell crossing and permanent central singularities. However, we did not discuss the properties of the energy density, some of which are surprising and seem not to have been known up to now. A singularity of infinite energy density does exist -- it is a point singularity situated on the world line of the center of symmetry. The condition that no mass shell collapses to $R = 0$ if it had $R > 0$ initially thus turns out to be still insufficient for avoiding a singularity. Moreover, at the singularity the energy density $\\epsilon$ is direction-dependent: $\\epsilon \\to - \\infty$ when we approach the singular point along a $t =$ const hypersurface and $\\epsilon \\to + \\infty$ when we approach that point along the center of symmetry. The appearance of negative-energy-density regions turns out to be inevitable. We discuss various aspects of this property of our configuration. We also show that a permanently pulsating configuration, with the period of pulsation independent of mass, is possible only if there exists a permanent central singularity. ", "machine_abstract": "We consider an electrically charged dust ball in the background of a static spherically symmetric black hole with charge Q and mass M . We show that if the charge is sufficiently large, then there exists no trajectory for which the dust ball can escape to infinity. The result holds even when we allow the charge on the dust ball to vary along its world line. This suggests that it may not be possible to send information across such a wormhole using this method. Introduction:-In recent years, several authors have studied traversable Lorentzian wormholes (see [1] - [4] ). These are hypothetical shortcuts between distant regions of spacetime whose existence would violate causality unless certain conditions are satisfied [5] , namely, the weak energy condition [6] or the averaged null energy condition [7] must hold at least locally around the throat of the wormhole [8] . The simplest example of a traversable wormhole is known as the Morris-Thorne wormhole [9] . It consists of two asymptotically flat spacetimes connected by a throat region called the bridge. In order to maintain the wormhole open, one requires exotic matter [10] satisfying the strong energy condition [11] but violating the dominant energy condition [12] . However, it has been shown [13] that these requirements cannot be met within classical general relativity [14] .", "paraphrased_abstract": "In the course of recent years, a number of scientists have studied Lorentzian wormholes. This is a hypothetical wormhole, which has the effect of breaking causality, if only certain conditions are satisfied, the weak energy condition, the averaged zero energy condition, and the spheres of the wormhole have the same characteristics, but the condition of the wormhole is different, and there is no way to escape from the wormhole. The most simple example is the Morris-Thorne wormhole, composed of two parallel flat spaces, each of which is connected by a sphere called the bridge. This sphere is composed of a resonant electrostatic sphere, with a charge Q and a mass M, which has a charge Q and a mass M. We show that if the charge is very large, there is no way for the wormhole to be escaped to infinity. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0777, "title": "Binary Quantum Search", "abstract": "  Database search has wide applications and is used as a subroutine in many important algorithms. We shall consider a database with one target item. Quantum algorithm finds the target item in a database faster than any classical algorithm. It frequently occurs in practice that only a portion of information about the target item is interesting, or we need to find a group of items sharing some common feature as the target item. This problem is in general formulated as search for a part of the database [a block] containing the target item, instead of the item itself. This is partial search. Partial search trades accuracy for speed, i.e. it works faster than a full search. Partial search algorithm was discovered by Grover and Radhakrishnan. We shall consider optimized version of the algorithm and call it GRK. It can be applied successively [in a sequence]. First the database is partitioned into blocks and we use GRK to find the target block. Then this target block is partitioned into sub-blocks and we use GRK again to find the target sub-block. [We can call it binary quantum search.] Another possibility is to partition the database into sub-blocks directly and use GRK to find the target sub-block in one time. In this paper we prove that the latter is faster [makes less queries to the oracle]. ", "machine_abstract": "We present an algorithm for quantum search that is based on the Grover's algorithm and uses only two qubits to represent one item in the database, which can be viewed as a binary number. The algorithm has been implemented using IBM Q Experience simulator with four different databases containing up to 16 items each. We have also compared our results against those obtained by running Grover's original algorithm on the same datasets. Our experimental results show that the proposed algorithm performs better than its classical counterpart when searching through small databases (up to 8 items). However, it becomes less efficient if we increase the size of the database beyond this limit. This work was supported by the Australian Research Council Discovery Project DP160103745. In recent years there has been significant interest in developing algorithms for performing quantum searches over large data sets [1] . These algorithms are expected to find applications in areas such as machine learning [2] , pattern recognition [3] , computer vision [4] , bioinformatics [5] , etc., where they will allow us to solve problems faster or more accurately [6] . In general, these algorithms use N qubits to encode M elements in the database [7, 8] . For example, Grover's algorithm [9] requires O( \u221a N/M ) iterations to find any single element out of M elements encoded into N qubits [10] . It should be noted here that the number of required iterations increases exponentially with respect to both N and M [11] . Therefore, these algorithms become inefficient when dealing with very large databases [12] .", "paraphrased_abstract": "And the numbers in the database are divided into two categories: N and M. In general, there are only two categories, each of which contains a single element. The number of these categories grows exponentially with N and M, and therefore, in the case of very large databases, the computational cost is greater. There is a great interest in obtaining algorithms for quantum-searching on large data, and these are expected to be of use to a wide variety of fields, such as machine learning, pattern recognition, computer vision, bioinformatics, etc., where they can be used to solve problems faster or more accurately. The present work is a result of the project of the Australian Research Council, project DP160103745. We have tested the proposed algorithm with four different databases containing eight objects each. The results show that the proposed algorithm performs better than the classical one, and searches better for smaller databases, as well as larger ones. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2515, "title": "Perspectives for the detection and measurement of Supersymmetry in the focus point region of mSUGRA models with the ATLAS detector at LHC", "abstract": "  This paper discusses the ATLAS potential to study Supersymmetry for the \"Focus-Point\" region of the parameter space of mSUGRA models. The potential to discovery a deviation from Standard Model expectations with the first few ${fb}^{-1}$ of LHC data was studied using the parametrized simulation of the ATLAS detector. Several signatures were considered, involving hard jets, large missing energy, and either $b$-tagged jets, opposite-sign isolated electron or muon pairs, or top quarks reconstructed exploiting their fully hadronic decays. With only 1 ${fb}^{-1}$ of data each of these signatures may allow to observe an excess of events over Standard Model expectation with a statistical significance exceeding 5 standard deviations. An analytical expression was derived for the shape of the distribution of the dilepton invariant mass arising from the three-body leptonic decay of the neutralinos under the hypothesis of heavy scalars, which is appropriate for the focus-point scenario. The resulting function was used to fit the distribution of the dilepton invariant mass obtained with simulated LHC data, and to extract the value of two kinematic endpoints measuring the $\\tilde \\chi^0_2 - \\tilde \\chi^0_1$ and the $\\tilde \\chi^0_3 - \\tilde \\chi^0_1$ mass differences. This information was used to constrain the MSSM parameter space compatible with the data. ", "machine_abstract": "The Minimal Supergravity (mSUGRA) model is one of the most studied supersymmetric extensions to the Standard Model, predicting five free parameters which are constrained by measurements performed at LEP1, Tevatron Run I and II experiments as well as by cosmological observations. In this work we present results on searches for direct production of squarks and gluinos decaying into jets plus missing transverse energy using data collected during 2010-2012 proton-proton collisions at sqrt(s) = 7TeV recorded by the ATLAS experiment at Large Hadron Collider. The analysis uses two different search strategies based either on jet multiplicity or on kinematic properties of events. No significant excess over the background expectation has been observed. Upper limits have been set on the cross section times branching ratio for each signal hypothesis considered. These results are interpreted within the context of the mSUGRA model assuming that R-parity is conserved.", "paraphrased_abstract": ". The results of this work show that the search for direct generation of squarks and gluinos has a direct and a transverse atom by the kinetic properties of the events is not incongruous, and that there is no significant difference between the two signals. In the present work we have found that the search for direct formation of squarks and gluinos has a direct and a partial atom of a fixed energy is not at all skewed by the expectation of the background expectation. The mSUGRA model is one of the most widely studied supersymmetric versions of the Standard Model and consists of five free parameters, and the corresponding conditions are derived from the measurements at LEP1, Tevatron, and I. and II, and from cosmological observations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0305, "title": "Thermodynamics of an Accretion Disk Annulus with Comparable Radiation and Gas Pressure", "abstract": "  We explore the thermodynamic and global structural properties of a local patch of an accretion disk whose parameters were chosen so that radiation pressure and gas pressure would be comparable in magnitude. Heating, radiative transport, and cooling are computed self-consistently with the structure by solving the equations of radiation MHD in the shearing-box approximation. Using a fully 3-d and energy-conserving code, we compute the structure and energy balance of this disk segment over a span of more than forty cooling times. As is also true when gas pressure dominates, the disk's upper atmosphere is magnetically-supported. However, unlike the gas-dominated case, no steady-state is reached; instead, the total (i.e., radiation plus gas) energy content fluctuates by factors of 3--4 over timescales of several tens of orbits, with no secular trend. Because the radiation pressure varies much more than the gas pressure, the ratio of radiation pressure to gas pressure varies over the approximate range 0.5--2. The volume-integrated dissipation rate generally increases with increasing total energy, but the mean trend is somewhat slower than linear, and the instantaneous dissipation rate is often a factor of two larger or smaller than the mean for that total energy level. Locally, the dissipation rate per unit volume scales approximately in proportion to the current density; the time-average dissipation rate per unit mass is proportional to m^{-1/2}, where m is the horizontally-averaged mass column density to the nearer of the top or bottom surface. As in our earlier study of a gas-dominated shearing-box, we find that energy transport is completely dominated by radiative diffusion, with Poynting flux carrying less than 1% of the energy lost from the box. ", "machine_abstract": "We present the results of our numerical simulations of accretion disk annuli in which radiation pressure is comparable to gas pressure, but not dominant.  We find that for such disks there are two distinct regimes depending on whether or not the luminosity is dominated by advection (i.e., Ladv/Lvisc \u226b 1). In one regime we find that the temperature profile has a power-law form T \u221d r^{-(3/2)}, while in another it follows a more complicated dependence on radius. The latter case occurs when the luminosity is dominated either by viscous dissipation or by advection. For both cases, however, the radial velocity profiles have similar shapes. Finally, we show how these results can be used to explain observed properties of X-ray binaries. Subject headings: Black holes -accretion disks -X-ray binaries: general -Accretion, accretion disks", "paraphrased_abstract": "- Accretion, accretion, X-ray binaries: general - accretion, accretion, X-ray binaries - accretion, accretion disks, x-ray binaries, general, accretion, accretion disks, general, accretion, accretion, accretion disks. The results of the numerical simulations show that for such disks there are two distinct regimes, depending on whether or not they are dominated by advection (i.e., Ladv/Lvisc) or not. One is characterized by a power-law of Trr =3, while in the other is marked by a more complicated scalarity. The former is characterized by a viscous dissipation, and the latter by advection. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2346, "title": "Non-Gaussianity analysis on local morphological measures of WMAP data", "abstract": "  The decomposition of a signal on the sphere with the steerable wavelet constructed from the second Gaussian derivative gives access to the orientation, signed-intensity, and elongation of the signal's local features. In the present work, the non-Gaussianity of the WMAP temperature data of the cosmic microwave background (CMB) is analyzed in terms of the first four moments of the statistically isotropic random fields associated with these local morphological measures, at wavelet scales corresponding to angular sizes between 27.5 arcminutes and 30 degrees on the celestial sphere. While no detection is made neither in the orientation analysis nor in the elongation analysis, a strong detection is made in the excess kurtosis of the signed-intensity of the WMAP data. The non-Gaussianity is observed with a significance level below 0.5% at a wavelet scale corresponding to an angular size around 10 degrees, and confirmed at neighbour scales. This supports a previous detection of an excess of kurtosis in the wavelet coefficient of the WMAP data with the axisymmetric Mexican hat wavelet (Vielva et al. 2004). Instrumental noise and foreground emissions are not likely to be at the origin of the excess of kurtosis. Large-scale modulations of the CMB related to some unknown systematics are rejected as possible origins of the detection. The observed non-Gaussianity may therefore probably be imputed to the CMB itself, thereby questioning the basic inflationary scenario upon which the present concordance cosmological model relies. Taking the CMB temperature angular power spectrum of the concordance cosmological model at face value, further analysis also suggests that this non-Gaussianity is not confined to the directions on the celestial sphere with an anomalous signed-intensity. ", "machine_abstract": "We analyze the non-Gaussianity in the temperature fluctuations of the Wilkinson Microwave Anisotropy Probe (WMAP) first year sky maps using three different estimators, namely Minkowski Functionals (MF), genus curve and correlation functions. We find that all these estimators show significant deviations from Gaussian predictions at large angular scales. The observed deviation is consistent with theoretical expectations for topological defects like cosmic strings or textures which are predicted to be present during inflationary phase transitions. These results provide further support for the presence of primordial non-Gaussianities in CMB anisotropies. This work was supported by the Department of Atomic Energy under grant No. 06(B)/ST-IISc/04. The authors thank Sourav Chatterjee for useful discussions. PACS numbers: 98.80.Cq, 95.35.+d, 04.20.Jb  Keywords: Cosmic microwave background radiation", "paraphrased_abstract": "The authors thank Sourav Chatterjee for his good work. The study was supported by the Department of Atomic Energy by grant No. 04 (B)/ST-IISc.04. This work was supported by the Department of Atomic Energy under contract No. 06(B)/ST-IISc/04. We examine the non-Gaussianity of the temperature fluctuations of the first year sky maps from the Wilkinson Microwave Anisotropy Probe, which was carried out in the year 1812 by the Wiseman Interceptor, using three different methods: Minkowski Functionals, Genus Curves and Correlations. All three methods show remarkable differences from the Gaussian predictions, which are in accordance with the theoretical predictions of the cosmic strings and textures, which are predicted to be present in the phase transitions, during the period of inflation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2147, "title": "On the freezing of variables in random constraint satisfaction problems", "abstract": "  The set of solutions of random constraint satisfaction problems (zero energy groundstates of mean-field diluted spin glasses) undergoes several structural phase transitions as the amount of constraints is increased. This set first breaks down into a large number of well separated clusters. At the freezing transition, which is in general distinct from the clustering one, some variables (spins) take the same value in all solutions of a given cluster. In this paper we study the critical behavior around the freezing transition, which appears in the unfrozen phase as the divergence of the sizes of the rearrangements induced in response to the modification of a variable. The formalism is developed on generic constraint satisfaction problems and applied in particular to the random satisfiability of boolean formulas and to the coloring of random graphs. The computation is first performed in random tree ensembles, for which we underline a connection with percolation models and with the reconstruction problem of information theory. The validity of these results for the original random ensembles is then discussed in the framework of the cavity method. ", "machine_abstract": "We study the problem of finding solutions to random instances of Constraint Satisfaction Problems (CSPs). We show that for many CSPs, including k-SAT and Max-Cut, there is an exponential number of solutions with high probability when the ratio between the number of constraints and the number of variables tends to infinity. This result implies that it may be impossible to find all solutions efficiently even if we allow ourselves to use randomization or approximation algorithms. In this work, we focus on the case where each solution has exactly one variable set to true. For such solutions, called pure solutions, we prove that they are exponentially rare as well. Finally, we present some experimental results which suggest that our theoretical predictions hold also for real-world instances of SAT and MAX-CUT. The main motivation behind studying these questions comes from the fact that several important combinatorial optimization problems can be formulated as CSPs.", "paraphrased_abstract": "A most important reason for pursuing these questions is that they are associated with the various combinatorial optimization problems that can be formulated as CSPs. This problem is formulated with the view that some of the important combinatorial optimization problems can be formulated as CSPs. This is the main reason for introducing these problems into study. In our work, we examine the case where each solution has a certain value of true. For this case, we call this pure solution, and we show that these solutions are exponentially rare. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2854, "title": "Scanning and Sequential Decision Making for Multi-Dimensional Data - Part II: the Noisy Case", "abstract": "  We consider the problem of sequential decision making on random fields corrupted by noise. In this scenario, the decision maker observes a noisy version of the data, yet judged with respect to the clean data. In particular, we first consider the problem of sequentially scanning and filtering noisy random fields. In this case, the sequential filter is given the freedom to choose the path over which it traverses the random field (e.g., noisy image or video sequence), thus it is natural to ask what is the best achievable performance and how sensitive this performance is to the choice of the scan. We formally define the problem of scanning and filtering, derive a bound on the best achievable performance and quantify the excess loss occurring when non-optimal scanners are used, compared to optimal scanning and filtering.   We then discuss the problem of sequential scanning and prediction of noisy random fields. This setting is a natural model for applications such as restoration and coding of noisy images. We formally define the problem of scanning and prediction of a noisy multidimensional array and relate the optimal performance to the clean scandictability defined by Merhav and Weissman. Moreover, bounds on the excess loss due to sub-optimal scans are derived, and a universal prediction algorithm is suggested.   This paper is the second part of a two-part paper. The first paper dealt with sequential decision making on noiseless data arrays, namely, when the decision maker is judged with respect to the same data array it observes. ", "machine_abstract": "In this work, we consider the problem of scanning data in multi-dimensional space with noisy measurements. We propose an algorithm that sequentially decides which dimension to scan next based on the current measurement vector. The proposed method is shown to be optimal under certain conditions. In addition, it can also handle cases where there are multiple targets present simultaneously. Finally, numerical results show that our approach outperforms existing methods by orders of magnitude.     Keywords: Scanning, sequential decision making, multi-object tracking, information-theoretic analysis     1 Introduction     In many applications such as radar detection [1] , sonar [2] or computer vision [3] , one needs to detect objects (e.g., aircrafts) in multi-dimensional spaces using limited resources. For example, in air traffic control [4] , radars need to track several aircrafts at once while minimizing false alarms due to clutter noise. This task requires efficient algorithms to decide how to allocate available resources among different dimensions so that the overall performance is optimized. A common strategy used in these problems is to perform scans along each dimension separately until some stopping criterion is met. However, this may lead to suboptimal solutions since the best solution depends not only on the current measurement but also on future measurements. Therefore, it becomes necessary to develop new techniques to solve these problems more efficiently.     In recent years, significant progress has been made towards solving various resource allocation problems related to multi-target tracking [5] . Most of them focus on optimizing the number of sensors [6] , their locations [7, 8] , or the sensor network topology [9] . These works assume that all target states are known exactly before performing any optimization. However, in practice, target state estimates are often uncertain because they are obtained through noisy measurements [10] . As a result, the aforementioned approaches cannot guarantee global optimality when applied directly to practical scenarios [11] .     To address this issue, researchers have developed robust versions of classical resource allocation strategies [12] . They typically use worst-case formulations [13] to ensure that the resulting allocations remain feasible even if the true target states deviate significantly...", "paraphrased_abstract": "However, there are few known ways to allocate resources, and even in practice these methods are not universally correct. In the field of vision, such as radars, sonars, or computer vision, it is necessary to detect objects in multidimensional spaces, and to minimize false alarms due to noise. This task is especially important for radar detection, sonar, or computer vision. Most of these methods require that the number of sensors be precisely defined before they are optimized, and the number of the sensors in their place and in the topology of the network are defined. Hence the problem is complicated and requires a strong system to manage this problem. The problem of scanning in multidimensional space, even with noisy measurements, is very complicated. Consequently, the optimal solution is not known for all the dimensions, but for the other dimensions too. For example, in the field of aviation, a radar must follow several aircrafts simultaneously, to avoid false alarms due to noise. In these problems, the algorithms are usually used to estimate the target states in all dimensions, and the result is often suboptimal, because the best solution is dependent not only on the current measurement, but also on future measurements. The work we have been carrying out has been very successful. In this work, we consider the problem of scanning the data in the multidimensional space with noisy measurements. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1467, "title": "Capillary ordering and layering transitions in two-dimensional hard-rod fluids", "abstract": "  In this article we calculate the surface phase diagram of a two-dimensional hard-rod fluid confined between two hard lines. In a first stage we study the semi-infinite system consisting of an isotropic fluid in contact with a single hard line. We have found complete wetting by the columnar phase at the wall-isotropic fluid interface. When the fluid is confined between two hard walls, capillary columnar ordering occurs via a first-order phase transition. For higher chemical potentials the system exhibits layering transitions even for very narrow slits (near the one-dimensional limit). The theoretical model used was a density-functional theory based on the Fundamental-Measure Functional applied to a fluid of hard rectangles in the restricted-orientation approximation (Zwanzig model). The results presented here can be checked experimentally in two-dimensional granular media made of rods, where vertical motions induced by an external source and excluded volume interactions between the grains allow the system to explore those stationary states which entropically maximize packing configurations. We claim that some of the surface phenomena found here can be present in two-dimensional granular-media fluids. ", "machine_abstract": "We study the phase behavior of a system of N identical hard rods confined to a square box with periodic boundary conditions, using Monte Carlo simulations at constant pressure P . We find that for sufficiently large values of P , there is an ordered state where all particles are aligned along one direction (the x-axis), forming layers perpendicular to this axis.  The transition between disordered and ordered states occurs via a first-order phase transition which we characterize by studying the density profiles across the simulation cell as well as the order parameter distribution function. For small values of P , however, no such ordered state exists. Instead, the system exhibits a glassy dynamics characterized by slow relaxation timescales. Finally, we show how our results can be used to explain recent experiments on colloidal suspensions under shear flow. In many physical systems, it has been observed that particles tend to align themselves into regular patterns when they interact strongly enough. This phenomenon is known as capillarity [1] or self-assembly [2] . In particular, in two dimensions, particles may form stripes [3] - [5] , squares [6] , hexagons [7] , or even more complex structures [8] depending on their shape [9] , size [10] , interactions [11] , and external fields [12] . These phenomena have attracted considerable attention over the past few years due to both fundamental interest [13] and potential applications [14] - [16] . A particularly interesting example is provided by colloidal suspensions [17] - [20] : When these are subjected to strong shear flows [21] , they often exhibit striped phases [22] - [24] whose formation mechanism remains poorly understood [25] .", "paraphrased_abstract": "\u201cThe phenomena we have studied have been especially interesting in the last few years, because of the importance they bring to the physical world and the potential applications that they may serve. We have emphasized that it is possible to produce striped particles when the tumbling process is strong and strong, and this is known as slendering. During strong shear flows, however, striped particles may form, in particular, squares, hexagons, or even more complex structures, depending on their shape, size, contact with the environment, and the properties of the external fields. It is particularly interesting to see the striped particles in colloidal suspensions confined to a square box, under the conditions of constant P. This is called capillary or self-assembling. During high pressures, particles tend to be ordered to one another in one direction (the x-axis), so that the layers are perpendicular to this x-axis. In the simplest form, the striped particles appear in two dimensions, forming squares, squares, hexagons, or even more complex structures depending on their shape, size, interactions, and external fields. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4041, "title": "Influence of the lattice topography on a three-dimensional, controllable Brownian motor", "abstract": "  We study the influence of the lattice topography and the coupling between motion in different directions, for a three-dimensional Brownian motor based on cold atoms in a double optical lattice. Due to controllable relative spatial phases between the lattices, our Brownian motor can induce drifts in arbitrary directions. Since the lattices couple the different directions, the relation between the phase shifts and the directionality of the induced drift is non trivial. Here is therefore this relation investigated experimentally by systematically varying the relative spatial phase in two dimensions, while monitoring the vertically induced drift and the temperature. A relative spatial phase range of 2pi x 2pi is covered. We show that a drift, controllable both in speed and direction, can be achieved, by varying the phase both parallel and perpendicular to the direction of the measured induced drift. The experimental results are qualitatively reproduced by numerical simulations of a simplified, classical model of the system. ", "machine_abstract": "We report an experimental realization of a novel type of 3D Brownian motors based on colloidal particles trapped in periodic potentials created by laser interference patterns. The potential landscape is engineered to have two different types of local minima separated by barriers with varying heights and widths. We show that this design allows for controlling both directional transport as well as its speed over several orders of magnitude. This work opens up new possibilities for designing active materials with tunable properties. A growing number of applications require systems capable of converting energy into directed motion at low Reynolds numbers [1] . In recent years, there has been significant progress towards realizing such devices known as \"Brownian motors\" [2] , which are typically composed of many interacting particles moving through complex environments [3] . In particular, it was shown theoretically [4] and experimentally [5] that one can create a unidirectional current of colloids using optical traps arranged in a 2D square or honeycomb pattern [6] . However, these designs cannot be easily extended to three dimensions (3D) due to technical limitations associated with creating stable trapping sites [7, 8] . Here we demonstrate how to overcome those challenges by engineering the shape of the potential wells and barriers in order to achieve robust 3D transport. Our approach relies on the use of holographic optical tweezers [9] to trap polystyrene microspheres suspended in water inside a glass capillary tube [10] . By changing the phase between the beams forming each individual trap [11] , we were able to generate a variety of potential landscapes [12] ranging from simple double-well structures [13] to more complicated ones containing multiple barriers [14] .", "paraphrased_abstract": "But these devices cannot be extended to the three-dimensional plane, because the trapping site is difficult to establish and is difficult to secure. This is a challenge in which we show how to overcome it by modifying the form of the wells and the barriers in order to create a robust 3D motion. Especially recently, a remarkable advance has been made in the field of brownian motion, i.e., a kind of cyclic current of colloids arranged in a honeycomb or a square pattern. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2655, "title": "658 GHz Vibrationally-Excited Water Masers with the Submillimeter Array", "abstract": "  Discovered in 1995 at the Caltech Submillimeter Observatory (CSO), the vibrationally-excited water maser line at 658 GHz (455 micron) is seen in oxygen-rich giant and supergiant stars. Because this maser can be so strong (up to thousands of Janskys), it was very helpful during the commissioning phase of the highest frequency band (620-700 GHz) of the Submillimeter Array (SMA) interferometer. From late 2002 to early 2006, brief attempts were made to search for emission from additional sources beyond the original CSO survey. These efforts have expanded the source count from 10 to 16. The maser emission appears to be quite compact spatially, as expected from theoretical considerations; thus these objects can potentially be used as atmospheric phase calibrators. Many of these objects also exhibit maser emission in the vibrationally-excited SiO maser at 215 GHz. Because both maser lines likely originate from a similar physical region, these objects can be used to test techniques of phase transfer calibration between millimeter and submillimeter bands. The 658 GHz masers will be important beacons to assess the performance of the Atacama Large Millimeter Array (ALMA) in this challenging high-frequency band. ", "machine_abstract": "We report on observations made in June and July 2005 using the Submillimeter Array (SMA) atop Mauna Kea, Hawaii to search for vibrationally-excited water maser emission toward two massive protostellar objects, IRAS 20126+4104 and G34.3+0.2. We detected strong 658 GHz maser emission toward both sources. The observed properties are consistent with those previously reported by other authors. In addition we found that the maser emission is highly variable over timescales as short as one day. This variability may be due to changes in physical conditions within the masing region or perhaps due to interstellar scintillation effects caused by intervening material along our line-of-sight. These results demonstrate the power of high sensitivity interferometric observations to study the temporal behavior of astrophysical phenomena. Keywords: Water maser, Interferometry, Time-variability", "paraphrased_abstract": "I have shown that the use of interferometric measurements can be applied to the study of astrophysics. The present study focuses on water masers, interferometry, and time. We describe, in particular, our observations of the Submillimeter Array at Mauna Kea, Hawaii, in June and July 2005, in order to detect the vibrational wave of water masers to two large, protostellar objects, IRAS 20126+4104 and G34.3+0.2. The observations are consistent with those previously reported. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3431, "title": "Suppressed radio emission in supercluster galaxies: enhanced ram pressure in merging clusters?", "abstract": "  The environmental influence on the 1.4 GHz continuum radio emission of galaxies is analyzed in a 600 deg2 region of the local Universe containing the Shapley Supercluster (SSC). Galaxies in the FLASH and 6dFGS redshift surveys are cross-identified with NVSS radio sources, selected in a subsample doubly complete in volume and luminosity. Environmental effects are studied through a smoothed density field (normalized with random catalogs with the same survey edges and redshift selection function) and the distance to the nearest cluster (R/r200, where r200 is the virial radius, whose relation to the aperture velocity dispersion is quantified). The fraction of high radio loudness (R_K=L_radio/L_K) galaxies in the 10 Mpc Abell 3558 cluster complex at the core of the SSC (SSC-CR) is half as large than elsewhere. In the SSC-CR, R_K is anti-correlated with the density of the large-scale environment and correlated with R/r200: central brightest cluster galaxies (BCGs) in the SSC-CR are 10x less radio-loud than BCGs elsewhere, with signs of suppressed radio loudness in the SSC-CR also present beyond the BCGs, out to at least 0.3 r200. This correlation is nearly as strong as the tight correlation of L_K with R/r200 (K-luminosity segregation), inside the SSC-CR. The suppression of radio loudness in SSC-CR BCGs can be attributed to cluster-cluster mergers that destroy the cool core and thus the supply of gas to the central AGN. We analytically demonstrate that the low radio loudness of non-BCG galaxies within SSC-CR clusters cannot be explained by direct major galaxy mergers or rapid galaxy flyby collisions, but by the loss of gas supply through the enhanced ram pressure felt when these galaxies cross the shock front between the 2 merging clusters and are later subjected to the stronger wind from the 2nd cluster. ", "machine_abstract": "We report on the detection of suppressed radio emission in two interacting galaxy clusters, Abell 2146 and Abell 2390 (z = 0.23). The cluster pair is part of an extended X-ray filamentary structure that extends over ~30 Mpc along the line-of-sight. We find that the fraction of radio-detected galaxies decreases with increasing projected distance to the nearest cluster center for both clusters individually as well as when combining all galaxies within the entire filamentary structure. This trend cannot be explained by differences in stellar mass or SFR between galaxies located at different distances from their host cluster centers. Instead we argue that it can be understood if the gas density increases towards the cluster cores due to gravitational compression and/or shock heating during cluster mergers. In this case, the increased ram pressure would suppress star formation activity in galaxies residing close to the cluster core. Our results are consistent with previous studies showing that the fraction of star-forming galaxies decreases rapidly with decreasing cluster-centric radius.", "paraphrased_abstract": "And it turns out that the gas density increases toward the core, and the pressure of the ram increases the activity of the galaxies close to the core. This is not due to the difference in mass and SFR of the galaxies located at different distances from their host clusters. In our experiments, the radio emission decreases as the distance to the central cluster increases. The clusters are in a wide X-ray filament that extends over a radius of about 30 m. Our study is consistent with the previous work that shows that the abundance of star-forming galaxies increases rapidly as the distance increases. We find that the fraction of radio-observed galaxies decreases with increasing the distance to the central cluster. We study the emission of radio waves in two interacting galaxy clusters, Abell 2146 and Abell 2390 (z = 0.62). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0461, "title": "What Gamma-Ray Bursts Explode Into", "abstract": "  The association of long gamma-ray bursts (GRBs) with Type Ib/c supernovae implies that they explode into the winds of their Wolf-Rayet progenitor stars. Although the evolution of some GRB afterglows is consistent with expansion into a free wind, there is also good evidence for expansion into a constant density medium. The evidence includes the evolution of X-ray afterglows (when X-rays are below the cooling frequency), the evolution of the pre-jet break optical and X-ray afterglow, and the sharp turn-on observed for some afterglows. Recent observations of short bursts, which are expected to be interacting with a constant density medium, provide a check on the standard afterglow model. Although radio observations do not support the constant density model for long bursts in some cases, the evidence for constant density interaction is strong. The most plausible way to produce such a medium around a massive star is to shock the progenitor wind. This requires a smaller termination shock than would be expected, possibly due to a high pressure surroundings, a high progenitor velocity, or the particular evolution leading to a GRB. However, the need for the termination shock near the deceleration radius cannot be plausibly accomodated and may indicate that some long bursts have compact binary progenitors and explode directly into the interstellar medium. ", "machine_abstract": "Gamma-ray bursts (GRBs) are the most energetic explosions in the universe, but their origin is still unknown. The leading model for GRB production involves two compact objects merging into one black hole and then exploding as a result of rapid accretion onto this black hole.  In this talk I will discuss how we can use gravitational waves to test this hypothesis by looking at the ringdown phase of these mergers. This is an exciting time for gravitational wave astronomy with Advanced LIGO/VIRGO starting taking data soon!  I will also present some recent results on using gravitational waves to study neutron star mergers that may be related to short gamma ray bursts. Finally, I'll give you my personal perspective on what it takes to become a successful scientist today. My research interests include gravitational waves, astrophysics, cosmology, and particle physics. I'm currently working at MIT Kavli Institute for Astrophysics and Space Research where I am part of the Gravitational Wave Cosmology Project.", "paraphrased_abstract": "Astrophysics, cosmology, and particle physics. My main research interests are gravitational waves, astrophysics, cosmology, and particle physics. I am a member of the Gravitational Wave Cosmology Project at the Kavli Institute for Astrophysics and Space Research. In the meantime, I will discuss some recent results from the study of the mergers of neutron stars, in which the mergers are related to short gamma ray bursts. The most recent example of the creation of GRBs is the merging of two compact objects into a black hole and then explosion, resulting from the rapid collapse of the black hole. This experiment is of particular interest to me, as it is the first to use gravitational waves to study the mergers of neutron stars, which can be related to short gamma ray bursts. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3533, "title": "Very High Energy $\\gamma$-ray and Near Infrared observations of 1ES2344+514 during 2004-05", "abstract": "  We have observed the BL Lac object 1ES2344+514 (z = 0.044) in Very High Energy (VHE) gamma-ray and near-infrared wavelength bands with TACTIC and MIRO telescopes respectively. The observations were made from 18th October to 9th December 2004 and 27th October 2005 to 1st January 2006. Detailed analysis of the TACTIC data indicates absence of a statistically significant gamma-ray signal both in overall data and on a nightly basis from the source direction. We estimate an upper limit of I($\\geq$1.5 TeV)$\\leq 3.84 \\times 10^{-12}$ photons cm$^{-2}$ s$^{-1}$ at a 3$\\sigma$ confidence level on the integrated $\\gamma$-ray flux. In addition, we have also compared TACTIC TeV light curves with those of the RXTE ASM (2-12keV) for the contemporary period and find that there are no statistically significant increases in the signal strengths from the source in both these energy regions. During 2004 IR observations, 1ES2344+514 shows low level (~0.06 magnitude) day-to-day variation in both, J & H bands. However, during 2005 observation epoch, the source brightens up by about 0.41 magnitude from its October 2005 level J magnitude= 12.64 to J = 12.23 on December 6, 2005. It then fades by about 0.2 magnitude during 6 to 10 December, 2005. The variation is seen in both, J & H, bands simultaneously. The light travel time arguments suggest that the emission region size is of the order of $10^{17}$ cms. ", "machine_abstract": "We report on the detection by HESS (High Energy Stereoscopic System) of VHE gamma rays from the BL Lac object 1ES2344+514; this is only the second time that such emission has been detected from an extragalactic source at energies above 100 GeV, after Mkn 421 in 1997.  The data were taken between February 2004 and May 2005 with the four telescopes of the system pointed towards the source for periods ranging from one to several hours per night; the total exposure was about 50 h live-time. We also present near-infrared photometry obtained simultaneously using REM (Rapid Eye Mount), which shows no evidence for variability over timescales as short as 30 minutes. No significant flux variations are seen within individual nights or between different nights. A simple power-law fit gives a photon index of 2.6 \u00b1 0.2 stat \u00b1 0.3 sys . This value is consistent with previous measurements made by EGRET and Whipple Observatory but significantly softer than those measured previously by CAT and MAGIC.", "paraphrased_abstract": "It is moreover the first time that a gamma ray can be detected from an extragalactic source at a rate of over 100 GeV, compared with Mk 421 in 1997. We present here a close-up view of the gamma rays, obtained by REM (Rapid Eye Mount) which shows that it is not affected by any amplification of the flux. The light of the gamma rays was detected by the HESS (High Energy Stereoscopic System) in the BL Lac, 1ES2344+514, which was the second time that such a ray could be detected from an extragalactic source, exceeding 100 GeV. The measurements were taken in May 2004 and May 2005, with the four telescopes pointed at the source, for about four hours at a time. The exposure was a total of about fifty hours. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.098, "title": "Trans-Planckian Issue in the Milne Universe", "abstract": "  The ``trans-Planckian'' challenge in cosmology appears when we trace the present physical wavelengths of fluctuations backwards in time. They become smaller and smaller until crossing the Planck scale where conventional QFT is challenged, so that unknown ultraviolet physics may be traced in the observable cosmological fluctuations. Usually this issue is addressed in the inflationary context, but trans-Planckian reasoning is much broader. We examine this logic in a simple example of scalar quantum field theory in the expanding and contracting Milne universes, where wavelengths of the eigenmodes are red- or blue-shifted. Trans-Planckian modifications of QFT should result in a UV-dependent VeV of the energy momentum tensor of a scalar field in the Milne universe. On the other hand, the Milne universe is another coordinate systems of flat Minkowski space-time, and the covariant energy momentum tensor should be the same (but vacuum-dependent) in different coordinates of flat space time. We explicitly demonstrate that in conventional QFT the energy momentum tensor, choosing the adiabatic vacuum, is identical to zero in Minkowski coordinates, and remains zero in the contracting Milne universe (due to non-trivial cancellations of contributions from particles which appear in the accelerating frame and from vacuum polarization there). In contrast to this, the trans-Planckian modification of the energy momentum tensor is not motivated. We provide a similar argument for the expanding Milne universe, where the energy momentum tensor in the conformal vacuum is non-zero. Similar arguments are applicable for other cosmological models where the curvature is much lower than Planckian which leads to conflicts with trans-Planckian considerations. ", "machine_abstract": "We consider the problem of time evolution for quantum fields on the Milne universe, which is an open FRW cosmology with negative spatial curvature and vanishing cosmological constant. We show that there are no normalizable solutions to the Klein-Gordon equation corresponding to massive particles at early times (t \u2192 \u2212\u221e). This result can be understood as follows. The Milne universe has a singularity at t = 0 where all physical quantities diverge. In particular, the energy density diverges like $1/t^2$ near this point. As a consequence, any particle state localized around $t=0$ will have infinite energy. Therefore it cannot correspond to a physically meaningful solution of the field equations. On the other hand, we find that there exist normalizable states describing massless particles at early times. These states represent gravitons or photons propagating into the past along geodesics towards the big bang singularity.", "paraphrased_abstract": "The Milne universe has a singularity at t0 which contains all physical quantities. Particularly, the energy density of the particles at t0 is like $1/t2$. Thus any particle, whose density is t0, has an infinite energy, and therefore cannot be a physically meaningful solution of the equations of the Klein-Gordon equation. It is the problem of time evolution in the Milne universe, which is an open, FRW cosmology with a negative spatial curvature and a vanishing cosmological constant. We will see the problem of time evolution in the Milne universe. We have a general problem of time evolution in the Milne universe, an open, FRW cosmology with negative spatial curvature and vanishing cosmological constant. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2678, "title": "X-ray and Sunyaev-Zel'dovich scaling relations in galaxy clusters", "abstract": "  [Abridged] We present an analysis of the scaling relations between X-ray properties and Sunyaev-Zel'dovich (SZ) parameters for a sample of 24 X-ray luminous galaxy clusters observed with Chandra and with measured SZ effect. These objects are in the redshift range 0.14--0.82 and have X-ray bolometric luminosity L>10^45 erg/s. We perform a spatially resolved spectral analysis and recover the density, temperature and pressure profiles of the ICM, just relying on the spherical symmetry of the cluster and the hydrostatic equilibrium hypothesis. We observe that the correlations among X-ray quantities only are in agreement with previous results obtained for samples of high-z X-ray luminous galaxy clusters. On the relations involving SZ quantities, we obtain that they correlate with the gas temperature with a logarithmic slope significantly larger than the predicted value from the self-similar model. The measured scatter indicates, however, that the central Compton parameter y_0 is a proxy of the gas temperature at the same level of other X-ray quantities like luminosity. Our results on the X-ray and SZ scaling relations show a tension between the quantities more related to the global energy of the system (e.g. gas temperature, gravitating mass) and the indicators of the structure of the ICM (e.g. gas density profile, central Compton parameter y_0), showing the most significant deviations from the values of the slope predicted from the self-similar model in the L-T, L-M_{tot}, M_{gas}-T, y_0-T relations. When the slope is fixed to the self-similar value, these relations consistently show a negative evolution suggesting a scenario in which the ICM at higher redshift has lower both X-ray luminosity and pressure in the central regions than the expectations from self-similar model. ", "machine_abstract": "We present the results on X-ray luminosity, temperature, gas mass fraction and YX (the product of these three quantities) for a sample of 62 galaxy clusters observed with XMM-Newton. We find that our data are consistent with self-similar evolution at low redshifts but show deviations from it at higher redshifts. The deviation is more pronounced when we use the SZ signal to estimate cluster masses instead of using the X-ray temperatures. This may be due to non-thermal pressure support or biases introduced by different selection effects between X-ray and SZ surveys. Our results suggest that the scatter around the mean relation increases towards high redshift. We also investigate how well the scaling relations can be used as cosmological probes. Using the current observational constraints on the Hubble constant H0 = 73 km s-1 Mpc-1 , matter density parameter Omega_m = 0.27 and dark energy equation-of-state w = -1, we find that the uncertainty in the derived values of Omega_m and w is dominated by systematic uncertainties rather than statistical errors.", "paraphrased_abstract": "But in general, the relationship between the two peaks becomes more skewed toward high redshifts. We examine the limits of the Hubble constant H0=73 kms s-1 Mpc-1. The matter density, Omega=m = 0.27, and the dark energy equation w = -1, show that the uncertainty in the derived values is dominated by systematic and systematic errors, rather than by statistical errors. We compare our results with those of the X-ray luminosity, the temperature, the mass of the gas, the mass fraction, and the YX (the product of these three quantities) in a sample of 62 galaxy clusters analyzed by X-ray and SZ. The results indicate that the scattering around the mean is increasing with redshift. We conclude that the distribution of the two measurements can be used to estimate the cosmological indices. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3583, "title": "Sub-arcsecond CO(1-0) and CO(2-1) observations of the ultraluminous infrared galaxy IRAS 10190+1322", "abstract": "  We present the results of high resolution mapping of the CO(1-0) and CO(2-1) emission of the ultraluminous infrared galaxy (ULIRG) IRAS 10190+1322, with the IRAM interferometer, down to an angular resolution of ~0.3 arcsec. This object is composed of two interacting galaxies with a projected nuclear separation of 6 kpc, and was selected to analyze the physical and dynamical properties of the molecular gas in each galaxy in order to study the conditions that lead a galaxy pair to become ultraluminous in the infrared. With the exception of Arp 220, the closest ULIRG, this is the first time that the CO emission is morphologically and kinematically resolved in the two interacting galaxies of a ULIRG system. In one of the galaxies the molecular gas is highly concentrated, distributed in a circumnuclear disk of 1.7 kpc in size. The molecular gas in the presumably less infrared luminous galaxy is distributed in a more extended disk of 7.4 kpc. The molecular gas mass accounts for ~10% of the dynamical mass in each galaxy. Both objects are rich enough in molecular gas, Mgas ~ 4 10^9 Msun, as to experience an infrared ultraluminous phase. ", "machine_abstract": "We present sub-arcsecond resolution interferometric observations in the millimeter continuum, 12CO(J=1-0), 13CO(J=1-0) and C18O(J=1-0). We find that the molecular gas is distributed into two components with different kinematics. The main component has an extent of 1 kpc (0.5 arcsec at z = 2.3) and shows blueshifted emission on its eastern side and redshifted emission on its western side. This suggests that this component may be associated to outflowing material. A second component is detected only in 12CO(J=1-1) and it is located 0.2 arcsec eastward of the first one. It also shows blueshifted velocities but they are smaller than those observed for the main component. Both components show similar line widths suggesting that both have comparable dynamical masses. These results suggest that the bulk of the molecular gas is concentrated in these two components which could represent the progenitors of massive elliptical galaxies seen today.     Keywords: Ultraluminous Infrared Galaxies", "paraphrased_abstract": "At the same time, we find that the molecular gas is divided into two groups. The first group, which is 0.1 kpc (0.1 arcsec at z=2), is characterized by blue refractions in the eastern part, and the red refractions in the western part, suggesting that it may be a source of outflow. In this second group, we present a sub-arcsecond resolution interferometer at z=2.5, 12CO(J) and 13CO(J); they are all located 0.2 arcsecs from each other and their line widths are equal to each other, which implies that the smattering of the molecules is concentrated in these two groups, and that the vast elliptical galaxies have been formed to this day. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0979, "title": "Precision measurements of large scale structure with future type Ia supernova surveys", "abstract": "  Type Ia supernovae are currently the best known standard candles at cosmological distances. In addition to providing a powerful probe of dark energy they are an ideal source of information about the peculiar velocity field of the local universe. Even with the very small number of supernovae presently available it has been possible to measure the dipole and quadrupole of the local velocity field out to z~0.025. With future continuous all-sky surveys like the LSST project the luminosity distances of tens of thousands of nearby supernovae will be measured accurately. This will allow for a determination of the local velocity structure of the universe as a function of redshift with unprecedented accuracy, provided the redshifts of the host galaxies are known. Using catalogues of mock surveys we estimate that future low redshift supernova surveys will be able to probe sigma-8 to a precision of roughly 5% at 95% C.L. This is comparable to the precision in future galaxy and weak lensing surveys and with a relatively modest observational effort it will provide a crucial cross-check on future measurements of the matter power spectrum. ", "machine_abstract": "We present forecasts for the precision on cosmological parameters that can be achieved by combining data from current and planned galaxy redshift surveys, weak lensing experiments, and future SNe Ia surveys. We find that these three probes will provide complementary information about dark energy and neutrino masses. The combination of all three datasets leads to constraints on w{\\displaystyle w} and m\u03bd{\\displaystyle m_{\\nu }} which are competitive with those expected from Planck CMB observations. In particular we show how future SNe Ia surveys could significantly improve our knowledge of the equation of state parameter w{\\displaystyle w} at redshifts z>2.5 where it is currently poorly constrained. Future SNe Ia surveys may also help break degeneracies between other cosmological parameters such as \u2126m{\\displaystyle \\odot }and h0{\\displaystyle H_{0}} . This work was supported in part by NASA grant NNX08AG84G (PI:KEY).", "paraphrased_abstract": "A future SNe Ia survey could be of great value for the understanding of the equation of state parameter wdisplaystyle w at redshifts z>2.5 where it is currently under constraint. We have derived estimates of the accuracy of cosmological parameters by combining the results of recent and planned galaxy redshift surveys, weak lens experiments, and future SNe Ia surveys. We have also derived predictions for the precision of dark energy and neutrino masses. We show that the combined data would be sufficient to give a better idea of the exact values of wdisplaystyle w and mdisplaystyle m_nu. The combination of the three data sets would be sufficiently precise to give us a precise estimate of dark energy and neutrino masses. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1515, "title": "Magnetization oscillations induced by a spin-polarized current in a point-contact geometry: mode hopping and non-linear damping effects", "abstract": "  In this paper we study magnetization excitations induced in a thin extended film by a spin-polarized dc-current injected through a point contact in the current-perpendicular-to-plane (CPP) geometry. Using full-scale micromagnetic simulations, we demonstrate that in addition to the oscillations of the propagating wave type, there exist also two localized oscillation modes. The first localized mode has a relatively homogeneous magnetization structure of its kernel and corresponds to the so called 'bullet' predicted analytically by Slavin and Tiberkevich (Phys. Rev. Lett., 95 (2005) 237201). Magnetization pattern of the second localized mode kernel is highly inhomogeneous, leading to a much smaller power of magnetoresistance oscillations caused by this mode. We have also studied the influence of a non-linear damping for this system and have found the following main qualitative effects: (i) the appearance of frequency jumps within the existence region of the propagating wave mode and (ii) the narrowing of the current region where the 'bullet' mode exists, until this mode completely disappears for a sufficiently strong non-linear damping. ", "machine_abstract": "We study the magnetization dynamics driven by an alternating spin polarized current (ASPC) flowing through a magnetic tunnel junction with perpendicular anisotropy. We show that, depending on the amplitude of the ASPC, two different regimes can be observed experimentally: i) for small amplitudes, we observe a single frequency corresponding to the ferromagnetic resonance; ii) when increasing the amplitude of the ASCP, several frequencies are excited simultaneously leading to a complex spectrum which is analyzed using numerical simulations based on the Landau-Lifshitz-Gilbert equation including spin-transfer torque terms. The results obtained are discussed in connection with recent experiments performed at room temperature.     PACS: 75.60.Cc, 76.30.+z, 77.20.Hs     Spin transfer torques have been extensively studied both theoretically and experimentally during last years [1-3]. In particular, it has been shown that they induce precessional motion of the magnetization [4-6] as well as steady-state phenomena [7-9] such as domain-wall motion [10-12] or vortex core reversal [13-15]. These effects have attracted great interest due to their potential applications in novel devices like microwave oscillators [16] , logic elements [17] , memories [18] . However, most studies were focused on macroscopic systems where the magnetization was uniform over large distances. Recently, there has been growing interest in studying these effects in nanostructures [19-21] since this allows one to explore new physical properties associated with reduced dimensions [22] .   In this work, we focus our attention on the magnetization dynamics driven out of equilibrium by an alternating spin polarized Current (ASPC). This problem has already been addressed theoretically [23] but only few experimental works have been reported so far [24] . Here, we present detailed measurements carried out on a magnetic tunnel junction (MTJ), made of CoFeB/MgO/CoFeB layers grown by sputtering [25] . By applying an external field Hext along the hard axis of the MTJ, we obtain a perpendicularly magnetized system whose static properties are described elsewhere [26] . When", "paraphrased_abstract": "He made the outer axis of the MTJ to be hardened with an external force Hext, and this produced a perpendicular magnetized structure. This system was studied, for the first time, both theoretically and experimentally, mainly in the form of nanostructures, because these systems allow one to explore new physical properties in dimensions that are reduced. In this study, we will present detailed measurements of a MTJ of CoFeB, with a high entropy, with a hard axis, which is magnetized by an external force Hext, whose static properties are described elsewhere. The main interest of these findings was the magnetization of an alternating spin polarized current (ASPC) in a microstructure, in which, in the case of a large amplitude, the alternating current produced two different, namely: a single frequency, which corresponds to the ferromagnetic resonance; b) if the amplitude of the ASCP increases, several frequencies, leading to a complex spectrum, analyzed with the Landau-Lifshitz-Gilbert equation and spin polarization terms. Recently, there has been an increasing interest in studying these phenomena in microstructures, in which the magnetization is uniform over large distances, as well as new physical properties, as well as new physical properties induced", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0764, "title": "Symmetries in Differential Geometry: A Computational Approach to Prolongations", "abstract": "  The aim of this work is to develop a systematic manner to close overdetermined systems arising from conformal Killing tensors (CKT). The research performs this action for 1-tensor and 2-tensors. This research makes it possible to develop a new general method for any rank of CKT. This method can also be applied to other types of Killing equations, as well as to overdetermined systems constrained by some other conditions.   The major methodological apparatus of the research is a decomposition of the section bundles where the covariant derivatives of the CKT land via generalized gradients. This decomposition generates a tree in which each row represents a higher derivative. After using the conformal Killing equation, just a few components (branches) survive, which means that most of them can be expressed in terms of lower order terms. This results in a finite number of independent jets. Thus, any higher covariant derivative can be written in terms of these jets.   The findings of this work are significant methodologically and, more specifically, in the potential for the discovery of symmetries. First, this work has uncovered a new method that could be used to close overdetermined systems arising from conformal Killing tensors (CKT). Second, through an application of this method, this research finds higher symmetry operators of first and second degree, which are known by other means, for the Laplace operator. The findings also reveal the first order symmetry operators for the Yamabe case. Moreover, the research leads to conjectures about the second order symmetries of the Yamabe operator. ", "machine_abstract": "The aim of this thesis is the study and development of computational methods for prolongation structures, which are used as tools in geometric analysis. The main focus lies on the construction of explicit formulas for the prolonged action of vector fields on tensor bundles over manifolds with symmetries. In particular we consider Lie groups acting by diffeomorphisms on Riemannian or pseudo-Riemannian manifolds. We present an algorithm that computes the prolonged action of a given vector field on any tensor bundle associated to such a manifold. This algorithm relies on the use of invariant bases adapted to the symmetry group at hand. As applications we compute the prolonged actions of some important examples like the Killing vector fields on homogeneous spaces and the conformal vector fields on conformally flat manifolds. Finally, we show how our results can be applied to construct new families of solutions to Einstein's equations. Keywords: Geometric Analysis, Manifold Symmetry Group", "paraphrased_abstract": "And we show how our methods can be used to solve Einstein's equations. The purpose of this thesis is to develop the methods of calculating the time required for the tensor fields to act on tensors on manifolds with symmetries. The main focus is on the computation of explicit formulas for the duration of vector fields on tensors on manifolds with symmetries. In particular, we examine the Lie groups which act in diffeomorphisms on Riemannian or pseudo-Riemannian manifolds. Then we present an algorithm for the computation of the time taken for a given vector field on a manifold associated with a manifold. It is based on the use of invariant bases adapted to the symmetry group at hand. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3649, "title": "Quantile and Probability Curves Without Crossing", "abstract": "  This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem. The method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve in finite samples than the original curve, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone econometric function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural quantile functions using data on Vietnam veteran status and earnings. ", "machine_abstract": "We present an algorithm for computing quantiles without crossing the probability curve, which is based on the concept of monotonicity preserving interpolation (MPI). The proposed method can be used to compute any number of quantiles in one pass over data with no extra storage overheads. We also show that our approach has better accuracy than existing methods when applied to real-world datasets. Finally, we demonstrate how this new technique can be used to efficiently solve several important problems such as finding the kth largest element or identifying outliers.  Keywords: Quantile, Interpolation, Outlier detection, Data mining, Computational complexity theory, Computational geometry, Statistics, Database systems, Data analysis, Machine learning, Algorithms, Computational biology, Computer science, Mathematics, Science and technology, Business intelligence, Big data, Data visualization, Data analytics, Data management, Data warehousing, Data streaming, Data compression", "paraphrased_abstract": "As a result, it was possible to solve various difficult problems such as finding the largest element of the population or identifying outliers. We present a new method for calculating the value of the number of particles that are not bound by the probability curve, based on the principle of monotonicity preserving interpolation (NPI). We present a method for computing the value of the entire population, without crossing the probability curve. We also present the fact that the proposed method is better than existing methods for calculating the number of particles, in that it is not necessary to add extra storage and storage. In addition, we show that our method is more precise than existing methods in calculating the size of the whole universe. We show that it is a very efficient method to solve some problems such as finding the largest element or detecting the outlier. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0518, "title": "Close Pairs as Proxies for Galaxy Cluster Mergers", "abstract": "  Galaxy cluster merger statistics are an important component in understanding the formation of large-scale structure. Unfortunately, it is difficult to study merger properties and evolution directly because the identification of cluster mergers in observations is problematic. We use large N-body simulations to study the statistical properties of massive halo mergers, specifically investigating the utility of close halo pairs as proxies for mergers. We examine the relationship between pairs and mergers for a wide range of merger timescales, halo masses, and redshifts (0<z<1). We also quantify the utility of pairs in measuring merger bias. While pairs at very small separations will reliably merge, these constitute a small fraction of the total merger population. Thus, pairs do not provide a reliable direct proxy to the total merger population. We do find an intriguing universality in the relation between close pairs and mergers, which in principle could allow for an estimate of the statistical merger rate from the pair fraction within a scaled separation, but including the effects of redshift space distortions strongly degrades this relation. We find similar behavior for galaxy-mass halos, making our results applicable to field galaxy mergers at high redshift. We investigate how the halo merger rate can be statistically described by the halo mass function via the merger kernel (coagulation), finding an interesting environmental dependence of merging: halos within the mass resolution of our simulations merge less efficiently in overdense environments. Specifically, halo pairs with separations less than a few Mpc/h are more likely to merge in underdense environments; at larger separations, pairs are more likely to merge in overdense environments. ", "machine_abstract": "We use the Millennium Simulation to study how close pairs of galaxies evolve into galaxy clusters, and we show that they are good proxies for mergers between dark matter halos.  We find that most (>80%) of the mass in galaxy clusters is assembled through mergers with other galaxy clusters or groups rather than by merging individual galaxies.   The merger rate increases strongly towards higher redshifts, but it decreases rapidly at lower masses because of dynamical friction.    Our results suggest that galaxy cluster formation can be studied using large samples of close galaxy pairs selected from optical surveys such as SDSS. This work was supported by NASA grant NAG5-12140. We thank J. S. Bullock, A. Kravtsov, D. Cohn, R. H. Richstone, B. Tinker, E. Tozzi, P. van Kampen, and W. Zhang for useful discussions.", "paraphrased_abstract": "We thank J. S. Bullock, A. Kravtsov, D. Cohn, R. H. Richstone, B. Tinker, E. Tozzi, P. van Kampen, and W. Zhang for their useful discussions. We will examine how close pairs of galaxies can grow into galaxy clusters and make them suitable for forming halos between dark matter. The data in this paper were supported by NASA grant NAG5-12140. We studied the formation of a cluster in the Millennium Simulation by studying the evolution of the star clusters, and we found that they are good proxies for mergers of dark matter halos. We observe that most of the mass in a cluster is formed by mergers of galaxies and groups, not by merging individual galaxies. The merger rate increases in the higher redshifts, but the merger rate declines rapidly with lower redshifts, because of friction. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.019, "title": "The Reliability on the Direction of the Incident Neutrino for the Fully Contained Events and Partially Contained Events due to QEL in the Super-Kamiokande", "abstract": "  In the SK analysis of the neutrino events for [Fully Contained Events] and [Partially Contained Events] on their zenith angle distribution, it is assumed that the zenith angle of the incident neutrino is the same as that of the detected charged lepton. In the present paper, we examine the validity of [the SK assumption on the direction] of the incident neutrinos. Concretely speaking, we analyze muon-like events due to QEL. For the purpose, we develop [Time Sequential Monte Carlo Simulation] to extract the conclusion on the validity of the SK assumption. In our [Time Sequential Simulation], we simulate every physical process concerned as exactly as possible without any approximation.   From the comparison between the zenith angle distributon of the emitted muons under [the SK assumption on the direction] and the corresponding one obtained under our [Time Sequential Simulation], it is concluded that the measurement of the direction of the incident neutrino for the neutrino events occurring inside the detector in the SK analysis turns out to be unreliable, which holds irrespective of the existence and/or non-existence of the neutrino oscillation. ", "machine_abstract": "The directional information is important to identify neutrinos coming from astrophysical sources, such as supernovae or gamma-ray bursts. In this study we investigate how reliable the incident neutrino directions are reconstructed by using fully contained events (FCEs) and partially contained events (PCEs). We find that the angular resolution of FCEs is better than that of PCEs because of their larger number of Cherenkov photons detected at the detector. The systematic uncertainty of the reconstruction efficiency depends strongly on the zenith angle of the event. For example, it becomes about 10% when the zenith angle is 60 degrees. This effect causes an error in the determination of the energy spectrum of atmospheric neutrinos. However, if one can reduce the systematic uncertainty down to 5% level, then the total flux of atmospheric neutrinos will be determined with accuracy within 20%.     Keywords: Neutrino, Super-Kamiokande", "paraphrased_abstract": "As we have discussed in this article, we examine how well the information of the incident neutrino is reconstructed by calculating it with the help of fully contained events and partially contained events. We find that the accuracy of the calculations is about half that of PCEs, which have more Cherenkov photons than PCEs. Therefore, in the case of a fully contained event, the resolution of the axes is greater than that of PCEs, because they contain a greater number of Cherenkov photons. The systematic uncertainty of the reconstruction process depends strongly on the angle of the event. For example, the angle of the event is about five degrees, and the total flux of atmospheric neutrinos is about ten times higher than that of the PCEs. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2082, "title": "Plasma effects in a micromachined floating-gate high-electron-mobility transistor", "abstract": "  We study plasma effects in a micromachined high-electron mobility transistor (HEMT) with the microcantilever serving as the gate using the developed a model. The model accounts for mechanical motion of the microcantilever and spatio-temporal variations (plasma effects) of the two-dimensional electron gas(2DEG) system in the transistor channel. The microcantilever mechanical motion is described in the point-mass approximation. The hydrodynamic electron transport model is used to describe distributed electron plasma phenomena in the 2DEG system. Using the developed model, we calculated the response function characterizing the amplitude microcantilever oscillations and the output electric signal as functions of the signal frequency and the bias voltage for the devices with different parameters. We find the voltage dependences of the frequency of the mechanical resonance and its damping. In particular, it is demonstrated that the amplitudes of the mechanical oscillations and output electric signal exhibit pronounced maxima at the bias voltages close to the voltage of the 2DEG channel depletion followed by a steep drop with further increase in the bias voltage. ", "machine_abstract": "We report on the fabrication and characterization of a novel floating gate (FG) HEMT with an AlGaN/GaN/AlN heterostructure grown by metal-organic chemical vapor deposition (MOCVD). The device is fabricated using standard photolithography techniques, followed by dry etching to define mesa structures. A thin layer of SiO2 is deposited as insulation between the source/drain contacts and FG. Electrical measurements show that this structure exhibits good electrical characteristics such as high drain-source current density (JDS), low threshold voltage (Vth), and small subthreshold swing (SS). In addition, we have observed a large negative shift in Vth after plasma treatment for 30 s at room temperature. This indicates that the charge trapping effect can be effectively controlled through plasma treatment.     Keywords: GaN-based transistors; Plasma effects; Floating gate devices; High electron mobility transistors", "paraphrased_abstract": "A thick layer of Si2 is deposited between the source and the drain. This enables the charge to be trapped. The device is built with GaN-based transistors, with a heterogeneous AlGaN-GaN-GaN heterostructure, which is formed by a process of chemical vapor deposition. We report here on the fabrication and characterization of a novel floating gate HEMT made with an AlGaN/GaN/AlN heterostructure grown by the use of the chemical vapor deposition method. The device is fabricated in the usual photolithographic method, and then we dry etch the mesa-shaped parts. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2346, "title": "Strings as sigma models and in the tensionless limit", "abstract": "  This thesis considers two different aspects of string theory, the tensionless limit of the string and supersymmetric sigma models. The tensionless limit is used to find a IIB supergravity background generated by a tensionless string. Quantization of the tensionless string in a pp-wave background is performed and the tensionless limit is found to commute with quantization. Further, the sigma model with N=(2,2) extended world-sheet supersymmetry is considered and the requirement on the target space to have a bi-Hermitean geometry is reviewed. It is shown that the equivalence between bi-Hermitean geometry and generalized Kahler follows, in this context, from the equivalence between the Lagrangian- and Hamiltonian formulation of the model. Moreover, the explicit T-duality transformation in the Hamiltonian formulation of the sigma model is constructed and shown to be a symplectomorphism. Under certain assumptions, the amount of extended supersymmetry present in the sigma model is shown to be preserved under T-duality. Further, by requiring N=(2,2) extended supersymmetry in a first order formulation of the sigma model an intriguing geometrical structure arises and in a special case generalized complex geometry is found to be contained in the new framework. ", "machine_abstract": "We consider strings propagating on curved backgrounds, with emphasis on their role as sigma-models. We show that the world-sheet action for such strings can be written in terms of an auxiliary field which is related to the extrinsic curvature of the target space. This allows us to study string propagation by solving the equations of motion for this auxiliary field instead of directly solving the equation of motion for the embedding coordinates. In particular we discuss how this approach simplifies calculations when studying strings in AdS spaces or near black holes. Finally, we use our formalism to investigate the behavior of strings at high energies where they become tensionless. We find that these strings are described by a conformal theory whose central charge vanishes. The corresponding Virasoro generators have negative norm states so that the Hilbert space has indefinite metric. These results provide further evidence that tensionless strings may play an important r\u00f4le in understanding quantum gravity.", "paraphrased_abstract": "We study the behavior of these strings at the high energies when they become distasteful. We find that the central charge vanishes, Virasoro generators are negative norm states, and Hilbert space has indefinite metric. This study shows that the behavior of strings at high energies, where they are disengaged, is governed by conformity theory, whose central charge is destroyed, and that the Virasoro generators have negative norm states, so that the Hilbert space has indefinite metric. Finally, we make use of our formalism to study strings at high energies, when they become unengaged. We show that the world sheet is a second field, connected to the extrusive curvature of the target space, and this allows us to account for the movement of the world sheet in terms of this auxiliary field, instead of solving the equation of motion for the embedding coordinates directly. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.149, "title": "Staggered Heavy Baryon Chiral Perturbation Theory", "abstract": "  Although taste violations significantly affect the results of staggered calculations of pseudoscalar and heavy-light mesonic quantities, those entering staggered calculations of baryonic quantities have not been quantified. Here I develop staggered chiral perturbation theory in the light-quark baryon sector by mapping the Symanzik action into heavy baryon chiral perturbation theory. For 2+1 dynamical quark flavors, the masses of flavor-symmetric nucleons are calculated to third order in partially quenched and fully dynamical staggered chiral perturbation theory. To this order the expansion includes the leading chiral logarithms, which come from loops with virtual decuplet-like states, as well as terms the order of the cubed pion mass, which come from loops with virtual octet-like states. Taste violations enter through the meson propagators in loops and tree-level terms the order of the squared lattice spacing. The pattern of taste symmetry breaking and the resulting degeneracies and mixings are discussed in detail. The resulting chiral forms are appropriate to lattice results obtained with operators already in use and could be used to study the restoration of taste symmetry in the continuum limit. I assume that the fourth root of the fermion determinant can be incorporated in staggered chiral perturbation theory using the replica method. ", "machine_abstract": "We present the results for the pion-nucleon scattering lengths in the framework of staggered heavy baryon chiral perturbation theory (HBChPT). We use lattice QCD data obtained with Nf = 2 + 1 flavors of dynamical quarks and an improved gauge action at three different values of the lattice spacing, corresponding to about 0.1 fm, 0.06 fm and 0.04 fm respectively. The light quark masses range between m\u03c0/\u039bQCD=0.2 and m\u03c0/\u039bQGP =0.4 where \u039bQCD is the scale parameter determined by the lattice regularization scheme used here. In order to extract physical observables we perform fits to the lattice data using HBChPT up to next-to-leading order (NLO) including finite volume effects as well as discretization errors. Our analysis shows that the inclusion of higher orders in the expansion leads to significant changes in the numerical value of the scattering lengths: while the leading-order result agrees within error bars with previous determinations based on continuum calculations, our final result differs significantly from these earlier estimates.", "paraphrased_abstract": "We use the QCD data of the pion-nucleon scattering in a stifled chiral perturbation theory. We are concerned with the physical observation of the scattering of pions. We compare our results with the results of the continuum calculations. We find that if we increase the order of the expansion by adding higher orders, the resulting numerical value is very different. It is observed that the optimum value of the radial weight is substantially lower than the original estimate of the radial weight. Consequently, the results of the radial weight of the pion and the nucleon scattering are different from the earlier estimates. In order to measure the scattering lengths, we have to obtain the following numerical results: while the radial weights agree with the previous results of continuum calculations, their resulting results are very different from those of the radial weights. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1975, "title": "Proper Motions in the Galactic Bulge: Plaut's Window", "abstract": "  A proper motion study of a field of 20' x 20' inside Plaut's low extinction window (l,b)=(0 deg,-8 deg), has been completed. Relative proper motions and photographic BV photometry have been derived for ~21,000 stars reaching to V~20.5 mag, based on the astrometric reduction of 43 photographic plates, spanning over 21 years of epoch difference. Proper motion errors are typically 1 mas/yr and field dependent systematics are below 0.2 mas/yr. Cross-referencing with the 2MASS catalog yielded a sample of ~8,700 stars, from which predominantly disk and bulge subsamples were selected photometrically from the JH color-magnitude diagram. The two samples exhibited different proper-motion distributions, with the disk displaying the expected reflex solar motion as a function of magnitude. Galactic rotation was also detected for stars between ~2 and ~3 kpc from us. The bulge sample, represented by red giants, has an intrinsic proper motion dispersion of (sigma_l,sigma_b)=(3.39, 2.91)+/-(0.11,0.09) mas/yr, which is in good agreement with previous results, and indicates a velocity anisotropy consistent with either rotational broadening or tri-axiality. A mean distance of 6.37^{+0.87}_{-0.77} kpc has been estimated for the bulge sample, based on the observed K magnitude of the horizontal branch red clump. The metallicity [M/H] distribution was also obtained for a subsample of 60 bulge giants stars, based on calibrated photometric indices. The observed [M/H] shows a peak value at [M/H]~-0.1 with an extended metal poor tail and around 30% of the stars with supersolar metallicity. No change in proper motion dispersion was observed as a function of [M/H]. We are currently in the process of obtaining CCD UBVRI photometry for the entire proper-motion sample of ~21,000 stars. ", "machine_abstract": "We present proper motions for stars with magnitudes between 8 and 16, obtained by combining data from two epochs of photographic plates taken at the Palomar Observatory (POSS-I) and one epoch of digital images taken with the Hubble Space Telescope (HST). The sample consists of about 1 million objects located within a region centered on the galactic center that is known as \"Plaut's window\".  We find that our results are consistent with previous measurements made using POSS-II plates combined with HST observations. However, we also find significant differences when compared to other recent studies based on similar datasets but different analysis techniques. These discrepancies may be due to systematic errors introduced during the reduction process or they could indicate real changes in the structure of the bulge over time. Our final catalogue will be available online through the CDS Vizier service. This work was supported by NASA grant NAG5-13523.", "paraphrased_abstract": "This work was supported by NASA Grant NAG-1303/03. Our results were in good agreement with previous studies of the POSS-II plates, which had been taken at the Palomar Observatory. The resulting dataset consisted of more than 1 million objects located within a region of the Galactic Center called the Plaut window. Our datasets were compared with previous measurements, and they showed remarkable differences when compared with similar experiments based on similar instruments but with different techniques. Our results compared with earlier studies using similar datasets and with the observations of HST conducted at the Palomar Observatory. Our data showed the correct motion of stars with magnitudes of 8 and 16, which we calculated from two eras of photographic plates taken by the Palomar Observatory and one era of digital images taken by the Hubble Space Telescope. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0035, "title": "Universality of Mixed Action Extrapolation Formulae", "abstract": "  Mixed action theories with chirally symmetric valence fermions exhibit very desirable features both at the level of the lattice calculations as well as in the construction and implementation of the low energy mixed action effective field theory. In this work we show that when such a mixed action effective field theory is projected onto the valence sector, both the Lagrangian and the extrapolation formulae become universal in form through next to leading order, for all variants of discretization methods used for the sea fermions. Our conclusion relies on the chiral nature of the valence quarks. The result implies that for all sea quark methods which are in the same universality class as QCD, the numerical values of the physical coefficients in the various mixed action chiral Lagrangians will be the same up to lattice spacing dependent corrections. This allows us to construct a prescription to determine the mixed action extrapolation formulae for a large class of hadronic correlation functions computed in partially quenched chiral perturbation theory at the one-loop level. For specific examples, we apply this prescription to the nucleon twist--2 matrix elements and the nucleon--nucleon system. In addition, we determine the mixed action extrapolation formula for the neutron EDM as this provides a nice example of a theta-dependent observable; these observables are exceptions to our prescription. ", "machine_abstract": "We present the results for mixed action extrapolations using Wilson and staggered fermions on N f = 2 + 1 flavors of gauge configurations generated by PACS-CS collaboration at $\\sqrt{s_{\\text{lat}}} \\sim 5$ GeV, which are available in public domain through the ILDG project.  We use two different types of lattice actions; one is the tree-level Symanzik improved gauge action with stout smeared links (Wilson type) while another is the Iwasaki gauge action with tadpole-improved L\u00fcscher-Weisz clover quark action (staggered type). The bare parameters used to generate these ensembles are listed in Table 1 . In this work we consider three different quantities as our observables; pion decay constant $F_\\pi$ , nucleon mass $M_N$ and chiral condensate $m_\\text{ch}$ . The numerical values obtained after applying mixed action extrapolation formulae are presented in Tables 2-4 together with their statistical errors estimated by jackknife method.  In order to check whether or not the mixed action extrapolation formula works well, we compare the results between mixed action and single action calculations. As can be seen from Figs. 1-3 , there seems no significant difference between them within the current accuracy.", "paraphrased_abstract": "In this paper, we consider three quantities of observables: pion decay constant F-pi, nucleon mass M-n and chiral condensate M-n. We compare these values with those of single-action calculations. Using Wilson and staggered fermions on N-f, we present the result of the joint PACS-CS-PACS collaboration at the level of 5 and the results of the tadpole-adapted L\u00fcscher-Weisz clover quark. We present here the results of the tadpole-adaptation and the deposition of the two types of ring-shaped tangles, which were obtained by the joint collaboration of PACS-CS and PACS-CS, which are in the public domain through the ILDG project. In order to test whether the mixed-action method works, we compared the results of the two methods with those of single-action. As you can see from Fig. 3, the difference in the accuracy of the calculations is not too great. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1648, "title": "Spectral Analysis of the Chandra Comet Survey", "abstract": "  We present results of the analysis of cometary X-ray spectra with an extended version of our charge exchange emission model (Bodewits et al. 2006). We have applied this model to the sample of 8 comets thus far observed with the Chandra X-ray observatory and ACIS spectrometer in the 300-1000 eV range. The surveyed comets are C/1999 S4 (LINEAR), C/1999 T1 (McNaught-Hartley), C/2000 WM1 (LINEAR), 153P/2002 (Ikeya-Zhang), 2P/2003 (Encke), C/2001 Q4 (NEAT), 9P/2005 (Tempel 1) and 73P/2006-B (Schwassmann-Wachmann 3) and the observations include a broad variety of comets, solar wind environments and observational conditions. The interaction model is based on state selective, velocity dependent charge exchange cross sections and is used to explore how cometary X-ray emission depend on cometary, observational and solar wind characteristics. It is further demonstrated that cometary X-ray spectra mainly reflect the state of the local solar wind. The current sample of Chandra observations was fit using the constrains of the charge exchange model, and relative solar wind abundances were derived from the X-ray spectra. Our analysis showed that spectral differences can be ascribed to different solar wind states, as such identifying comets interacting with (I) fast, cold wind, (II), slow, warm wind and (III) disturbed, fast, hot winds associated with interplanetary coronal mass ejections. We furthermore predict the existence of a fourth spectral class, associated with the cool, fast high latitude wind. ", "machine_abstract": "We present an analysis of the first year (1999) of data obtained by the Chandra X-ray Observatory's Advanced CCD Imaging Spectrometer (ACIS). The survey was designed to detect and study bright, soft X-ray emitting comets in our solar system. We have detected four new cometary sources with ACIS-S3 that are not previously known as optical or radio emitters. These objects were found using a novel technique for identifying point-like sources based on their spatial extent along the detector readout direction. This method is particularly useful at low count rates where traditional source detection algorithms fail due to statistical fluctuations. All four newly discovered comets show significant variability over time scales ranging from hours to days. Two of these comets also exhibit periodicities consistent with rotation periods near one day. In addition we report on two other faint comets which may be related to the recently discovered Jupiter family of asteroids.", "paraphrased_abstract": "It was the first year of a survey undertaken by the Chandra X-ray Observatory, with the objective of detecting and studying comets of the soft X-ray type. It was a survey of a large number of comets which was designed to discover, to our knowledge, a wide variety of bright and soft X-rays. It consisted of four comets, each of which was different from the others in its period of rotation. Two of these comets also displayed periodicities, consistent with rotations of more than a day. In addition, we also describe two other comets that were found by the asteroids of Jupiter. These comets were spotted in a manner different from that used by the Chandra X-ray Observatory. These camets were identified by means of a novel technique, which distinguished the source from the source in terms of the direction of the telescope. These camets were discovered using a new method of identifying point-like sources based on their relative position along the inclination of the instrument. This method of finding points is particularly useful at low Counting rates, when conventional methods are inefficient. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0146, "title": "Vortices in Bose-Einstein Condensates: Theory", "abstract": "  Vortices are pervasive in nature, representing the breakdown of laminar fluid flow and hence playing a key role in turbulence. The fluid rotation associated with a vortex can be parameterized by the circulation $\\Gamma=\\oint {\\rm d}{\\bf r}\\cdot{\\bf v}({\\bf r})$ about the vortex, where ${\\bf v}({\\bf r})$ is the fluid velocity field. While classical vortices can take any value of circulation, superfluids are irrotational, and any rotation or angular momentum is constrained to occur through vortices with quantized circulation. Quantized vortices also play a key role in the dissipation of transport in superfluids. In BECs quantized vortices have been observed in several forms, including single vortices, vortex lattices, and vortex pairs and rings. The recent observation of quantized vortices in a fermionic gas was taken as a clear signature of the underlying condensation and superfluidity of fermion pairs. In addition to BECs, quantized vortices also occur in superfluid Helium, nonlinear optics, and type-II superconductors. ", "machine_abstract": "We present an overview of the theory for vortices in trapped, dilute atomic gases at low temperatures. We discuss how these systems can be described by macroscopic wave functions and show that they are governed by nonlinear Schr\u00f6dinger equations with external potentials. The solutions to this equation have been studied extensively over many years and we review some of their properties relevant to vortex formation. In particular, we consider stationary states which correspond to condensate configurations without rotation (vortex-free) as well as rotating ones where quantized angular momentum is carried by phase singularities known as vortices. Finally, we briefly describe recent experiments on vortex production in cold atom clouds. Vortices occur naturally in superfluids such as liquid helium or dilute atomic gases. They carry quantized angular momenta and play important roles in various physical phenomena including turbulence and quantum transport processes. Here we give an introduction into the theoretical description of vortices in trapped atomic gases.", "paraphrased_abstract": "A vortex is a naturally occurring phenomenon in superfluids such as liquid helium or dilute atomic gases, and has an effect on a number of physical phenomena, including turbulence and quantum transport. We discuss the theory of vortexes in subterranean atoms at low temperatures. In this chapter, we present the theory of vortexes in subterranean atoms, a phenomenon that occurs naturally in superfluids, such as liquid helium or dilute atoms. These phenomena are governed by nonlinear Schr\u00f6dinger equations with external potentials. We present some of the most recent studies on the theory of vortexes in subterranean atoms. The solution of this equation is found to be a nonlinear Schr\u00f6dinger equation with external potentials. We then present the theory of vapor-swarming and vapor-swarming in subterranean gases. We also present a summary of the recent experiments on vapor-swarming in cold atom clouds. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1269, "title": "Phase Transitions in the Coloring of Random Graphs", "abstract": "  We consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. Using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   We show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. First, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. Afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. Another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. Eventually, above the coloring threshold, no more solutions are available. We compute all the critical connectivities for Erdos-Renyi and regular random graphs and determine their asymptotic values for large number of colors.   Finally, we discuss the algorithmic consequences of our findings. We argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. We also discuss the performance of a simple local Walk-COL algorithm and of the belief propagation algorithm in the light of our results. ", "machine_abstract": "We study phase transitions for coloring random graphs with bounded maximum degree using the first moment method and the second moment method.  We show that if the number of colors is at least $\\Omega(\\log n)$, then there exists an absolute constant $c > 0$ such that any graph on $n$ vertices with average degree $< c \\log n$ can be colored with probability $1 - O(1/n)$ by choosing each vertex independently to have one of its colors uniformly at random (Theorem 1).  This result improves upon previous results which required the number of colors to grow faster than linearly as a function of the number of vertices.  In addition, we prove that if the number of available colors is only $O(1)$, then there exist graphs on $n$ vertices with average degrees $< c \\log n$ that cannot be colored with high probability unless $P = NP$ (Theorem 2). Finally, we use our techniques to obtain new bounds on the chromatic thresholds of several families of random graphs including the Erd\u0151s-R\u00e9nyi model $G_{n, p}$, the configuration model, and the preferential attachment model.", "paraphrased_abstract": "In addition, we prove that if the number of colors is at least O(1), there are graphs on all vertices of average degree  c log n that are unable to be colored with high probability, unless P = NP (Theorem 2). Finally, we study phase transitions of the random graph, bounded in a limited number of degrees. In our experiments, we test the chromatic thresholds of several random graphs, including the Erds-R\u00e9nyi-Gan-Pn, the configuration model, the preferential-attachment model, and the preferential attachment model. Theorem 1 is a new model for the chromatic threshold of some of the families of random graphs, such as the Erds-R\u00e9nyi-Gan, pn, the configuration model, and the preferred-attachment model. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2852, "title": "Global polarization of QGP in non-central heavy ion collisions at high energies", "abstract": "  Due to the presence of a large orbital angular momentum of the parton system produced at the early stage of non-central heavy-ion collisions, quarks and anti-quarks are shown to be polarized in the direction opposite to the reaction plane which is determined by the impact-parameter and the beam momentum. The global quark polarization via elastic scattering was first calculated in an effective static potential model, then using QCD at finite temperature with the hard-thermal-loop re-summed gluon propagator. The measurable consequences are discussed. Global hyperon polarization from the hadronization of polarized quarks are predicted independent of the hadronization scenarios. It has also been shown that the global polarization of quarks and anti-quarks leads also to spin alignment of vector mesons. Dedicated measurements at RHIC are underway and some of the preliminary results are obtained. In this presentation, the basic idea and main results of global quark polarization are presented. The direct consequences such as global hyperon polarization and spin alignment are summarized. ", "machine_abstract": "We study the global polarization of quark gluon plasma (QGP) produced in non central heavy-ion collisions by using an effective chiral model with vector and axial-vector mesons as well as quarks and anti-quarks. We find that the global polarization is mainly determined by the initial angular momentum carried by the colliding nuclei, which can be estimated through the Glauber model. The magnitude of the global polarization decreases rapidly when the collision energy increases due to the increasing number of particles involved in the reaction. Our results show that the global polarization may reach about 10% for RHIC energies but it will decrease significantly if one goes up to LHC energies.     Introduction     In recent years there has been growing interest on studying the global polarization of quark-gluon plasma(QGP), especially its dependence on the collision energy [1\u20133] . It was found that the global polarization could reach about 20% for RHIC energies [4] , while it would drop down to less than 1% for LHC energies [5] .     This phenomenon is closely related to the initial angular momenta carried by the colliding nuclei; therefore, it provides us a new way to probe the nuclear structure [6] . On the other hand, since the global polarization is also sensitive to the temperature evolution [7, 8] , it might provide some information on the thermalization process of QGP [9] .", "paraphrased_abstract": "As a result, we have calculated the global polarization of quark gluon plasmas produced in the absence of central heavy ions, and are therefore concerned with the angular motion of the nuclei, which is measured by the Glauber equation. Among these angular motions, the global polarization is particularly affected by the collision energy of the nuclei, and it is thus a new method for investigating the structure of the nuclear structure. The present study, however, is directed to the study of the global polarization of quark gluon plasmas produced in the absence of central heavy ions, especially the effect of the collision energy on the angular motion of the nuclei, and the dependence of the collision energy on the collision energy. The results show that the global polarization of quarks is mainly influenced by the initial angular moment of the nuclei, which is computed by Glauber\u2019s theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4674, "title": "Evolution of complex modular biological networks", "abstract": "  Biological networks have evolved to be highly functional within uncertain environments while remaining extremely adaptable. One of the main contributors to the robustness and evolvability of biological networks is believed to be their modularity of function, with modules defined as sets of genes that are strongly interconnected but whose function is separable from those of other modules. Here, we investigate the in silico evolution of modularity and robustness in complex artificial metabolic networks that encode an increasing amount of information about their environment while acquiring ubiquitous features of biological, social, and engineering networks, such as scale-free edge distribution, small-world property, and fault-tolerance. These networks evolve in environments that differ in their predictability, and allow us to study modularity from topological, information-theoretic, and gene-epistatic points of view using new tools that do not depend on any preconceived notion of modularity. We find that for our evolved complex networks as well as for the yeast protein-protein interaction network, synthetic lethal pairs consist mostly of redundant genes that lie close to each other and therefore within modules, while knockdown suppressor pairs are farther apart and often straddle modules, suggesting that knockdown rescue is mediated by alternative pathways or modules. The combination of network modularity tools together with genetic interaction data constitutes a powerful approach to study and dissect the role of modularity in the evolution and function of biological networks. ", "machine_abstract": "We study the evolution of complex networks with multiple modules, where each module is an Erd\u0151s-R\u00e9nyi random graph and all nodes are connected to one another within their own module but not across different modules. We show that this model can be used to describe many real-world systems such as metabolic pathways in yeast cells or social interactions between individuals in animal groups. In particular we find that: (i) The number of links per node scales linearly with system size. (ii) The clustering coefficient decreases logarithmically with system size. (iii) The average path length increases logarithmically with system size. These results agree well with those observed for both metabolic networks and social networks. Finally, by using our evolutionary approach, we predict new functional relationships among genes in the yeast cell cycle pathway. Complex networks have been found to play important roles in various fields ranging from physics [1] , biology [2] , sociology [3] , computer science [4] , etc.. Many real world networks exhibit common statistical properties including power-law degree distribution [5] , small diameter [6] , high clustering coefficients [7, 8] . However, it remains unclear how these networks evolve over time [9] . In recent years there has been growing interest in studying the evolution of complex networks [10 -12] . For example, Barab\u00e1si-Albert proposed a simple growth mechanism which leads to scale-free networks [13] . Dorogovtsev et al studied the evolution of hierarchical networks [14] . Caldarelli et al investigated the evolution of clustered networks [15] . Newman introduced a fitness-based model [16] . This model was further developed into a more realistic version [17] . Recently, Jeong et al showed that some metabolic networks share similar topological features [18] . They also suggested that the underlying mechanisms responsible for generating these networks may be related to natural selection [19] .", "paraphrased_abstract": "We analyzed the evolution of complex networks with a number of modules, each of which has an Erds-R\u00e9nyi random graph, and which is connected with all nodes of the same module but does not connect with different modules. We show that our model can be used in many real world systems, such as the metabolic pathways in yeast or the social relationships between members of the same animal group. The results are in agreement with those of metabolic networks and of social networks. In recent years, researchers have been busy studying complex networks. They have discovered some common statistical features, such as a power-law degree distribution, a small radius, a high clustering coefficient, and so on. Many complex networks have been studied, for example, in physics, biology, sociology, computer science, and so on. We show that this model can be applied to many real-world systems, such as metabolic networks in yeast cells, social networks in animals, and so on. In our study, we show that the number of links per node scales linearly with the size of the system, and the clustering coefficient decreases linearly with the size of the system, and the average path length increases linearly with the size of the system, and finally, by means of the evolutionary approach, we predict new functional relationships among genes of the yeast cell. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1225, "title": "Enrichment of the ISM by metal-rich droplets and the abundance bias in HII regions", "abstract": "  We critically examine a scenario for the enrichment of the interstellar medium (ISM) in which supernova ejecta follow a long (10^8 yr) journey before falling back onto the galactic disk in the form of metal-rich ``droplets'', These droplets do not become fully mixed with the interstellar medium until they become photoionized in HII regions. We investigate the hypothesis that the photoionization of these highly metallic droplets can explain the observed ``abundance discrepancy factors'' (ADFs), which are found when comparing abundances derived from recombination lines and from collisionally excited lines, both in Galactic and extragalactic HII regions. We derive bounds of 10^{13}--10^{15} cm on the droplet sizes inside HII regions in order that (1) they should not have already been detected by direct imaging of nearby nebulae, and (2) they should not be too swiftly destroyed by diffusion in the ionized gas. From photoionization modelling we find that, if this inhomogeneous enrichment scenario holds, then the recombination lines strongly overestimate the metallicities of the fully mixed HII regions. The abundances derived from collisionally excited lines also suffer some bias, although to a much lesser extent. In the absence of any recipe for correcting these biases, we recommend the discarding of all objects showing large ADFs from studies of galactic chemical evolution. These biases must also be kept in mind when comparing the galactic abundance gradients for elements derived from recombination lines with those derived from collisionally excited lines. Finally, we propose a set of observations that could be undertaken to test our scenario and improve our understanding of element mixing in the ISM. ", "machine_abstract": "We present new results on the chemical composition of interstellar matter (ISM) based on high-resolution spectroscopy of two bright Galactic planetary nebulae, NGC 6153 and IC 4593. We find that these objects are characterized by an overabundance of oxygen relative to iron compared with solar values. This is consistent with previous studies of other PNe but our data show for the first time that this effect persists even when we consider only those ions which are not affected by photoionization effects.  The observed O/Fe ratio can be explained if most of the Fe atoms were locked up into dust grains prior to their destruction during the PN phase. In addition, we detect significant amounts of sulfur in both targets indicating that at least some fraction of S was also incorporated into dust grains before being released back into the gas phase after grain destruction. Finally, we argue that the presence of such large quantities of metals in the ISM may lead to systematic errors in determinations of elemental abundances using emission-line ratios measured in H II regions.", "paraphrased_abstract": "The study revealed the presence of the metals in the ISM, in particular in the presence of atoms of the Fe and the Iron, in which it is found that the Fe atoms were incorporated in the dust of the PN, and therefore, that, after the PN phase, they were released back into the gas phase. Moreover, the presence of the sulfur in the two targets reveals that, when the S was destroyed, it dissolved into the dust and remained there in the gas phase, after the dissolution of the grains. Moreover, in the presence of the O/Fe ratio, we also observe that most of the Fe atoms were dissolved in the dust before the PN phase, which we expect is also true, as it is a consequence of the presence of sulfur in the dust, and that the sulfur in the grain was also incorporated into the dust, and that, when the grains were destroyed, the S was released into the gas phase. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0082, "title": "Matter-Wave Bright Solitons with a Finite Background in Spinor Bose-Einstein Condensates", "abstract": "  We investigate dynamical properties of bright solitons with a finite background in the F=1 spinor Bose-Einstein condensate (BEC), based on an integrable spinor model which is equivalent to the matrix nonlinear Schr\\\"{o}dinger equation with a self-focusing nonlineality. We apply the inverse scattering method formulated for nonvanishing boundary conditions. The resulting soliton solutions can be regarded as a generalization of those under vanishing boundary conditions. One-soliton solutions are derived in an explicit manner. According to the behaviors at the infinity, they are classified into two kinds, domain-wall (DW) type and phase-shift (PS) type. The DW-type implies the ferromagnetic state with nonzero total spin and the PS-type implies the polar state, where the total spin amounts to zero. We also discuss two-soliton collisions. In particular, the spin-mixing phenomenon is confirmed in a collision involving the DW-type. The results are consistent with those of the previous studies for bright solitons under vanishing boundary conditions and dark solitons. As a result, we establish the robustness and the usefulness of the multiple matter-wave solitons in the spinor BECs. ", "machine_abstract": "We study the formation and dynamics of matter-wave bright solitons in spin-1 condensates, which are described by the Gross-Pitaevskii equation for the macroscopic wave function $\\psi$ coupled to an additional nonlinear Schr\u00f6dinger equation for the magnetization vector $\\mathbf{m}$. We show that stable soliton solutions exist only if the background density is finite. In this case we find two types of stationary states corresponding to different orientations of the magnetization vector. The stability analysis reveals that one type of solution is always unstable while the other can be either stable or unstable depending on the values of the system parameters. Finally, we present numerical results showing how these solutions evolve dynamically when perturbed slightly away from their equilibrium positions. DOI: 10.1063/1.3189000 I. INTRODUCTORY REMARkS The recent experimental realization of Bose-Einstein condensation (BEC) in dilute atomic gases has opened up new possibilities for studying quantum phenomena at the macroscopic level [1] . One of the most interesting features observed experimentally is the existence of solitary waves known as \"bright solitons\" [2] , which have been predicted theoretically [3] . In this work we consider a model describing spin-1 bosons trapped inside a magnetic trap [4] . This problem was first studied numerically in Ref. [5] where it was shown that there exists a family of localized solutions whose properties depend strongly on the orientation of the external magnetic field. It turns out that some of them correspond to stable configurations [6] . However, all previous studies were performed within the mean-field approximation neglecting fluctuations around the ground state [7, 8] . Here we perform a detailed investigation of the full many-body problem using both analytical and numerical methods.", "paraphrased_abstract": "The most interesting feature observed in the experiment is the existence of a single wave known as bright solitons, whose scalability has been predicted theoretically... But all the experiments conducted so far have been performed in the flat-bottomed approximation, neglecting fluctuations in the ground state. This problem has been studied in the first place in Ref., where it has been shown that the properties of a certain number of localized solutions depend strongly on the direction of the external magnetic field, and of which some are stable. In the second place, we study the formation and dynamics of light solitons in spin-1 sulfates, as described by Gross-Pitaevskii's equation for the macroscopic wave function, 'psi', and by an additional Schr\u00f6dinger equation for the magnetic vector'mathbf'm'. The first result is that the existence of a single, solitary wave, called 'bright', has been predicted theoretically. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0511, "title": "Vibration-Induced Conductivity Fluctuation Measurement for Soil Bulk Density Analysis", "abstract": "  Soil bulk density affects water storage, water and nutrient movement, and plant root activity in the soil profile. Its measurement is difficult in field conditions. Vibration-induced conductivity fluctuation was investigated to quantify soil bulk density with possible field applications in the future. The AC electrical conductivity of soil was measured using a pair of blade-like electrodes while exposing the soil to periodic vibration. The blades were positioned longitudinally and transversally to the direction of the induced vibration to enable the calculation of a normalized index. The normalized index was expected to provide data independent from the vibration strength and to reduce the effect of soil salinity and water content. The experiment was conducted on natural and salinized fine sand at two moisture conditions and four bulk densities. The blade-shaped electrodes improved electrode-soil contact compared to cylindrical electrodes, and thereby, reduced measurement noise. Simulations on a simplified resistor lattice indicate that the transversal effect increases as soil bulk density decreases. Measurement of dry sand showed a negative correlation between the normalized conductivity fluctuation and soil bulk density for both longitudinal and transversal settings. The decrease in the transversal signal was smaller than expected. The wet natural and salinized soils performed very similarly as hypothesized, but their normalized VICOF response was not significant to bulk density changes. ", "machine_abstract": "The soil bulk density is an important parameter in the study of soil properties and agricultural production, which can be measured by using vibration-induced conductivity fluctuation (VICF) method. In this work, we propose to use VICF measurement system with two parallel plates as excitation source to measure the soil bulk density. The experimental results show that the proposed method has high sensitivity and accuracy compared with traditional methods such as gamma ray attenuation technique. This research provides new insights into the application of VICF technology on measuring soil bulk density.     Keywords: Vibration-induced conductivity fluctuation; Soil bulk density; Parallel-plate excitation source; Gamma-ray attenuation technique         1 Introduction     Soil bulk density is one of the most basic physical parameters used to describe soil characteristics [1] . It plays an important role in studying soil properties [2] , analyzing crop growth [3] , and optimizing fertilizer management [4] . Currently, there are many techniques available for measuring soil bulk density [5] - [8] . Among them, the gamma-ray attenuation technique [9] is widely applied because it is non-destructive and easy-to-use [10] . However, due to its low spatial resolution [11] , the gamma-ray attenuation method cannot accurately determine the distribution of soil bulk density [12] . To overcome these shortcomings, some researchers have developed other technologies including electrical resistivity tomography [13] , ground penetrating radar [14] , neutron radiography [15] , and X-ray computed tomography [16] . These technologies provide higher spatial resolutions than the gamma-ray attenuation approach [17] but they require expensive equipment or complicated procedures [18] .     Recently, vibration-induced conductivity fluctuation(VICF), also known as electromechanical impedance spectroscopy [19] , has been introduced to detect changes in soil moisture [20] , soil salinity [21] , and soil texture [22] . Compared with conventional methods, VICF measurement does not need any special instruments [23] . Moreover, it only requires simple sample preparation [24] . Therefore, VICF measurement may become a promising tool for detecting soil properties [25] .  In recent years, several studies have investigated the relationship between soil bulk density and VICF [26] - [28]", "paraphrased_abstract": "In recent years, however, there has been a great deal of research on the relationship between soil-density and vibration-induced conductivity. Various methods are being used to measure the density of soil, to detect the moisture content, the salinity of soil, and the texture of the soil. These methods have been introduced to improve the accuracy of the measurement, but they are expensive, and there are complicated procedures. Therefore, it may be a promising method to study the soil properties. To overcome this problem, several methods have been introduced, including acoustic thermography, acoustic spectroscopy, the ground-particle radar, the neutron imaging, the X-ray computed tomography. Besides, these methods require no special equipment or complicated procedures. The solubility of soil is one of the most basic physical characteristics of a soil, and it is necessary to study the soil properties, to calculate the fertilizer, to determine the fertility of the soil. The Solubility of soil is a vital parameter in the study of soil properties and agriculture, and can be measured by using VICF method. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1178, "title": "Refined asymptotics around solitons for gKdV equations", "abstract": "  We consider the generalized Korteweg-de Vries equation $$ \\partial_t u + \\partial_x (\\partial_x^2 u + f(u))=0, \\quad (t,x)\\in [0,T)\\times \\mathbb{R}$$ with general $C^2$ nonlinearity $f$. Under an explicit condition on $f$ and $c>0$, there exists a solution in the energy space $H^1$ of the type $u(t,x)=Q_c(x-x_0-ct)$, called soliton. Stability theory for $Q_c$ is well-known.   In previous works, we have proved that for $f(u)=u^p$, $p=2,3,4$, the family of solitons is asymptotically stable in some local sense in $H^1$, i.e. if $u(t)$ is close to $Q_{c}$ (for all $t\\geq 0$), then $u(t,.+\\rho(t))$ locally converges in the energy space to some $Q_{c_+}$ as $t\\to +\\infty$, for some $c^+\\sim c$.   Then, the asymptotic stability result could be extended to the case of general assumptions on $f$ and $Q_c$.   The objective of this paper is twofold.   The main objective is to prove that in the case $f(u)=u^p$, $p=2,3,4$, $\\rho(t)-c_+ t$ has limit as $t\\to +\\infty$ under the additional assumption $x_+ u\\in L^2$.   The second objective of this paper is to provide large time stability and asymptotic stability results for two soliton solutions for the case of general nonlinearity $f(u)$, when the ratio of the speeds of the solitons is small. The motivation is to accompany forthcoming works devoted to the collision of two solitons in the nonintegrable case. The arguments are refinements of previous works specialized to the case $u(t)\\sim Q_{c_1}+Q_{c_2}$, for $0< c_2 \\ll c_1$. ", "machine_abstract": "We consider the generalized Korteweg-de Vries (gKdV) equation with periodic boundary conditions and initial data in Sobolev spaces, which are close to solitary waves. We prove that solutions exist globally in time and converge exponentially fast towards the corresponding solitary wave as t goes to infinity.  The main idea is to use an appropriate Lyapunov functional together with suitable energy estimates. The proof relies on refined asymptotic expansions near solitary waves developed by Martel & Merle [J. Math. Pures Appl. (9), vol. 84, no. 1, pp. 1-33, 2004] .  Introduction  In this work we study the following nonlinear evolution equation:         u_t + 6uu_x - u_{xxx} = 0 , x \\in [0,1] , t > 0 ,  u(x,0) = u_0(x) , x \\in [0,1] .   Here u : [0,1] \\times [0,\\infty) \\to \\mathbb{R}$ denotes the unknown function. This problem arises naturally when studying water waves over a flat bottom under the influence of gravity. In fact, if one considers the flow of shallow water at rest above a horizontal bottom, then it can be shown that the free surface elevation satisfies the so-called Benney-Luke system, see e.g.    [1] or [2] . If one neglects higher order terms arising from the dispersion relation, then one obtains the classical Korteweg-de Vires (KdV) equation. For more details about the derivation of the model considered here, we refer to [3] .", "paraphrased_abstract": "But the problem, for example, arises naturally from the study of water waves on a flat bottom under the influence of gravity. This is called the \u201cBenney-Luke system\u201d: see, for example, 1 and 2, the so-called Benney-Luke system, as enumerated in the table below. The equivalences are, for instance, denoted by the following: ut - 6uu x - u___xxx 0 - u[0], u[0] - u[0] - u[0] - u[0] - u[0] - x - u[0] - x - u[0] - x - x - a[0] = u[0] - x - [0] - to [0][0], [6] [7] [8] [9] - in which the basic conditions are given and the corresponding boundary conditions are given in Sobolev spaces close to solitary waves. This solution is based on a generalized KdV equation, with periodic boundary conditions and initial data in Sobolev spaces", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1151, "title": "Old Main-Sequence Turnoff Photometry in the Small Magellanic Cloud. I. Constraints on the Star Formation History in Different Fields", "abstract": "  We present ground-based B and R-band color-magnitude diagrams (CMDs), reaching the oldest main-sequence turnoffs with good photometric accuracy for twelve fields in the Small Magellanic Cloud (SMC). Our fields, located between ~1 and ~4 degrees from the center of the galaxy, are situated in different parts of the SMC such as the \"Wing'' area, and towards the West and South. In this paper we perform a first analysis of the stellar content in our SMC fields through comparison with theoretical isochrones and color functions (CFs). We find that the underlying spheroidally distributed population is composed of both intermediate-age and old stars and that its age composition does not show strong galacto-centric gradients. The three fields situated toward the east, in the Wing region, show very active current star formation. However, only in the eastern field closest to the center do we find an enhancement of recent star formation with respect to a constant SFR(t). The fields corresponding to the western side of the SMC present a much less populated young MS, and the CF analysis indicates that the SFR(t) greatly diminished around 2 Gyr ago in these parts. Field smc0057, the closest to the center of the galaxy and located in the southern part, shows recent star formation, while the rest of the southern fields present few bright MS stars. The structure of the red clump in all the CMDs is consistent with the large amount of intermediate-age stars inferred from the CMDs and color functions. None of the SMC fields presented here are dominated by old stellar populations, a fact that is in agreement with the lack of a conspicuous horizontal branch in all these SMC CMDs. This could indicate that a disk population is ruling over a possible old halo in all the observed fields. ", "machine_abstract": "We present new photometric data for stars near the main-sequence turn-off (MSTO) in three fields in the Small Magellanic: Cloud, obtained with the Wide Field Imager at the MPG/ESO 2.2 m telescope. The observations were carried out during two runs in August and September 2002. We have used these data to derive ages and metallicities for about 1000 MSTO stars using theoretical isochrones. Our results show that there are significant differences between the age distributions derived for different fields. In particular we find evidence for an extended period of star formation in field 1, which contains the open cluster NGC 1818. This result supports previous suggestions based on studies of red giant branch stars.     Keywords: Age distribution; Open clusters; Small Magellanic Cloud; Stellar population synthesis; Wide-field imaging. Subject headings: Galaxy: Magellanic Clouds", "paraphrased_abstract": "The title is: The Galaxy and the Magellanic Cloud. The data are presented in two separate runs in August and September 2002, and in two other runs in September and October. We present new data for stars close to the main-sequence, the MSTO, in three fields in the Small Magellanic Cloud, gathered with the Wide Field Imager of the 2.2 m telescope of the ESSO. We based our analysis on the data from two different runs in August and September 2002. The results of this analysis confirm the results of previous studies of the red giant branch stars. The new data show that there are substantial differences in the age distributions of the three fields of the Cloud. In the first field, which contains the open cluster NGC 1818, the occurrence of the star-forming, and the elongation of the star, is well supported by the previous observation of the red giant branch stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0226, "title": "Correlated modulation between the redshifted Fe K alpha line and the continuum emission in NGC 3783", "abstract": "  It has been suggested that X-ray observations of rapidly variable Seyfert galaxies may hold the key to probe the gas orbital motions in the innermost regions of accretion discs around black holes and, thus, trace flow patterns under the effect of the hole strong gravitational field. We explore this possibility analizing XMM-Newton observations of the seyfert 1 galaxy NGC 3783. A detiled time-resolved spectral analysis is performed down to the shortest possible time-scales (few ks) using \"excess maps\" and cross-correlating light curves in different energy bands. In addition to a constant core of the Fe K alpha line, we detected a variable and redshifted Fe K alpha emission feature between 5.3-6.1 keV. The line exhibits a modulation on a time-scale of 27 ks that is similar to and in phase with a modulation of the 0.3-10 keV source continuum. The time-scale of the correlated variability of the redshifted Fe line and continuum agrees with the local dynamical time-scale of the accretion disc at 10 r_g around a black hole of 10^7 M_sun. Given the shape of the redshfted line emission and the overall X-ray variability pattern, the line is likely to arise from the relativistic region near the black hole. ", "machine_abstract": "We report on an observation made with Suzaku satellite to study X-ray variability of active galactic nucleus (AGN) NGC 3783, which is known as one of brightest Seyfert 1 galaxies at soft X-rays. We found that the fluxes of both the iron K\u03b1 line and the continuum are modulated by a factor of about two over a period of 100 ks. The phase lag between these two components was estimated to be ~0.1 s using cross-correlation analysis. This result suggests that there exists some physical connection between them. In addition, we detected significant time lags for higher-order harmonics of the fundamental frequency component up to the third harmonic. These results suggest that the observed variability may originate from reverberation effects caused by variable illumination of the accretion disk around supermassive black hole. Keywords: Reverberation; Time lag; Iron line; AGN", "paraphrased_abstract": "Then, as we examine X-ray fluxes of the AGN, NGC 3783, which is known as one of the brightest in the Seyfert 1 class of galaxies, was observed to be in a manner similar to that described by the Suzaku satellite in the case of NGC 3783, which is one of the brightest in the Seyfert 1 class. We have been observing the reverberation and the reverberation of the accretion disk around the supermassive black hole. We have observed that the magnetic field between the equilinear and equilinear peaks is about two tenths of a tenth of a sigma, and that the sigma is only a fraction of a sigma. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1573, "title": "Thermal derivation of the Coleman-De Luccia tunneling prescription", "abstract": "  We derive the rate for transitions between de Sitter vacua by treating the field theory on the static patch as a thermal system. This reproduces the Coleman-De Luccia formalism for calculating the rate, but leads to a modified interpretation of the bounce solution and a different prediction for the evolution of the system after tunneling. The bounce is seen to correspond to a sequence of configurations interpolating between initial and final configurations on either side of the tunneling barrier, all of which are restricted to the static patch. The final configuration, which gives the initial data on the static patch for evolution after tunneling, is obtained from one half of a slice through the center of the bounce, while the other half gives the configuration before tunneling. The formalism makes no statement about the fields beyond the horizon.   This approach resolves several puzzling aspects and interpretational issues concerning the Coleman-De Luccia and Hawking-Moss bounces. We work in the limit where the back reaction of matter on metric can be ignored, but argue that the qualitative aspects remain in the more general case. The extension to tunneling between anti-de Sitter vacua is discussed. ", "machine_abstract": "We present an alternative thermal derivation of the Coleman-DeLuccia tunneling prescription for black hole formation in de Sitter space-time, which is based on the Euclidean path integral approach to quantum gravity and uses only standard techniques of statistical mechanics. We show that this method reproduces exactly the same results as those obtained by Hawking's original calculation using semi-classical methods. The advantage of our new approach lies in its conceptual simplicity and generality - it can be applied not only to Schwarzschild-deSitter but also to Kerr-deSitter solutions with arbitrary angular momentum. In addition we discuss possible extensions of these ideas beyond the semiclassical approximation. PACS: 04.20.-q; 98.80.Cq Keywords: Black holes, Quantum gravity, Tunneling, Thermalization, Path integrals, Statistical mechanics, De Sitter space time. 1 Introduction Black holes are one of the most fascinating predictions of Einstein's theory of gravitation [1] . They have been studied extensively over many years both theoretically [2] , numerically [3] and experimentally [4] . The main difficulty in studying black holes comes from their singular nature at the horizon. This problem has been overcome recently by Hawking [5] who showed how black holes evaporate due to quantum effects. He found that black holes emit particles according to a Planck spectrum [6]  where T H =h/8\u03c0M P l c 2 is the temperature of the black hole (with M being its mass) and \u03ba = 1/4G N is the surface gravity of the event horizon. It was shown later [7] that the emission rate \u0393 \u221d exp(\u22128\u03c0M/M P l ) agrees very well with numerical calculations [8] . However, despite all efforts made so far there still remains some controversy about whether or not black holes really exist [9] .", "paraphrased_abstract": "The simplest difficulty in studying black holes is their singular nature at the horizon. Recent work by Hawking has revealed that black holes emit particles according to the Planck spectrum, where the temperature of the black hole is h/m/m P lc 2 where the mass of the black hole is M, and the tangent  is the surface gravity of the event horizon. In addition, we discuss some possibilities for extending this hypothesis beyond the semi-classical calculations. This new approach of mine is based on the Euclidean Path Integral Method to the Graveline Gravity, based on the Euclidean Path Integral Method, which consists of a thermodynamical approach, based on the Euclidean Path Integral Method, which combines the basic techniques of statistical mechanics and a purely arithmetic approach to the question of black holes. Moreover, we discuss the extensions of this method beyond the semi-classical method. Black holes are one of the most fascinating predictions of the theory of gravity. In the recent years, Hawking showed the evaporation of black holes due to quantum effects. He showed how they emitted particles according to a Planck spectrum, where the temperature of the black hole is 8 M/M P l c 2 (as the mass of the black hole) and = ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1274, "title": "Parametric Learning and Monte Carlo Optimization", "abstract": "  This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and `blackbox' or `oracle'-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO. ", "machine_abstract": "We present an algorithm for learning the parameters of a probabilistic model by optimizing its log-likelihood using Monte Carlo (MC) sampling, which we call Parametric Learning with MC Sampling or PLMS. We show that this approach is equivalent to maximum likelihood estimation when the number of samples goes to infinity. In practice however, it can be much more efficient than standard gradient-based optimization methods because it does not require computing gradients over all data points at each iteration. Instead, only one sample per iteration needs to be generated in order to compute the objective function value and update the parameter values accordingly. This makes our method particularly suitable for large datasets where evaluating the objective function may take too much time. Our experiments on synthetic and real-world datasets demonstrate that PLMS outperforms state-of-the-art algorithms such as stochastic gradient descent and Adam in terms of both accuracy and efficiency. Code available at: https://github.com/sunyuan-chen/PLMS", "paraphrased_abstract": "The results show that PLMS is far more accurate than the traditional gradient-based optimizations, such as Adam's and stochastic gradient descent, but it also uses a single sampling. The algorithm is also useful for large data, since it does not need to compute the gradients at every iteration, so it can calculate the objective function in a single hexagram, and it is a very efficient method for learning the parameters of a model, but it does not require any gradients, and it only needs one sample to find the objective function, and it updates the parameters accordingly. In our simulation, we demonstrate that the model has reached the maximum likelihood, and we can use it to find the corresponding log-likelihood. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4109, "title": "Dynamical models with a general anisotropy profile", "abstract": "  Both numerical simulations and observational evidence indicate that the outer regions of galaxies and dark matter haloes are typically mildly to significantly radially anisotropic. The inner regions can be significantly non-isotropic, depending on the dynamical formation and evolution processes. In an attempt to break the lack of simple dynamical models that can reproduce this behaviour, we explore a technique to construct dynamical models with an arbitrary density and an arbitrary anisotropy profile. We outline a general construction method and propose a more practical approach based on a parameterized anisotropy profile. This approach consists of fitting the density of the model with a set of dynamical components, each of which have the same anisotropy profile. Using this approach we avoid the delicate fine-tuning difficulties other fitting techniques typically encounter when constructing radially anisotropic models. We present a model anisotropy profile that generalizes the Osipkov-Merritt profile, and that can represent any smooth monotonic anisotropy profile. Based on this model anisotropy profile, we construct a very general seven-parameter set of dynamical components for which the most important dynamical properties can be calculated analytically. We use the results to look for simple one-component dynamical models that generate simple potential-density pairs while still supporting a flexible anisotropy profile. We present families of Plummer and Hernquist models in which the anisotropy at small and large radii can be chosen as free parameters. We also generalize these two families to a three-parameter family that self-consistently generates the set of Veltmann potential-density pairs. (Abridged...) ", "machine_abstract": "We present dynamical models for the Milky Way halo that include an arbitrary number of spherical shells, each characterized by its own density and velocity dispersion profiles. The model is based on Jeans' theorem applied to the phase-space distribution function (DF) in order to obtain the DF as well as the projected surface mass density along any line-of-sight. We show how this approach can be used to fit observational data such as those obtained from kinematic tracers or gravitational lensing measurements. In particular we apply our method to two different sets of observations: 1) A sample of RR Lyrae stars observed towards the Galactic bulge; 2) Gravitational lensing measurements towards the Bullet Cluster. Our results are compared against previous works using similar techniques but assuming either constant anisotropies across all radii or simple power-law radial dependences. We find that these assumptions lead to significant biases when fitting the data.", "paraphrased_abstract": "There is a problem of the spherical density of the halo and a number of spheres which impose their own density and velocity. We based our methods on two different observations: the RR Lyrae star which is observed towards the Galactic bulge; the gyroscopic light of the Bullet Cluster. This is in particular applied to two different observations: 1) A sample of RR Lyrae stars, observed towards the Galactic bulge; 2) Gravitational rays measurements of the Bullet Cluster. We use this method to obtain the density of the surface along all the lines of view, and the mass density of the surface at all points. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1869, "title": "The Magnetic Field of the Solar Corona from Pulsar Observations", "abstract": "  We present a novel experiment with the capacity to independently measure both the electron density and the magnetic field of the solar corona. We achieve this through measurement of the excess Faraday rotation due to propagation of the polarised emission from a number of pulsars through the magnetic field of the solar corona. This method yields independent measures of the integrated electron density, via dispersion of the pulsed signal and the magnetic field, via the amount of Faraday rotation. In principle this allows the determination of the integrated magnetic field through the solar corona along many lines of sight without any assumptions regarding the electron density distribution. We present a detection of an increase in the rotation measure of the pulsar J1801$-$2304 of approximately 160 \\rad at an elongation of 0.95$^\\circ$ from the centre of the solar disk. This corresponds to a lower limit of the magnetic field strength along this line of sight of $> 393\\mu\\mathrm{G}$. The lack of precision in the integrated electron density measurement restricts this result to a limit, but application of coronal plasma models can further constrain this to approximately 20mG, along a path passing 2.5 solar radii from the solar limb. Which is consistent with predictions obtained using extensions to the Source Surface models published by Wilcox Solar Observatory ", "machine_abstract": "We present an analysis of pulsar observations to determine the magnetic field strength in the solar corona at heights between 1 and 3 R_Sun . We use data obtained with the Nan\u00e7ay Radio Telescope (NRT) for two different radio frequencies, 327 MHz and 1420 MHz, corresponding to emission heights of about 2 and 5 R_Sun , respectively. The observed pulse profiles are modeled using a simple model that includes contributions from both the local interstellar medium and the solar wind plasma. From these models we derive estimates for the coronal magnetic field strengths as well as the electron density distribution along the line-of-sight towards PSR B1133+16 . The results show that the magnetic field decreases rapidly with height above the photosphere but is still strong enough to confine energetic particles up to several solar radii away from the Sun's surface. This suggests that particle acceleration processes may be taking place throughout most of the solar atmosphere.", "paraphrased_abstract": "The measured signals were obtained from Nan\u00e7ay's radio telescope at two different frequencies, 327 MHz and 1420 MHz, each of which attained about 2 and 5 radii above the sun, respectively. The obtained signals, which are analyzed in a simple model, are derived from the local interstellar medium and the radiated plasma of the sun. The results show that the magnetic field grew slowly at higher altitudes and that it was still strong enough to envelop the energy of the accelerated particles in a few solar radii. The accelerations were observed throughout the entire atmosphere. We have a study of the pulsars, and we present an analysis of the pulses from the Nan\u00e7ay radio telescope, in which the heights of the radii are compared with a height of about 3 radii. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0784, "title": "Space Velocities of L- and T-type Dwarfs", "abstract": "  (Abridged) We have obtained radial velocities of a sample of 18 ultracool dwarfs (M6.5-T8) using high-resolution, near-infrared spectra obtained with NIRSPEC and the Keck II telescope. We have confirmed that the radial velocity of Gl 570 D is coincident with that of the K-type primary star Gl 570 A, thus providing additional support for their true companionship. The presence of planetary-mass companions around 2MASS J05591914-1404488 (T4.5V) has been analyzed using five NIRSPEC radial velocity measurements obtained over a period of 4.37 yr. We have computed UVW space motions for a total of 21 L and T dwarfs within 20 pc of the Sun. This population shows UVW velocities that nicely overlap the typical kinematics of solar to M-type stars within the same spatial volume. However, the mean Galactic (44.2 km/s) and tangential (36.5 km/s) velocities of the L and T dwarfs appear to be smaller than those of G to M stars. A significant fraction (~40%) of the L and T dwarfs lies near the Hyades moving group (0.4-2 Gyr), which contrasts with the 10-12% found for earlier-type stellar neighbors. Additionally, the distributions of all three UVW components (sigma_{UVW} = 30.2, 16.5, 15.8 km/s) and the distributions of the total Galactic (sigma_{v_tot} = 19.1 km/s) and tangential (sigma_{v_t} = 17.6 km/s) velocities derived for the L and T dwarf sample are narrower than those measured for nearby G, K, and M-type stars, but similar to the dispersions obtained for F stars. This suggests that, in the solar neighborhood, the L- and T-type ultracool dwarfs in our sample (including brown dwarfs) is kinematically younger than solar-type to early M stars with likely ages in the interval 0.5-4 Gyr. ", "machine_abstract": "We present new space velocities for the nearest late-M, L-, and T-dwarf stars based on high-precision radial velocity measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile over an 8-year period (2003-2009). We find that all but one of these objects are members of young nearby open clusters or associations. The only exception is LP 944-20 which has been shown to be a member of the Hyades supercluster by its proper motion.  These results confirm previous suggestions that most brown dwarfs have ages less than 100 Myr. They also show that there may exist two populations among the very low-mass stars and brown dwarfs: those associated with open clusters/associations and those apparently not gravitationally bound to any cluster. This suggests that either some brown dwarfs form outside open clusters and/or they evaporate rapidly after formation. Finally we discuss possible explanations for the observed kinematics of the lowest mass stars and brown dwarfs.", "paraphrased_abstract": "The results confirm previous estimates of ages for most brown dwarfs. In fact, the results confirm that brown dwarfs are mostly older than 100 Myr. In addition, we present two different populations of brown dwarfs, which are found in the vicinity of open clusters and association, and that apparently not rooted in any cluster, but dissolved in the air at once. These results confirm the previous assumption that most brown dwarfs are older than 100 Myr. The spectral velocity of the stars is estimated for the nearest open clusters, L, and T, and they are all from young clusters or alliances. The LP 944-20 is, for instance, a member of the Hyades supercluster, and consists of a motion that is exactly like a movement of the Hyades supercluster. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0626, "title": "Covering Homology", "abstract": "  We introduce the notion of \"covering homology\" of a commutative ring spectrum with respect to certain families of coverings of topological spaces. The construction of covering homology is extracted from Bokstedt, Hsiang and Madsen's topological cyclic homology. In fact covering homology with respect to the family of orientation preserving isogenies of the circle is equal to topological cyclic homology.   Our basic tool for the analysis of covering homology is a cofibration sequence involving homotopy orbits and a restriction map similar to the restriction map used in Bokstedt, Hsiang and Madsen's construction of topological cyclic homology.   Covering homology with respect to families of isogenies of a torus is constructed from iterated topological Hochschild homology. It receives a trace map from iterated algebraic K-theory and the hope is that the rich structure, and the calculability of covering homology will make covering homology useful in the exploration of J. Rognes' ``red shift conjecture''. ", "machine_abstract": "We introduce the notion of covering homology, which is an extension of ordinary homology to infinite graphs and simplicial complexes. The main idea behind this concept is that we consider not only finite subcomplexes but also their infinite analogues - so-called coverings. We show how covering homology can be used for studying properties of infinite graphs such as amenability or hyperbolicity. In particular, we prove that if a graph has infinitely many ends then its first covering homology group vanishes. This result generalizes the classical theorem by Stallings on groups with infinitely many ends. Covering homology provides us with new tools for investigating various classes of infinite objects in mathematics. It turns out that it is closely related to other concepts like equivariant homology theory (in the case when our object admits actions of some group) or higher-order algebraic K-theory. Finally, we present several examples illustrating applications of covering homology.", "paraphrased_abstract": "It turns out that this theory is closely related to other theories, such as the equivariant homology (when a object admits the action of some group), or the higher-order algebraic K-theory. We propose a new method for studying infinite objects, such as amenability or hyperbolicity. We demonstrate the use of covering homology in analyzing the properties of infinite graphs, such as the existence of amenability or hyperbolicity. Then, we introduce the theory of covering homology, which is an extension of homology to infinite graphs and simplicial complexes. The main idea of this method is that we consider not only finite structures, but also their infinite analogues, so-called coverings. We prove that if a graph has infinitely many ends, the first group of covering homology disappears. This result generalizes the classic theorem of Stallings on groups with infinitely many ends. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2943, "title": "Multiperiodicity in the newly discovered mid-late Be star V2104 Cygni", "abstract": "  We obtained the first long, homogenous time-series of V2104Cyg, consisting of 679 datapoints, with the uvbybeta photometers of Sierra Nevada and San Pedro Martir Observatories with the aim to detect and subsequently interpret the intrinsic frequencies of this previously unstudied variable star, which turned out to be a Be star. We try to figure out its place among the variable B stars on the upper Main Sequence. In order to obtain additional information on physical parameters we collected a few spectra with the ELODIE and FIES instruments. We searched for frequencies in the uvby passbands using 2 different frequency analysis methods and used the S/N>4 criterion to select the significant periodicities. We obtained an estimate of the physical parameters of the underlying B star of spectral type between B5 and B7, by correcting for the presence of a circumstellar disk, using a formalism based on the strenght of the Halpha line emission. We detected 3 independent frequencies with amplitudes below 0.01mag, f1 = 4.7126 c/d, f2 = 2.2342 c/d and f3 = 4.671 c/d, and discovered that V2104Cyg is a Be star. The fast rotation (vsini=290+/-10 km/s, and 27<i<45) hampered the investigation of the associated pulsational parameters l. Nevertheless, the most plausible explanation for the observed variability of this mid-late type Be star is a non-radial pulsation model. This paper is based on observations obtained at the Observatorio Astronomico Nacional San Pedro Martir (Mexico), Observatorio de Sierra Nevada (Spain), Observatoire de Haute Provence (France), and on observations made with the Nordic Optical Telescope, Observatorio Roque de los Muchachos, La Palma, Spain. ", "machine_abstract": "We report on new photometric and spectroscopic observations of the recently discovered mid- to late-Be star V2104 Cyg, which show that it is multiperiodic with two periodicities at P = 0. d . The first periodicity (P_1) was found by analyzing our own data as well as those available in literature; its value agrees very well with previous determinations. The second one (P_2), however, has never been detected before. We have also analyzed archival IUE spectra for this object and found evidence for long-term variability in both emission-line profiles and continuum fluxes. Our results suggest that the observed double-periodic behavior may be caused by non-radial pulsation or rotationally modulated magnetic activity.     Keywords: Multiperiodicity, Nonradial pulsation, Rotational modulation, Mid-to-late B-type stars, Photometry, Spectroscopy", "paraphrased_abstract": "The multiperiodicity is caused by non-radial pulsation or by rotational motion. This luminous spectroscopic result is based on the recently discovered late B-type star V2104 Cyg. The first periodicity (P. 1) was determined from our own observations and from the literature and was a very significant one. The second periodicity (P.2) was found not only from our observations, but also from the spectral spectra. Moreover, we have examined spectroscopic data from the archival spectra of the object and found long-term variability in the luminous spectrum and the continuum fluxes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2943, "title": "Impact of dimensionless numbers on the efficiency of MRI-induced turbulent transport", "abstract": "  The magneto-rotational instability is presently the most promising source of turbulent transport in accretion disks. However, some important issues still need to be addressed to quantify the role of MRI in disks; in particular no systematic investigation of the role of the physical dimensionless parameters of the problem on the dimensionless transport has been undertaken yet. First, we complete existing investigations on the field strength dependence by showing that the transport in high magnetic pressure disks close to marginal stability is highly time-dependent and surprisingly efficient. Second, we bring to light a significant dependence of the global transport on the magnetic Prandtl number, with $\\alpha\\propto Pm^\\delta$ for the explored range: $0.12<Pm<8$ and $200<Re<6400$ ($\\delta$ being in the range 0.25 to 0.5). We show that the dimensionless transport is not correlated to the dimensionless linear growth rate, contrarily to a largely held expectation. More generally, these results stress the need to control dissipation processes in astrophysical simulations. ", "machine_abstract": "The effect of magnetic Reynolds number (Rm), Lundquist number (S) and plasma beta (\u03b2) on the efficiency of MRI-driven turbulence is investigated by means of direct numerical simulations in cylindrical geometry with periodic boundary conditions. The results show that Rm, S and \u03b2 have significant effects on the saturation level of the Maxwell stress tensor as well as the growth rate of the kinetic energy density. In particular, it was found that for fixed values of other parameters, increasing Rm leads to an increase in both the saturation value of the Maxwell stress tensor and the growth rate of the total kinetic energy density.  Increasing S also increases the saturation value of the stress tensor but has no influence on its growth rate. On the contrary, decreasing \u03b2 decreases the saturation value of the tensor while having little or no effect on its growth rate. It should be noted that these trends are observed only when the initial equilibrium state satisfies certain constraints which depend on the dimensionless numbers under consideration.", "paraphrased_abstract": "The result was that Rm, S and  had the greatest influence on the saturation of the Maxwell stress tensor and the growth of the kinetic energy density. The proportion of these parameters, compared with other factors, showed that for fixed values of other parameters, an increase in Rm would increase the saturation of the Maxwell stress tensor and the growth of the kinetic energy density. In particular, the increase in S and the decrease in  would decrease the saturation of the Maxwell stress tensor and reduce the growth rate. The result showed that the number of magnetic Reynolds, the number of Lundquist and the number of beta beta, compared with the proportion of the flux density, were very significant and, in the presence of the constants of the kinetic energy density, were very significant. The results show that the magnetic Reynolds, the number of S, the number of  and the number of beta beta, are very significant in the development of MRI-induced turbulence. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.044, "title": "What Makes a System Complex? an Approach to Self-Organization and Emergence", "abstract": "  The fast changing reality in technical and natural domains perceived by always more accurate observations has drawn attention on new and very broad class of systems with specific behaviour represented under the common wording complexity. From elementary system graph representation with components as nodes and interactions as vertices, systems are shown to belong to only three states : simple, complicated, and complex, the main properties of which are discussed. The first two states have been studied at length over past centuries, and last one finds its origin in the elementary fact that when system performance is pushed up, there exists a threshold above which interaction between components overtake outside interaction. Then system self-organizes and filters corresponding outer action, making it more robust to outer effect, with emergence of new behaviour not predictable from only components study. Examples in Physics and Biology are given. Three main classes of complexity behaviour are distinguished corresponding to different grades of difficulty to handle their dynamics. The interest of using complex state properties in man-made systems is stressed. Important issues concentrating on the difficult balance between relative system isolation when becoming complex and required delegation of corresponding new capability from (outside) operator are discussed. This implies giving the system some intelligence in an adequate frame between new augmented system state and supervising operator, with consequences on canonical system triplet {effector-sensor-controller} to be reorganized in this new setting. Moreover, entering complexity state opens the possibility for function to feedback onto structure, ie to mimic at technical level the invention of Nature over Her very long history. ", "machine_abstract": "The complexity sciences are concerned with the study of complex systems, which can be defined as open dynamical systems that evolve in time through interactions between their components. The main goal is to understand how these systems self-organize into coherent structures without any central control or external intervention. This book presents a new approach for studying such phenomena by introducing the concept of emergence. It also discusses some important issues related to the foundations of science and proposes a novel methodological framework based on the idea of \"complexity reductionism\". In addition, it provides several examples illustrating its concepts and methods. The book will appeal to scientists interested in complexity theory, computational biology, artificial intelligence, cognitive neuroscience, social science, economics, physics, mathematics, computer science, engineering, medicine, and other fields where complex systems play an important role. The complexity sciences are concerned with understanding how complex systems emerge spontaneously out of simple interacting elements.", "paraphrased_abstract": "The book will be of use to anyone interested in the study of complex systems, computational biology, artificial intelligence, cognitive neuroscience, social science, economics, physics, mathematics, computer science, engineering, medicine and all the other areas in which complex systems play a part. The book will be of use to anyone who is interested in complex systems, in computational biology, in artificial intelligence, in cognitive neuroscience, in social science, economics, physics, mathematics, computer science, engineering, medicine, and other fields in which complex systems play an important role. The study of complex systems is based on the study of the open dynamical systems which emerge in time by their interaction with one another, and its members. In this sense it is the study of the structure of the systems of the organisms, the systems of the body, the systems of the body, the 'whole' of the organisms, the 'whole' of the organisms, the 'whole' of the organism. This book also proposes a new method of study and introduces a new theoretical framework of complexity, which is based on the idea of complexity reductionism. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2949, "title": "Optimizing future imaging survey of galaxies to confront dark energy and modified gravity models", "abstract": "  We consider the extent to which future imaging surveys of galaxies can distinguish between dark energy and modified gravity models for the origin of the cosmic acceleration. Dynamical dark energy models may have similar expansion rates as models of modified gravity, yet predict different growth of structure histories. We parameterize the cosmic expansion by the two parameters, $w_0$ and $w_a$, and the linear growth rate of density fluctuations by Linder's $\\gamma$, independently. Dark energy models generically predict $\\gamma \\approx 0.55$, while the DGP model $\\gamma \\approx 0.68$. To determine if future imaging surveys can constrain $\\gamma$ within 20 percent (or $\\Delta\\gamma<0.1$), we perform the Fisher matrix analysis for a weak lensing survey such as the on-going Hyper Suprime-Cam (HSC) project. Under the condition that the total observation time is fixed, we compute the Figure of Merit (FoM) as a function of the exposure time $\\texp$. We find that the tomography technique effectively improves the FoM, which has a broad peak around $\\texp\\simeq {\\rm several}\\sim 10$ minutes; a shallow and wide survey is preferred to constrain the $\\gamma$ parameter. While $\\Delta\\gamma < 0.1$ cannot be achieved by the HSC weak-lensing survey alone, one can improve the constraints by combining with a follow-up spectroscopic survey like WFMOS and/or future CMB observations. ", "machine_abstract": "We present the results of an optimization study for future galaxy surveys, aimed at constraining cosmological parameters in the context of general relativity (GR) and alternative theories of gravitation. We consider two different classes of experiments: weak lensing tomography with Euclid-like specifications, and 21cm intensity mapping observations with SKA1-LOW-like specifications. In both cases we assume that systematics can be controlled down to percent level accuracy. For each experiment we compute the Fisher matrix associated with the measurement of several relevant observables as functions of the underlying cosmology. The resulting covariance matrices are then used to perform parameter forecasts using Monte Carlo Markov Chains. Our main findings are:  - Weak lensing tomography is able to provide competitive constraints on w0 and wa when combined with Planck data.  - Intensity mapping experiments will not significantly improve our knowledge about the equation-of-state of dark energy beyond what has already been achieved by current CMB measurements.", "paraphrased_abstract": "We compare this with two other studies: weak lensing tomography with Euclidean specifications, and the 21 cm, intensifying maps with SKA2 low specifications. In both cases, we assume that systematics can be controlled to a tenth of a percent accuracy. We conclude that the use of a weak lensing technique can give us competitive constraints on w0 and w2 when combined with Planck data. We show that a higher intensity mapping of a dark energy will not, for instance, improve the equation of dark energy beyond what is currently obtained by CMB measurements. We introduce here an optimization of the Galaxy Survey in order to constrain cosmological parameters in the context of general relativity and alternative theories of gravitation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0977, "title": "Light Heavy MSSM Higgs Bosons at Large tan_beta", "abstract": "  The region of MSSM Higgs parameter space currently excluded by the CDF Collaboration, based on an analysis of ~1 fb^-1 of integrated luminosity, is less than the expected sensitivity. We analyze the potential implications of the persistence of this discrepancy within the MSSM, assuming that the soft supersymmetry-breaking contributions to scalar masses are universal, apart from those to the Higgs masses (the NUHM model). We find that a light heavy MSSM Higgs signal in the unexcluded part of the sensitive region could indeed be accommodated in this simple model, even after taking into account other constraints from cold dark matter, electroweak precision observables and B physics observables. In this case the NUHM suggests that supersymmetric signatures should also be detectable in the near future in some other measurements such as BR(B_s -> mu+ mu-), BR(b -> s gamma) and (g-2)_mu, and M_h would have to be very close to the LEP exclusion limit. In addition, the dark matter candidate associated with this model should be on the verge of detection in direct detection experiments. ", "machine_abstract": "We study the lightest and heaviest CP-even neutral Higgs boson masses in the Minimal Supersymmetric Standard Model (MSSM) with large values of tan(beta). We find that for large values of tan(betas), there is an upper bound on mH,max which depends only weakly on tan(beta). This upper bound can be as low as 130 GeV if we allow for nonuniversal soft supersymmetry breaking terms. The lower limit on mH,min increases rapidly with increasing tan(beta).  For small values of tan(beta) (tan(beta) < 3), the mass difference between the two CP-even Higgs bosons decreases slowly with increasing tan(beta). However, this decrease becomes more rapid when tan(beta) > 5.  In addition to these results, we also present the dependence of the lightest CP-odd Higgs boson mass on tan(beta).  I. INTRODUCTORY REMARkS The Minimal Supersymmetric Standard model (MSSM) [1] has been studied extensively over the past few years [2] . It contains many new parameters beyond those of the Standard Model (SM). These include the gaugino masses M1 , M2 , M3 , the higgsino mass parameter \u00b5, the trilinear scalar couplings A f , and the ratio of vacuum expectation values of the two Higgs doublets tan(beta). In general, it is difficult to obtain analytical expressions for all the physical quantities in the MSSM [3] . Therefore, one usually resorts to numerical methods [4] or approximations [5] . Recently, several groups have used approximate techniques [6] - [8] to calculate various properties of the MSSM Higgs sector. In particular, Ref. [7] presents analytic formulas for calculating the masses of the three neutral Higgs bosons in the MSSM upto leading order corrections in 1/tan(beta). They show that their results agree well with exact calculations [9] .", "paraphrased_abstract": "There is, for instance, a maximum limit for mH, max, which is weakly dependent on tan. This limit can be as low as a factor of a hundred, if we allow for the nonuniversal soft supersymmetry of the terms of the model. There is, in particular, a work in which Ref. (6) presents analytic formulas for the mass of the three neutral Higgs Bosons, as well as a first order correction for tan. In the last few years, a number of studies have been carried out in the field of analytic geometry of the model, and it contains many new parameters, not in the usual sense, of the supersymmetric structure of the SM. This includes, for example, the gaugino mass of M1 and M2 and the Higgs mass of M3; the ratio of the vacuum spectra of the Higgs doublets tan. We present here the results of an analysis of the lightest and heaviest CP-neutral Higgs Bosons in the SMSM with large values of tan., which, however, is much more rapid than tan (beta) > 5. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1819, "title": "Production of magnetic energy by macroscopic turbulence in GRB afterglows", "abstract": "  Afterglows of gamma-ray bursts are believed to require magnetic fields much stronger than that of the compressed pre-shock medium. As an alternative to microscopic plasma instabilities, we propose amplification of the field by macroscopic turbulence excited by the interaction of the shock with a clumpy pre-shock medium, for example a stellar wind. Using a recently developed formalism for localized perturbations to an ultra-relativistic shock, we derive constraints on the lengthscale, amplitude, and volume filling factor of density clumps required to produce a given magnetic energy fraction within the expansion time of the shock, assuming that the energy in the field achieves equipartion with the turbulence. Stronger and smaller-scale inhomogeneities are required for larger shock Lorentz factors. Hence it is likely that the magnetic energy fraction evolves as the shock slows. This could be detected by monitoring the synchrotron cooling frequency if the radial density profile ahead of the shock, smoothed over clumps, is known. ", "machine_abstract": "We study the production of magnetic fields and their subsequent evolution during the relativistic blast wave phase of gamma-ray burst (GRB) afterglow emission, using numerical simulations with high spatial resolution. We find that the magnetic field is amplified to equipartition strength within several hundred milliseconds at most for typical parameters. The amplification occurs mainly through turbulent dynamo action driven by the kinetic energy of the shocked fluid. After reaching its peak value, the magnetic field decays gradually due to adiabatic expansion losses as well as Ohmic dissipation. Our results suggest that the observed X-ray flares are likely produced by internal shocks between shells ejected from different regions inside the progenitor star.     Keywords: Gamma-Ray Bursts, Magnetic Fields, Dynamo Action, Relativistic Blast Wave, Turbulence     1. Introduction     In recent years there has been growing evidence suggesting that gamma-ray burst (GRBs) may be associated with massive stars (e.g., Woosley & Bloom 2006) . If this is true, then it would imply that some fraction of these stars explode into space while still surrounded by dense stellar winds or envelopes. These environments can significantly affect the dynamics of the explosion and the properties of the emitted radiation. For example, Chevalier et al. (2004) showed that if the density profile of the surrounding medium follows an r-2 power law, then the resulting light curve will exhibit a plateau followed by a steep decay phase. This behavior was later confirmed observationally (e.g., Panaitescu 2005; Kumar & Panaitescu 2008) , which led to the suggestion that many GRBs might originate from such progenitors (e.g., Zhang 2007). However, other authors have argued against this scenario on theoretical grounds (e.g., Ramirez-Ruiz et al. 2005 ) and observational ones (e.g., Lazzati et al. 2009 ). It should also be noted that even though the majority of GRBs seem to follow this general trend, there exist cases where no clear signature of interaction with a wind-like environment could", "paraphrased_abstract": "He has recently discovered the existence of the gamma-ray pulse, which can be seen in a large number of stars. The theory is based on a density law of r-2; if the density of the surrounding medium possesses an r-2 power, the light curve will become flat and slow, resulting in a trough, and with a slow decay. This effect was later confirmed by the observations of Panaitescu and Panaitescu, who argued that a large number of GRBs could be derived from such progenitors. However, in certain cases, there are some instances where the emitted radiation has not yet been observed in this environment. It should be noted that, in spite of the fact that most GRBs seem to follow this general pattern, there are instances when there is no clear sign of interaction with a wind-like atmosphere. For example, Chevalier et al. found that, as long as the density of the surrounding medium is a power law of r-2, the resulting light curve will be of a low frequency, with a gradual absorption of the emitted light, and a gradual decrease in the flux of the emitted light. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3778, "title": "BL Lac Contribution to the Extragalactic Gamma-Ray Background", "abstract": "  Very high energy gamma-rays from blazars traversing cosmological distances through the metagalactic radiation field can convert into electron-positron pairs in photon-photon collisions. The converted gamma-rays initiate electromagnetic cascades driven by inverse-Compton scattering off the microwave background photons. Using a model for the time-dependent metagalactic radiation field consistent with all currently available far-infrared-to-optical data, we calculate the cascade contribution from faint, unresolved high- and low-peaked blazars to the extragalactic gamma-ray background as measured by EGRET. For low-peaked blazars, we adopt a spectral index consistent with the mean spectral index of EGRET detected blazars, and the luminosity function determined by Chiang and Mukherjee (1998). For high-peaked blazars, we adopt template spectra matching prototype sources observed with air-Cherenkov telescopes up to 30 TeV, and a luminosity function based on X-ray measurements. The low number of about 20 for nearby high-peaked blazars with a flux exceeding 10^-11 cm^-2 s^-1 above 300 GeV inferred from the luminosity function is consistent with the results from air-Cherenkov telescope observations. Including the cascade emission from higher redshifts, the total high-peaked blazar contribution to the observed gamma-ray background at GeV energies can account up to about 30. ", "machine_abstract": "We present new results on the contribution of BL Lacs (blazars) to the extragalactic gamma-ray background based on data collected by the Fermi Large Area Telescope between August 2008 and December 2010, corresponding to an effective exposure time of 1.6 yr for each source in our sample. We use two different methods to estimate this contribution: i) we calculate the number counts above 100 MeV as function of redshift using a maximum likelihood method; ii) we fit the observed spectral energy distribution with a log-parabola model and derive the integrated fluxes at 0.1 GeV and 10 TeV energies. The resulting contributions are consistent within statistical uncertainties. Our best-fit value is F(>100 MeV) = 2.2 x 10^{\u22128} ph cm\u22122 s\u22121 sr\u22121 which corresponds to ~20% of the measured EGB intensity. This result confirms that blazars are one of the main contributors to the EGB emission.", "paraphrased_abstract": "Using a maximum likelihood method, we compute the number of MeV above 100 mV corresponding to the redshift of the sky. Then we estimate the integrated fluxes at 0.1 GeV and at ten TeV energies. The result is a consistent and consistent one. The new results demonstrate the presence of BL Lacs (blazars) in the cosmic rays. We calculate the sum of BL Lacs as a function of redshift, a maximum likelihood method, and a log-parabola model, to estimate the integrated fluxes of the spectra at 0.1 GeV and at ten TeV energies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2582, "title": "Corbino experimental set-up for Cooper pair mass spectroscopy and determination of mobility in normal phase", "abstract": "  We are suggesting an electronic method for Cooper pair mass spectroscopy of thin superconducting films. The method can be applied, for example, for 100 nm thin high-T_c films grown on insulator substrate 10 mm X 10 mm. In Corbino geometry two Ohmic contacts have to be made on the film evaporating Ag or using silver paste: one circle with radius r_a (1) and a ring electrode with internal radius r_b (2). In the in-between space (r_a,r_b) a ring electrode from mylar assures a capacitive connection between the superconducting layer and the metallized surface (3) of the mylar. In such a way we have an field effect transistor (FET) type structure with a circular gate. When at low enough temperatures T << T_c an AC current with frequency omega is applied between the circle source (1) and the ring-shaped drain (2) an AC Bernoulli voltage with double frequency 2f appears between the gate (3) and the source (1). The 2f signal depends on Cooper pair effective mass and its systematic investigation gives a Cooper pair mass spectroscopy. In the normal phase 2f gives logarithmic derivative of the density of states (DOS) with respect of Fermi energy. Applying a gate voltage in the same structure gives the mobility of the normal phase. ", "machine_abstract": "We present an experimental setup to measure the superfluid density $\\rho_{s}$ by means of microwave transmission measurements on thin films of high-temperature superconductors (HTS). The method is based on the measurement of the complex impedance $Z$ of a Corbino disk geometry, which can be related to the surface resistance $R_{surf}$ via the expression $$R_{surf} = \\frac{Z_0 - Z_{in}}{2}\\tag{1}$$ where $Z_0$ is the characteristic impedance of free space and $Z_{in}$ is the input impedance of our system. In order to determine the latter we use a numerical model that takes into account all relevant geometrical parameters as well as the dielectric properties of the substrate material.  We demonstrate this technique with two different HTS materials, namely YBaCuO and BiSrCaCuO. For both compounds we find good agreement between theory and experiment. Finally, we show how one can extract information about the charge carrier mobility from these data.", "paraphrased_abstract": "It is the result of the analysis of the elasticity of a thin, high-temperature superconductor. This method is based on the measurement of the density of superfluids, by means of a microwave spectrometer. The method is based on the complex impedance of a Corbino disk, which can be compared with the surface resistance R surf, by the expression: fracZ0 - Z in2tag12tag14, where the characteristic impedance of the space, and the input impedance of the system. The numerical equation is used to find the exact solution. We have demonstrated this technique on two different materials of high-temperature superconducting material YbCuO and BiSrCaCuO. We demonstrate this technique with two different HTS materials, YbCuO and BiSrCaCuO, respectively. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.117, "title": "The Collision Between The Milky Way And Andromeda", "abstract": "  We use a N--body/hydrodynamic simulation to forecast the future encounter between the Milky Way and the Andromeda galaxies, given current observational constraints on their relative distance, relative velocity, and masses. Allowing for a comparable amount of diffuse mass to fill the volume of the Local Group, we find that the two galaxies are likely to collide in a few billion years - within the Sun's lifetime. During the the interaction, there is a chance that the Sun will be pulled away from its present orbital radius and reside in an extended tidal tail. The likelihood for this outcome increases as the merger progresses, and there is a remote possibility that our Sun will be more tightly bound to Andromeda than to the Milky Way before the final merger. Eventually, after the merger has completed, the Sun is most likely to be scattered to the outer halo and reside at much larger radii (>30 kpc). The density profiles of the stars, gas and dark matter in the merger product resemble those of elliptical galaxies. Our Local Group model therefore provides a prototype progenitor of late--forming elliptical galaxies. ", "machine_abstract": "The collision between the Milky Way and its nearest neighbor, M31 (Andromeda), is predicted to occur in about 4 billion years.  This will be one of the most spectacular events ever witnessed by humans.   In this talk I will describe how we can use observations made with telescopes on Earth as well as space-based observatories such as Hubble Space Telescope to study these collisions and learn more about dark matter, galaxies, stars, black holes, and other cosmic phenomena that are part of our universe. I will also discuss some of my research projects related to studying galaxy mergers using data obtained at the W.M. Keck Observatory located on Mauna Kea, Hawaii. Finally, I'll share what it was like for me to work there during my summer internship last year. Keywords: Dark Matter, Galaxy merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics", "paraphrased_abstract": "I will then describe my research project at the W.M. Keck Observatory in Mauna Kea, Hawaii. I will also discuss the study of the merger of the Milky Way and the neighbouring constellation M31, in which the collision will be about four billion years in the future. This will be one of the most spectacular events in the history of mankind. I will also explain how I have incorporated data from the Observation of the Galaxy to investigate the formation of the Black Hole, the Cosmic Evolution, the Cosmology, and Astrophysics. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3292, "title": "Coalition Games with Cooperative Transmission: A Cure for the Curse of Boundary Nodes in Selfish Packet-Forwarding Wireless Networks", "abstract": "  In wireless packet-forwarding networks with selfish nodes, applications of a repeated game can induce the nodes to forward each others' packets, so that the network performance can be improved. However, the nodes on the boundary of such networks cannot benefit from this strategy, as the other nodes do not depend on them. This problem is sometimes known as the curse of the boundary nodes. To overcome this problem, an approach based on coalition games is proposed, in which the boundary nodes can use cooperative transmission to help the backbone nodes in the middle of the network. In return, the backbone nodes are willing to forward the boundary nodes' packets. The stability of the coalitions is studied using the concept of a core. Then two types of fairness, namely, the min-max fairness using nucleolus and the average fairness using the Shapley function are investigated. Finally, a protocol is designed using both repeated games and coalition games. Simulation results show how boundary nodes and backbone nodes form coalitions together according to different fairness criteria. The proposed protocol can improve the network connectivity by about 50%, compared with pure repeated game schemes. ", "machine_abstract": "We consider a wireless network where nodes selfishly forward packets to their neighbors, and study how cooperation among neighboring nodes can be used to improve performance. We propose a novel cooperative transmission scheme that allows each node to transmit its packet simultaneously on multiple channels using different power levels. The proposed scheme is shown to achieve higher throughput than existing schemes by exploiting spatial diversity across channels as well as power control over these channels. In addition, we show that our scheme achieves fairness between nodes without requiring any centralized controller or global information about the network topology. Finally, we present simulation results showing significant improvement in terms of both throughput and fairness compared to existing non-cooperative schemes. This work was supported by NSF under Grants CNS-0721440 (CAREER), ECCS-0701260, and CNS-0834753. Abstract: We consider a wireless network where all nodes are selfish and only cooperate when it benefits them. We propose a new cooperative transmission scheme which enables each node to transmit its own packet simultaneously on multiple channels at different power levels. Our scheme exploits spatial diversity across channels as wells as power control over those channels to increase the overall system throughput while maintaining fairness amongst competing nodes.", "paraphrased_abstract": "As a result of this study we have come up with a new, cooperative transmission system, which allows each node to transmit a packet at the same time on multiple channels at different power levels. We have introduced this system to the world by leveraging the diversity of channels and the influence of power on the transmission of the packet, thereby increasing the throughput and the fairness of the nodes. In our experiments we show that our system has the highest throughput and the highest fairness without any centrally-arranged management or any global information on the network topology. We have also developed an experimental method for the analysis of the system using data collected on the Internet. We have a wireless system in which all nodes are selfish and only cooperate when it is convenient for them. We have a novel transmission method, which combines the transmission of the packet on multiple channels, the control of the channel, and the centralized control of the channels, so as to increase the total throughput and the fairness of the competing nodes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0329, "title": "Solutions of fractional reaction-diffusion equations in terms of the H-function", "abstract": "  This paper deals with the investigation of the solution of an unified fractional reaction-diffusion equation associated with the Caputo derivative as the time-derivative and Riesz-Feller fractional derivative as the space-derivative. The solution is derived by the application of the Laplace and Fourier transforms in closed form in terms of the H-function. The results derived are of general nature and include the results investigated earlier by many authors, notably by Mainardi et al. (2001, 2005) for the fundamental solution of the space-time fractional diffusion equation, and Saxena et al. (2006a, b) for fractional reaction- diffusion equations. The advantage of using Riesz-Feller derivative lies in the fact that the solution of the fractional reaction-diffusion equation containing this derivative includes the fundamental solution for space-time fractional diffusion, which itself is a generalization of neutral fractional diffusion, space-fractional diffusion, and time-fractional diffusion. These specialized types of diffusion can be interpreted as spatial probability density functions evolving in time and are expressible in terms of the H-functions in compact form. ", "machine_abstract": "We present an explicit formula for solutions to some classes of nonlinear fractional diffusion equations with variable coefficients and nonlocal initial conditions, which are expressed by means of the generalized Mittag-Leffler function E\u03b1(\u2212z) and the H-function. The results obtained can be used as a basis for numerical methods for solving such problems. We also discuss applications of our approach to the description of anomalous transport processes in complex systems. Introduction. The theory of fractional calculus has been developed rapidly during recent years due to its numerous applications in various fields of science and engineering (see e.g., [1] - [4] ). In particular, it is widely applied to describe anomalous transport phenomena observed in many physical models [5] . For example, this approach was successfully employed to model subdiffusion [6] , superdiffusion [7] , and L\u00e9vy flights [8] . In general, fractional differential equations have no exact analytical solution [9] . Therefore, there exists great interest in developing efficient numerical algorithms for their approximate treatment [10] . However, even if we use powerful computers, these approaches may not always provide sufficiently accurate results [11] . This problem becomes especially important when one deals with strongly nonlinear problems [12] or considers very large time intervals [13] . To overcome difficulties associated with the lack of exact solutions, several authors proposed different techniques based on special functions [14] - [16] . Among them, the most popular ones include the Mittag-Leffler [17] and Wright [18] functions, Fox's H-functions [19] , and others [20] - [22] .", "paraphrased_abstract": "In recent years the theory of fractional calculus has been widely applied to the study of anomalous transport phenomena in many physical models. The latter have been successfully used to describe subdiffusion, superdiffusion and \u00e9vy flights. The latter are, in particular, popular among students, notably the Mittag-Leffler and Wright-Haeser functions, the Fox H-functions, and others. We show here the explicit formula for solving some classes of nonlinear fractional equations, which have variable coefficients and nonlocal initial conditions, and in particular the generalized Mittag-Leffler function, E()[2] and the H-function, respectively. Consequently, we are occupied with the problem of estimating the exact inverse in the inverse. It is necessary, for instance, to obtain the exact inverse of these equations, and to obtain their exact numerical solution. However, the methods we develop are not always exact. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4221, "title": "Controllability of the heat and wave equations and their finite difference approximations by the shape of the domain", "abstract": "  In this article we study a controllability problem for a parabolic and a hyperbolic partial differential equations in which the control is the shape of the domain where the equation holds. The quantity to be controlled is the trace of the solution into an open subdomain and at a given time, when the right hand side source term is known. The mapping that associates this trace to the shape of the domain is nonlinear. We show (i) an approximate controllability property for the linearized parabolic problem and (ii) an exact local controllability property for the linearized and the nonlinear equations in the hyperbolic case. We then address the same questions in the context of a finite difference spatial semi-discretization in both the parabolic and hyperbolic problems. In this discretized case again we prove a local controllability result for the parabolic problem, and an exact controllability for the hyperbolic case, applying a local surjectivity theorem together with a unique continuation property of the underlying adjoint discrete system. ", "machine_abstract": "We study controllability properties for the heat equation, the wave equation in one space dimension with Dirichlet boundary conditions, and their finite-difference discretizations on rectangular domains. We prove that these problems are not exactly controllable if the length of the longest side is less than or equal to two times the length of the shortest side. For the case where this ratio is larger we show exact controllability under some additional assumptions on the initial data. The proofs rely on Carleman estimates combined with suitable observability inequalities. These results can be extended to higher dimensions using similar techniques. Controllability questions have been studied extensively over the past decades both theoretically as well as numerically (see e.g., [1] , [4] - [6] ). In particular, it has been shown that many partial differential equations cannot be controlled globally even though they may admit an infinite number of controls. This phenomenon was first observed by J. Lagnese [7] who showed that the linear Schr\u00f6dinger equation with potential $|x|^{-2}$ does not admit any global control function when the spatial domain $\\Omega$ is bounded below by a half-line. Later, Fursikov [8] proved that the same result holds true also for other potentials such as $|x|^{-1}$ and $|x|^{-3/2}$. More recently, Burq et al. [9] generalized this result to nonlinear Schr\u00f6dinger equations.", "paraphrased_abstract": "This fact was first noticed by J. Lagnese, who showed that the linear Schr\u00f6dinger equation with potential $x  2 does not admit any global control function when the spatial domain is bounded by a half-line. More recently, Burq et al. generalized this result to nonlinear Schr\u00f6dinger equations. We study the control of heat equations, the wave equation in one space dimension, the Dirichlet boundary conditions, and their finite-difference discretizations in rectangular domains. We demonstrate that these problems are not precisely controlled if the length of the longest line is less than or equal to two times the length of the shortest line. We show that these problems are not exactly controlled if the length of the longest line is less than two times the length of the shortest line. This is particularly demonstrated by the work of J. Lagnese, who showed that the linear Schr\u00f6dinger equation with potential $x--2 has no global control function when the spatial domain is bounded below by half a line. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2764, "title": "Complementarity in the Einstein-Bohr Photon Box", "abstract": "  The photon box thought experiment can be considered a forerunner of the EPR-experiment: by performing suitable measurements on the box it is possible to ``prepare'' the photon, long after it has escaped, in either of two complementary states. Consistency requires that the corresponding box measurements be complementary as well. At first sight it seems, however, that these measurements can be jointly performed with arbitrary precision: they pertain to different systems (the center of mass of the box and an internal clock, respectively). But this is deceptive. As we show by explicit calculation, although the relevant quantities are simultaneously measurable, they develop non-vanishing commutators when calculated back to the time of escape of the photon. This justifies Bohr's qualitative arguments in a precise way; and it illustrates how the details of the dynamics conspire to guarantee the requirements of complementarity. In addition, our calculations exhibit a ``fine structure'' in the distribution of the uncertainties over the complementary quantities: depending on when the box measurement is performed, the resulting quantum description of the photon differs. This brings us close to the argumentation of the later EPR thought experiment. ", "machine_abstract": "We present an experimental investigation into complementarity and quantum nonlocality using a photonic implementation of the Einstein-Bohr photon box thought experiment. We demonstrate that our system exhibits both classical correlations, which are consistent with local hidden variable theories, as well as quantum correlations, which cannot be explained by any such theory. Our results show that this system is capable of exhibiting all three types of Bell inequalities simultaneously. The Einstein-Bohr (EB) photon-box [1] , also known as the EPRB [2] or the two-slit experiment [3] , has been used to investigate many aspects of quantum mechanics including entanglement [4] , Bell's theorem [5] , and quantum teleportation [6] . In its original form it consists of a source emitting pairs of photons at random times; one photon passes through a beam splitter while the other travels directly towards a detector. If we measure whether each photon arrives at either output port of the beam splitter then there will always be exactly one photon arriving at each detector. This measurement can be performed locally on each side without disturbing the state of the other particle. However if instead we perform measurements on both particles jointly then they must arrive together at the same detector [7, 8] . In order for these experiments to exhibit genuine quantum effects, the detectors need to have high efficiency so that the probability of detecting more than one photon per pair is negligible [9] . Previous implementations of EB boxes have relied upon inefficient single-photon counting detectors [10] or inefficient avalanche photo diodes [11] . These devices do not allow us to distinguish between different numbers of detected photons and therefore prevent us from observing truly quantum behaviour [12] .", "paraphrased_abstract": "The Einstein-Bohr (EB) experiment is an experiment for investigating many aspects of quantum mechanics, such as entanglement, Bell's theorem, and teleportation. In its original form it consists of a source emitting a pair of electrons at random times; one of them passes through a beam splitter and the other passes directly to the detector; we can measure it directly from each end of the beam splitter, and we know exactly one electron is at each end of the beam splitter. The experiment can be done either in the individual detectors or in the pair, so that it can be observed at the same time. In order to produce true quantum effects, the detectors must be of a high efficiency, so that the probability of detecting more than one electron per pair is negligible. We also show that our system is capable of exhibiting both classical correlations, which are consistent with the hidden variable theory, and quantum correlations, which are impossible to explain with any theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0937, "title": "Stability of spinor Fermi gases in tight waveguides", "abstract": "  The two and three-body correlation functions of the ground state of an optically trapped ultracold spin-1/2 Fermi gas (SFG) in a tight waveguide (1D regime) are calculated in the plane of even and odd-wave coupling constants, assuming a 1D attractive zero-range odd-wave interaction induced by a 3D p-wave Feshbach resonance, as well as the usual repulsive zero-range even-wave interaction stemming from 3D s-wave scattering. The calculations are based on the exact mapping from the SFG to a ``Lieb-Liniger-Heisenberg'' model with delta-function repulsions depending on isotropic Heisenberg spin-spin interactions, and indicate that the SFG should be stable against three-body recombination in a large region of the coupling constant plane encompassing parts of both the ferromagnetic and antiferromagnetic phases. However, the limiting case of the fermionic Tonks-Girardeau gas (FTG), a spin-aligned 1D Fermi gas with infinitely attractive p-wave interactions, is unstable in this sense. Effects due to the dipolar interaction and a Zeeman term due to a resonance-generating magnetic field do not lead to shrinkage of the region of stability of the SFG. ", "machine_abstract": "We study the stability of spin-1/2 fermions confined to one dimension by an external potential and interacting via contact interactions, using the Bethe ansatz solution for the Lieb-Liniger model. We find that there is no instability at zero temperature when the chemical potential lies between two consecutive energy levels of the system. This result holds true even if we consider finite temperatures as well. In particular, this implies that the ground state remains stable against collapse into a single particle state (fermionization) or formation of bound states with more than 2 particles (bosonization). The results are also valid for higher spins. Our analysis can be extended to other models such as those describing cold atoms trapped inside optical lattices. Introduction:-In recent years, ultracold atomic systems have been used extensively to simulate various physical phenomena [1] . One-dimensional quantum gases provide particularly interesting examples because they allow us to explore many-body physics in regimes where analytical solutions cannot be obtained [2] . The most common experimental setup consists of confining bosonic or fermionic atoms along one spatial direction within a harmonic trap [3] , which leads to the emergence of quasi-one dimensional behavior [4] . However, it has recently become possible to confine these atoms tightly enough so that their motion becomes truly onedimensional [5] . For example, experiments performed with Bose-Einstein condensates [6] and degenerate Fermi gases [7, 8] show that confinement in a narrow channel gives rise to new phases of matter [9] . These include superfluidity [10] , supersolids [11] , Luttinger liquids [12] , Tonks-Girardeau gas [13] , and Mott insulators [14] . It would therefore be very useful to develop theoretical tools capable of predicting the properties of these novel phases [15] . One of the main challenges associated with studying strongly correlated quantum systems is determining whether certain configurations are energetically favorable [16] . If the answer turns out to be yes, then we say that the configuration is metastable [17] . On the other hand, if the answer is no, then the configuration is unstable [18] . Instabilities may occur due to spontaneous symmetry", "paraphrased_abstract": "One of the difficulties facing scientists studying strongly correlated quantum systems is determining whether certain configurations are energetically favorable... On the other hand, if they are not, they are unstable. It is particularly interesting to investigate many-body physics, which is difficult to solve analytically. The most common example is the confinement of bosonic and fermionic atoms in a harmonic trough, which results in the formation of quasi-one-dimensional physics. In the recent years, experiments on Bose-Einstein condensates and Fermi gases have been carried out to study the phenomena of new phases of matter. They include superfluidity, supersolids, Luttinger liquids, Tonks-Girardeau gas, and Mott insulators. Our study has shown that there is no instability at zero temperature, and when the chemical potential lies between two energy levels, it is stable. However, it has recently been possible to confine these atoms to one dimension, so that their motion is truly one-dimensional. It will be very useful to develop theoretical methods for the analysis of such new phases. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2193, "title": "Linear pre-metric electrodynamics and deduction of the light cone", "abstract": "  We formulate a general framework for describing the electromagnetic properties of spacetime. These properties are encoded in the `constitutive tensor of the vacuum', a quantity analogous to that used in the description of material media. We give a generally covariant derivation of the Fresnel equation describing the local properties of the propagation of electromagnetic waves for the case of the most general possible linear constitutive tensor. We also study the particular case in which a light cone structure is induced and the circumstances under which such a structure emerges. In particular, we will study the relationship between the dual operators defined by the constitutive tensor under certain conditions and the existence of a conformal metric. Closure and symmetry of the constitutive tensor will be found as conditions which ensure the existence of a conformal metric. We will also see how the metric components can be explicitly deduced from the constitutive tensor if these two conditions are met. Finally, we will apply the same method to explore the consequences of relaxing the condition of symmetry and how this affects the emergence of the light cone. ", "machine_abstract": "The author considers an alternative approach to classical electrodynamics, which is based on the concept of \"pre-metricity\" introduced by Einstein in his work \"On the Electrodynamics of Moving Bodies\" (1905). The main idea behind this approach consists in introducing into consideration not only the usual electromagnetic field tensor F\u03bc\u03bd but also its dual counterpart *F\u03bc\u03bd . In order to obtain equations for both fields simultaneously one has to introduce additional terms into Lagrangian density L(F\u03bc\u03bd ,*F\u03bc\u03bd ) that are quadratic with respect to derivatives of these fields. It turns out that such modified Lagrangian leads to the following set of equations:  where  is the d'Alembert operator,  is the energy-momentum tensor of matter fields, and  is the current four-vector describing the flow of electric charge.  In addition to the standard Maxwell's equations, Eqs. (1)-(3), there appears another equation -the so-called \"duality condition\":", "paraphrased_abstract": "The authors take the notion of pre-metrics, which Einstein introduced in his book \u201cThe Electrodynamics of Moving Bodies\u201d (1954). In order to obtain equations for both the fields, it is necessary to introduce terms into the density of the Lagrangian density of L(F,*F) which are quadratic for derivatives of these fields. Besides the equations of the usual Maxwell equations, they also contain the following one: where is the operator of the d\u2019Alembert operator, where is the energy of the field, where is the current of electric charge, where is the current of the current of the electric charge. The author proposes a certain approach to electrodynamics, which is based on the principle of \u2018pre-metricity\u2019 which Einstein formulated in his work on the Electrodynamics of Moving Bodies (ca. 1857). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3132, "title": "Direct CP violation in $\\bar{B}^0 \\to \\rho^0(\\omega)\\rho^0(\\omega) \\to \\pi^+\\pi^-\\pi^+\\pi^-$", "abstract": "  We study the direct CP violation in $\\bar{B}^0 \\to \\rho^0(\\omega)\\rho^0(\\omega) \\to \\pi^+\\pi^-\\pi^+\\pi^-$ (with unpolarized $\\rho^0(\\omega)$) via the $\\rho-\\omega$ mixing mechanism which causes a large strong phase difference and consequently a large CP violating asymmetry when the masses of the $\\pi^+\\pi^-$ pairs are in the vicinity of the $\\omega$ resonance. Since there are two $\\rho (\\omega)$ mesons in the intermediate state $\\rho-\\omega$ mixing contributes twice to the first order of isospin violation, leading to an even larger CP violating asymmetry (could be 30% -- 50% larger) than in the case where only one $\\rho (\\omega)$ meson is involved. The CP violating asymmetry depends on the Cabibbo-Kobayashi-Maskawa (CKM) matrix elements and the hadronic matrix elements. The factorization approach is applied in the calculation of the hadronic matrix elements with the nonfactorizable effects being included effectively in an effective parameter, $N_c$. We give the constraint on the range of $N_c$ from the latest experimental data for the branching ratios for $\\bar{B}^0 \\to\\rho^0\\rho^0$ and $\\bar{B}^0 \\to\\rho^+\\rho^-$. We find that the CP violating asymmetry could be very large (even more than 90% for some values of $N_c$). It is shown that the sensitivity of the CP violating asymmetry to $N_c$ is large compared with its smaller sensitivity to the CKM matrix elements. We also discuss the possibility to remove the mod $(\\pi)$ ambiguity in the determination of the CP violating phase angle $\\alpha$ through the measurement of the CP violating asymmetry in the decay $\\bar{B}^0\\to \\rho^0(\\omega)\\rho^0(\\omega) \\to \\pi^+\\pi^-\\pi^+\\pi^-$. ", "machine_abstract": "We report the first observation of direct CP violation in B decays to two vector mesons, using $2\\times 10^8$ BB pairs collected by the BABAR experiment at SLAC National Accelerator Laboratory.  We measure the branching fraction and CP asymmetry for the decay $$\\bar{B}^0 \\rightarrow \\rho^0(\\omega ) \\rho^0(\\omega)  \\rightarrow \\pi^+ \\pi^- \\pi^+ \\pi^-\\tag{1}$$ where one or both of the rho mesons are longitudinally polarized. The measured values are:  $$B = (1.27 \\pm 0.10 \\pm 0.06) \\times 10^{-6}$$ $$A_CP = -0.0016 \\pm 0.0006 \\pm 0.0003 \\tag{2}$$  The results agree with Standard Model predictions within uncertainties. This is the first measurement of this type made on a hadronic final state containing four pions.", "paraphrased_abstract": "A CP of -0.0016 pm 0.0006 pm 0.0003 tag2$$. The data are: B = (1.27 pm 0.05 pm 0.04) times 106$$. The decays of B and C are directly affected by the CP. This is the first measurement in a state with four pions. We report the first direct observation of the CP of B decays in a case of two vectors, as measured by the BABAR experiment of the National Accelerator Laboratory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1255, "title": "Measuring the spin polarization and Zeeman energy of a spin-polarized electron gas: Comparison between Raman scattering and photoluminescence", "abstract": "  We compare resonant electronic Raman scattering and photoluminescence measurements for the characterization of a spin-polarized two-dimensional electron gas embedded in $\\text{Cd}_{1-x}\\text{Mn}_x\\text{Te}$ single quantum wells. From Raman scattering by single-particle excitations in a zero magnetic field, we measure the Fermi velocity and then obtain the Fermi energy (as well as the electron density), which is comparable to that extracted from photoluminescence for moderate electron densities, assuming a bare band-edge mass. At large electron densities, the Fermi energies derived from Raman scattering and photoluminescence differ. For an applied in-plane magnetic field and zero wave vector transferred to the electron gas, Raman scattering spectra show peaks at both the Zeeman energy $Z$, resulting from collective excitations of the spin-polarized electron gas, and the one electron spin-flip energy $Z^*$. Magneto-photoluminescence spectra show conduction band splitting that are equivalent to $Z$, suggesting that collective effects are present in the photoluminescence spectra. Assuming (as before) an uncorrected mass, the degree of spin polarization $\\zeta$ determined from the magneto-photoluminescence lineshape is found to differ from that derived from the magnetic field dependent Raman scattering measurements for large electron densities. We attribute the discrepancy in measuring $\\zeta$ and the Fermi energy to the renormalized mass resulting from many-body electron-electron interactions. ", "machine_abstract": "We report on an experimental study of the spin-dependent properties of a two-dimensional electron system in GaAs/AlGaAs quantum wells by means of polarized light scattering (PL) and magneto-optical spectroscopy (Raman). The PL experiments are performed at low temperatures, where we observe circularly polarized emission with opposite helicity for right- and left-circularly polarized excitation. We show that this effect is due to the presence of a strong exchange interaction which leads to a splitting of the excitonic states into two branches corresponding to different spins of the carriers. In addition, we find that the degree of circular polarization depends strongly on temperature and magnetic field strength. By comparing our results with those obtained by polarized Raman scattering under similar conditions, we demonstrate that both techniques provide complementary information about the electronic structure of the investigated samples. Our findings can be used as input parameters for theoretical models describing the transport phenomena in semiconductor nanostructures.", "paraphrased_abstract": "I will give a preliminary account of the theoretical interpretation of the transport of the electrons in the semiconductor nanostructure. In this respect, I will describe a comparison between the results of polarized Raman and Raman, and we show that these two methods yield complementary information about the electronic structure of the fabricated samples. I will also describe the effect of polarization in the polarized electron system in GaAs and AlAs, by combining polarized PL and Raman spectroscopy. I will discuss here an experimental investigation of the two-dimensional electron structure of a quantum well of Al-As and Ga-Al. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2959, "title": "Geometry of PT-symmetric quantum mechanics", "abstract": "  Recently, much research has been carried out on Hamiltonians that are not Hermitian but are symmetric under space-time reflection, that is, Hamiltonians that exhibit PT symmetry. Investigations of the Sturm-Liouville eigenvalue problem associated with such Hamiltonians have shown that in many cases the entire energy spectrum is real and positive and that the eigenfunctions form an orthogonal and complete basis. Furthermore, the quantum theories determined by such Hamiltonians have been shown to be consistent in the sense that the probabilities are positive and the dynamical trajectories are unitary. However, the geometrical structures that underlie quantum theories formulated in terms of such Hamiltonians have hitherto not been fully understood. This paper studies in detail the geometric properties of a Hilbert space endowed with a parity structure and analyses the characteristics of a PT-symmetric Hamiltonian and its eigenstates. A canonical relationship between a PT-symmetric operator and a Hermitian operator is established. It is shown that the quadratic form corresponding to the parity operator, in particular, gives rise to a natural partition of the Hilbert space into two halves corresponding to states having positive and negative PT norm. The indefiniteness of the norm can be circumvented by introducing a symmetry operator C that defines a positive definite inner product by means of a CPT conjugation operation. ", "machine_abstract": "We study the geometry of the space of states in PT-symmetric quantum mechanics, and show that it is related to the geometry of the complex projective line. We also discuss how this relates to the concept of pseudo-Hermiticity.  The article was published on September 24th 2014 by ArXiv.org as part of their open access initiative. The full text can be found here. This work has been made available under a Creative Commons Attribution License (CC BY). You may use and distribute this material for any purpose provided that you give appropriate credit to the original author(s) and the source, provide a link to the license, and indicate if changes were made. If you remix, transform, or build upon this material, you must distribute your contributions under the same license as the original. Quantum mechanics with parity-time symmetry (PTQM), which includes non-Hermitian Hamiltonians, has attracted considerable interest recently because it appears naturally in many physical systems such as optical waveguides, Bose-Einstein condensates, and microwave cavities.     In this paper we study the geometry of the state space in PTQM. First, we introduce an equivalence relation between two states $|\\psi\\rangle$ and $|\\phi\\rangle$ when they are connected by a unitary transformation $U$ satisfying $\\langle \\psi | U^\\dagger H U | \\phi \\rangle = 0$. Then we define the geometric phase factor associated with each closed loop in the state space as the holonomy of the connection 1-form defined by the Berry curvature. Finally, we prove that the geometric phase factors corresponding to different loops are additive up to a global phase factor.", "paraphrased_abstract": "This article has been published by ArXiv.org on September 24, 2014. The article is available for free download. The article is distributed under a Creative Commons License. The use of this work is permitted for all purposes, so long as you give appropriate credit to the author, the source, and the license, and give the necessary conditions for the revisions. The study of the state space in PTQM is based on a non-Hermitian Hamiltonian PTQM, which is not Hermitian Hamiltonian, and is naturally found in many physical systems, such as optical waveguides, Bose-Einstein condensates, and microwave cavities. We present the geometry of the state space of PTQM, and show how it corresponds to the geometry of the complex projective line, and we illustrate how this relates to the idea of pseudo-Hermiticity. We introduce a holonomy of the closed loops in the state space, and give them the holonomy of the connection -1 form, i.e., the holonomy of the connection -1 form, and conclude that the geometric phase factors of each loop are multiplied, and add to it the global phase factor. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1637, "title": "Comparative study of complex N- and O-bearing molecules in hot molecular cores", "abstract": "  We have observed several emission lines of two Nitrogen-bearing (C2H5CN and C2H3CN) and two Oxygen-bearing (CH3OCH3 and HCOOCH3) molecules towards a sample of well-known hot molecular cores (HMCs) in order to check whether the chemical differentiation seen in the Orion-HMC and W3(H_2O) between O- and N-bearing molecules is a general property of HMCs. With the IRAM-30m telescope we have observed 12 HMCs in 21 bands, centered at frequencies from 86250 to 258280 MHz. The rotational temperatures obtained range from ~100 to ~150 K in these HMCs. Single Gaussian fits performed to unblended lines show a marginal difference in the line peak velocities of the C2H5CN and CH3OCH3 lines, indicating a possible spatial separation between the region traced by the two molecules. On the other hand, neither the linewidths nor the rotational temperatures and column densities confirm such a result. By comparing the abundance ratio of the pair C2H5CN/C2H3CN with the predictions of theoretical models, we derive that the age of our cores ranges between 3.7 and 5.9x10^{4} yrs. The abundances of C2H5CN and C2H3CN are strongly correlated, as expected from theory which predicts that C2H3CN is formed through gas phase reactions involving C2H5CN. A correlation is also found between the abundances of C2H3CN and CH3OCH3, and C2H5CN and CH3OCH3. In all tracers the fractional abundances increase with the H_2 column density while they are not correlated with the gas temperature. ", "machine_abstract": "We present the results of an unbiased survey for N-bearing species (NH3, N2H+) and O-bearing species (H2O, OH, CH3OH, HCO+, H2S) toward two massive star forming regions associated with high-mass protostars. The observations were carried out using the Submillimeter Array at 1.4 mm wavelength. We detected all these species except NH3 towards both sources. In addition to detections we also made upper limits on some other species like SO2, SiO, HNCO etc.. Our main findings are as follows:  -The abundance ratios between different species show that H2O is more abundant than any other molecule by several orders of magnitude.  -The derived column densities suggest that the physical conditions vary significantly across the observed region. -The chemical composition of the gas shows significant differences between the two sources. -The comparison of our data with previous studies suggests that there may be multiple evolutionary stages within each source.", "paraphrased_abstract": "And we have compared the results with the other studies to show that there are many evolutionary stages in each of the sources. We are therefore reporting the results of a comprehensive survey of the N-bearing (NH3) and O-bearing species (H2O, OH, CH3OH, H2S) toward two massive regions associated with a protostar. The data we have collected indicate that the total number of species is many orders of magnitude greater than any other. The abundance ratios of the various species indicate that H2O is in the highest abundance among the other molecules by several orders of magnitude. The physical properties of the gaseous materials show that the conditions are significantly different between the two sources. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1603, "title": "Measuring energy dependent polarization in soft gamma-rays using Compton scattering in PoGOLite", "abstract": "  Linear polarization in X- and gamma-rays is an important diagnostic of many astrophysical sources, foremost giving information about their geometry, magnetic fields, and radiation mechanisms. However, very few X-ray polarization measurements have been made, and then only mono-energetic detections, whilst several objects are assumed to have energy dependent polarization signatures. In this paper we investigate whether detection of energy dependent polarization from cosmic sources is possible using the Compton technique, in particular with the proposed PoGOLite balloon-experiment, in the 25-100 keV range. We use Geant4 simulations of a PoGOLite model and input photon spectra based on Cygnus X-1 and accreting magnetic pulsars (100 mCrab). Effective observing times of 6 and 35 hours were simulated, corresponding to a standard and a long duration flight respectively. Both smooth and sharp energy variations of the polarization are investigated and compared to constant polarization signals using chi-square statistics. We can reject constant polarization, with energy, for the Cygnus X-1 spectrum (in the hard state), if the reflected component is assumed to be completely polarized, whereas the distinction cannot be made for weaker polarization. For the accreting pulsar, constant polarization can be rejected in the case of polarization in a narrow energy band with at least 50% polarization, and similarly for a negative step distribution from 30% to 0% polarization. ", "machine_abstract": "PoGOLite is an experiment to measure the degree and angle of linear polarization for photons with energies between 100 MeV and 1 GeV, which are emitted by astrophysical sources such as pulsars or active galactic nuclei (AGN). The measurement principle relies on Compton scattering off electrons bound into atoms inside a scintillator crystal. In this work we present results obtained during commissioning runs at the Paul Scherrer Institute (PSI) in Switzerland. We show that the detector response function can be described well within statistical uncertainties by Monte Carlo simulations based on Geant4. Furthermore, we demonstrate how the measured data can be used to extract information about the source's polarization properties. Finally, we discuss possible systematic effects related to the experimental setup. Keywords: Polarization measurements; Gamma-ray polarimetry; PoGOLite Experiment; Scintillation detectors; Compton scattering; Commissioning", "paraphrased_abstract": "I present a simulation of the detector response function which is the basis of the present work at the PSP Institute in Switzerland. Its purpose is to measure the polarisation of photons whose amplitudes range from 100 to one volt, in which pulsars or active galactic nuclei are generated. The principle of the experiment is that of capturing the polarization from atoms bound to atoms in a scintillator crystal. We have thus obtained results from the trial run at the PSP Institute in Switzerland. The results of the experiment have been presented, obtained from the trials of the PSP Institute, Switzerland. The PSP Institute commissioning of the experiment is based on the principle of the Compton scattering of the electrons bound to atoms within a scintillator crystal. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3448, "title": "Finite Euler products and the Riemann Hypothesis", "abstract": "  We show that if the Riemann Hypothesis is true, then in a region containing most of the right-half of the critical strip, the Riemann zeta-function is well approximated by short truncations of its Euler product. Conversely, if the approximation by products is good in this region, the zeta-function has at most finitely many zeros in it. We then construct a parameterized family of non-analytic functions with this same property. With the possible exception of a finite number of zeros off the critical line, every function in the family satisfies a Riemann Hypothesis. Moreover, when the parameter is not too large, they have about the same number of zeros as the zeta-function, their zeros are all simple, and they \"repel\". The structure of these functions makes the reason for the simplicity and repulsion of their zeros apparent and suggests a mechanism that might be responsible for the corresponding properties of the zeta-function's zeros. Computer evidence suggests that the zeros of functions in the family are remarkably close to those of the zeta-function (even for small values of the parameter), and we show that they indeed converge to them as the parameter increases. Furthermore, between zeros of the zeta-function, the moduli of functions in the family tend to twice the modulus of the zeta-function. Both assertions assume the Riemann Hypothesis. We end by discussing analogues for other L-functions and show how they give insight into the study of the distribution of zeros of linear combinations of L-functions. ", "machine_abstract": "The main result is that if we have an infinite product of finite number fields, then it can be written as a sum over primes in those number fields.  This theorem has been used to prove many results about prime numbers including the Prime Number Theorem for arithmetic progressions (PNTAP) which states that the density of primes in any arithmetic progression equals 1 / li(n), where n is the common difference between consecutive terms of the progression.   The PNTAP was first proved by Chebyshev using analytic methods but this proof does not give information on how large the primes are or what their distribution looks like.    We will also discuss some applications of our theorem such as proving the existence of infinitely many primes with certain properties. In this lecture I will talk about my recent work on finite Euler products and its connection to the Riemann hypothesis. First let me define what a finite Euler product is:  Let K be a number field, i.e., a finite extension of Q. Then we say that f = \\prod_{P \\in S}f_P$ is a finite Euler product over $K$ iff:   $f_P$ is a rational function in $P - 1$, $|S| < \\infty$,   $f$ converges absolutely for all $\\Re(s) > 0$,  If $s = \\sigma + it$ satisfies $\\sigma > 1$, then there exists a constant $C > 0$ so that: $$ |f(\\sigma+it)| \\leq C t^{-\\sigma}$$ Here $S$ denotes the set of primes in $K$. Now let's see why these conditions make sense intuitively. For example, if you take $K = \\mathbb{Q}(\\sqrt{2})$, then the only prime in $K$ is 2. So if we want to write down a finite Euler product over $\\mathbb{Q}(\\sqrt{2}) = \\{a + b \\sqrt{2}\\mid a,b \\in \\mathbb{Q}\\}$, then we should just look at the factorization of polynomials into irreducible factors modulo 2:  $$\\prod_{P\\in S}f_P = \\prod_{n=0}^{\\infty} \\left(1 - \\frac{x^{2^n}}{2^{n+1}}\\right).$$", "paraphrased_abstract": "So if you take K as a digit, it is a digit, it is a digit, it is a digit, and there are digits in it, in the form of digits, in the form of a number, and you think it is a digit, you can write the number 'K' as a digit. Now let's say that if we write the digit 'P' on K, then the digit is 'P', if: 'P'''n=0 infty', 'P'''infty', 'p''is the rational function in P - 1, 'S' infty', 'P' 'n'''' infty', 'p' i'' = 'c' 'c', 'c' = 'c'-c'' - 'c' = 'c' 'c' t' - 'c' 'c' 't' 'c' t' 'c' 'c' 'c' 't' 'c' 'c' 'c", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0697, "title": "Higher order antibunching in intermediate states", "abstract": "  Since the introduction of binomial state as an intermediate state, different intermediate states have been proposed. Different nonclassical effects have also been reported in these intermediate states. But till now higher order antibunching or higher order subpoissonian photon statistics is predicted only in one type of intermediate state, namely shadowed negative binomial state. Recently we have shown the existence of higher order antibunching in some simple nonlinear optical processes to establish that higher order antibunching is not a rare phenomenon (J. Phys. B 39 (2006) 1137). To establish our earlier claim further, here we have shown that the higher order antibunching can be seen in different intermediate states, such as binomial state, reciprocal binomial state, hypergeometric state, generalized binomial state, negative binomial state and photon added coherent state. We have studied the possibility of observing the higher order subpoissonian photon statistics in different limits of intermediate states. The effect of different control parameters have also been studied in this connection and it has been shown that the depth of nonclassicality can be tuned by controlling various physical parameters. ", "machine_abstract": "We study the second-order correlation function for an atom interacting with two modes of light, one resonant and another off-resonant to atomic transition frequency. We show that higher order antibunching can be observed when the atom is initially prepared in an excited state or ground state superposition. The effect is more pronounced if the initial state has some population on the excited state. This phenomenon may have applications in quantum information processing.     Introduction:-In recent years there has been considerable interest in studying nonclassical properties of radiation fields generated by atoms [1] . In particular, it was shown that the photon statistics of such systems are governed by the first-order coherence function g (1) (\u03c4) [2] , which describes bunching behavior at short times and anti-bunching at longer times [3] . It is well known that this property arises due to destructive interference between different pathways leading to emission of photons [4] . Recently, several authors studied the effects of spontaneous emission on the second-order correlation functions [5] - [8] . They showed that the presence of spontaneous emission leads to sub-Poissonian statistics [6]-[8] . However, these studies were restricted only to the case where the atom interacts with a single mode of field. On the other hand, many experiments involving atoms interacting simultaneously with multiple modes of electromagnetic field have also been performed [9] - [11] . For example, in Ref. [10] , the authors investigated the influence of vacuum fluctuations on the fluorescence spectrum of a three-level system driven by two laser beams. In addition, they found that the intensity noise of the emitted light depends strongly on the relative phase difference between the driving lasers. Motivated by these experimental results we consider here the problem of calculating the second-order correlation function of an atom interacting simultaneously with two modes of light [12] .", "paraphrased_abstract": "But these studies were limited to one mode of interaction; i.e., only to one mode of interaction. Other studies, however, have mainly focused on the atoms' interaction with multiple modes of electromagnetic radiation. Several studies, including that of Ref. 1, investigated the effect of vacuum changes on the fluorescence spectrum of a three-level system driven by two lasers. The effect of the vacuum is strongly influenced by the relative phase difference between the lasers. We have recently discovered the effects of spontaneous emission on the second-order correlation function of an atom, and we have discovered that this is the result of an interaction between two types of electromagnetic radiation. It is well known that the entanglement of photons is the result of a destructive interference of different pathways that lead to the emission of photons. However, these studies are limited to atoms interacting with only a single type of electromagnetic radiation. Some studies have been performed on atoms interacting with different types of electromagnetic fields, and some of them have been performed in a single mode of the electromagnetic field. The latter have been studied in detail. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0502, "title": "Relating two standard notions of secrecy", "abstract": "  Two styles of definitions are usually considered to express that a security protocol preserves the confidentiality of a data s. Reachability-based secrecy means that s should never be disclosed while equivalence-based secrecy states that two executions of a protocol with distinct instances for s should be indistinguishable to an attacker. Although the second formulation ensures a higher level of security and is closer to cryptographic notions of secrecy, decidability results and automatic tools have mainly focused on the first definition so far.   This paper initiates a systematic investigation of the situations where syntactic secrecy entails strong secrecy. We show that in the passive case, reachability-based secrecy actually implies equivalence-based secrecy for digital signatures, symmetric and asymmetric encryption provided that the primitives are probabilistic. For active adversaries, we provide sufficient (and rather tight) conditions on the protocol for this implication to hold. ", "machine_abstract": "We relate the notion of information-theoretic security to that of computational indistinguishability, and show how this relationship can be used in practice for proving security properties of cryptographic protocols.  We also present an algorithm which transforms any protocol secure against passive adversaries into one secure against active ones (under some reasonable assumptions). The transformation preserves all communication complexity measures such as round-complexity or number of messages exchanged between parties.   Finally we give examples where our approach is useful by applying it to prove security of several well-known protocols. In recent years there has been much interest in developing techniques for analyzing the security of cryptographic protocols. One important goal is to develop tools for showing that certain protocols are secure with respect to various definitions of security. For example, many researchers have studied the problem of transforming a protocol secure only against passive attackers into one secure against active attackers. This problem was first considered by Canetti et al., who gave a general solution based on universal composability [Can01] . However their method does not preserve communication complexity measures like round-complexity or message-exchange patterns. Subsequently, Micali proposed another transformation [MIC02] , but his technique requires additional assumptions about the underlying primitives being used.", "paraphrased_abstract": "For example, many researchers have studied the problem of transforming a protocol that is secure only against passive attackers into a secure one that is active against active attackers. This problem was first proposed by Canetti et al., who first presented a general solution, which was based on universal composability. But the method was not able to preserve all the complexity measures of communication, like roundness and number of messages exchanged between parties. Micali, however, formulated another approach, but this one required additional assumptions for the underlying primitives. In our approach we introduce the concept of information-theoretic security to the idea of computational indistinguishability and show how it can be applied in practice to prove the security of cryptographic protocols. We compared the idea of information-theoretic security with the idea of computational indistinguishability, and show how this relationship can be applied to the security of cryptographic systems. We discuss the application of information-theoretic security to the theory of computational indistinguishability, and demonstrate how this can be used in practice to prove the security of a variety of cryptographic systems. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0298, "title": "J1128+592: a highly variable IDV source", "abstract": "  Short time-scale radio variations of compact extragalactic radio quasars and blazars known as IntraDay Variability (IDV) can be explained in at least some sources as a propagation effect; the variations are interpreted as scintillation of radio waves in the turbulent interstellar medium of the Milky Way. One of the most convincing observational arguments in favor of a propagation-induced variability scenario is the observed annual modulation in the characteristic time scale of the variation due to the Earth's orbital motion. So far there are only two sources known with a well-constrained seasonal cycle. Annual modulation has been proposed for a few other less well-documented objects. However, for some other IDV sources source-intrinsic structural variations which cause drastic changes in the variability time scale were also suggested. J1128+592 is a recently discovered, highly variable IDV source. Previous, densely time-sampled flux-density measurements with the Effelsberg 100-m radio telescope (Germany) and the Urumqi 25-m radio telescope (China), strongly indicate an annual modulation of the time scale. The most recent 4 observations in 2006/7, however, do not fit well to the annual modulation model proposed before. In this paper, we investigate a possible explanation of this discrepancy. ", "machine_abstract": "We report on the discovery and study of J1128+592, an intraday variable (IDV) radio quasar at redshift z = 1.8. The object was found in our search for new IDVs using data obtained with the Very Large Array (VLA). We have monitored this source over two epochs separated by one year to look for variability on timescales ranging between 10 minutes and several days. Our results show that it is a strong IDV source which varies up to 50% peak-to-peak amplitude on time scales as short as 20 min. This makes J1128+592 one of the most rapidly varying quasars known so far. In addition we find evidence for long-term variations on timescales longer than 100 days. These are likely due to interstellar scintillation caused by density fluctuations along the line-of-sight towards the source. Finally, we present optical spectroscopy taken with the Keck telescope showing broad emission lines typical of quasars.", "paraphrased_abstract": "The goal of this research was to find out a new source of IDVs. We used the VLA to look for the source, and we found that it was a very strong IDV, which varies in the range of about 20 Hz to about 60 Hz. It was the source of the most rapid and pronounced oscillations, which are at least 50% in amplitude on times of 20 minutes. This is the largest oscillation known so far. Our data are from a search for the new IDV, which was conducted with the VLA. We have been following it for a year and a half, and have compared its intensity to that of the VLA. This source is a very strong oscillation, with a maximum of about 60 Hz peak to peak, which makes it the most rapid oscillation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3436, "title": "The Minimum Gap-opening Planet Mass in an Irradiated Circumstellar Accretion Disk", "abstract": "  We consider the minimum mass planet, as a function of radius, that is capable of opening a gap in an $\\alpha$-accretion disk. We estimate that a half Jupiter mass planet can open a gap in a disk with accretion rate $\\dot M \\la 10^{-8} M_\\odot$/yr for viscosity parameter $\\alpha =0.01$, and Solar mass and luminosity. The minimum mass is approximately proportional to $\\dot M^{0.48} \\alpha^{0.8} M_*^{0.42} L_*^{-0.08}$. This estimate can be used to rule out the presence of massive planets in gapless accretion disks. We identify two radii at which an inwardly migrating planet may become able to open a gap and so slow its migration; the radius at which the heating from viscous dissipation is similar to that from stellar radiation in a flared disk, and the radius at which the disk has optical depth order 1 in a self-shadowed disk. If a gap opening planet cuts off disk accretion allowing the formation of a central hole or clearing in the disk then we would estimate that the clearing radius would approximately be proportional to the stellar mass. ", "machine_abstract": "We present the results of hydrodynamic simulations that show how planets open gaps in circumstellar disks and determine their minimum mass for gap opening as a function of disk viscosity, surface density profile slope, and irradiation by the central star.  We find that the planet's ability to open a gap is determined primarily by its accretion rate onto the planet (which depends on the planet's mass) rather than by its luminosity or temperature. The critical planet mass decreases with increasing disk viscosity because higher viscosities lead to more rapid angular momentum transport across the planet's orbit which reduces the planet's ability to accrete gas. For fixed viscosity, we find that the critical planet mass increases with decreasing surface density power law index because lower values of this parameter correspond to steeper radial profiles of the disk's surface density which leads to stronger gravitational torques exerted by the planet. Finally, we find that the planet's critical mass decreases with increasing stellar irradiation due to increased temperatures at small radii where most of the torque is generated.", "paraphrased_abstract": "The critical mass decreases with the elongation of the disk and decreases with the elongation of the surface of the disk, and the gravity is stronger in the lower part of the disk, and the critical mass decreases with the increase of the elongation of the central star. In fact, the critical mass decreases with the elongation of the disk and the height of the radii. It is observed that the critical mass decreases with the elongation of the surface, and the lower part of the power law increases the height of the surface, which increases the gravitational force. The critical mass increases with the decreasing of the density of the disk, for higher density leads to a steeper radial density, and the elongation of the surface leads to a stronger gravitational pull. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.448, "title": "Cassini UVIS Observations of the Io Plasma Torus. IV. Modeling Temporal and Azimuthal Variability", "abstract": "  In this fourth paper in a series, we present a model of the remarkable temporal and azimuthal variability of the Io plasma torus observed during the Cassini encounter with Jupiter. Over a period of three months, the Cassini Ultraviolet Imaging Spectrograph (UVIS) observed a dramatic variation in the average torus composition. Superimposed on this long-term variation, is a 10.07-hour periodicity caused by an azimuthal variation in plasma composition subcorotating relative to System III longitude. Quite surprisingly, the amplitude of the azimuthal variation appears to be modulated at the beat frequency between the System III period and the observed 10.07-hour period. Previously, we have successfully modeled the months-long compositional change by supposing a factor of three increase in the amount of material supplied to Io's extended neutral clouds. Here, we extend our torus chemistry model to include an azimuthal dimension. We postulate the existence of two azimuthal variations in the number of super-thermal electrons in the torus: a primary variation that subcorotates with a period of 10.07 hours and a secondary variation that remains fixed in System III longitude. Using these two hot electron variations, our model can reproduce the observed temporal and azimuthal variations observed by Cassini UVIS. ", "machine_abstract": "We present new results on temporal variability in the ultraviolet (UV) emission observed by Cassini's Ultraviolet Imaging Spectrograph (UVIS). The data were obtained during orbits that crossed the equatorial plane at distances between 1,000 km and 5,500 km above Jupiter's surface. We find that the brightness distribution is highly variable both temporally and azimuthally with respect to Jupiter's magnetic field direction. This behavior can be explained if we assume that the plasma torus consists of multiple co-rotating structures which are not uniformly distributed around the planet but instead form an arc-like structure centered near the magnetic equator. These observations provide further evidence for the existence of such arcs as predicted by theoretical models. They also suggest that these arcs may have significant effects on the global magnetosphere-ionosphere coupling processes.     Keywords: Jupiter, Saturn, Magnetospheres, Plasmas", "paraphrased_abstract": "A third method of observing this plasma structure is presented. It shows that the sphere of liquid is composed of several co-rotating structures that do not spread all around the planet but form an arc around the magnetic equator. These co-rotating structures, which are not necessarily circular, are distributed around the planet in a circular form, which is a great effect on the global magnetosphere-ionosphere kinship. This result is confirmed by the observations of the ultraviolet (UV) emission from Cassini\u2019s ultraviolet spectrometer, which has been measuring the equator at a distance of 1,000 km and 5,500 km above the surface of Jupiter. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2088, "title": "Laplacian instability of planar streamer ionization fronts - an example of pulled front analysis", "abstract": "  Streamer ionization fronts are pulled fronts propagating into a linearly unstable state; the spatial decay of the initial condition of a planar front selects dynamically one specific long time attractor out of a continuous family. A transverse stability analysis has to take these features into account. In this paper we introduce a framework for this transverse stability analysis, involving stable and unstable manifolds in a weighted space. Within this framework, a numerical dynamical systems method for the calculation of the dispersion relation as an eigenvalue problem is defined and dispersion curves for different values of the electron diffusion constant and of the electric field ahead of the front are derived. Numerical solutions of the initial value problem confirm the eigenvalue calculations. The numerical work is complemented with analytical expressions for the dispersion relation in the limit of small and large wave numbers and with a fit formula for intermediate wave numbers. This empirical fit supports the conjecture that the smallest unstable wave length of the Laplacian instability is proportional to the diffusion length that characterizes the leading edge of the pulled ionization front. ", "machine_abstract": "We study the stability properties of planar streamers in air, which are ionized regions that propagate into un-ionized gas under the influence of an applied electric field. We show how to use the so-called \"pulled front approach\" to derive a nonlinear evolution equation for the shape of such fronts and then analyze this equation using standard techniques from dynamical systems theory. In particular we find that there is a critical value of the applied voltage beyond which the planar solution becomes unstable with respect to small perturbations. This result explains why it has been so difficult to observe stable propagation of streamers at high voltages experimentally. The results presented here should be relevant not only to atmospheric discharges but also to other physical situations where similar phenomena occur, e.g., combustion flames or chemical waves on catalytic surfaces. Streamers are thin channels filled with highly conducting plasma that can form when strong electric fields are present between two electrodes immersed in non-conducting gases [1] . They have attracted considerable interest over many years because they play important roles in various applications including lightning [2] , sprites [3] , and high-voltage switches [4] . In recent years much progress has been made towards understanding their formation mechanisms [5, 6, 7, 8] as well as their dynamics [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96,", "paraphrased_abstract": "We investigate the stability of planar streamers in air, which are ionized regions which propagate into non-ionized gas when an applied electric field is applied. We find that the applied voltage is of the smallest value and that the solution is unstable even at very small perturbations. The results of this work are not limited to the atmosphere but to other physical phenomena as well, e.g., in the form of sparks, sparks in a circuit, a flame, or a chemical wave on a catalytic surface. In recent years, much research has been made into their formation and dynamics, as well as their dynamics. Thus, they have been studied to great detail, in particular with regard to the formation of ionized regions of gases that have been ionized by strong electric fields. The study shows how to use the so-called \u2018pulled front approach\u2019 to derive a nonlinear evolution equation of their shape, and apply it to the analysis of this equation by standard techniques of dynamical systems theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1798, "title": "A unified shear-thinning treatment of both film thickness and traction in EHD", "abstract": "  A conclusive demonstration has been provided that the nature of the shear-thinning, that affects both film thickness and traction in EHL contacts, follows the ordinary power-law rule that has been described by many empirical models of which Carreau is but one example. This was accomplished by accurate measurements in viscometers of the shear response of a PAO that possesses a very low critical stress for shear-thinning and accurate measurements in-contact of film thickness and traction under conditions which accentuate the shear-thinning effect. The in-contact central film thickness and traction were entirely predictable from the rheological properties obtained from viscometers using simple calculations. These data should be invaluable to researchers endeavoring to accurately simulate Hertz zone behavior since the shear-thinning rheology is extensively characterized and accurate in-contact data are available to test. In addition, a new model has been introduced that may be useful for the rheological characterization of mixtures. ", "machine_abstract": "We present an extension to the classical lubrication theory that accounts for shear thinning effects on both the fluid viscosity and the friction coefficient, which are assumed to be functions of local pressure only.  The resulting model is applied to study the effect of non-Newtonian rheology on the elastohydrodynamic (EHD) behavior of a circular contact between two elastic solids with different surface roughnesses under pure rolling conditions.  We show that the presence of shear thinning leads to significant changes in the predicted load-carrying capacity as well as the distribution of the normal stress across the contact area compared to those obtained using Newtonian models.  In particular, we find that the maximum value of the dimensionless pressure increases significantly when the fluids exhibit strong shear thinning characteristics.  Moreover, our results indicate that the inclusion of shear thinning effects can lead to substantial reductions in the magnitude of the dimensionless tangential stresses at the centerline of the contact region.  Finally, it should be noted that the proposed theoretical framework may also be used to investigate other important phenomena such as thermal effects or mixed lubrication regimes.", "paraphrased_abstract": "In the future, however, the proposed theory may be extended to other important aspects, such as the thermoelectric effect or the mixing of lubricants. Moreover, the results will demonstrate that the presence of shear thinning leads to significant changes in the predicted load-carrying capacity and the normal stress distribution in the contact area. In addition, the effects of shear thinning can reduce the magnitude of the dimensionless tangential stresses in the center of the contact. In this paper, we introduce a new formulation of lubrication based on non-Newtonian rheology, and present an extension of the classical lubrication theory to account for shear thinning of the fluid viscosity and friction coefficient, which are only functions of pressure. We show that, when the fluid is not thinning, the maximum dimensionless pressure increases a great deal, and, moreover, the magnitude of dimensionless tangential stresses decreases. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.104, "title": "H$_2$D$^+$ line emission in Proto-Planetary Disks", "abstract": "  %Context: {Previous studies have indicated that the 372.4 GHz ground transition of ortho-H$_2$D$^+$ might be a powerful probe of Proto-Planetary Disks. The line could be especially suited for study of the disk mid-plane, where the bulk of the mass resides and where planet formation takes place.} %Aims: {Provide detailed theoretical predictions for the line intensity, profile and maps expected for representative disk models.} %Methods: {We determine the physical and chemical structure of the disks from the model developed by Ceccarelli & Dominik (2005). The line emission is computed with the new radiative transfer method developed recently by Elitzur & Asensio Ramos (2006).} %Results: {We present intensity maps convolved with the expected ALMA resolution, which delineate the origin of the H$_2$D$^+$ 372.4 GHz line. In the disk inner regions, the line probes the conditions in the mid-plane out to radial distances of a few tens of AU, where Solar-like planetary systems might form. In the disk outermost regions, the line originates from slightly above the mid-plane. When the disk is spatially resolved, the variation of line profile across the image provides important information about the velocity field. Spectral profiles of the entire disk flux show a double peak shape at most inclination angles.} %Conclusions: {Our study confirms that the 372.4 GHz H$_2$D$^+$ line provides powerful diagnostics of the mid-plane of Proto-Planetary Disks. Current submillimeter telescopes are capable of observing this line, though with some difficulties. The future ALMA interferometer will have the sensitivity to observe and even spatially resolve the H$_2$D$^+$ line emission.} ", "machine_abstract": "We present new observations of the H$_2$D$^+(1s)$ line at $230.538 nm$ toward two young protoplanetary disks, TW Hydrae and HD 163296.  The data were obtained with the Vacuum Tower Telescope (VTT), located on Roque de los Muchachos Observatory, La Palma, Canaries Islands. We have detected this line for both sources using an echelle grating centered around 230nm. For TW Hydrae we find that the line is blueshifted by about -40 km/s compared to the systemic velocity of the source. This result suggests that there are outflows or winds associated with the disk. In addition, we detect several other lines such as He I $\\lambda = 587.6 nm$, O I $\\lambda = 630.0 nm$, C II $\\lambda = 723.3 nm$, Si III $\\lambda = 455.4 nm$, Fe II $\\lambda = 468.7 nm$, and Mg II $\\lambda = 448.2 nm$. These lines can be used to determine physical conditions within the disk atmosphere.", "paraphrased_abstract": "Besides this, we find many other lines: H-I-Lambda = -580.0 nm, O-I-Lambda = - 630.0 nm, C-I-Lambda = -723.3 nm, Si-III - 455.4 nm, Fe-III - 468.2 nm, and Mg-III - 448.2 nm. We spotted this line at the price of 230 nm. We also spotted several other lines, namely: He I-Lambda = 587.6 nm, O I-Lambda = 6636.0 nm, C II - 6700.0 nm, C III - 468.7 nm, Fe-III - 468.2 nm, and Mg-III - 448.2 ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1324, "title": "Identifying Dark Matter Burners in the Galactic center", "abstract": "  If the supermassive black hole (SMBH) at the center of our Galaxy grew adiabatically, then a dense \"spike\" of dark matter is expected to have formed around it. Assuming that dark matter is composed primarily of weakly interacting massive particles (WIMPs), a star orbiting close enough to the SMBH can capture WIMPs at an extremely high rate. The stellar luminosity due to annihilation of captured WIMPs in the stellar core may be comparable to or even exceed the luminosity of the star due to thermonuclear burning. The model thus predicts the existence of unusual stars, i.e. \"WIMP burners\", in the vicinity of an adiabatically grown SMBH. We find that the most efficient WIMP burners are stars with degenerate electron cores, e.g. white dwarfs (WD) or degenerate cores with envelopes. If found, such stars would provide evidence for the existence of particle dark matter and could possibly be used to establish its density profile. In our previous paper we computed the luminosity from WIMP burning for a range of dark matter spike density profiles, degenerate core masses, and distances from the SMBH. Here we compare our results with the observed stars closest to the Galactic center and find that they could be consistent with WIMP burners in the form of degenerate cores with envelopes. We also cross-check the WIMP burner hypothesis with the EGRET observed flux of gamma-rays from the Galactic center, which imposes a constraint on the dark matter spike density profile and annihilation cross-section. We find that the EGRET data is consistent with the WIMP burner hypothesis. New high precision measurements by GLAST will confirm or set stringent limits on a dark matter spike at the Galactic center, which will in turn support or set stringent limits on the existence of WIMP burners at the Galactic center. ", "machine_abstract": "We present an analysis of data obtained with the Chandra X-ray Observatory to search for evidence that dark matter particles are being annihilated by collisions at the Galactic Center (GC). We find no significant excess emission above backgrounds, and we set upper limits on the fluxes of gamma rays produced by such processes. These results can be used to constrain models of particle physics beyond the Standard Model which predict the existence of new stable massive particles whose interactions produce gamma-ray signatures similar to those expected from dark matter annihilation products.     The GC is one of the most promising targets for indirect searches for dark matter because it contains large amounts of dark matter concentrated within a small volume. In addition, there may exist astrophysical objects near the GC capable of accelerating cosmic ray protons up to energies high enough to produce gamma-rays through pion decay or inverse Compton scattering off ambient photons. However, these sources could also contribute significantly to any observed gamma-ray signal associated with dark matter annihilation.", "paraphrased_abstract": "The GC, as a reference point for indirect research, has one of the most promising properties: there are a lot of dark matter concentrated in a small space, and the astrophysical objects, near the GC, are capable of accelerating the cosmic rays and protons to a level such as that at the rate of pion decay or by inverse Compton scattering, but these sources are not insignificant to the gamma rays that are emitted by the annihilation of the dark matter. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0744, "title": "Quantifying social group evolution", "abstract": "  The rich set of interactions between individuals in the society results in complex community structure, capturing highly connected circles of friends, families, or professional cliques in a social network. Thanks to frequent changes in the activity and communication patterns of individuals, the associated social and communication network is subject to constant evolution. Our knowledge of the mechanisms governing the underlying community dynamics is limited, but is essential for a deeper understanding of the development and self-optimisation of the society as a whole. We have developed a new algorithm based on clique percolation, that allows, for the first time, to investigate the time dependence of overlapping communities on a large scale and as such, to uncover basic relationships characterising community evolution. Our focus is on networks capturing the collaboration between scientists and the calls between mobile phone users. We find that large groups persist longer if they are capable of dynamically altering their membership, suggesting that an ability to change the composition results in better adaptability. The behaviour of small groups displays the opposite tendency, the condition for stability being that their composition remains unchanged. We also show that the knowledge of the time commitment of the members to a given community can be used for estimating the community's lifetime. These findings offer a new view on the fundamental differences between the dynamics of small groups and large institutions. ", "machine_abstract": "We present an approach to quantify the evolutionary dynamics of social groups, based on their internal structure and interactions with other groups. We use this method to study how cooperation evolves in structured populations composed by different types of agents (cooperators or defectors). Our results show that cooperator clusters are more likely to survive than isolated ones when they interact with defector clusters. This effect is stronger for larger cluster sizes. The survival probability decreases as the number of competing clusters increases. Finally we find that the presence of cooperator clusters can lead to the extinction of all defector clusters if there exists at least one large enough cooperator cluster. In recent years it has been shown that cooperative behavior among unrelated individuals may evolve spontaneously even under strong competition between selfish strategies [1-3]. However, these studies have focused mainly on well-mixed populations where each individual interacts equally often with every other member of the population [4] . On the contrary, real-world systems such as biological communities [5] , human societies [6] , or computer networks [7] usually exhibit some degree of spatial organization [8] . In order to understand better the role played by space in the emergence of cooperation, several authors have studied the evolution of cooperation in spatially-structured populations [9] . These works typically consider two main classes of models: lattice-based [10] and agent-based [11] . Lattice-based models assume that the whole population lives on a regular grid [12] while agent-based models allow for arbitrary topologies [13] . Despite the differences between both approaches, most previous work agrees that the topology of the underlying network plays a crucial role in determining whether cooperation will be able to thrive [14] .", "paraphrased_abstract": "On the contrary, real societies, human societies, and computer networks, usually have some sort of spatial organization in them. This work focuses mainly on the well-mixed, well-mixed, well-formed populations, in which each individual is closely related to the others, and so on. However, it has been found that cooperative activities can be spontaneously cultivated even in a strong competition. In the last few years, however, many studies have shown that cohesion can be established in the group of unrelated people even when they are compelled by a certain selfish strategy. It has also been found that when a group of cohesion is joined together with another, the survival probability increases. The results show that cooperation is more probable in the group of cohesions when they are joined by a group of defectors than in the absence of defectors. In order to better understand the role of space in the emergence of cooperation, many authors have studied the evolution of cohesion in complex populations. These studies are usually based on two main categories of analysis: the -Lattice and the -Lattice, which assume that the whole population lives on a regular grid, and the -Lattice, which is based on arbitrary topologies, is based on a very important principle. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3898, "title": "Quantum-like Representation of Macroscopic Configurations", "abstract": "  The aim of this paper is to apply a contextual probabilistic model (in the spirit of Mackey, Gudder, Ballentine) to represent and to generalize some results of quantum logic about possible macroscopic quantum-like (QL) behaviour. The crucial point is that our model provides QL-representation of macroscopic configurations in terms of complex probability amplitudes -- wave functions of such configurations. Thus, instead of the language of propositions which is common in quatum logic, we use the language of wave functions which is common in the conventional presentation of QM. We propose a quantum-like representation algorithm, QLRA, which maps probabilistic data of any origin in complex (or even hyperbolic) Hilbert space. On the one hand, this paper clarifyes some questions in foundations of QM, since some rather mystical quantum features are illustrated on the basis of behavior of macroscopic systems. On the other hand, the approach developed in this paper may be used e.g. in biology, sociology, or psychology. Our example of QL-representation of hidden macroscopic configurations can find natural applications in those domains of science. ", "machine_abstract": "We present an approach to the description of macroscopic configurations in terms of quantum states, which is based on the concept of entanglement entropy and its generalization for mixed states. We show that this representation allows one to obtain exact results for some statistical properties of systems with many degrees of freedom. In particular, we consider the problem of calculating the partition function of classical spin models at high temperatures. The proposed method can be used as a basis for constructing approximate algorithms for solving problems related to the calculation of thermodynamic characteristics of complex systems. Introduction. The main goal of statistical mechanics is to describe the behavior of macroscopic objects (for example, gases) by using microscopic information about their constituents (atoms). This task becomes especially difficult when dealing with large systems consisting of many particles or spins. For such cases, it is necessary to use approximations, since direct calculations are impossible due to the exponential growth of the number of possible microstates with increasing system size N . One of these approaches is the so-called mean-field approximation [1] , according to which each particle interacts only with all other particles simultaneously; i.e., the interaction between different pairs of particles is neglected. However, even within this simplified model, the calculation of the partition function Z = Tr exp(\u2212\u03b2H) (1) remains extremely complicated [2] . In recent years, there has been growing interest in developing new methods for describing macroscopic configurations in terms similar to those used in quantum physics [3] - [8] . These studies were inspired by the fact that both classical and quantum descriptions have certain common features [9] : they are formulated in terms of wave functions \u03c8(x), where x denotes either positions of particles or spins, respectively. Moreover, the evolution of these wave functions obeys the same Schr\u00f6dinger equation ih\u2202 t |\u03c8(t) = H|\u03c8(t) , where H is the corresponding Hamiltonian operator. It should also be noted that the density matrix \u03c1 = |\u03c8(t) \u03c8(t)| plays the role of a probability distribution in both theories [10] .", "paraphrased_abstract": "It is particularly difficult in the large systems, in which the number of particles and the number of spins is increasing. This is done by approximations, because the number of states in the system grows exponentially, and it is impossible to calculate directly, because the increasing size of the system grows exponentially. This is especially difficult when there are many particles and spins, and the corresponding equations are invariable. However, even in this simplified model the calculating function of the partition function of classical spins remains extremely complicated. We introduce a method to the computation of the thermodynamic properties of complex systems. In this way we will discuss the problems of calculating the thermodynamic properties of complex systems., - in which we will mention the density matrix  =  =    t, which is used in both theories to define the probability distribution. The main aim of statistical mechanics is to describe the behavior of macroscopic objects (for example, gases), by means of the microscopic information of atoms. In this method there are approximations, because the number of possible microstates is exponentially increasing, with N - increasing. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1852, "title": "Cooperative action in eukaryotic gene regulation: physical properties of a viral example", "abstract": "  The Epstein-Barr virus (EBV) infects more than 90% of the human population, and is the cause of several both serious and mild diseases. It is a tumorivirus, and has been widely studied as a model system for gene (de)regulation in human. A central feature of the EBV life cycle is its ability to persist in human B cells in states denoted latency I, II and III. In latency III the host cell is driven to cell proliferation and hence expansion of the viral population, but does not enter the lytic pathway, and no new virions are produced, while the latency I state is almost completely dormant. In this paper we study a physico-chemical model of the switch between latency I and latency III in EBV. We show that the unusually large number of binding sites of two competing transcription factors, one viral and one from the host, serves to make the switch sharper (higher Hill coefficient), either by cooperative binding between molecules of the same species when they bind, or by competition between the two species if there is sufficient steric hindrance. ", "machine_abstract": "The human genome contains thousands of enhancers, which are non-coding DNA sequences that can activate transcription at distant sites. Enhancer activity is often cell-type specific and regulated by the binding of sequence-specific transcription factors (TFs). However, how TFs cooperate to regulate enhancer function remains poorly understood. Here we show that cooperative interactions between two distinct classes of TFs play an important role in regulating enhancer activity during lytic infection with herpes simplex virus 1 (HSV-1), a large double-stranded DNA virus. We find that HSV-1 encodes three proteins - VP16, ICP4 and ICP27 - each containing multiple domains capable of interacting directly or indirectly with other cellular proteins. Using biophysical methods, we demonstrate that these three viral proteins interact cooperatively on a single promoter element to drive expression of genes required for efficient viral replication. Our results suggest that cooperative protein-protein interactions may be a general mechanism used by viruses to control host gene expression.", "paraphrased_abstract": "I show that a few molecules of the herpes simplex family of viruses (SSVs) have been assembled into a protein called VP16, VP4 and VP27, a protein which has multiple domains which bind to other proteins and are then integrated into a single promoter, to enable the synthesis of a single gene required for the rapidity of viral replication. We have found that a protein called VP16 is enriched in a few different domains, and the three proteins are linked in one way or another by their interaction with a single promoter. This, in combination with the TFs, was able to control the expression of the genes necessary for bacterial replication. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2832, "title": "Endogenous and exogenous dynamics of pressure fluctuations in an impinging entrained-flow gasifier", "abstract": "  This paper has been withdrawn by the authors due to a fatal error in the analysis. The manuscript was submitted to Chemical Engineering Science. To clarify the situation, we copy the main comment from an anonymous referee here: \"To my understanding, the authors analyze i = 1 ... 63 time series and calculate their mean and standard deviation. These time series correspond to individual, single ignition processes. Is this correct? If yes, these processes, as Fig. 3 shows very clearly, are not stationary, and the pressure difference (i.e., the signal) quickly decays to zero. In this case both the mean and the standard deviation are poorly defined, for example because both depend in a trivial fashion on the observation period T. I am not aware of any study (including those cited by the authors) which allows for any conclusion from such non-stationary signals. The results of Menezes and Barabasi are strictly only valid for stationary time series, and they cannot be applied at all in this case.\" We agree with this insightful comment that our data are not stationary and the method adopted in our manuscript does not apply. We do not see any possibility to correct this error and decide to withdraw it. We would like to thank gratefully the referee and apologize for any inconvenience caused by our oversight. ", "machine_abstract": "A mathematical model is developed to study the endogenous and exogenous dynamics associated with pressure fluctuations within an entrained flow gasifier, which are caused by the interaction between fluid mechanics and chemical reactions. The proposed model consists of two parts: one for describing the hydrodynamics inside the gasifier; another for representing the thermochemical processes occurring at different locations along the reactor axis. In order to investigate how these interactions affect the system stability, we perform linear analysis on both parts separately as well as their combined effect. It is found that the thermochemical process can be stabilized if the residence time of particles in the reaction zone is sufficiently large. However, this may not always be possible due to practical constraints such as high temperature requirement or limited space available. Finally, numerical simulations are performed using realistic operating conditions to verify our theoretical results. Pressure fluctuations have been observed experimentally in many types of gasifiers including bubbling bed, circulating fluidized bed (CFB), and entrained flow gasifiers [1] . These fluctuations often lead to unstable operation of the gasification systems [2] , resulting in poor quality syngas production [3] . The main cause of pressure fluctuation lies in the coupling between fluid mechanics and chemical kinetics [4] . For example, when the fuel feed rate increases suddenly, more reactants enter into the reaction zone causing higher temperatures there. This leads to faster chemical reactions and thus larger heat release rates. As a result, the local pressure rises rapidly. On the other hand, when the fuel feed decreases quickly, less reactants enter into the combustion chamber leading to lower temperatures and slower chemical reactions. Consequently, the local pressure drops sharply.", "paraphrased_abstract": "The study aimed to find the influence of the two on the stability of the gasifier. The model is composed of two parts: one of the hydrodynamics of the gasifier, and the other of the thermochemical reactions at different places along the axis of the reactor. To evaluate the effect of this, we developed the following equation. The entrainment of particles in the reaction zone is increased suddenly, and the temperature of the reactant increases, and the temperature increases with it. The pressure rises with the increase of the fuel. When the fuel is swollen, more reactants enter the reaction zone, and the thermochemical reactions are faster, the temperature of the reaction is lower, and the pressure falls. The model, which is developed in two parts, will be used to describe the hydrodynamics of the gasifier, and the thermochemical reactions to be performed at the different locations of the reactor. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0189, "title": "Diagnostic tools for 3D unstructured oceanographic data", "abstract": "  Most ocean models in current use are built upon structured meshes. It follows that most existing tools for extracting diagnostic quantities (volume and surface integrals, for example) from ocean model output are constructed using techniques and software tools which assume structured meshes. The greater complexity inherent in unstructured meshes (especially fully unstructured grids which are unstructured in the vertical as well as the horizontal direction) has left some oceanographers, accustomed to traditional methods, unclear on how to calculate diagnostics on these meshes. In this paper we show that tools for extracting diagnostic data from the new generation of unstructured ocean models can be constructed with relative ease using open source software. Higher level languages such as Python, in conjunction with packages such as NumPy, SciPy, VTK and MayaVi, provide many of the high-level primitives needed to perform 3D visualisation and evaluate diagnostic quantities, e.g. density fluxes. We demonstrate this in the particular case of calculating flux of vector fields through isosurfaces, using flow data obtained from the unstructured mesh finite element ocean code ICOM, however this tool can be applied to model output from any unstructured grid ocean code. ", "machine_abstract": "The increasing amount and complexity of oceanographic data requires new approaches to the analysis, visualization and interpretation of these datasets. In this work we present an interactive visual analytics system that allows users to explore large volumes of 3D oceanographic data in order to detect patterns and anomalies. The main goal is to provide scientists with powerful tools to analyze their data without having to be experts on computer graphics or visualization techniques. We use state-of-the-art volume rendering algorithms combined with advanced interaction techniques such as brushing, linking and querying. Our approach has been tested by using real-world oceanographic data sets collected during several research cruises around Europe. This article presents our results and discusses future directions. Oceanographers are increasingly collecting massive amounts of data about the oceans' physical properties (e.g., temperature, salinity) and biological processes (e.g., plankton blooms). These data can be used to study phenomena like global warming, pollution spreading, marine life migration etc.. However, analyzing and interpreting these huge volumes of data remains challenging due to its high dimensionality and complex relationships between variables. Interactive visual analytics systems have shown great potential in helping scientists discover hidden patterns and anomalies within their data [1] . They allow researchers to interactively explore their data through different views while simultaneously performing queries and applying filters [2] . In this work we present Diagnostic Tools for Unstructured Data (DUT), a novel visual analytics tool designed specifically for exploring large volumes of 3D oceanic data. DUT provides scientists with powerful tools to perform exploratory analyses of their data without requiring them to be experts on computer science or visualization techniques [3] , thus allowing them to focus more on the actual content of their data rather than how it should be presented [4] .", "paraphrased_abstract": "There are vast numbers of oceans, and the oceans are constantly gathering data about them\u2014temperature, salinity, and dissolved gases\u2014and the biological processes of these waters, such as global warming, the spread of ozone, the migration of sea life, and so on. This data is often used in studies to study global warming, pollution, marine life, and other things. This data, however, is often difficult to interpret and interpreted because of its high complexity and the complex relationships between variables. Our goal is to provide scientists with powerful tools for analyzing their data without needing to be experts in chemistry or visualization, and this will allow them to focus on the content of their data rather than on the presentation of the data. We present a new, interactive visualization system to explore large amounts of data, in order to identify the hidden patterns and anomalies. We apply a combination of high-performance volume rendering and a new set of interactions, such as brushing, linking and querying. In this work, we describe our results and present a future direction. The increasing amount and complexity of oceanographic data demands new approaches to analyzing and interpreting it. This new approach combines the latest and most advanced statistical techniques, such as blundering, tagging, combining, and interpolating the data into a single view, while at the same time also performing queries and applying filters. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3071, "title": "Converting genetic network oscillations into somite spatial pattern", "abstract": "  In most vertebrate species, the body axis is generated by the formation of repeated transient structures called somites. This spatial periodicity in somitogenesis has been related to the temporally sustained oscillations in certain mRNAs and their associated gene products in the cells forming the presomatic mesoderm. The mechanism underlying these oscillations have been identified as due to the delays involved in the synthesis of mRNA and translation into protein molecules [J. Lewis, Current Biol. {\\bf 13}, 1398 (2003)]. In addition, in the zebrafish embryo intercellular Notch signalling couples these oscillators and a longitudinal positional information signal in the form of an Fgf8 gradient exists that could be used to transform these coupled temporal oscillations into the observed spatial periodicity of somites. Here we consider a simple model based on this known biology and study its consequences for somitogenesis. Comparison is made with the known properties of somite formation in the zebrafish embryo . We also study the effects of localized Fgf8 perturbations on somite patterning. ", "machine_abstract": "The segmentation clock is an oscillatory gene regulatory network that controls the periodic formation of segments in vertebrates, such as vertebrae and ribs.  The segmentation clock consists of two interlocked feedback loops with different time scales; one loop involves genes Hes1-7 (Hairy/Enhancer-of-split related) which are expressed periodically at every cycle of the other loop involving genes Notch-Delta-Stat3-Gata6-8-Hes7.  In this study we show how these two loops can be combined to produce stable limit cycles for all parameter values using a mathematical model based on ordinary differential equations.  We also demonstrate how the periodicity of the segmentation clock can be converted into a spatial pattern by coupling it to a reaction-diffusion system describing the production of morphogens.  Finally, we use our results to explain why mutations in some components of the segmentation clock lead to skeletal abnormalities while others do not.", "paraphrased_abstract": "In addition, we present a mathematical model that can be derived from the differential equations and which can be applied to any number of variables. In this study, we show how the regularity of the segmentation can be made spatial by combining the periodicity of the segmentation with a spatial pattern of morphogenetics, a mechanism for the production of morphogenetics. The segmentation network is a synapse whose function is to generate a periodicity of segments in vertebrates and ribs. The segmentation network is composed of two interconnected loops that have different time scales. One loop involves the gene Hes7 (as the hair-binding hormone), which is expressed on each cycle at each cycle. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1786, "title": "Learning more from the Lorentz transformations", "abstract": "  Admitting the validity of Lorentz transformations for the space as time coordinates of the same event we derive their differential form in order to underline the correct prerequisites for the application of time and length contraction or dilation effects. Furthermore we quantify the simultaneity error occurring in the relativity theory. Having done this, we analyse the root cause of these effects and identify it with a finite phase velocity associated with the moving frame. We define this phase velocity by analogy to the de Broglie wave associated with a moving particle. Based on this construct we demonstrate that the phase of the de Broglie waves further extended for stationary particles is a relativistic invariant being the same for all corresponding observers. Also the phase of the electromagnetic waves transporting energy at light speed is a relativistic invariant. Therefore the universe and its matter / energy may be seen as a superposition of waves propagating such that their phase is the same for all corresponding observers. The wave phase may replace the time as an invariant and universal reference. ", "machine_abstract": "The author presents an alternative approach to teaching special relativity by using the concept of transformation groups and their generators, which are used in physics for describing symmetry properties of physical systems.  The main idea is that students can learn about the foundations of special relativity without having to deal with complicated mathematical concepts such as tensors or spinor fields. This article describes how this method works on examples related to the Lorentz transformations. It also discusses some possible applications of these ideas in other areas of physics. In particular, it shows how one can use them to explain the origin of gauge symmetries in quantum field theory. Special relativity (SR) has been taught at many universities since its discovery in 1905 [1] . However, despite numerous attempts [2] , there still exists no generally accepted way of introducing SR into undergraduate courses [3] . In recent years, several authors have proposed new approaches to teaching SR [4] - [8] . These methods usually involve presenting the basic principles of SR through simple experiments performed in different reference frames [9] - [11] . They often require only minimal knowledge of mathematics [12] - [14] . Some of these proposals were inspired by Feynman's lectures [15] . Other authors tried to develop similar techniques based on modern computer technology [16] - [18] .", "paraphrased_abstract": "In recent years, however, several authors have developed new approaches to teaching special relativity. Usually, they merely demonstrate the basic principles of SR through simple experiments carried out in different reference frames.... It usually requires no mathematical knowledge at all, and often requires little mathematical knowledge... The main idea is that the students can learn about the fundamental principles of special relativity without dealing with the complicated mathematical concepts of tensors and spinors. The article discusses how this method is applied to the Lorentz transformations. It explains how the method works on the Lorentz transformations and, in particular, how one can explain gauge symmetries in quantum field theory. This article describes the application of this method to examples of the Lorentz transformations. This article describes the transformations of the Lorentz transformations. The author gives an alternative way of teaching special relativity with the synthesis of transformations and generators, which are used in physics to describe symmetry of physical systems. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2261, "title": "Uniqueness of $\\bf C^*$- and $\\bf C_+$-actions on Gizatullin surfaces", "abstract": "  A Gizatullin surface is a normal affine surface $V$ over $\\bf C$, which can be completed by a zigzag; that is, by a linear chain of smooth rational curves. In this paper we deal with the question of uniqueness of $\\bf C^*$-actions and $\\bf A^1$-fibrations on such a surface $V$ up to automorphisms. The latter fibrations are in one to one correspondence with $\\bf C_+$-actions on $V$ considered up to a \"speed change\".   Non-Gizatullin surfaces are known to admit at most one $\\bf A^1$-fibration $V\\to S$ up to an isomorphism of the base $S$. Moreover an effective $\\bf C^{*}$-action on them, if it does exist, is unique up to conjugation and inversion $t\\mapsto t^{-1}$ of $\\bf C^*$. Obviously uniqueness of $\\bf C^*$-actions fails for affine toric surfaces; however we show in this case that there are at most two conjugacy classes of $\\bf A^1$-fibrations. There is a further interesting family of non-toric Gizatullin surfaces, called the Danilov-Gizatullin surfaces, where there are in general several conjugacy classes of $\\bf C^*$-actions and $\\bf A^1$-fibrations.   In the present paper we obtain a criterion as to when $\\bf A^1$-fibrations of Gizatullin surfaces are conjugate up to an automorphism of $V$ and the base $S$. We exhibit as well a large subclasses of Gizatullin $\\bf C^{*}$-surfaces for which a $\\bf C^*$-action is essentially unique and for which there are at most two conjugacy classes of $\\bf A^1$-fibrations over $\\bf A^1$. ", "machine_abstract": "We prove that the only actions by groups with Kazhdan's property (T) on Gizatullin surface are trivial, i.e., they are either free or fixed point free.  This is joint work with A. Lando. We also show that there exists an infinite family of non-isomorphic actions of $SL(2,R)$ on Gizatullin threefolds which have no global fixed points but do not admit any equivariant complex structure. These results were announced in our preprint [BL] .  Introduction  Let $X$ be a smooth projective variety over $\\mathbb{C}$ equipped with an action of a group $G$ such that the induced map $G\\times X\\to X$ is holomorphic. The action is called algebraic if it lifts to an action of some connected reductive algebraic group $H$ defined over $\\mathbb{Q}$. In this case we say that $X$ admits an $H$-structure.  The main goal of this note is to study algebraic actions of groups with Kazhdan\u2019s property (T). Recall that a locally compact topological group has Kazhdan\u2019s property T if every unitary representation of $G$ which does not contain almost invariant vectors is finite dimensional. Examples include all simple Lie groups without center and many other interesting classes of groups including lattices in higher rank semisimple Lie groups.   Kazhdan\u2019s property T was introduced by M. Gromov [Gro81] who used it to establish rigidity properties for Riemannian manifolds with negative sectional curvature. Since then it turned out to play important role in various branches of mathematics ranging from geometry to number theory. For example, one can find applications of property T in the works of J.-P. Serre [S89] , D. Toledo [T92] ,  S. Adams [A94]  and others.    In particular, property T plays crucial role in classification problems related to algebraic actions. Indeed, let us consider two algebraic actions of a group $G$ on varieties $X$ and $Y$ respectively. If both actions lift to actions of some connected reductive groups $H_X$ and $H_Y$ then the corresponding", "paraphrased_abstract": ", a group of complex Lie groups, with no center, and in many other interesting Lie groups, and in certain other interesting classes. For instance, one of the local-complex topological groups has Kazhdan's property T if the unitary representation of the group of Lie is not invariant with the vectors of the Riemannian. The property T is, moreover, necessary in the classification of algebraic actions. Let X be a smooth radial variation of a mathematical group of the class G, so that a map of the number G is holomorphic, and in this case we say that X is an axiom of the class H. In our preprint he says that X is an axiom of the class H. It is a joint work with A. Lando. Besides, there are infinite classes of the actions of the group with the property of Kazhdan on the surface of the tiger, which are neither free nor fixed-pointed. The main thing of this paper is to study the algebraic actions of groups with the property of Kazhdan. For example, there are Lie groups with no center, and many others, like the lattices of higher ranks, of Lie groups with a fixed point. This property of Kazhdan was discovered by Gromov, who", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0687, "title": "Guiding-fields for phase-separation: Controlling Liesegang patterns", "abstract": "  Liesegang patterns emerge from precipitation processes and may be used to build bulk structures at submicron lengthscales. Thus they have significant potential for technological applications provided adequate methods of control can be devised. Here we describe a simple, physically realizable pattern-control based on the notion of driven precipitation, meaning that the phase-separation is governed by a guiding field such as, for example, a temperature or a pH field. The phase-separation is modeled through a non-autonomous Cahn-Hilliard equation whose spinodal is determined by the evolving guiding field. Control over the dynamics of the spinodal gives control over the velocity of the instability front which separates the stable and unstable regions of the system. Since the wavelength of the pattern is largely determined by this velocity, the distance between successive precipitation bands becomes controllable. We demonstrate the above ideas by numerical studies of a 1D system with diffusive guiding field. We find that the results can be accurately described by employing a linear stability analysis (pulled-front theory) for determining the velocity -- local-wavelength relationship. From the perspective of the Liesegang theory, our results indicate that the so-called revert patterns may be naturally generated by diffusive guiding fields. ", "machine_abstract": "We study the formation and control of Liesegang patterns in an electrochemical system by applying external electric fields to modulate the local concentration gradients during precipitation reactions.  We show that, under certain conditions, the applied field can be used as a guiding field to direct the growth of precipitate bands along specific directions. The results are explained using a simple model based on the competition between diffusion and reaction rates at different locations within the sample. This work provides new insights into how chemical systems may self-organize through interactions with their environment. Chemical systems often exhibit complex spatial structures such as stripes or rings which form spontaneously without any externally imposed symmetry breaking [1] . These structures have been observed in many natural phenomena including mineral deposits [2] , biological tissues [3] , and even living organisms [4] . The most famous example is the so-called \"Liesegang ring\" formed when two solutions containing metal ions react chemically [5] . In this case, the initial solution contains both cations (e.g., Ag+) and anions (e.g., Cl-). When these two solutions come into contact, they begin to diffuse across each other until they meet another interface where the opposite charges neutralize one another [6] . At some point after mixing, precipitation occurs leading to the formation of a band of solid material separating the original solutions [7, 8] . As more bands grow, they eventually overlap forming concentric rings around the center of the sample [9] . Although the exact mechanism behind the formation of Liesegang rings remains unclear [10] , it has been shown experimentally that the spacing between successive rings depends strongly on the concentrations of the starting solutions [11] .", "paraphrased_abstract": "The Liesegang Ring is a structure formed by the synthesis of two solutions containing metal ions; the first solution contains cations, Ag and anions, the opposite ions are cations, the opposite ions neutralize each other; the first two are dissolved, and the second is dissolved, forming a solid ring around the centre of the sample, so that it equilibrates and merges. Then the Liesegang Ring is formed, which is a phenomenon, and which does not require any external order, in the absence of external symmetry. The results are presented by a simple model of the competition between the different diffusion rates and the rate of reaction. In the process of the formation of these ringlets and ringlets, the emergence of which is induced by a force of electric field, and which is not governed by any external symmetry, can be analyzed. In a case of water liquification, the water liquification results in a solid material separating the two ringlets; while, after mixing, the ringlets are separated by a ringlet, where the opposite ringlets are neutralized, and more ringlets grow together, and a ringlets are formed around the center of the ringlets. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3623, "title": "Casimir effect in a 6D warped flux compactification model", "abstract": "  We discuss Casimir effect of a massless, minimally coupled scalar field in a 6D warped flux compactification model and its implications for the hierarchy and cosmological constant problems, which are longstanding puzzles in phenomenology and cosmology. Due to the scale invariance of the background theory, the 4D effective theory contains a volume modulus. To stabilize the modulus, we consider one-loop corrections to the effective potential by the Casimir effect. The one-loop effective potential for the volume modulus has a form which is very similar to Coleman-Weinberg potential. We evaluate coefficients appearing in the effective potential by employing zeta function regularization and heat kernel analysis. The volume modulus is stabilized for smaller degrees of warping, below a critical value, which depends on deficit angle of the reference brane. After stabilizing the modulus, it is possible to obtain observed values of the mass ratio between the fundamental energy scales and a tiny effective cosmological constant (though its sign is negative). The degree of warping should be tuned to be close to the critical value, not as severely as the original fine-tuning. ", "machine_abstract": "We study the Casimir effect for fermions and scalars on an orbifolded six-dimensional space-time with nontrivial topology, which is obtained by compactifying M-theory on a Calabi-Yau threefold X times S 1 /Z2. We find that the vacuum energy density depends only on the volume of X and not on its shape or size. This result can be understood as follows. The effective action contains terms proportional to the Euler characteristic of X, which are independent of the metric moduli. These terms cancel out when we take into account all Kaluza-Klein modes. In addition, there exist contributions coming from the winding states localized at fixed points of the orbifolding. However these contributions also vanish because they depend on the complex structure moduli of X, whose values cannot be determined within our framework. Finally, we show how this cancellation mechanism works explicitly using toric geometry techniques.", "paraphrased_abstract": "This equivalence, as we see, is due to the metric properties of X, but without the moduli of metric properties, which cannot be defined within our framework. The result is as follows. We will examine the Casimir effect for fermions and scalars in an orbifold of six dimensions with non-trivial topology. We will combine the M-theory on a Calabi-Yau with threefold X times S / Z2. We will also prove that the vacuum density depends only on the volume of X and not on its shape or size. Moreover, there are contributions from the windings, located at fixed points of the orbifolding, and they disappear because of the complex structure moduli of X, which cannot be calculated in our framework. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0069, "title": "Dynamical Objects for Cohomologically Expanding Maps", "abstract": "  The goal of this paper is to construct invariant dynamical objects for a (not necessarily invertible) smooth self map of a compact manifold. We prove a result that takes advantage of differences in rates of expansion in the terms of a sheaf cohomological long exact sequence to create unique lifts of finite dimensional invariant subspaces of one term of the sequence to invariant subspaces of the preceding term. This allows us to take invariant cohomological classes and under the right circumstances construct unique currents of a given type, including unique measures of a given type, that represent those classes and are invariant under pullback. A dynamically interesting self map may have a plethora of invariant measures, so the uniquess of the constructed currents is important. It means that if local growth is not too big compared to the growth rate of the cohomological class then the expanding cohomological class gives sufficient \"marching orders\" to the system to prohibit the formation of any other such invariant current of the same type (say from some local dynamical subsystem). Because we use subsheaves of the sheaf of currents we give conditions under which a subsheaf will have the same cohomology as the sheaf containing it. Using a smoothing argument this allows us to show that the sheaf cohomology of the currents under consideration can be canonically identified with the deRham cohomology groups. Our main theorem can be applied in both the smooth and holomorphic setting. ", "machine_abstract": "We introduce the notion of dynamical objects associated to cohomologically expanding maps, and prove that they are equivalent to the usual ones in many cases.  We also show how these new objects can be used to study the dynamics of such maps on infinite-dimensional spaces. Let X be an infinite dimensional Banach space with norm . For each integer n \u2265 1 we define the open ball B(n) = {x \u2208 X : x < n}. A map T : X \u2192 X is said to be cohomologically expanding if there exists some constant C > 0 so that for all integers m, n \u2265 1 one has  diam (T \u2212m (B(n))) \u2264 Cn. In this case it follows easily that T satisfies the following properties: (1)  T is continuous;  (2)  T is surjective;  The main result of our work shows that under certain conditions, the existence of a dynamical object implies the existence of another one which behaves well when restricted to finite-dimensional subspaces.  Let us now recall what a dynamical object is. Given any point x \u2208 X , let O(x) denote the orbit of x; i.e., O(x) := {T k (x), k \u2208 Z}. The set O(x) equipped with the metric dO defined by dO((x1, x2)) = sup{d(x1, x2), x1 \u2208 O(x2), x2 \u2208 O(x1)} becomes a compact metric space called the orbital space at x. If T is cohomologically expanding then every orbital space is homeomorphic to a Cantor set.", "paraphrased_abstract": "In fact, if T is cohomologically expanding, all orbital spaces are homeomorphic. The map T \u2013 X \u2013 X is said to be cohomologically expanding if a constant C is in excess of 0 so that for all integers m, n, \u2013 1 the diam (T \u2013 m (B \u2013 n)) = Cn \u2013 the diam becomes diam (T \u2013 m (B \u2013 n) \u2013 n). In this case we may readily find the following properties: (1) T is continuous, (2) T is equilateral; (3) T is equilateral; and (4) T is equilateral. Suppose, for instance, that O, by the metric dO ((x1, x2)), where x = supd(x1, x2), x2 = do. This metric metric is in the sense that it is homeomorphosed to a cantor set. We have given here the idea of dynamical objects, which, by their nature, can be compared with those normally associated with the maps, show that they are invariably equivalent to the corresponding ones. This, moreover, demonstrates that in certain cases, the existence of a dynamical object means that it is a", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.391, "title": "Cascading on extragalactic background light", "abstract": "  High-energy gamma-rays propagating in the intergalactic medium can interact with background infrared photons to produce e+e- pairs, resulting in the absorption of the intrinsic gamma-ray spectrum. TeV observations of the distant blazar 1ES 1101-232 were thus recently used to put an upper limit on the infrared extragalactic background light density. The created pairs can upscatter background photons to high energies, which in turn may pair produce, thereby initiating a cascade. The pairs diffuse on the extragalactic magnetic field (EMF) and cascade emission has been suggested as a means for measuring its intensity. Limits on the IR background and EMF are reconsidered taking into account cascade emissions. The cascade equations are solved numerically. Assuming a power-law intrinsic spectrum, the observed 100 MeV - 100 TeV spectrum is found as a function of the intrinsic spectral index and the intensity of the EMF. Cascades emit mainly at or below 100 GeV. The observed TeV spectrum appears softer than for pure absorption when cascade emission is taken into account. The upper limit on the IR photon background is found to be robust. Inversely, the intrinsic spectra needed to fit the TeV data are uncomfortably hard when cascade emission makes a significant contribution to the observed spectrum. An EMF intensity around 1e-8 nG leads to a characteristic spectral hump in the GLAST band. Higher EMF intensities divert the pairs away from the line-of-sight and the cascade contribution to the spectrum becomes negligible. ", "machine_abstract": "We study the effect of cascades initiated by ultra-high energy cosmic rays (UHECRs) in intergalactic space, and their contribution to the diffuse gamma-ray emission observed at Earth. We find that cascade photons can be produced over cosmological distances with energies up to 10 TeV or more. The resulting flux is strongly suppressed for sources located beyond redshifts z > 0.5 due to absorption effects caused by pair production interactions between cascade photons and infrared radiation fields associated with galaxies. For nearby source models we show that this suppression leads to an upper limit on the photon fraction f\u03b3 < 10% \u2212 20% depending on the UHECR injection spectrum assumed. This constraint is consistent with current measurements of the diffuse gamma-ray background as well as recent results obtained using Fermi/LAT data. In addition, our analysis shows that cascade photons are likely to dominate the total diffuse gammaray emission below 1 GeV if they originate within a distance of about 100 Mpc around us.", "paraphrased_abstract": "It is also shown that the gamma rays produced by a gamma ray are likely to dominate the total gamma ray emission, below 1 GeV., in which the rays of a gamma ray are produced. In this way, we will calculate the effect of a gamma ray on the diffuse gamma ray emission of Earth, if they originate within a radius of 100 million meters around us. We investigate the effect of UHECRs in the space of the universe, and the contribution of these rays to the diffuse gamma ray emission of the Earth. In this study, we find that the rays of a gamma ray can be produced for cosmological distances up to 10 TeV, but their source is largely suppressed, for the sources surrounded by redshifts of z >0.5, by absorption by the pair formation of photons with infrared fields associated with galaxies. We have shown that the resulting rays are not a negligible fraction of the radiance f1 10 10, depending on the radiance of the radiance. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2684, "title": "Effects of Lepton Flavour Violation on Chargino Production at the Linear Collider", "abstract": "  We study the effects of lepton flavour violation (LFV) on the production processes e+e- --> \\chi+_i \\chi-_j at a linear collider with longitudinal e+ and e- beam polarizations. In the case of LFV the sneutrino mass eigenstates have no definite flavour, therefore, in the t-channel more than one sneutrino mass eigenstate can contribute to the chargino production cross sections. Our framework is the Minimal Supersymmetric Standard Model (MSSM) including LFV terms. We show that in spite of the restrictions on the LFV parameters due to the current limits on rare lepton decays, the cross section \\sigma(e+e- --> \\chi+_1 \\chi-_1) can change by a factor of 2 or more when varying the LFV mixing angles. We point out that even if the present bound on BR(tau- --> e- gamma) improves by a factor of thousand the influence of LFV on the chargino production cross section can be significant. These results could have an important impact on the strategies for determining the underlying model parameters at the linear collider. ", "machine_abstract": "We study effects of lepton flavour violation (LFV) in chargino production and decay at linear colliders, taking into account all relevant LFV interactions. We find that for large values of tan \u03b2 , where tan \u03b2 is defined as the ratio between vacuum expectation values of up-type and down-type Higgs fields, the branching ratios to tau leptons can be enhanced by several orders of magnitude compared with those expected within the Standard Model. This enhancement results mainly from the fact that the coupling of neutralinos to taus is proportional to tan \u03b2 . The effect of this enhancement depends strongly on the mass difference between the lightest and second-lightest neutralinos.     For small values of tan \u03b2 we find only moderate enhancements or even suppression of the branching ratios due to destructive interference among different contributions. In addition, we show how these effects depend on the masses of the supersymmetric particles involved.", "paraphrased_abstract": "In short, we find that for very high values of tan, a measure of the vacuum expectations of the upper and lower Higgs fields, the branching ratios to the tau leptons are increased by several orders of magnitude, compared with those expected by the Standard Model. The increase of the branching ratio is mainly due to the fact that the coupling of neutrals to taus is proportional to tan. This increase depends strongly on the mass of the lightest neutrals. We show that the effects of lepton flavouring are of a kind which has a great influence on the chargino production and decay at linear colliders, and consider all the relevant LFV interactions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1459, "title": "The production of Tsallis entropy in the limit of weak chaos and a new indicator of chaoticity", "abstract": "  We study the connection between the appearance of a `metastable' behavior of weakly chaotic orbits, characterized by a constant rate of increase of the Tsallis q-entropy (Tsallis 1988), and the solutions of the variational equations of motion for the same orbits. We demonstrate that the variational equations yield transient solutions, lasting for long time intervals, during which the length of deviation vectors of nearby orbits grows in time almost as a power-law. The associated power exponent can be simply related to the entropic exponent for which the q-entropy exhibits a constant rate of increase. This analysis leads to the definition of a new sensitive indicator distinguishing regular from weakly chaotic orbits, that we call `Average Power Law Exponent' (APLE). We compare the APLE with other established indicators of the literature. In particular, we give examples of application of the APLE in a) a thin separatrix layer of the standard map, b) the stickiness region around an island of stability in the same map, and c) the web of resonances of a 4D symplectic map. In all these cases we identify weakly chaotic orbits exhibiting the `metastable' behavior associated with the Tsallis q-entropy. ", "machine_abstract": "We study the statistical properties of time series generated by weakly chaotic dynamical systems, which are characterized by an exponential decay of correlations with a finite correlation length. We show that for such systems the Tsallis entropy is produced as the number of degrees of freedom increases to infinity. This result allows us to define a new indicator of chaoticty based on the rate at which the Tsallis entropy grows when the system size increases. The proposed indicator can be used both for deterministic and stochastic processes. It has been applied to several examples including the logistic map, the H\u00e9non map, the Ikeda map, the Lorenz attractor, and the Ornstein-Uhlenbeck process. In particular we have shown how this indicator can be used to detect nontrivial scaling laws associated with intermittent dynamics.  I. INTRODUCTORY REMARK The concept of entropy was introduced into physics by Boltzmann [1] . Since then it has become one of the most important concepts in statistical mechanics [2] , information theory [3] , and many other fields [4] . In recent years there has been growing interest in generalized entropies [5] - [8] . One of these generalizations is due to Tsallis [9] who defined his entropy Sq (p) = \u2212 \u2211\ufe00 i pi ln q [pi] where p denotes probability distribution function and q > 0 is called the non-extensivity parameter. For q \u2192 1 Eq. (1) reduces to Shannon's entropy [10]  Sq(p) = \u2212 \u2211\ufe00 i pini ln[pini]. (2) * Electronic address: yuri@ipmras.ru \u2020 Electronic address: alexey@ipmras. ru \u2021 Electronic address: vladimir@ipmras.r", "paraphrased_abstract": "The idea of entropy was introduced into physics by Boltzmann, and it has since been established as one of the most important concepts in the field of statistics, information theory, and many other fields. In recent years, however, interest in generalized entropy has increased. For example, we have applied it to the logistical map, the H\u00e9non map, the Ikeda map, the Lorenz attractor, and the Ornstein-Uhlenbeck process. In this study, we have derived the statistical properties of time series derived from weakly chaotic dynamical systems, which are characterized by an exponential decay of correlations with a finite correlation length. Then we have derived a new measure of entropy, which is based on the rate of increase in the entropy as the size of the system increases. The result we have shown is that the Tsallis entropy increases with the size of the system. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0999, "title": "Einstein in the crossroad of the sciences, arts and humanities", "abstract": "  Western thinking underwent a turning point between 1885 and 1925. Einstein in 1905 symbolizes the emblematic hinge of this change of direction. To find an equivalent phenomenon in the past we need to go back to the period from the XV century to the XVII century. It was not a mere reform of codes but a new code. In 1905 the perception of reality changed through the introduction of relativism in three levels of the reference systems: the ego-other relation, the world perceived by the senses, and the 'ideal' universe of the concepts. We will try to identify how this change was expressed and developed in the cited levels, both in the physic and nature based sciences, in the social sciences, and in literature and fine arts. Today, 100 years after, the 1905's generation still proposes us two options: to live sheltered by the dogma that reassures us with its only and exclusive viewpoint claimed objective, or to dare to live with the multiple, the transient, the relative. The first way led to the worst regimes and intolerant ideas of the XX century, the second option throws us into the uncertainty of the creative adventure, but also leads us to the possibility of a fairer society in the XXI century. ", "machine_abstract": "The conference will be held on September 24-25 at the University of Milan-Bicocca (Italy). The main topics to be discussed are Einstein's role as an intellectual figure; his contribution to science, art and culture; his influence on contemporary society and its future development.  The event is organized by the Italian National Committee for UNESCO with the support of the International Union of Pure and Applied Physics (IUPAP), the European Physical Society (EPS) and the Institute of Advanced Studies of the University of Bologna.     The conference aims to celebrate the centenary of Albert Einstein's birth and to promote the dissemination of knowledge about him among scientists, artists, students and the general public. It also intends to foster international cooperation between researchers working in different fields related to Einstein's work.      The program includes plenary lectures given by internationally renowned speakers, parallel sessions dedicated to specific themes, poster presentations and cultural events such as exhibitions, concerts and performances.", "paraphrased_abstract": "The conference will take place on September 24-25 at the University of Milan-Bicocca. The theme is \"Einstein as a person\"; he is an intellectual figure, his role in science, in art and culture; he is a major influence on the world of tomorrow; he is a representative of the European Physical Society; he is also the leading scientific figure in the field of knowledge; he is a representative of the renown of mankind, and the most important influence on human life. The conference is organized by the Italian National Committee for the UNESCO, with the support of the International Union of Pure and Applied Physics (IUPAP), the European Physical Society (EPS), and the Institute of Advanced Studies at the University of Bologna. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1091, "title": "Information feedback and mass media effects in cultural dynamics", "abstract": "  We study the effects of different forms of information feedback associated with mass media on an agent-agent based model of the dynamics of cultural dissemination. In addition to some processes previously considered, we also examine a model of local mass media influence in cultural dynamics. Two mechanisms of information feedback are investigated: (i) direct mass media influence, where local or global mass media act as an additional element in the network of interactions of each agent, and (ii) indirect mass media influence, where global media acts as a filter of the influence of the existing network of interactions of each agent. Our results generalize previous findings showing that cultural diversity builds-up by increasing the strength of the mass media influence. We find that this occurs independently of the mechanisms of action (direct or indirect) of the mass media message. However, through an analysis of the full range of parameters measuring cultural diversity, we establish that the enhancement of cultural diversity produced by interaction with mass media only occurs for strong enough mass media messages. In comparison with previous studies a main different result is that weak mass media messages, in combination with agent-agent interaction, are efficient in producing cultural homogeneity. Moreover, the homogenizing effect of weak mass media messages are more efficient for direct local mass media messages than for global mass media messages or indirect global mass media influences. ", "machine_abstract": "The present study examines the role played by information feedbacks on cultural evolution, focusing on how they affect the transmission of culture between generations through social learning mechanisms such as imitation or teaching. We show that when individuals are able to observe their own performance relative to others' performances (i.e., when there is information feedback), this can lead to an increase in cultural diversity at both individual and population levels. In particular, we find that under certain conditions, information feedback may cause cultural traits to become more variable across individuals within each generation, thereby increasing the likelihood for new cultural variants to emerge. This effect occurs because individuals who perform better than average tend to be imitated more often than those who perform worse than average; thus, over time, the distribution of cultural traits becomes skewed towards higher values. Finally, we demonstrate that these results hold true even if individuals have imperfect knowledge about other's performances.", "paraphrased_abstract": "The present study examines the influence of information on the development of the human race, namely, how it influences the transmission of culture between generations through social learning, such as imitation and instruction. We show that under certain conditions information can cause the variation in the traits of individuals, as a result of which there is more variability in the distribution of their traits. This is because the individuals who perform better than average tend to be influenced more often than the ones who perform worse. Thus, the distribution of the traits becomes skewed towards the higher. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4579, "title": "Hamiltonian and Brownian systems with long-range interactions: IV. General kinetic equations from the quasilinear theory", "abstract": "  We develop the kinetic theory of Hamiltonian systems with weak long-range interactions. Starting from the Klimontovich equation and using a quasilinear theory, we obtain a general kinetic equation that can be applied to spatially inhomogeneous systems and that takes into account memory effects. This equation is valid at order 1/N in a proper thermodynamic limit and it coincides with the kinetic equation obtained from the BBGKY hierarchy. For N tending to infinity, it reduces to the Vlasov equation describing collisionless systems. We describe the process of phase mixing and violent relaxation leading to the formation of a quasi stationary state (QSS) on the coarse-grained scale. We interprete the physical nature of the QSS in relation to Lynden-Bell's statistical theory and discuss the problem of incomplete relaxation. In the second part of the paper, we consider the relaxation of a test particle in a thermal bath. We derive a Fokker-Planck equation by directly calculating the diffusion tensor and the friction force from the Klimontovich equation. We give general expressions of these quantities that are valid for possibly spatially inhomogeneous systems with long correlation time. We show that the diffusion and friction terms have a very similar structure given by a sort of generalized Kubo formula. We also obtain non-markovian kinetic equations that can be relevant when the auto-correlation function of the force decreases slowly with time. An interest of our approach is to develop a formalism that remains in physical space (instead of Fourier space) and that can deal with spatially inhomogeneous systems. ", "machine_abstract": "We present here an extension to the results obtained in Ref.    [Phys. Rev. E 79, 046211 (2009)] on the derivation of general kinetic equations for Hamiltonian and Brownian systems with  long-range interactions. We show that these kinetic equations can be derived by using only the first two moments of the distribution function instead of all its moments as done previously. This is achieved by introducing new variables which are related to the second moment of the distribution function. The resulting kinetic equation contains memory effects due to the presence of non-Markovian terms. These memory effects have been studied numerically for different types of potentials including power-law potentials.  I. INTRODUCTORY REMARK In this work we consider a system composed of N particles interacting via pairwise additive potentials V(r). In particular, we assume that the potential decays at large distances r according to some power law V(r) ~ 1/r\u03b1 where \u03b1 > 2. For such potentials it has been shown recently [1] that the dynamics of the system cannot be described by standard Fokker-Planck or Langevin equations but rather requires more sophisticated approaches based either on generalized Langevin equations [2] , fractional diffusion equations [3] , or integro-differential equations [4] . Here we follow another approach proposed originally by Mori [5] who showed how one could derive kinetic equations starting directly from the microscopic description of the system without any reference to phenomenological arguments [6] . The main idea behind Mori's method consists in expressing the time evolution of the probability density P(r1,\u2026,rn;t) (where ri denotes the position vector of particle i = 1,...,n) in terms of correlation functions Cij(t1,...", "paraphrased_abstract": "... This is a general approach to the development of kinetics. In this paper we describe the first kinetic theory of the Hamiltonian and Brownian systems of the long term. We show that these kinetic theories are derived by using only the first two moments of the distribution, rather than by using all the moments. This is achieved by introducing new variables which are related to the second moment of the distribution. We are now following the example of Mori, who developed a method for the computation of kinetics in a very direct manner from the microcosmology of the system, without reference to phenomenological assumptions. In this paper we will use the method of Mori, which is an extension of the method of Ehle-Planck and Langevin, which have recently been proved to be incompatible with the standard Fokker-Planck equations, or the fractional diffusion equations, or the integro-differential equations. We will take advantage of the previous result of the kinetics, in which the first two moments of the distribution are taken in as opposed to the first. We will take advantage of the fact that the resulting equation is composed of memory effects due to the presence of non-Markovian terms. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3908, "title": "Microlens Parallax Measurements with a Warm Spitzer", "abstract": "  Because Spitzer is an Earth-trailing orbit, losing about 0.1 AU/yr, it is excellently located to perform microlens parallax observations toward the Magellanic Clouds (LMC/SMC) and the Galactic bulge. These yield the so-called ``projected velocity'' of the lens, which can distinguish statistically among different populations. A few such measurements toward the LMC/SMC would reveal the nature of the lenses being detected in this direction (dark halo objects, or ordinary LMC/SMC stars). Cool Spitzer has already made one such measurement of a (rare) bright red-clump source, but warm (presumably less oversubscribed) Spitzer could devote the extra time required to obtain microlens parallaxes for the more common, but fainter, turnoff sources. Warm Spitzer could observe bulge microlenses for 38 days per year, which would permit up to 24 microlens parallaxes per year. This would yield interesting information on the disk mass function, particularly old brown dwarfs, which at present are inaccessible by other techniques. Target-of-Opportunity (TOO) observations should be divided into RTOO/DTOO, i.e., ``regular'' and ``disruptive'' TOOs, as pioneered by the Space Interferometry Mission (SIM). LMC/SMC parallax measurements would be DTOO, but bulge measurements would be RTOO, i.e., they could be scheduled in advance, without knowing exactly which star was to be observed. ", "machine_abstract": "We present the first microlensing parallax measurements made using infrared observations by the Wide-field Infrared Survey Explorer (WISE). We use these data to measure the mass and distance to two lensing systems, OGLE-2011-BLG-0341L/MOA-2011-BLG-262S and MOA-2012-BLG-0171. The former is an event in which the source star passes close to both lenses; we find that it has a total mass of 1.4 solar masses at a distance of 4 kpc. The latter system consists of three bodies -the lens, its host star, and another distant companion-that are all gravitationally bound together. This binary-lens event exhibits significant deviations from standard single-lens behavior due to the presence of this third body. Using our new measurement technique, we determine the mass ratio between the lens components as well as their projected separation on the sky.", "paraphrased_abstract": "The corresponding spectra of the two lenses are: obliquely a source star passes through the two lenses. Its mass is 1.4 solar masses at a distance of four kilometers. In this system, a third one, a third one, is tethered to it. This tethered event is much different from the typical single-lens event, for the presence of this third body. We present the first measurements of the microlensing angle with the use of the infrared. We use the measurements made by the Wide-field Infrared Survey Explorer (WISE). The first is a binary lensing event in which the source star passes between the two lenses, and it is found that the mass is a thousand solar masses at a distance of 4 km. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0839, "title": "Correlated bosons on a lattice: Dynamical mean-field theory for Bose-Einstein condensed and normal phases", "abstract": "  We formulate a bosonic dynamical mean-field theory (B-DMFT) which provides a comprehensive, thermodynamically consistent framework for the theoretical investigation of correlated lattice bosons. The B-DMFT is applicable for arbitrary values of the coupling parameters and temperature and becomes exact in the limit of high spatial dimensions d or coordination number Z of the lattice. In contrast to its fermionic counterpart the construction of the B-DMFT requires different scalings of the hopping amplitudes with Z depending on whether the bosons are in their normal state or in the Bose-Einstein condensate. A detailed discussion of how this conceptual problem can be overcome by performing the scaling in the action rather than in the Hamiltonian itself is presented. The B-DMFT treats normal and condensed bosons on equal footing and thus includes the effects caused by their dynamic coupling. It reproduces all previously investigated limits in parameter space such as the Beliaev-Popov and Hartree-Fock-Bogoliubov approximations and generalizes the existing mean-field theories of interacting bosons. The self-consistency equations of the B-DMFT are those of a bosonic single-impurity coupled to two reservoirs corresponding to bosons in the condensate and in the normal state, respectively. We employ the B-DMFT to solve a model of itinerant and localized, interacting bosons analytically. The local correlations are found to enhance the condensate density and the Bose-Einstein condensate (BEC) transition temperature T_{BEC}. This effect may be used experimentally to increase T_{BEC} of bosonic atoms in optical lattices. ", "machine_abstract": "We present the results of our study of correlated bosonic systems in one dimension using dynamical mean field theory (DMFT). We consider both the case where all particles are condensed into a single state, as well as that with no condensation at all. In particular we focus on the effect of interactions between the bosons on their dynamics.     The model is defined by the following Hamiltonian:        H = \\sum_{i < j} \\Sigma_{ij} b_i^\\dagger b_j + V(b_i) - \\mu \\sum_{i} b_i^\\dagger + \\mu \\sum_{i} b_i,   where $\\mu$ is an external chemical potential which controls the number of particles in the system. Here $b_i$ denotes the annihilation operator for a particle in site $i$, while $\\Sigma_{ij}$ describes nearest neighbor hopping. Finally $V(b_i)$ represents the interaction energy between two bosons occupying neighboring sites.", "paraphrased_abstract": "mu sum i bdagger bdagger bdagger bdagger, bdagger bdagger, bdagger bdagger, where mu sum i bdagger bdagger bi, where mu sum i bi, where mu sum i bi, where mu is a chemical potential which determines the number of particles in the system. Moreover, the coefficient of friction is given by the equations of the following Hamiltonian: H = sum i j Sigma ij bdagger bdagger + mu sum i bi, where mu is an external chemical potential that controls the number of particles in the system. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2551, "title": "Network Topology of an Experimental Futures Exchange", "abstract": "  Many systems of different nature exhibit scale free behaviors. Economic systems with power law distribution in the wealth is one of the examples. To better understand the working behind the complexity, we undertook an empirical study measuring the interactions between market participants. A Web server was setup to administer the exchange of futures contracts whose liquidation prices were coupled to event outcomes. After free registration, participants started trading to compete for the money prizes upon maturity of the futures contracts at the end of the experiment. The evolving `cash' flow network was reconstructed from the transactions between players. We show that the network topology is hierarchical, disassortative and scale-free with a power law exponent of 1.02+-0.09 in the degree distribution. The small-world property emerged early in the experiment while the number of participants was still small. We also show power law distributions of the net incomes and inter-transaction time intervals. Big winners and losers are associated with high degree, high betweenness centrality, low clustering coefficient and low degree-correlation. We identify communities in the network as groups of the like-minded. The distribution of the community sizes is shown to be power-law distributed with an exponent of 1.19+-0.16. ", "machine_abstract": "We present the network topology and trading volume for the first experimental futures exchange, which was launched in September 2009 as part of the University of Chicago's Financial Markets Lab (FML). The FML is designed to provide students with hands-on experience in designing, implementing, and operating financial markets. We find that the network exhibits small-world properties similar to those observed on other real world networks such as social networks or the Internet. In addition we show how the network evolves over time by analyzing its growth rate and degree distribution. Finally, we study the relationship between the number of transactions executed at each node and their respective degrees. Our results suggest that nodes with higher degrees tend to trade more than lower-degree nodes. This finding has important implications for market design since it suggests that traders should be incentivized to increase their connectivity within the network if they want to maximize their trading activity. We also examine whether there are any differences across different types of contracts traded on the exchange.", "paraphrased_abstract": "Then, we study the number of transactions executed by each node and their degree of connectivity. We find that the network has the appearance of a network that is similar to the network of social networks or the Internet. The network is a small world network, and our results are in agreement with those observed on the network of the Internet and other real world networks. We study the network\u2019s characteristics and the corresponding degree of growth, and examine the relationship between the degree of connectivity and the number of transactions. This is of great importance for market-design, since it will encourage the traders to connect and make use of the network, if they wish to increase their trading. We also study the relationship between the degree of trading and the number of transactions executed. We conclude that the higher the degree of trading, the more active it is. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.39, "title": "The infrared environments of masers associated with star formation", "abstract": "  The near infrared (1-2um) and the thermal infrared (3-25um) trace many of the environments in which masers are thought to reside, including shocks, outflows, accretion disks, and the dense medium near protostars. After a number of recent surveys it has been found that there is a higher detection rate of mid-IR emission towards masers than cm radio continuum emission from UC HII regions, and that the mid-IR emission is actually more closely cospatial to the maser locations. A high percentage of water and methanol masers that are not coincident with the UC HII regions in massive star forming regions are likely to be tracing outflows and extremely young high mass stars before the onset of the UC HII region phase. After a decade of groundwork supporting the hypothesis that linarly distributed class II methanol masers may generally trace accretion disks around young massive stars, compelling evidence is mounting that these masers may generally be associated with outflows instead. Substantiation of this claim comes from recent outflow surveys and high angular resolution mid-IR imaging of the maser environments. ", "machine_abstract": "We have studied the infrared (IR) properties of water and methanol maser sources in nearby galaxies using data obtained by the Infrared Space Observatory (ISO). We find that most of these masers are located near luminous IR objects, such as ultraluminous infrared galaxies or active galactic nuclei. The majority of the masers appear to be excited by shocks produced by outflows driven by massive stars. However, we also found some masers which may be excited by accretion onto young stellar objects. These results suggest that both massive star formation and low-mass star formation can produce masing gas clouds. Masers are powerful tools for studying physical conditions in interstellar media because they provide information on molecular abundances and kinematics at high spatial resolution. Water and methanol masers are commonly observed toward star-forming regions in our Galaxy and other nearby galaxies. They are thought to trace dense molecular gas where protostars form. Since their discovery more than 30 years ago, many studies have been carried out to investigate the relationship between masers and various phenomena related to star formation processes.     In this study, we investigated the infrared (IR) environment around masers detected in nearby galaxies using ISO observations. Our sample consists of all known extragalactic water and methanol masers listed in the catalogs compiled by Caswell & Haynes(1987), Hoffman et al.(1989), and Pestalozzi et al. (2005) . Most of them were discovered serendipitously during surveys conducted with single-dish radio telescopes. Although there is no complete census of masers in external galaxies yet, it has been estimated that about 10 percent of local ULIRGs show maser emission (e.g., Gao 1996; Braatz et al. 1997 ). This suggests that masers play an important role in understanding the nature of ULIRGs.", "paraphrased_abstract": "In the last thirty years, many studies have been carried out to discover the relations between masers and various phenomena, and we have studied them, in our study, the IR properties of water and methanol masers, which have been detected in several galaxies by means of the information provided by the IR Observatory at the International Space Station, in particular by using the data of the IR-MISTRADO. It is thought that masers are an important tool for studying the inner workings of stars, since they give information about the size and composition of the gas. In this study, we compared the IR properties of water and methanol masers from near galaxies by using data obtained by the IR-MISTRADO. The study is performed on a sample of the water and methanol masers found in the nearby galaxies, in the catalog of Caswell & Haynes, Hoffman et al., Pestalozzi et al., (2000). These masers were mainly observed in the upper regions of the luminous IR galaxies or active galactic nuclei, but a few of them were emitted from the outflows by large stars. Most of them were observed by chance during surveys of single-dish radios. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2208, "title": "Contractions, deformations and curvature", "abstract": "  The role of curvature in relation with Lie algebra contractions of the pseudo-ortogonal algebras so(p,q) is fully described by considering some associated symmetrical homogeneous spaces of constant curvature within a Cayley-Klein framework. We show that a given Lie algebra contraction can be interpreted geometrically as the zero-curvature limit of some underlying homogeneous space with constant curvature. In particular, we study in detail the contraction process for the three classical Riemannian spaces (spherical, Euclidean, hyperbolic), three non-relativistic (Newtonian) spacetimes and three relativistic ((anti-)de Sitter and Minkowskian) spacetimes. Next, from a different perspective, we make use of quantum deformations of Lie algebras in order to construct a family of spaces of non-constant curvature that can be interpreted as deformations of the above nine spaces. In this framework, the quantum deformation parameter is identified as the parameter that controls the curvature of such \"quantum\" spaces. ", "machine_abstract": "The aim of this article is to present the basic concepts in Riemannian geometry that are needed for understanding the main results presented here.  The first section introduces some notation and definitions used throughout the text.   In particular we define what it means for two points on an n-dimensional manifold M to be close together (in terms of geodesic distance) or far apart.    We also introduce the concept of a local coordinate system at each point p \u2208 M which allows us to describe any other point q near p by giving its coordinates with respect to these local charts.   Finally we give a brief description of how one can construct such a coordinate system locally around a given point using parallel transport along curves starting at p.    The second section describes the notion of a vector field X defined over all of M.   This is done by defining a map F : T M \u2192 R where T M denotes the tangent bundle of M.   Then we show that if X satisfies certain conditions then there exists a unique smooth function f : M \u2192 R such that X = grad(f).   Here grad(f) denotes the gradient of f.   For example, if M is a surface embedded in R3 then X could represent the velocity of a particle moving across M.   If we assume that the particles move according to Newton's laws of motion then the function f would correspond to the potential energy of the system under consideration.   The third section defines the concept of a tensor field as a generalization of vector fields.   Tensor fields allow us to associate several vectors...", "paraphrased_abstract": "Then, if X is satisfied in some condition, then a unique smooth function is defined, f : M  R, where f : M  R : x. Then, if X is satisfied in some condition, then a smooth function f: M  R, where f = x. Then, if the particle moves in the sense of Newton's laws of motion, then f is a function of the potential energy of the system. The third section describes the concept of a tensor field, which is a generalization of vector fields. The tensor field is an integral of several vectors. The first section describes the general concept of a Riemannian field X, which is defined by a map X - T M - R, which consists of the tangent bundle of M, whose tangent bundle consists of T M, where T M is the tangent bundle of M, and whose tangent bundle is a vector. The second section defines the concept of a vector field, X, over all M, in which it is defined by a tangent map F, T M - R, where T M - R is the tangent bundle of M, and a special smooth function f: X", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1528, "title": "An Abelian two-Higgs model of strongly correlated electrons: phase structure, strengthening of phase transition and QCD at finite density", "abstract": "  We investigate non-perturbative features of a three-dimensional Abelian Higgs model with singly- and doubly-charged scalar fields coupled to a single compact Abelian gauge field. The model is pretending to describe various planar systems of strongly correlated electrons such as high-Tc superconductivity in the overdoped regime and exotic materials possessing excitations with fractionalized quantum numbers. The complicated phase structure of the model is studied thoroughly using numerical tools and analytical arguments. In the three-dimensional space of coupling parameters we identify the Fermi liquid, the spin gap, the superconductor and the strange metallic phases. The behavior of three kinds of topological defects -- holon and spinon vortices and monopoles - is explored in various phases. We also observe a new effect, the strong enhancement of the phase transition strength reflected in a lower order of the transition: at sufficiently strong gauge coupling the two second order phase transitions -- corresponding to spinon-pair and holon condensation lines - join partially in the phase diagram and become a first order phase transition in that region. The last observation may have an analogue in Quantum Chromodynamics at non-zero temperature and finite baryon density. We argue that at sufficiently large baryon density the finite-temperature transition between the (3-flavor paired) color superconducting phase and the quark-gluon plasma phases should be much stronger compared with the transition between 2-flavor paired and 3-flavor paired superconducting phases. ", "machine_abstract": "We study the phase diagram of an Abelian Higgs model with two complex scalar fields in which one field is coupled to fermions via Yukawa interaction while another couples to gauge bosons through its kinetic term. We find that there are three phases depending on the values of coupling constants: (i) The normal Fermi liquid, (ii) A superfluid state where both scalars condense simultaneously, and (iii) A superconducting state where only the scalar field coupled to fermions condenses. In addition we show that the second order phase transitions between these states can be strengthened into first-order ones by introducing higher derivative terms for the scalar fields. Finally, we discuss possible relevance of our results to Quantum Chromodynamics at finite baryon chemical potential. PACS numbers: 11.10.Wx, 12.38.Mh, 11.15.Tk  I. INTRODUCTORY REMARK In this work we consider an Abelian Higgs model consisting of two complex scalar fields $\\psi$ and $\\phi$ interacting with each other as well as with fermions and gauge bosons. This system has been studied extensively in the literature [1] . However most previous works have focused on the case when the scalar fields couple directly to fermions or gauge bosons but not both. Here we will allow them to interact with all particles present in the theory. As such it should provide us with some insight about how different types of interactions affect the properties of matter under extreme conditions like high temperature and/or high density.  The motivation behind studying this particular model comes mainly from condensed matter physics. It was shown recently [2] that the effective action describing the low energy dynamics of certain classes of strongly correlated electron systems contains a number of operators similar to those appearing in the standard electroweak Lagrangian. These include the usual Maxwell term for electromagnetism, the kinetic term for the Higgs field, and various four-fermion interactions. Since the latter are responsible for many interesting phenomena observed experimentally [3] , it would be very useful if we could understand their effects better using simple models.", "paraphrased_abstract": "It is a recursive structure. Its structure is a condensed matter physics. It is known that the good action of some very weakly coupled electrons is based on a number of operators, such as Maxwell's electromagnetism, the Higgs' field, and four-fermion interactions. In our work we will consider the phase of a Higgs-based system, consisting of two complex scalar fields, psi and phi, interacting with each other and with fermions and gauge particles. This system has been studied extensively in the literature. In general, the most extensive work has been done on the case of the scalar field and of the kinetic interaction of a Higgs-field, but fewer work has been done on the case of a pair of fermions interacting with each other. Besides, we show how the scalar field combines with both fermions and gauge particles with a kinetic term. In this work we have studied the dynamics of the Higgs-field by using two large, complex, and interacting fields, which are Yukawa, and the other is interacting with gauge particles. We find that there are three phases, depending on the degree of coupling: the normal liquid, the superfluid, where both scalars cond", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.096, "title": "Uniqueness Domains in the Workspace of Parallel Manipulators", "abstract": "  This work investigates new kinematic features of parallel manipulators. It is well known that parallel manipulators admit generally several direct kinematic solutions for a given set of input joint values. The aim of this paper is to characterize the uniqueness domains in the workspace of parallel manipulators, as well as their image in the joint space. The study focuses on the most usual case of parallel manipulators with only one inverse kinematic solution. The notion of aspect introduced for serial manipulators in [Borrel 86] is redefined for such parallel manipulators. Then, it is shown that it is possible to link several solutions to the forward kinematic problem without meeting a singularity, thus meaning that the aspects are not uniqueness domains. An additional set of surfaces, namely the characteristic surfaces, are characterized which divide the workspace into basic regions and yield new uniqueness domains. This study is illustrated all along the paper with a 3-RPR planar parallel manipulator. An octree model of spaces is used to compute the joint space, the workspace and all other newly defined sets. ", "machine_abstract": "In this work, we study uniqueness domains for parallel manipulators with closed-loop kinematics and redundant actuation. We show that these manipulators can be modeled as an underactuated serial chain whose joints are driven by motors located at their base. The workspace is then defined as the set of all poses reachable by such chains when they are connected to any possible end-effector pose. This definition allows us to use tools developed for serial robots to analyze parallel manipulators. In particular, we prove that the workspace boundary consists of two types of surfaces: singularity surfaces corresponding to configurations where one or more joint velocities become zero; and self-motion surfaces corresponding to configurations where the robot's motion becomes indeterminate because it has multiple solutions. Finally, we present numerical results on several examples including the Stewart platform and the Delta robot. Keywords: parallel manipulator, workspace, redundancy resolution, singularities", "paraphrased_abstract": "The name of this work is Parallel, and the name consists of the same meaning as \u201cSingle Object\u201d (i.e., a resonant Object). We have a notion of the Parallel that parallel Object can be described by a serially driven chain of the bottom, connected at the base by a motor driven by the motors of the base, so that the Space Object is the set of all possible Possessings which can be achieved by these Possesses, when connected with any Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing, so that, by means of the Selected Possessing Possessing, we can obtain in the Study of Uniqueness, the entailment of Single Action, and the Self-Motion, that is, the Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing Possessing", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0917, "title": "Reggeization of N=8 Supergravity and N=4 Yang-Mills Theory II", "abstract": "  The loop expansion for the n-point functions of N=4 Yang-Mills theory and N=8 supergravity can be formulated as the loop expansion of scalar field theory with an infinite subclass being the ladder diagrams. We consider the sum of ladder diagrams for gluon-gluon and graviton-graviton scattering in the Regge limit. The reggeization of the gluon and the graviton is discussed in this context and that of hep-th/0701217. If the Bern, Dixon, Smirnov conjecture for planar gluon-gluon scattering is correct, then the ladder sum for SU(N) gauge theory at large N, correctly gives the Regge limit, with Regge trajectory function proportional to the cusp anomalous dimension.   In graviton-graviton scattering it is argued that the graviton lies on a Regge trajectory. Regge cuts are also present due to infinite sums of non-planar graphs. The multiple exchange of Regge poles in non-planar graphs can give a countable infinite number of moving Regge cuts which accumulate near s=0. It is conjectured that this may be related to the infinite number of non-perturbative massless states which remain in the limit discussed by Green, Ooguri and Schwarz. ", "machine_abstract": "We present the results for the one-loop amplitudes in N=8 supergravity, which are obtained by using the spinor helicity formalism with dimensional reduction to four dimensions.  We show that our result agrees with the previous calculation based on Feynman diagrams at high energy limit. In addition we find that the amplitude vanishes when all external particles have negative helicities. This is consistent with the fact that there exists no graviton state with negative helicity in the massless spectrum of N=8 supergravity. The vanishing property also holds true even if some gravitons carry positive helicities but others do not.  The organization of this article is as follows. In section 2, we briefly review the basic properties of N=8 supergravity theory. In section 3, we explain how to calculate the scattering amplitudes in N=8 supergravitvity. In section 4, we give explicit expressions for the tree-level amplitudes. In sections 5-8, we present the results for the 1-loop amplitudes. Section 9 contains discussions about the physical meaning of these results.", "paraphrased_abstract": "The organization of this paper is as follows: in section 2 we briefly describe the basic properties of N=8 gravity, in section 3 we explain how to calculate the scattering amplitudes of the supergravity. In section 4 we present the corresponding calculations of the tree-level amplitudes. In section 5 we present the results of the tree-level amplitudes. Then in section 9 we will discuss the physical significance of these results. We find that the amplitude vanishes when all external particles have negative helicity. This is also a feature that does not exist in the massless spectrum of N=8 supergravity. Besides, the amplitude vanishes in the presence of all external particles, namely, that is, when all external particles have negative helicity, that is, we find that there is no helicity in the massless spectrum of N=8 supergravity. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2236, "title": "Squashed entanglement for multipartite states and entanglement measures based on the mixed convex roof", "abstract": "  New measures of multipartite entanglement are constructed based on two definitions of multipartite information and different methods of optimizing over extensions of the states. One is a generalization of the squashed entanglement where one takes the mutual information of parties conditioned on the state's extension and takes the infimum over such extensions. Additivity of the multipartite squashed entanglement is proved for both versions of the multipartite information which turn out to be related. The second one is based on taking classical extensions. This scheme is generalized, which enables to construct measures of entanglement based on the {\\it mixed convex roof} of a quantity, which in contrast to the standard convex roof method involves optimization over all decompositions of a density matrix rather than just the decompositions into pure states. As one of the possible applications of these results we prove that any multipartite monotone is an upper bound on the amount of multipartite distillable key. The findings are finally related to analogous results in classical key agreement. ", "machine_abstract": "We study squashed entanglement, an operational measure of quantum correlations in bipartite systems, for general multipartite states. We show that it can be computed efficiently by solving semidefinite programs (SDPs). Moreover we prove that this SDP is equivalent to another one which computes the so-called geometric measure of entanglement. Finally, we present several examples where our results are applied. Quantum information theory has been developed as a branch of physics since Bell's theorem [1] . In particular, many efforts have been made towards understanding how much information can be stored or transmitted using quantum resources [2] . One important concept in quantum information theory is entanglement [3] , i.e., nonclassical correlation between two subsystems of a composite system [4] . Entangled states play crucial roles in various applications such as quantum teleportation [5] , superdense coding [6] , quantum key distribution [7] , and quantum metrology [8] . The amount of entanglement contained in a given state is usually quantified by means of entanglement measures [9] . These quantities provide useful tools for studying properties of quantum states [10] and their relations with other physical phenomena [11] . In recent years there has been growing interest in extending these concepts beyond pure-state scenarios [12] . For instance, the notion of entanglement distillation [13] was extended to mixed states [14] . This led to the introduction of new measures of entanglement [15] - [17] . However, most of them cannot be evaluated analytically except for some special classes of states [18] - [20] . Therefore numerical methods [21] - [23] were proposed to compute them [24] - [26] . Among those approaches, semidefinite programming (SDP) [27] plays a central role [28] - [30] . It allows us to find solutions to optimization problems involving linear objective functions subject to linear constraints and quadratic equality/inequality constraints [31] . Recently, SDP techniques have also been used to investigate different aspects of quantum mechanics [32] - [38] . The aim of this work is to introduce efficient computational schemes for evaluating certain entanglement measures for arbitrary multipartite states. More specifically, we focus on the squashed entanglement [39] , [42] , [", "paraphrased_abstract": "The idea of entanglement has been a central concept in quantum physics since Bell\u2019s theorem, and much research has been made into entanglement and the integration of quantum materials. Since Bell\u2019s theorem, much research has been conducted into the integration of quantum states. The results of this research are complemented by several cases of entanglement, and this is discussed in the following chapters. There are many ways of analyzing entanglement in the quantum sphere, in particular in the area of quantum teleportation, in the field of quantum key distribution, in the field of quantum metrology. The entanglement of a given state is usually quantified by entanglement measures, such as entanglement of the tangled systems, entanglement of the two components of a composite sphere, entanglement of the tangled systems, entanglement of the tangled systems, entanglement of the tangled systems, and so on. There is much interest in the study of entanglement of a state, especially in the area of mixed states. For example, the notion of entanglement of a system of mixed states was extended to mixed states, and in the course of this, new entanglement measures were introduced, which were more and more useful in the estimation of the properties of ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1678, "title": "Nonequilibrium Steady States of Matrix Product Form: A Solver's Guide", "abstract": "  We consider the general problem of determining the steady state of stochastic nonequilibrium systems such as those that have been used to model (among other things) biological transport and traffic flow. We begin with a broad overview of this class of driven diffusive systems - which includes exclusion processes - focusing on interesting physical properties, such as shocks and phase transitions. We then turn our attention specifically to those models for which the exact distribution of microstates in the steady state can be expressed in a matrix product form. In addition to a gentle introduction to this matrix product approach, how it works and how it relates to similar constructions that arise in other physical contexts, we present a unified, pedagogical account of the various means by which the statistical mechanical calculations of macroscopic physical quantities are actually performed. We also review a number of more advanced topics, including nonequilibrium free energy functionals, the classification of exclusion processes involving multiple particle species, existence proofs of a matrix product state for a given model and more complicated variants of the matrix product state that allow various types of parallel dynamics to be handled. We conclude with a brief discussion of open problems for future research. ", "machine_abstract": "The nonequilibrium steady states (NESS) of matrix product form are the focus of this work, which is intended to be useful for researchers in computational physics and chemistry who wish to solve problems with such NESSs using numerical methods.  The first part of the article introduces the concept of NESSs as well as some basic properties that they possess. In particular, we show how one can construct an explicit representation of any given NESS by solving a linear system of equations whose coefficient matrices depend on the underlying transition rates between different microstates. We also discuss several important issues related to the construction of these coefficient matrices. The second part of the article presents two examples illustrating our approach. Finally, we provide a detailed discussion about various aspects of the proposed method along with possible extensions. Nonequilibrium steady states (NESs), i.e., time-independent solutions of master equations describing open systems far away from equilibrium, have been studied extensively over the past few decades [1] . They play crucial roles in many areas ranging from statistical mechanics [2] , quantum optics [3] , chemical reaction dynamics [4] , and biophysics [5] . In recent years there has been growing interest in developing efficient algorithms for computing NESs [6] - [8] . This is mainly due to their importance in applications where it may not always be feasible or desirable to obtain exact analytical results [9] - [11] . For example, in molecular dynamics simulations [12] , Monte Carlo sampling techniques [13] , and kinetic Monte Carlo schemes [14] , only approximate values of NESs are available. Moreover, even if the exact solution were known, its direct use would still require significant amount of storage space [15] . Therefore, it becomes necessary to develop fast and accurate numerical methods for calculating NESs [16] - [18] . There exist numerous approaches for numerically approximating NESs [19] - [21] . Among them, the most popular ones include the eigenvector-following algorithm [22] , the power iteration scheme [23] , and the Krylov subspace projection technique [24] . These methods usually involve repeated application of the original master equation until convergence is reached [25] . However, since the number of...", "paraphrased_abstract": "A large number of numerical methods have been developed for the numerical determination of the NESs. These methods are based on the eigenvector, on the power-iteration method, on the Krylov subspace, and are well known. They are applied in many fields, from statistical mechanics to quantum optics, to chemical reaction dynamics, to biophysics. During the past few decades, there has been an increasing interest in numerical methods to calculate NESs, which are primarily used in certain areas, namely, in statistical mechanics, in quantum optics, in chemical reactions, in biophysics, in the sciences. In recent years, however, there has been a growing interest in numerical methods to calculate NESs, and the method is of an explicit form, and the coefficients are calculated based on the sum of the transitions between the different microstates. In general, it is possible to calculate NESs, but it is necessary to calculate NESs in numerical terms, mainly by numerical means, and presently there are numerous methods for the calculation of NESs, which is usually performed repeatedly, until the equilibrium is reached. Moreover, although the exact solution is known, it would still take considerable space to compute NESs, and thus the need to develop numerical methods for calculating them is heightened. In this article we will discuss the NESs in matrix form, and give", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1249, "title": "IGR J17254-3257, a new bursting neutron star", "abstract": "  The study of the observational properties of uncommonly long bursts from low luminosity sources with extended decay times up to several tens of minutes is important when investigating the transition from a hydrogen-rich bursting regime to a pure helium regime and from helium burning to carbon burning as predicted by current burst theories. IGR J17254-3257 is a recently discovered X-ray burster of which only two bursts have been recorded: an ordinary short type I X-ray burst, and a 15 min long burst. An upper limit to its distance is estimated to about 14.5 kpc. The broad-band spectrum of the persistent emission in the 0.3-100 keV energy band obtained using contemporaneous INTEGRAL and XMM-Newton data indicates a bolometric flux of 1.1x10^-10 erg/cm2/s corresponding, at the canonical distance of 8 kpc, to a luminosity about 8.4x10^35 erg/s between 0.1-100 keV, which translates to a mean accretion rate of about 7x10^-11 solar masses per year. The low X-ray persistent luminosity of IGR J17254-3257 seems to indicate the source may be in a state of low accretion rate usually associated with a hard spectrum in the X-ray range. The nuclear burning regime may be intermediate between pure He and mixed H/He burning. The long burst is the result of the accumulation of a thick He layer, while the short one is a prematurate H-triggered He burning burst at a slightly lower accretion rate. ", "machine_abstract": "We report the discovery and analysis of an X-ray transient source IGR J17254-3257 in outburst during June 2009 with INTEGRAL/Swift/BAT. The source was detected at high flux levels (>10 Crab) for about one week by Swift/XRT and XMM-Newton/EPIC-pn. We find that this is likely to be another example of a short-hard gamma-ray burst associated with a binary system containing a black hole or neutron star accretor. A comparison between our results on IGR J17254\u22123257 and those obtained previously for other similar sources suggests that there may exist two different classes of such systems. In particular we suggest that some of these objects are powered by super-Eddington accretion onto rapidly rotating black holes while others are powered by sub-Eddington accretion into slowly spinning neutron stars. This work has been supported by NASA under contract NAS8-03060.", "paraphrased_abstract": "This study was funded by NASA under contract No. NAS-8-03060. We report on the discovery and analysis of a transient source, XGR J17254-3257, which was a transient source in June 2009, in the X-ray-band of the INTEGRAL/Swift-BAT, which had a gamma-ray source at the moment when a black hole or neutron star had been accreted. We show that the source was another short-lived gamma-ray burst, probably caused by a binary system of black holes or neutron stars. This is probably another instance of the short-lived gamma-ray burst of a binary system with a black hole or a neutron star. We show that this is another type of short-lived gamma-ray burst, caused by a black hole or a neutron star accreted. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1757, "title": "Scalability and Optimisation of a Committee of Agents Using Genetic Algorithm", "abstract": "  A population of committees of agents that learn by using neural networks is implemented to simulate the stock market. Each committee of agents, which is regarded as a player in a game, is optimised by continually adapting the architecture of the agents using genetic algorithms. The committees of agents buy and sell stocks by following this procedure: (1) obtain the current price of stocks; (2) predict the future price of stocks; (3) and for a given price trade until all the players are mutually satisfied. The trading of stocks is conducted by following these rules: (1) if a player expects an increase in price then it tries to buy the stock; (2) else if it expects a drop in the price, it sells the stock; (3)and the order in which a player participates in the game is random. The proposed procedure is implemented to simulate trading of three stocks, namely, the Dow Jones, the Nasdaq and the S&P 500. A linear relationship between the number of players and agents versus the computational time to run the complete simulation is observed. It is also found that no player has a monopolistic advantage. ", "machine_abstract": "The use of agents in the real world is becoming more widespread, especially for applications that require autonomous decision making or learning. However, there are many challenges associated with developing such systems including scalability issues when dealing with large numbers of agents. In this work we present an approach to optimising the performance of a committee of agents using genetic algorithms (GAs). We demonstrate how GAs can be used to evolve optimal agent behaviours within a simulated environment where each agent has different capabilities. The results show that our method improves upon existing approaches by increasing the overall efficiency of the system while reducing the number of messages required between agents. This research was funded by EPSRC grant EP/G066058/1. Agent-based computing is increasingly being applied to problems requiring autonomous decision-making and/or learning [1] . Examples include intelligent transportation [2] , smart grids [3] , healthcare [4] , manufacturing [5] , robotics [6] , and military operations [7] . However, there are several challenges associated with developing these types of systems [8] . One particular challenge relates to scalability as the number of agents increases [9] . For example, if one thousand agents need to make decisions on their own then it may not be possible to develop individualised behavioural models for all of them [10] . Instead, they must rely on some form of collective intelligence [11] which requires communication [12] . If too much information is communicated between agents then the network will become overloaded [13] resulting in poor performance [14] . Therefore, it becomes important to minimise the amount of data transmitted across the network [15] whilst still maintaining high levels of performance [16] . In order to address these issues, researchers have proposed various techniques [17] ranging from simple heuristics [18] to sophisticated machine-learning methods [19] . These techniques typically involve either centralised [20] or decentralised [21] control mechanisms [22] . Centralised control involves a single entity controlling all aspects of the system [23] whereas decentralised control allows multiple entities to act independently [24] .", "paraphrased_abstract": "- These techniques are based on either central or decentral control - central control means the control of the entire system - or - if the system is divided into groups - - decentral control - it is an independent, independent control of the entire system - - In general, these systems are used for autonomous decision-making and learning - such as the smart grids, the smart grids, the health, the factory, the manufacturing, the robotics, the military. The use of agents in real life is becoming more widespread, especially in the field of autonomous decision-making or learning - especially in the field of autonomous learning. - The use of agents in the real world is growing more and more in the field of intelligent enlightenment, especially in the field of autonomous learning - in the case of intelligent transportation, in the case of intelligent grids, in the case of intelligent grids, in the case of intelligent hospitals, in the case of smart grids, in the case of manufacturing, in the case of robotics, in the case of military operations - and the scalability of these systems, which is particularly prone to scalability when it is many agents, and so we propose a method to optimize the performance of an envoy in a simulated environment, where each agent has different capabilities. This method increases the efficiency of the system, while decreasing", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1185, "title": "Observation of Landau levels of Dirac fermions in graphite", "abstract": "  The low energy electronic excitations in single layer and bilayer graphite (graphene) resemble quantum-relativistic particles also known as Dirac Fermions (DF). They possess an internal degree of freedom, chirality, that leads to unusual Landau Level (LL) energy sequences in a magnetic field and profoundly alters the magneto-transport properties. One of the consequences is an anomalous Quantum-Hall effect, recently detected in both single layer and bi-layer graphene. However the underlying cause, the unusual LL sequence, was never observed. Here we report the direct observation of LL of DF by means of low temperature Scanning-Tunnelling-Spectroscopy (STS) on the surface of graphite in fields up to 12 Tesla. We find evidence of coexistence of massless and massive DF, and identify the zero-energy LL which is a unique consequence of their quantum-relativistic nature. Surprisingly these strictly two-dimensional properties emerge even on bulk graphite in samples where the interlayer coupling is weak. ", "machine_abstract": "We report the observation of Landau levels (LLs) of Dirac fermions in single-layered graphite by using scanning tunneling microscopy and spectroscopy at low temperatures.     The LLs are observed as periodic oscillations with magnetic field B, which is perpendicular to the sample surface. We find that the energy spacing between adjacent LLs decreases with increasing B. This behavior agrees well with theoretical predictions for massless Dirac fermions. In addition, we observe an unexpected peak structure near zero bias voltage Vbias = 0 mV on top of each LL. These peaks can be explained by assuming that they originate from localized states induced by disorder or impurities. Our results provide direct evidence for the existence of Dirac fermions in graphene-like materials such as graphite. Graphene has attracted much attention because it exhibits unique electronic properties originating from its two-dimensional honeycomb lattice [1] . Recently, several groups have succeeded in isolating monolayer sheets of carbon atoms arranged in a similar manner [2] , leading to renewed interest in this material [3] . In contrast to conventional semiconductors, where electrons behave like massive particles, the charge carriers in graphene obey relativistic quantum mechanics [4] . As a result, their dynamics exhibit many unusual features including Klein tunnelling [5] , Zitterbewegung [6] , and half-integer quantum Hall effect [7, 8] . Moreover, the low-energy excitations in graphene are described by massless Dirac fermions [9] whose dispersion relation E(k) shows linear dependence around two inequivalent points K and K' in momentum space [10] . Because of these remarkable characteristics, graphene is considered one of the most promising candidates for future applications in electronics [11] . Recently, there has been growing interest in other layered materials having a similar atomic arrangement [12] . Among them, graphite is particularly interesting since it consists of stacked layers of graphene [13] . Although the interlayer coupling leads to a gap opening [14] , the band structure still retains some resemblance to that of graphene [15] . For example, the Fermi velocity vF ~ 10 6 m/s [16] is almost identical to that of graphene [17] . Furthermore,", "paraphrased_abstract": "And yet, by resolving the enigma of dirac fermions, he was able to obtain a band of Dirac fermions, in the form of dirac fermions in graphite. In the case of graphite, we show the Dirac fermions in the form of dirac fermions, in the form of dirac fermions, the density of which is equidistant from the Dirac fermions. In our study, we show the presence of Dirac fermions in graphite. Graphene has attracted much attention because of its unique electrochemical properties, originating from its two-dimensional honeycomb structure, and because of its unique electrical properties. Recently, several groups have succeeded in isolating monolayers of carbon atoms, which have been arranged in a similar manner. We have been able to observe the formation of dirac fermions in graphene by using microscopy and spectroscopy at low temperatures. We show that the energy spacing between the dirac fermions is decreasing with increasing B. This observation is in keeping with theoretical predictions for the massless Dirac fermions. Furthermore, we show that the density of the Dirac fermions in graphene is very low and does not exhibit a massless Dirac fermions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0727, "title": "Luminous Red Galaxy Population in Clusters at $0.2\\le z\\le 0.6$", "abstract": "  We investigate statistical properties of LRGs in a sample of X-ray selected galaxy clusters at intermediate redshift ($0.2\\le z\\le0.6$). The LRGs are selected based on carefully designed color criteria, and the cluster membership is assessed via photometric redshifts. As clusters and LRGs are both viewed as promising tracer of the underlying dark matter distribution, understanding the distribution of LRGs within clusters is an important issue.   Our main findings include:   1. The halo occupation distribution of LRGs inside our cluster sample is $N(M) = k\\times (M/10^{14})^{a}$ where $a=0.620\\pm 0.105 $ and $k=1.425\\pm0.285 $ assuming a Poisson distribution for $N(M)$.   2. The halo occupation distribution of LRGs ($N(M)$) and the satellite distribution of LRGs ($N-1(M)$) are both consistent with being Poisson. To be more quantitative, we find $Var(N)/<N>= 1.428\\pm 0.351$ and $Var(N-1)/<N-1> = 1.823 \\pm 0.496$   3. The radial profile of LRGs within clusters when fitted with a NFW profile gives a concentration of $17.5^{+7.1}_{-4.3}$ ($6.0^{+3.2}_{-1.9}$) including (excluding) BLRGs (Brightest LRGs).   We also discuss the implications of these observations on the evolution of massive galaxies in clusters. ", "machine_abstract": "We present the results on the luminosity function (LF) and color-magnitude relation (CMR) for luminous red galaxies (LRGs) in clusters with redshift range between $z=0.2$ to $z=0.6$, using data obtained by Subaru/Suprime-Cam. We use two different methods to select LRGs: one is based on photometric colors; another uses spectroscopic information. The LF shows that there are more bright LRGs than faint ones in all samples except for the sample selected only by photometry. This result suggests that we can obtain better statistics if we combine both selection criteria. In addition, our results show that the number density of LRGs decreases as the cluster mass increases. On the other hand, the CMRs do not depend strongly on the cluster masses or redshifts. These results suggest that the formation epoch of massive elliptical galaxies may be earlier than that of less massive ones.      Keywords: Luminous Red Galaxies", "paraphrased_abstract": "The luminosity is calculated by means of two methods: the first is based on spectroscopic observations, and the second is on spectroscopic observations. Besides, the number density of the LRGs increases with the density of the cluster, but not on the size of the cluster. Moreover, the CMRs do not depend on the number of the clusters or redshifts. We present the results on the luminosity of a red galaxie in clusters with redshifts between 0.2 and 0.6, from the data of Subaru/Suprime Cannon. Moreover, we show that the density of the LRGs decreases with the number of clusters and redshifts, but that it is not dependent on the size of the clusters and the redshifts. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3468, "title": "Linear Tabling Strategies and Optimizations", "abstract": "  Recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. Linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. One decision concerns when answers are consumed and returned. This paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. The results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. Linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. Each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. Naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. In this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. We give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. Benchmarking in B-Prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of SLG. ", "machine_abstract": "We present an algorithm for the linear tabling problem that is based on dynamic programming, but avoids storing all possible tables in memory at once by using lazy evaluation to compute table entries as needed.  We show how this approach can be extended with optimizations such as memoization and caching to improve performance further. Our implementation outperforms previous approaches significantly while still being able to handle large inputs efficiently. The source code of our implementation is available online at: http://codeforces.com/contest/1522/problem/A .  Introduction  The linear tabling problem (LTP) asks whether there exists a sequence of integers $(a_1, ..., a_n)$ where each integer $a_i$ belongs to the set $\\{0,...,m-1\\}$ satisfying the following conditions:   1. No two consecutive elements are equal.  2. The sum of all numbers equals zero.   3. All numbers between 0 and m - 1 occur exactly once.    For example, consider the input $m = 3$ and $n = 4$. Then the valid sequences are:  $$\\{\\{1,0,0\\}, \\{0,1,1\\}, \\{0,0,2\\}\\}$$     Lazy Evaluation", "paraphrased_abstract": "\" A linear tally problem is a type of problem of integers, which is composed of two integers: a, a, a, a, a., a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1079, "title": "Co-orbital Oligarchy", "abstract": "  We present a systematic examination of the changes in semi-major axis caused by the mutual interactions of a group of massive bodies orbiting a central star in the presence of eccentricity dissipation. For parameters relevant to the oligarchic stage of planet formation, dynamical friction keeps the typical eccentricities small and prevents orbit crossing. Interactions at impact parameters greater than several Hill radii cause the protoplanets to repel each other; if the impact parameter is instead much less than the Hill radius, the protoplanets shift slightly in semi-major axis but remain otherwise unperturbed. If the orbits of two or more protoplanets are separated by less than a Hill radius, they are each pushed towards an equilibrium spacing between their neighbors and can exist as a stable co-orbital system. In the shear-dominated oligarchic phase of planet formation we show that the feeding zones contain several oligarchs instead of only one. Growth of the protoplanets in the oligarchic phase drives the disk to an equilibrium configuration that depends on the mass ratio of protoplanets to planetesimals, $\\Sigma/\\sigma$. Early in the oligarchic phase, when $\\Sigma/\\sigma$ is low, the spacing between rows of co-orbital oligarchs are about 5 Hill radii wide, rather than the 10 Hill radii cited in the literature. It is likely that at the end of oligarchy the average number of co-orbital oligarchs is greater than unity. In the outer solar system this raises the disk mass required to form the ice giants. In the inner solar system this lowers the mass of the final oligarchs and requires more giant impacts than previously estimated. This result provides additional evidence that Mars is not an untouched leftover from the oligarchic phase, but must be composed of several oligarchs assembled through giant impacts. ", "machine_abstract": "We study the orbital evolution and stability properties of oligarchic co-orbitals in the Solar System, i.e., bodies with masses comparable to that of Jupiter which are trapped on orbits close to those of Neptune or Uranus for billions of years.  We show how these objects can be identified by their long-term dynamical behavior as well as by their current positions relative to Neptune's orbit. The existence of such bodies is confirmed by numerical integrations over timescales up to 10 billion years using the symplectic N-body code SyMBA. In addition we find that there exist at least two other stable regions where oligarchs may reside. These results suggest that the Solar System contains several dozen oligarchic co-orbitals:  - At least four known trans-Neptunian objects (Pluto, Charon, Haumea, Makemake) have been found to exhibit this type of dynamics; - There exists another region around 30 AU containing three additional bodies (Sedna, 2000 CR 105 , 2003 SQ 317 ); - Finally, our simulations indicate that there might also be an additional group of oligarchs located between 50-60 AU.", "paraphrased_abstract": "The study reveals that the solar system is home to several dozen oligarchs. We have identified four of the known trans-Neptunian objects (Pluto, Charon, Haumea, Makema) and have found two more bodies (Sedna, 2000 CR 105, 2003 SM 317); we have found, besides, two other stable regions where oligarchs may be found. We show how these objects are shaped by the long-term dynamics of their environment, as well as by their current position relative to the orbit of Neptune or Uranus. The existence of these objects is confirmed by numerical integrations for the period of ten billion years with the symplectic N-body code. These results suggest that the solar system contains more than a dozen oligarchic co-orbitals: at least four known objects, Pluto, Charon, Haumea, Makem, have been characterized. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0731, "title": "The tensor part of the Skyrme energy density functional. I. Spherical nuclei", "abstract": "  We perform a systematic study of the impact of the J^2 tensor term in the Skyrme energy functional on properties of spherical nuclei. In the Skyrme energy functional, the tensor terms originate both from zero-range central and tensor forces. We build a set of 36 parameterizations, which covers a wide range of the parameter space of the isoscalar and isovector tensor term coupling constants, with a fit protocol very similar to that of the successful SLy parameterizations. We analyze the impact of the tensor terms on a large variety of observables in spherical mean-field calculations, such as the spin-orbit splittings and single-particle spectra of doubly-magic nuclei, the evolution of spin-orbit splittings along chains of semi-magic nuclei, mass residuals of spherical nuclei, and known anomalies of charge radii. Our main conclusion is that the currently used central and spin-orbit parts of the Skyrme energy density functional are not flexible enough to allow for the presence of large tensor terms. ", "machine_abstract": "We present an improved version of our previous calculation for the nuclear matter properties and the single-particle energies in spherical nuclei based on the covariant density functional theory with the tensor terms included. The new results are obtained by using the recently developed method to calculate the exchange-correlation contributions to the energy density functionals, which is free from any adjustable parameters. We find that the calculated binding energies per particle agree well with those observed experimentally except for some light nuclei such as 4He and 8Be. In particular, we obtain a reasonable value for the spin-orbit splitting between the p1/2 and p3/2 states in 16O nucleus. This indicates that the tensor force plays an important role in determining the shell structure around the Fermi surface. It should be noted here that this result has been achieved without introducing any additional parameter into the original formalisms proposed previously.     Keywords: Tensor force, Energy density functional", "paraphrased_abstract": "\u201cTensor force, Energy density function, density function tensor.  The energy density function has recently been modified, and it is free from any fixed parameters. The tensor force has an important role in the formation of the enveloping shells around the Fermi. The new calculations, which use the newly developed method of the density function, are calculated, without any addition of any variable parameter. The results are obtained, based on the density function of a spherical nucleus. The calculated binding energies are a close match to those of the experiments, except for the light spherical nuclei, such as 3He and 8Be. The spin of the spin orbit in the 16O nucleus, which is of an equal or inferior form, is a sufficient parameter to determine the shell structure around the Fermi surface. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2245, "title": "Wightman function and vacuum densities for a Z_2-symmetric thick brane in AdS spacetime", "abstract": "  Positive frequency Wightman function, vacuum expectation values of the field square and the energy-momentum tensor induced by a Z_{2}-symmetric brane with finite thickness located on (D+1)- dimensional AdS background are evaluated for a massive scalar field with general curvature coupling parameter. For the general case of static plane symmetric interior structure the expectation values in the region outside the brane are presented as the sum of free AdS and brane induced parts. For a conformally coupled massless scalar the brane induced part in the vacuum energy-momentum tensor vanishes. In the limit of strong gravitational fields the brane induced parts are exponentially suppressed for points not too close to the brane boundary. As an application of general results a special model is considered in which the geometry inside the brane is a slice of the Minkowski spacetime orbifolded along the direction perpendicular to the brane. For this model the Wightman function, vacuum expectation values of the field square and the energy-momentum tensor inside the brane are evaluated as well and their behavior is discussed in various asymptotic regions of the parameters. It is shown that for both minimally and conformally coupled scalar fields the interior vacuum forces acting on the brane boundaries tend to decrease the brane thickness. ", "machine_abstract": "We study the Wightman functions and vacuum densities on a Z_2-symmetric, thick brane embedded in an anti-de Sitter (AdS) space-time with one extra dimension. We find that there are two types of solutions to the corresponding equations depending on whether or not the bulk mass is zero. In both cases we show how these quantities can be expressed as sums over modified Bessel functions. The results obtained here may have applications in quantum field theory at finite temperature and/or density. PACS: 11.10.Kk, 12.20.Ds, 98.80.Cq Keywords: Vacuum expectation value, Anti-de Sitter space time, Thick brane, Modified Bessel function. 1 Introduction An interesting feature of string theories is their ability to incorporate gravity into the fundamental description of nature. This has led to renewed interest in studying gravitational backgrounds which admit supersymmetry [1] . One such class of spacetimes is given by the so-called warped product spaces [2] , where the metric takes the form ds2 = e2A(y)(\u03b7\u03bc\u03bddx\u03bc dx\u03bd + dy 2 ), (1) where y denotes the coordinate along the extra dimension, A(y) is called the warp factor and \u03b7\u03bc\u03bd is the Minkowski metric. For example, if we consider the five-dimensional case then this corresponds to the Randall-Sundrum model [3] . In recent years it was shown [4] - [8] that the presence of a nontrivial warp factor leads to new features in the physics associated with fields propagating in the bulk. These include modifications to the standard dispersion relations [9] , spontaneous symmetry breaking [10] , fermion localization [11] , etc.. It turns out [12] that the effects due to the warp factor depend crucially upon its behaviour near the boundary of the extra dimension. If the warp factor vanishes sufficiently rapidly at infinity then all physical observables will be identical to those computed using ordinary flat-space techniques. However, if the warp factor does not vanish fast enough then some novel phenomena occur.", "paraphrased_abstract": "It has been found that a non-trivial, non-zero-mass factor (as opposed to a negligible factor) leads to new physical phenomena, which include the spontaneous symmetry breaking, the localization of fermions, etc. ; it is known, therefore, that the effects of the non-zero-mass factor depend very much on its being near the edge of the extra-dimensional space; if this non-zero-mass factor does not disappear quickly enough, then the physical observables will be as good as those calculated with ordinary flat-space techniques; however, if the non-zero-mass factor does not disappear rapidly enough, new phenomena may occur. We study the Wightman functions and the vacuum density of a thick boras in an anti-de-de-de-de-de space. We obtain two solutions to the equations, depending on whether the mass of the bulk is zero or not; we show that these solutions are expressed as the sums of modified Bessel functions, and this can be used in a field theory of quantum field theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2244, "title": "Viscous Torque and Dissipation in the Inner Region of a Thin Accretion Disk: Implications for Measuring Black Hole Spin", "abstract": "  We consider a simple Newtonian model of a steady accretion disk around a black hole. The model is based on height-integrated hydrodynamic equations, alpha-viscosity, and a pseudo-Newtonian potential that results in an innermost stable circular orbit (ISCO) that closely approximates the one predicted by GR. We find that the hydrodynamic models exhibit increasing deviations from the standard disk model of Shakura & Sunyaev as disk thickness H/R or the value of alpha increases. The latter is an analytical model in which the viscous torque is assumed to vanish at the ISCO. We consider the implications of the results for attempts to estimate black hole spin by using the standard disk model to fit continuum spectra of black hole accretion disks. We find that the error in the spin estimate is quite modest so long as H/R < 0.1 and alpha < 0.2. At worst the error in the estimated value of the spin parameter is 0.1 for a non-spinning black hole; the error is much less for a rapidly spinning hole. We also consider the density and disk thickness contrast between the gas in the disk and that inside the ISCO. The contrast needs to be large if black hole spin is to be successfully estimated by fitting the relativistically-broadened X-ray line profile of fluorescent iron emission from reflection off an accretion disk. In our hydrodynamic models, the contrast in density and thickness is low when H/R>0.1, sugesting that the iron line technique may be most reliable in extemely thin disks. We caution that these results have been obtained with a viscous hydrodynamic model and need to be confirmed with MHD simulations of radiatively cooled thin disks. ", "machine_abstract": "We present new results on viscous torque and dissipation in thin accretion disks, focusing on their implications for measuring black hole spin using continuum-fitting techniques. We find that the magnitude of the viscous torque is strongly dependent upon the radial location at which it is evaluated; this dependence arises because the disk's surface density profile varies with radius. The net effect is to produce an apparent warp in the inner region of the disk (r < 10 GM/c2), where the observed flux depends sensitively on the viewing angle. This warp can be misinterpreted as evidence for retrograde precession if one assumes that the disk is axisymmetric. In addition, we show that the total energy dissipated within r = 3 GM/c2 may exceed the value inferred by fitting the spectrum with a standard Shakura-Sunyaev model. These effects are particularly important when attempting to measure the spins of supermassive black holes in AGN.", "paraphrased_abstract": "A certain extent of viscous torque, however, is strongly affected by the radius at which it is analyzed; this is due to the fact that the density of the surface of the disk is constantly changing. Moreover, we show that the total energy of the 3 GM/c2 spectrum is not equal to the value of the formula in which we are able to compute the spin of supermassive black holes. This is particularly important for measuring the spin of supermassive black holes. In particular, we present a new analysis of viscous force and the dissipation of the thin accretion disks, and present the implications for the measurement of the spin of supermassive black holes in AGN. We show that the magnitude of the viscous force is highly dependent on the radial location of the disk; the radial location depends on the density of the surface. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4198, "title": "Exotic Mott phases of the extended t--J model on the checkerboard lattice at commensurate densities", "abstract": "  Coulomb repulsion between electrons moving on a frustrated lattice can give rise, at simple commensurate electronic densities, to exotic insulating phases of matter. Such a phenomenon is illustrated using an extended t--J model on a planar pyrochlore lattice for which the work on the quarter-filled case [cond-mat/0702367] is complemented and extended to 1/8- and 3/8-fillings. The location of the metal-insulator transition as a function of the Coulomb repulsion is shown to depend strongly on the sign of the hopping. Quite generally, the metal-insulator transition is characterized by lattice symmetry breaking but the nature of the insulating Mott state is more complex than a simple Charge Density Wave. Indeed, in the limit of large Coulomb repulsion, the physics can be described in the framework of (extended) quantum fully-packed loop or dimer models carrying extra spin degrees of freedom. Various diagonal and off-diagonal plaquette correlation functions are computed and the low-energy spectra are analyzed in details in order to characterize the nature of the insulating phases. We provide evidence that, as for an electronic density of n=1/2 (quarter-filling), the system at $n=1/4$ or $n=3/4$ exhibits also plaquette order by forming a (lattice rotationally-invariant) Resonating-Singlet-Pair Crystal, although with a quadrupling of the lattice unit cell (instead of a doubling for $n=1/2$) and a 4-fold degenerate ground state. Interestingly, qualitative differences with the bosonic analog (e.g. known to exhibit columnar order at n=1/4) emphasize the important role of the spin degrees of freedom in e.g. stabilizing plaquette phases w.r.t. rotational symmetry-breaking phases. ", "machine_abstract": "We study the ground state properties and excitations in the extended t-J model with nearest-neighbor hopping (NNH) and next-nearest neighbor hopping (NNN), which is defined by the following Hamiltonian:  where c,c are fermion annihilation operators for electrons with spin , respectively; n,n denote electron number operators for sites i,j, respectively.         We consider this model on the checkerboard square lattice at half-filling where each site has one electron. The NNH term breaks time-reversal symmetry while the NNN term preserves it. In order to investigate the effect of these terms we use exact diagonalization method combined with density matrix renormalization group technique. We find that there exist three different types of magnetic orders depending on the ratio between NNH and NNN hoppings. These results suggest that the extended t-J model can be used as an effective model for describing high-Tc cuprates.", "paraphrased_abstract": "The NNH term disturbs the time-reversal symmetry, while the NNN term maintains it. To investigate the effects of these terms, we use an exact diagonalization and density matrix renormalization technique. We investigate the effects of the NNH and NNN terms, which are defined by the following Hamiltonian: c, c are fermions annihilation operators for electrons spinnable, n, n are number operators for electrons i, j. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2674, "title": "Hydrodynamical simulation of detonations in superbursts. I. The hydrodynamical algorithm and some preliminary one-dimensional results", "abstract": "  Aims. This work presents a new hydrodynamical algorithm to study astrophysical detonations. A prime motivation of this development is the description of a carbon detonation in conditions relevant to superbursts, which are thought to result from the propagation of a detonation front around the surface of a neutron star in the carbon layer underlying the atmosphere. Methods. The algorithm we have developed is a finite-volume method inspired by the original MUSCL scheme of van Leer (1979). The algorithm is of second-order in the smooth part of the flow and avoids dimensional splitting. It is applied to some test cases, and the time-dependent results are compared to the corresponding steady state solution. Results. Our algorithm proves to be robust to test cases, and is considered to be reliably applicable to astrophysical detonations. The preliminary one-dimensional calculations we have performed demonstrate that the carbon detonation at the surface of a neutron star is a multiscale phenomenon. The length scale of liberation of energy is $10^6$ times smaller than the total reaction length. We show that a multi-resolution approach can be used to solve all the reaction lengths. This result will be very useful in future multi-dimensional simulations. We present also thermodynamical and composition profiles after the passage of a detonation in a pure carbon or mixed carbon-iron layer, in thermodynamical conditions relevant to superbursts in pure helium accretor systems. ", "machine_abstract": "We present the results of our first attempt to simulate superbursts using two-dimensional hydrodynamics with realistic nuclear physics input. We use an Eulerian scheme for solving the equations of motion, which is based on operator splitting into advection and source terms. Our code includes detailed models for the equation of state (EOS) as well as for the reaction rates of all relevant nuclear reactions. In particular we include electron captures by nuclei up to nickel, beta-decays, fission processes, photodisintegrations, photoabsorptions, and inverse beta decays. For the EOS we employ a tabulated model that was constructed recently within the framework of the H-He-C-O-Ne-S system. This table contains thermodynamic quantities such as pressure, energy density, entropy density, specific heat capacity at constant volume, etc., as functions of temperature and composition. It covers temperatures between 10^7 K and 5\u00d710^9 K and compositions ranging from pure helium to pure iron.", "paraphrased_abstract": "\u201cSince you have been so firmly established in the knowledge of hydrodynamics, the two-dimensional geometry is now possible. The system consists of temperatures from a few degrees to about ten degrees Celsius, and in all its forms, helium and iron are represented. Our system is based on an Eulerian model of the motion of electrons, which is based on the concept of advection and source. Our method considers the equation of state (EOS), the reaction rates of all nuclear reactions. The EOS is divided into the four equations: the pressure, the energy density, the entropy density, the specific heat capacity at constant volume, and so on. We consider the presence of electrons in the nuclei of nickel, beta decays, photodisintegrations, photoabsorptions, and inverse beta decays. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3647, "title": "The Baryon Fractions and Mass-to-Light Ratios of Early-Type Galaxies", "abstract": "  We jointly model 22 early-type gravitational lens galaxies with stellar dynamical measurements using standard CDM halo models. The sample is inhomogeneous in both its mass distributions and the evolution of its stellar populations unless the true uncertainties are significantly larger than the reported measurement errors. In general, the individual systems cannot constrain halo models, in the sense that the data poorly constrains the stellar mass fraction of the halo. The ensemble of systems, however, strongly constrains the average stellar mass represented by the visible galaxies to $0.026\\pm0.006$ of the halo mass if we neglect adiabatic compression, rising to $0.056\\pm0.011$ of the halo mass if we include adiabatic compression. Both estimates are significantly smaller than the global baryon fraction, corresponding to a star formation efficiency for early-type galaxies of $10%-30%$. In the adiabatically compressed models, we find an average local B-band stellar mass-to-light ratio of $(M/L)_0 = (7.2\\pm0.5)(M_{\\sun}/L_{\\sun})$ that evolves by $d\\log(M/L)/dz = -0.72\\pm0.08$ per unit redshift. Adjusting the isotropy of the stellar orbits has little effect on the results. The adiabatically compressed models are strongly favored if we impose either local estimates of the mass-to-light ratios of early-type galaxies or the weak lensing measurements for the lens galaxies on 100 kpc scales as model constraints. ", "machine_abstract": "We present new measurements of the baryonic mass fractions (f bar ) in early-type galaxies, based on spatially resolved kinematics for a sample of 12 nearby elliptical/S0 galaxies observed with integral field spectroscopy at optical wavelengths.  We use these data to measure f bar , as well as the total stellar masses M * . The results are compared against predictions from semi-analytic models of galaxy formation within the standard \u039bCDM cosmology. Our main conclusions are:  1) For our sample we find that the mean value of f bar is 0.16 \u00b1 0.04, which agrees very well with previous estimates obtained using different techniques. 2) There exists no significant correlation between f bar and either luminosity or velocity dispersion. 3) Semi-analytic models predict values of f bar that are systematically lower than those measured here by about a factor of two. 4) In order to match the observations, it appears necessary to invoke additional physical processes beyond those included in current models.", "paraphrased_abstract": "In our first paper we present a new estimation of the f-bar in the early universe from the kinematics of twelve elliptical and S0 galaxies, which were observed with integral field spectroscopy at optical wavelengths, and a new measure of the total stellar mass. Our results show that for our sample f-bar is 0.1  0.14, which agrees very well with the estimations of other cosmopolitans. We compare these estimates with the predictions of semi-analytical models of the formation of galaxyes in the Standard Model of CDM. The results show that f-bar has a significant relationship with the luminosity of the galaxy, and with the velocity of the accretion. This seems to be an unusual phenomenon. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2604, "title": "Universal Substructure Distributions in LCDM halos: Can we find a Fossil Group?", "abstract": "  We use large cosmological N-body simulations to study the subhalo population in galaxy group sized halos. In particular, we look for fossil group candidates with typical masses ~10-25% of Virgo cluster but with an order of magnitude less substructure. We examine recent claims that the earliest systems to form are deficient enough in substructure to explain the luminosity function found in fossil groups. Although our simulations show a correlation between the halo formation time and the number of subhalos, the maximum suppression of subhalos is a factor of 2-2.5, whereas a factor of 6 is required to match fossil groups and galaxies. While the number of subhalos depends weakly on the formation time, the slope of the halo substructure velocity function does not. The satellite population within Cold Dark Matter (CDM) halos is self-similar at scales between galaxies and galaxy clusters regardless of mass, whereas current observations show a break in self-similarity at a mass scale corresponding to group of galaxies. ", "machine_abstract": "We study the universal substructure distribution (USSD) for dark matter halos in the Lambda-Cold Dark Matter model, and its dependence on halo mass and redshift. We use high-resolution N-body simulations to calculate the USSDs at redshifts z = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 . The results show that the USSDs are independent of redshift within our numerical accuracy. However, they do depend strongly on halo mass; more massive halos have larger USSDs than less massive ones. This is consistent with previous studies based on semi-analytic models or hydrodynamic simulations. In addition, we also find that there exists an upper limit to the number density of subhalos around any given host halo. Finally, by comparing the simulated USSDs with observations, we conclude that it may be possible to detect fossil groups using future surveys such as LSST.", "paraphrased_abstract": "We compared these simulated indices with those derived from observations and concluded that it may be possible to discover the occurrence of fossil groups through future surveys such as the LSST. We then compared the distribution of the sphere of dark matter in the model of the Lambda-Cold Dark Matter, and found that it was independent of the sphere of dark matter. Compared with the sphere of dark matter, it was much more significant than the sphere mass. The sphere of dark matter was investigated using high-resolution N-body simulations and found that they were independent of the sphere of dark matter. In this respect we found that the distribution of the sphere of dark matter was independent of the sphere of dark matter and the redshift of the sphere. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1083, "title": "An X-ray Spectral Analysis of the Central Regions of NGC 4593", "abstract": "  We present a detailed analysis of XMM-Newton EPIC-pn data for the Seyfert-1 galaxy NGC 4593. We discuss the X-ray spectral properties of this source as well as its variations with time. The 0.5-10 keV spectrum shows significant complexity beyond a simple power-law form, with clear evidence existing for a \"soft excess\" as well as absorption by highly ionized plasma (a warm absorber) within the central engine of this active galactic nucleus. We show that the soft excess is best described as originating from thermal Comptonization by plasma that is appreciably cooler than the primary X-ray emitting plasma; we find that the form of the soft excess cannot be reproduced adequately by reflection from an ionized accretion disk. The only measurable deviation from the power-law continuum in the hard spectrum comes from the presence of cold and ionized fluorescent iron-K emission lines at 6.4 and 6.97 keV, respectively. While constraints on the ionized iron line are weak, the cold line is found to be narrow at CCD-resolution with a flux that does not track the temporal changes in the underlying continuum, implying an origin in the outer radii of the accretion disk or the putative molecular torus of Seyfert unification schemes. The X-ray continuum itself varies on all accessible time scales. We detect a ~230-second time-lag between soft and hard EPIC-pn bands that, if interpreted as scattering timescales within a Comptonizing disk corona, can be used to constrain the physical size of the primary X-ray source to a characteristic length scale of ~2 gravitational radii. Taken together, the small implied coronal size and the large implied iron line emitting region indicate a departure from the current picture of a \"typical\" AGN geometry. ", "machine_abstract": "We have performed an analysis of archival Chandra data for the elliptical galaxy NGC 4593, which is known to harbor a supermassive black hole (SMBH) with mass M BH = 1.7 x 10^8 M sun . We find that there are two distinct components in its nuclear spectrum; one component has a power-law shape and dominates at energies above 2 keV while another component shows strong emission lines below 2 keV. The former can be explained by Comptonization of soft photons emitted from the accretion disk around SMBHs, whereas the latter may originate from photoionized plasma surrounding the central engine or from relativistic jets. In addition, we found evidence for absorption features due to ionized iron K-shell transitions near 6.4-6.9 keV. These results suggest that this object harbors both a standard thin accretion disk as well as a hot corona.", "paraphrased_abstract": "It is shown that this object contains both a slit accretion disk and a hot corona. This is consistent with the fact that this object is composed of a thin accretion disk and a hot corona. We have studied the archival Chandra data for the elliptical galaxy NGC 4593, and have observed that it contains a supermassive black hole (SMBH) with a mass of M BH = 1.7 x 108 M. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2745, "title": "Long-Term Evolution of Massive Black Hole Binaries. III. Binary Evolution in Collisional Nuclei", "abstract": "  [Abridged] In galactic nuclei with sufficiently short relaxation times, binary supermassive black holes can evolve beyond their stalling radii via continued interaction with stars. We study this \"collisional\" evolutionary regime using both fully self-consistent N-body integrations and approximate Fokker-Planck models. The N-body integrations employ particle numbers up to 0.26M and a direct-summation potential solver; close interactions involving the binary are treated using a new implementation of the Mikkola-Aarseth chain regularization algorithm. Even at these large values of N, two-body scattering occurs at high enough rates in the simulations that they can not be simply scaled to the large-N regime of real galaxies. The Fokker-Planck model is used to bridge this gap; it includes, for the first time, binary-induced changes in the stellar density and potential. The Fokker-Planck model is shown to accurately reproduce the results of the N-body integrations, and is then extended to the much larger N regime of real galaxies. Analytic expressions are derived that accurately reproduce the time dependence of the binary semi-major axis as predicted by the Fokker-Planck model. Gravitational wave coalescence is shown to occur in <10 Gyr in nuclei with velocity dispersions below about 80 km/s. Formation of a core results from a competition between ejection of stars by the binary and re-supply of depleted orbits via two-body scattering. Mass deficits as large as ~4 times the binary mass are produced before coalescence. After the two black holes coalesce, a Bahcall-Wolf cusp appears around the single hole in one relaxation time, resulting in a nuclear density profile consisting of a flat core with an inner, compact cluster, similar to what is observed at the centers of low-luminosity spheroids. ", "machine_abstract": "We present the results of long-term numerical simulations of binary black hole (BBH) evolution, including gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption. We focus on binaries with total mass M = 100-1000M\u2299 that evolve through collisional nuclear environments at high redshifts z > 10. Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds. In particular we investigate whether these systems can reach masses above 1000M\u2299 before merging within a Hubble time. The initial conditions for our models were obtained using Monte Carlo sampling of the distribution function of isolated BBHs constructed by Belczynski et al. (2010) . For each model we performed several runs starting from different orbital configurations. All calculations were carried out assuming circular orbits. We find that most of the massive binaries merge within a few hundred million years after formation due to emission of gravitational waves. However, some of them survive until today if they form in regions where the density of surrounding gas exceeds $10^{9}$ cm$^{-3}$. These binaries may be detectable by future space-based gravitational wave observatories like LISA or DECIGO/BBO.", "paraphrased_abstract": "For each of these calculations, we applied a Monte Carlo sampling method to the distribution of isolated BBHs, based on the study of the distribution of isolated BBHs by Belczynski and others. The first conditions of our models are: they are based on the distribution of a large mass of BBHs, M = 100-1000M, and whose mass is about 100-1000M, and which, as a rule, can reach a mass of over 1000M. Moreover, we investigate whether they can be attained before Hubble time. In the course of our work we apply the results of long-term numerical simulations of BBHs, with gravitational radiation and general relativistic effects, e.g., dispersion of tidal frames and the collapse of tidal frames. In the course of our work we use Monte Carlo sampling, which is derived from the distribution function of isolated BBHs by Belczynski et al. (2010a), a swarm of BBHs that have a mass of at least 100,000,000,000,000,000, which in turn can be studied by means of future space-based gravitational waves, such as LISA or DECIGO/BBO. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2115, "title": "Uncovering the Internal Structure of the Indian Financial Market: Cross-correlation behavior in the NSE", "abstract": "  The cross-correlations between price fluctuations of 201 frequently traded stocks in the National Stock Exchange (NSE) of India are analyzed in this paper. We use daily closing prices for the period 1996-2006, which coincides with the period of rapid transformation of the market following liberalization. The eigenvalue distribution of the cross-correlation matrix, $\\mathbf{C}$, of NSE is found to be similar to that of developed markets, such as the New York Stock Exchange (NYSE): the majority of eigenvalues fall within the bounds expected for a random matrix constructed from mutually uncorrelated time series. Of the few largest eigenvalues that deviate from the bulk, the largest is identified with market-wide movements. The intermediate eigenvalues that occur between the largest and the bulk have been associated in NYSE with specific business sectors with strong intra-group interactions. However, in the Indian market, these deviating eigenvalues are comparatively very few and lie much closer to the bulk. We propose that this is because of the relative lack of distinct sector identity in the market, with the movement of stocks dominantly influenced by the overall market trend. This is shown by explicit construction of the interaction network in the market, first by generating the minimum spanning tree from the unfiltered correlation matrix, and later, using an improved method of generating the graph after filtering out the market mode and random effects from the data. Both methods show, compared to developed markets, the relative absence of clusters of co-moving stocks that belong to the same business sector. This is consistent with the general belief that emerging markets tend to be more correlated than developed markets. ", "machine_abstract": "The present study examines the cross correlation between stock returns and volatility on the National Stock Exchange (NSE) for the period January 2000 to December 2010 using daily data. The results show that there is significant positive relationship between return-volatility pairs, which implies that past information about one variable can be used to predict future values of another variable. Further analysis reveals that this relationship varies across different market segments as well as over time. In addition, we find evidence of asymmetry in the relationship between returns and volatility. Finally, our findings suggest that the relationship between returns and their corresponding volatilities are not stable but change with changes in economic conditions. This finding has important implications for risk management practices by investors and portfolio managers. Keywords: Volatility, Returns, Correlations, Asymmetries, Economic Conditions, Risk Management Practices. 1 Introduction Investors often use historical information regarding asset prices or returns to make investment decisions. However, it may also be useful to consider how past information about an asset's price movements affects its subsequent volatility. For example, if past information about an asset leads to higher returns then it should lead to greater uncertainty among investors resulting in increased volatility. Similarly, if past information about returns leads to lower returns then it would reduce investor uncertainty leading to reduced volatility. Thus, understanding the nature of the relationship between returns and subsequent volatility could help investors better manage risks associated with their portfolios. A number of studies have examined the relationship between returns and various measures of volatility such as variance, standard deviation, absolute value of returns etc. (see e.g., Engle et al., 1987; Bollerslev et al., 1992; Glosten et al., 1993) . These studies generally report a positive relationship between returns and volatilities suggesting that past information about returns helps explain current levels of volatility.", "paraphrased_abstract": "The present paper investigates the relationship between returns and volatility of the N.S. market, comparing the data from the two weeks of January 2000 to the month of December 2010, in the present study. We found a positive relationship between returns and volatility, which suggests that the past information about returns can explain the current volatility. We also find that the correlation between returns and volatility is not constant but changes with the changes in economic conditions. For example, if the past information about an asset is higher, the uncertainty of the market will increase, which increases volatility. For example, if the previous information about an asset is higher, then the uncertainty increases, which reduces volatility, and so on. In the present study, we examine the correlation between returns and volatility in the N.S. in the period from January 2000 to December 2010. We study the cross-correlation between returns and volatility on the N.S. from January 2000 to December 2010. The results show that the correlation between returns and volatility varies across markets, time and time again. In addition, there is a symmetry in the correlation between returns and volatility. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2415, "title": "Near-Infrared and X-ray Observations of the Enigmatic G70.7+1.2", "abstract": "  We present high resolution imaging of the puzzling radio and optical nebula G70.7+1.2 with the Keck Observatory's laser guide star adaptive optics (LGS-AO) system and the Chandra X-ray Observatory. The archival X-ray observations show a hard (Gamma ~ 1.8), low luminosity (L_X ~ 4 x 10^31 ergs/s) point source at the center of the nebula. Follow-up LGS-AO near-infrared imaging of the Chandra error circle reveals a relatively bright (K' ~ 14 magnitude) counterpart. Both its color and brightness are consistent with a heavily obscured B-star or possibly a late-G/early-K giant. The most plausible explanation is that this newly discovered X-ray source is a non-accreting B-star/pulsar binary powering the radio and optical nebula. If so, the luminous Be-star discussed in the literature seemingly embedded in the nebula is not the dominant force responsible for shaping G70.7+1.2. Thus, we suggest that G70.7+1.2 is the result of two unrelated objects (a B-star X-ray binary and a Be star) interacting with a dense molecular cloud. With this explanation we believe we have solved the mystery of the origin of G70.7+1.2. ", "machine_abstract": "We report on near-infrared (NIR) photometry, optical spectroscopy, and X-ray observations of the enigmatic radio source G70.7+1.     The NIR data show that this object is an extremely reddened star with A V = 25 mag located at a distance of about 5 kpc in front of the supernova remnant W28. We find no evidence for any infrared excess emission indicative of dust formation around the central star. Optical spectra reveal strong P-Cygni profiles indicating mass loss rates as high as 10^-5 M_sol/yr. In addition to these features we detect narrow absorption lines which are likely due to interstellar material along our line-of-sight towards the star. Finally, we present Chandra ACIS-S X-ray images showing extended diffuse emission surrounding the radio source. This emission may be associated with shocked gas produced by stellar winds or outflows from the central star.", "paraphrased_abstract": "\u201cG70.7+1 is a radio source with a reddish-red hue and a ten-percent angular mass, and it is about five kilometers from the center of the supernova remnant, W28. Then we present Chandra ACIS-S X-ray images, which reveal a diffuse gaseous irradiation that was attributed to the evaporation of gaseous gas from the central star. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3718, "title": "Gamma-Ray Burst Afterglows as Probes of Environment and Blastwave Physics II: the Distribution of p and Structure of the Circumburst Medium", "abstract": "  We constrain blastwave parameters and the circumburst media of a subsample of ten BeppoSAX Gamma-Ray Bursts. For this sample we derive the values of the injected electron energy distribution index, p, and the density structure index of the circumburst medium, k, from simultaneous spectral fits to their X-ray, optical and nIR afterglow data. The spectral fits have been done in count space and include the effects of metallicity, and are compared with the previously reported optical and X-ray temporal behaviour. Using the blastwave model and some assumptions which include on-axis viewing and standard jet structure, constant blastwave energy and no evolution of the microphysical parameters, we find a mean value of p for the sample as a whole of 2.04 +0.02/-0.03. A statistical analysis of the distribution demonstrates that the p values in this sample are inconsistent with a single universal value for p at the 3-sigma level or greater, which has significant implications for particle acceleration models. This approach provides us with a measured distribution of circumburst density structures rather than considering only the cases of k=0 (homogeneous) and k=2 (wind-like). We find five GRBs for which k can be well constrained, and in four of these cases the circumburst medium is clearly wind-like. The fifth source has a value of 0<k<1, consistent with a homogeneous circumburst medium. ", "machine_abstract": "We present new results on the distribution of the electron index, p, in GRB afterglow models using data from Swift/BATSE bursts with known redshifts (z < 5). We find that the distribution is consistent with being log-normal for both low-z and high-z samples. The mean value of p decreases slightly with increasing redshift but there are no significant differences between the two distributions at fixed z. This suggests that the physical processes which determine the value of p do not evolve significantly over cosmic time scales. Using our best-fit values we show how the observed X-ray light curves can be used to constrain the structure of the circumburst medium. In particular, we demonstrate that it should be possible to distinguish between different density profiles by comparing the temporal decay slopes of the early-time X-ray emission with those measured later during the prompt phase.     Keywords: Gamma-ray bursts, Electron index", "paraphrased_abstract": "Using the best-fitting value of the X-ray light curve, we show how the light curve of the circumburst medium can be used to constrain the structure of the inner sphere. In particular, we show that one can distinguish between different density profiles by comparing the early decay rates of the X-ray light with those that are measured later in the prompt phase. We are presenting new results on the distribution of the electron indices, p, in the GRB afterglow, from the Swift-BATSE bursts, with known redshifts of z5. The distribution of p increases slightly with increasing redshifts, but there are no significant differences between the two distributions at fixed z. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3685, "title": "Flow instabilities of magnetic flux tubes II. Longitudinal flow", "abstract": "  Flow-induced instabilities are relevant for the storage and dynamics of magnetic fields in stellar convection zones and possibly also in other astrophysical contexts. We continue the study started in the first paper of this series by considering the stability properties of longitudinal flows along magnetic flux tubes. A linear stability analysis was carried out to determine criteria for the onset of instability in the framework of the approximation of thin magnetic flux tubes. In the non-dissipative case, we find Kelvin-Helmholtz instability for flow velocities exceeding a critical speed that depends on the Alfv{\\'e}n speed and on the ratio of the internal and external densities. Inclusion of a friction term proportional to the relative transverse velocity leads to a friction-driven instability connected with backward (or negative energy) waves. We discuss the physical nature of this instability. In the case of a stratified external medium, the Kelvin-Helmholtz instability and the friction-driven instability can set in for flow speeds significantly lower than the Alfv{\\'e}n speed. Dissipative effects can excite flow-driven instability below the thresholds for the Kelvin-Helmholtz and the undulatory (Parker-type) instabilities. This may be important for magnetic flux storage in stellar convection zones and for the stability of astrophysical jets. ", "machine_abstract": "We study the stability properties of longitudinal flows in straight and curved magnetic flux tubes using linearized ideal MHD equations. We find that for sufficiently large values of plasma beta, there is always an unstable mode with zero frequency (i.e., static) which grows exponentially fast at small wavenumbers. The growth rate increases monotonically as we increase the value of plasma beta.     For smaller values of plasma beta, however, this instability disappears completely. In fact, we show analytically that if the plasma beta is less than some critical value then all modes are stable irrespective of their frequencies or wavelengths. This result agrees well with our numerical simulations. Finally, we also present results on the effect of curvature on the stability properties of longitudinal flow. It turns out that the presence of curvature has no significant effect on the stability properties of these flows. However, it does affect the nature of the eigenfunctions associated with different eigenvalues.", "paraphrased_abstract": "A measure of the stability of longitudinal flows in straight and curved magnetic flanges is given. The steady state of the flow is given as a constant, or constant, value, and as a constant, but the stability is not at all affected by the existence of curvature. However, curvature has a negative effect on the stability of the flows, although it does influence the eigenfunctions associated with different values. We find, for sufficiently high beta, that the stable state, with a zero frequency, has always a steady state, which grows exponentially and inversely with the number of waves. However, the stability of the system becomes totally destroyed, with the increasing of beta. The numerical results agree with our numerical simulations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3174, "title": "A new chaotic attractor in a basic multi-strain epidemiological model with temporary cross-immunity", "abstract": "  An epidemic multi-strain model with temporary cross-immunity shows chaos, even in a previously unexpected parameter region. Especially dengue fever models with strong enhanced infectivity on secondary infection have previously shown deterministic chaos motivated by experimental findings of antibody-dependent-enhancement (ADE). Including temporary cross-immunity in such models, which is common knowledge among field researchers in dengue, we find a deterministically chaotic attractor in the more realistic parameter region of reduced infectivity on secondary infection (''inverse ADE'' parameter region). This is realistic for dengue fever since on second infection people are more likely to be hospitalized, hence do not contribute to the force of infection as much as people with first infection.   Our finding has wider implications beyond dengue in any multi-strain epidemiological systems with altered infectivity upon secondary infection, since we can relax the condition of rather high infectivity on secondary infection previously required for deterministic chaos. For dengue the finding of wide ranges of chaotic attractors open new ways to analysis of existing data sets. ", "machine_abstract": "We study the dynamics of an epidemic system consisting of two strains and three classes, susceptible (S), infected (I) and recovered/removed (R). The infection rates are assumed to be bilinear functions of both strain densities. We show that this system has a unique positive equilibrium which is globally asymptotically stable if one of the following conditions holds: (i) the recovery rate for each strain is greater than its corresponding infection rate; or  (ii) there exists at least one strain whose recovery rate is equal to its infection rate.  However, we prove that when condition (i) fails but condition (ii) holds true, then the system exhibits chaos through numerical simulations. Finally, we present some results on global stability by using Lyapunov functionals. In recent years, many mathematical models have been proposed to describe the transmission dynamics of infectious diseases [1] . These models can be classified into single-strain models [2] , multi-strain models [3] - [6] and metapopulation models [7] . In particular, multi-strain models play important roles in understanding how different pathogens interact within hosts [8] . For example, it was shown that co-infection may lead to extinction [9] ; while superinfection may cause periodic oscillations [10] . Recently, Li et al. [11] studied a multi-strain epidemic model with nonlinear incidence rates and found that the disease-free equilibrium is locally asymptotically stable under certain conditions. However, they did not consider the effect of cross immunity between strains. Cross immunity refers to partial protection against subsequent infections caused by other strains [12] . It plays an important role in preventing epidemics [13] . Therefore, it should be taken into account in modeling the spread of infectious diseases [14] .", "paraphrased_abstract": "It is important to know the dynamics of diseases. The latest models, including single strains, multi-strain models, and metapopulation models, have been developed. These models, as well as multi-strain models, have been used to describe the dynamics of the disease transmission in humans. In this regard, multi-strain models have been of great interest, especially in the study of how different organisms intertwine. For instance, a co-infection leads to extinction, while a superinfection may lead to periodic oscillations. It is also important to recognize the influence of cross-strain immunity in preventing the spread of infectious diseases. Recently, Li et al. studied the dynamics of an epidemic system with nonlinearity, and found that the equilibrium of the epidemic was in fact asymptotically stable under certain conditions. We proved that this system was unique in that it contained a unique positive equilibrium, which is universally stable under one of the following conditions: (i) the recovery rate for each strain is greater than its infection rate; or (ii) there is at least one strain whose recovery rate is equal to its infection rate. We also show how global stability is achieved by arithmetic. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1958, "title": "Line and continuum variability of two intermediate-redshift, high-luminosity quasars", "abstract": "  It has been shown that the luminosity of AGNs and the size of their broad line region obey a simple relation of the type R=a L^g, from faint Seyfert nuclei to bright quasars, allowing single-epoch determination of the central black hole mass M=b L^g D^2 from their luminosity L and width of H_beta emission line. Adopting this mass determination for cosmological studies requires the extrapolation to high z and L of a relation whose calibration relies so far on reverberation mapping measurements performed for L<10^46 erg/s and z<0.4. We initiated a campaign for the monitoring of a few luminous, intermediate z quasars whose apparent magnitude V<15.7 allows observations with a 1.8m telescope, aimed at proving that emission lines vary and respond to continuum variations even for luminosities >10^47 erg/s, and determining eventually their M_BH from reverberation mapping. We have repeatedly performed simultaneous observations of quasars and reference stars to determine relative variability of continuum and emission lines. We describe the observations and methods of analysis. For the quasars PG1634+706 and PG1247+268 we obtain light-curves respectively for CIII], MgII and for CIV, CIII] emission lines with the relevant continua. During 3.2 years of observation, in the former case no continuum variability has been detected and the evidence for line variability is marginal, while in the latter case both continuum and line variability are detected with high significance and the line variations appear correlated with continuum variations. The detection of the emission line variability in a quasar with L~10^47 erg/s encourages the prosecution of the campaign which should provide a black hole mass estimate in other 5-6 years, constraining the M_BH-L relation in a poorly explored range of luminosity. ", "machine_abstract": "We present the results of an optical monitoring campaign on two luminous quasars at redshifts z = 1.7 and 2.1 with the aim to study their long-term line and continuum variability properties. The observations were carried out in the period between September 2005 and December 2007 using the Nordic Optical Telescope (NOT) equipped with ALFOSC. We find that both objects show significant variations over time scales ranging from months up to years. In particular we detect strong changes in the H\u03b2 emission-line profiles which are accompanied by corresponding flux density fluctuations in the adjacent continuum regions. These findings suggest that the observed spectral changes can be explained as being due to variable obscuration effects caused by clouds moving across our line-of-sight towards the central engine. This scenario is supported by the fact that the detected variabilities appear to occur simultaneously for all three Balmer lines studied here. Furthermore, we find evidence for additional short-term variability events occurring within individual nights.", "paraphrased_abstract": "The observed variations are particularly significant in the H and in the corresponding flux density of the continuum. This explanation is supported by the fact that they all occur simultaneously, for all three Balmer lines. The results of a scientific study on two luminous quasars z = 1.7 and 2.1 were performed, comparing them in the period between September 2005 and December 2007, on the Nordic optical telescope (NOT) equipped with ALFOSC. We observe the changes in the spectral properties of the two objects, which are of significant radiance over the entire length of time. Moreover, we observe that the radiance of the two objects is influenced by variable obscuration, the effect of clouds, which are moving in a variable direction to the central engine. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0027, "title": "High-energy Cosmic Rays and Neutrinos from Semi-relativistic Hypernovae", "abstract": "  The origin of the ultrahigh-energy (UHE) cosmic rays (CRs) from the second knee ($\\sim6\\times10^{17}$eV) above in the CR spectrum is still unknown. Recently, there has been growing evidence that a peculiar type of supernovae, called hypernovae, are associated with sub-energetic gamma-ray bursts (GRBs), such as SN1998bw/GRB980425 and SN2003lw/GRB031203. Such hypernovae appear to have high (up to mildly relativistic) velocity ejecta, which may be linked to the sub-energetic GRBs. Assuming a continuous distribution of the kinetic energy of the hypernova ejecta as a function of its velocity $E_k\\propto (\\Gamma\\beta)^{-\\alpha}$ with $\\alpha\\sim 2$, we find that 1) the external shock wave produced by the high velocity ejecta of a hypernova can accelerate protons up to energies as high as $10^{19} {\\rm eV}$; 2) the cosmological hypernova rate is sufficient to account for the energy flux above the second knee; and 3) the steeper spectrum of CRs at these energies can arise in these sources. In addition, hypernovae would also give rise to a faint diffuse UHE neutrino flux, due to $p\\gamma$ interactions of the UHE CRs with hypernova optical-UV photons. ", "machine_abstract": "We propose that the most energetic cosmic rays are accelerated in supernova remnants by relativistic jets powered by hypernova explosions, which may be associated with gamma-ray bursts (GRBs). We show how this model can explain several observed features of GRBs: their duration distribution; their association with massive star formation regions; their high luminosities; and their large redshifts. The proposed mechanism is also able to accelerate protons up to energies beyond 10^20 eV without violating current observational constraints on the diffuse fluxes of high-energy neutrinos or photons produced during the acceleration process. This scenario provides an explanation for the origin of ultra-high energy cosmic rays as well as for the production of the highest energy neutrinos detected so far. In addition, it offers a natural explanation for the recent detection of very bright optical flashes following some GRBs.     High-energy cosmic rays have been measured at Earth over many decades [1] . Their spectrum extends up to energies above 1020 eV [2] , but no astrophysical source has yet been identified that accelerates particles to such extreme energies [3] . It seems likely that these cosmic rays were accelerated in distant sources billions of years ago [4] .   The most powerful known explosion in our Universe occurs when a massive star collapses into a black hole after exhausting its nuclear fuel supply [5] . Such events release huge amounts of gravitational binding energy [6] , which powers relativistic outflows called \"jets\"; they are believed to produce gamma-ray bursts [7, 8] . These jets could provide the necessary power to accelerate cosmic rays to extremely high energies [9] .     However, there are two major difficulties in explaining the origin of the most energetic cosmic ray particles using conventional models [10] :     1) Conventional jet-powered models cannot accelerate protons to energies greater than ~10^19 eV [11] because the maximum Lorentz factor \u0393max of the flow decreases rapidly with distance r from the central engine [12] . As a result, the total kinetic energy available to accelerate particles drops dramatically with increasing particle energy E [13] . For example, if we assume that the bulk Lorentz factor of the", "paraphrased_abstract": "At Earth, for instance, over many decades, a great number of cosmic rays have been measured. Their spectrum extends to about 1020 eV, and, without any astrophysical source, there is no astrophysical source that accelerates cosmic rays to such a high energy. This astrophysical source has not yet been established. In fact, it is possible that these cosmic rays have been accelerated millions of years ago by distant sources. In fact, we find that the most energetic particles, in the ashes of supernovae, are accelerated by relativistic jets, which are accompanied by a gamma ray, namely by a ray burst. This type of jets, if properly accelerated, can accelerate the rays to the very highest degree, without violating the constraints of the diffusion coefficient of neutrinos and photons produced during the accelerated process. We have shown that the most energetic particles are accelerated in supernovae and in a swarm of supernovae, in which a gamma ray bursts, as well as in the accelerated gamma rays. It is possible to accelerate the rays to such a high rate, but it is also possible to accelerate protons to a level of more than 1020 eV", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2294, "title": "Sensitivity of low degree oscillations to the change in solar abundances", "abstract": "  Context. The most recent determination of the solar chemical composition, using a time-dependent, 3D hydrodynamical model of the solar atmosphere, exhibits a significant decrease of C, N, O abundances compared to their previous values. Solar models that use these new abundances are not consistent with helioseismological determinations of the sound speed profile, the surface helium abundance and the convection zone depth. Aims. We investigate the effect of changes of solar abundances on low degree p-mode and g-mode characteristics which are strong constraints of the solar core. We consider particularly the increase of neon abundance in the new solar mixture in order to reduce the discrepancy between models using new abundances and helioseismology. Methods. The observational determinations of solar frequencies from the GOLF instrument are used to test solar models computed with different chemical compositions. We consider in particular the normalized small frequency spacings in the low degree p-mode frequency range. Results. Low-degree small frequency spacings are very sensitive to changes in the heavy-element abundances, notably neon. We show that by considering all the seismic constraints, including the small frequency spacings, a rather large increase of neon abundance by about (0.5 +/- 0.05)dex can be a good solution to the discrepancy between solar models that use new abundances and low degree helioseismology, subject to adjusting slightly the solar age and the highest abundances. We also show that the change in solar abundances, notably neon, considerably affects g-mode frequencies, with relative frequency differences between the old and the new models higher than 1.5% ", "machine_abstract": "We have studied how changes in the abundance pattern affect the frequencies and amplitudes of low-degree p-mode oscillations using two different methods, namely forward modelling (with the help of MESA stellar evolution code) and inverse analysis with the help of GYRE pulsation code. We find that for both methods there is an increase in the frequency difference between observed and theoretical values when we decrease the metallicity by 0.1 dex or more. The effect on amplitude differences are less pronounced but still significant. This shows that it will be very difficult to determine accurate absolute metallicities of stars based only on asteroseismic data.  For example, if one were to use the results obtained here as priors in the Bayesian framework then this would lead to underestimated uncertainties in the derived parameters such as age and mass. However, our study also suggests that relative metallicities can be determined quite accurately even without any additional information about other physical properties of the star.", "paraphrased_abstract": "Then, if we rely on the asteroseistic results as a starting point for the Bayesian method, then we may have underestimated the uncertainty in derived parameters, like age and mass. However, we show that relative metallicities of stars can be estimated in the most accurate way, despite all the other physical properties of the star. During our investigation, we have compared the frequencies and amplitudes of p-mode oscillations with the two methods. For one, using the MESA stellar evolution codes, we find that we have a doubling of the metallicity of the amplitude of the oscillations; whereas the amplitude increases with an increase of 0.1 dex, but is still significant. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1636, "title": "AMBER on the VLTI: data processing and calibration issues", "abstract": "  We present here the current performances of the AMBER / VLTI instrument for standard use and compare these with the offered modes of the instrument. We show that the instrument is able to reach its specified precision only for medium and high spectral resolution modes, differential observables and bright objects. For absolute observables, the current achievable accuracy is strongly limited by the vibrations of the Unit Telescopes, and also by the observing procedure which does not take into account the night-long transfer function monitoring. For low-resolution mode, the current limitation is more in the data reduction side, since several effects negligible at medium spectral resolution are not taken into account in the current pipeline. Finally, for faint objects (SNR around 1 per spectral channel), electromagnetic interferences in the VLTI interferometric laboratory with the detector electronics prevents currently to get unbiased measurements. Ideas are under study to correct in the data processing side this effect, but a hardware fix should be investigated seriously since it limits seriously the effective limiting magnitude of the instrument. ", "machine_abstract": "The AMBER instrument is one of three instruments that will be available for use with the Very Large Telescope Interferometer (VLTI) in Paranal, Chile when it opens to astronomers next year. The main goal of this work was to develop an efficient pipeline for reducing AMBER observations using the latest version of the software package PIMA developed by ESO. We have tested our reduction pipeline on simulated data as well as real data obtained during commissioning runs at the telescope. In addition we have investigated several aspects related to the calibration of AMBER data including the effects of atmospheric dispersion correction, detector non-linearity corrections and fringe tracking errors. Finally we present some first results obtained with our pipeline applied to actual science targets observed during commissioning runs. Keywords: Data Reduction Software, Astronomical Instrumentation, Optical/Infrared Interferometry, Science Instruments, Calibration, Data Analysis, VLTI", "paraphrased_abstract": "Besides, we also studied several aspects of the calibration of AMBER, including atmospheric dispersion, the effect of detector nonlinearity, and the fringes of the observation. We have developed a pipeline for the reduction of the observational data, based on PIMA, developed by ESO. The main goal of this research was to develop a tool to reduce the observational data of AMBER, with the latest version of PIMA, which is the latest edition of ESO\u2019s PIMA software. The AMBER instrument is one of three instruments that will be available for use in the VLTI, which opens in Chile in the spring. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.028, "title": "Common Envelope Evolution Redux", "abstract": "  Common envelopes form in dynamical time scale mass exchange, when the envelope of a donor star engulfs a much denser companion, and the core of the donor plus the dense companion star spiral inward through this dissipative envelope. As conceived by Paczynski and Ostriker, this process must be responsible for the creation of short-period binaries with degenerate components, and, indeed, it has proven capable of accounting for short-period binaries containing one white dwarf component. However, attempts to reconstruct the evolutionary histories of close double white dwarfs have proven more problematic, and point to the need for enhanced systemic mass loss, either during the close of the first, slow episode of mass transfer that produced the first white dwarf, or during the detached phase preceding the final, common envelope episode. The survival of long-period interacting binaries with massive white dwarfs, such as the recurrent novae T CrB and RS Oph, also presents interpretative difficulties for simple energetic treatments of common envelope evolution. Their existence implies that major terms are missing from usual formulations of the energy budget for common envelope evolution. The most plausible missing energy term is the energy released by recombination in the common envelope, and, indeed, a simple reformulation the energy budget explicitly including recombination resolves this issue. ", "machine_abstract": "We present the results of our study on common envelope evolution (CEE) in binary systems with helium white dwarfs and red giant secondaries, using detailed stellar evolutionary models for both components. We find that CEE is very efficient at removing orbital angular momentum when the secondary star fills its Roche lobe during core helium burning or later phases. The efficiency increases as the mass ratio decreases because more energy can be extracted by tidal friction if the primary has less mass than the secondary.     In addition to this effect, we also found that the amount of orbital angular momentum removed depends strongly on whether the system undergoes stable or unstable mass transfer after the onset of RLOF. If the system experiences unstable mass transfer, then it will lose much more orbital angular momentum compared to cases where the mass transfer proceeds stably. This occurs mainly due to the fact that the former case leads to stronger tidal interaction between the two stars which causes them to spiral inward faster.", "paraphrased_abstract": "It is important to note that this effect is most strongly influenced by the degree of stable or unstable mass transfer following the onset of RLOF. The latter is more strongly influenced by the fact that the tidal interactions between the two stars are stronger and their rotations are slowed down. We show how CEE is very efficient at removing orbital angular momentum when the secondary star fills its Roche lobe during the core helium burning, and at decreasing the mass of the primary. We show that CEE is also very efficient in removing orbital angular momentum when the secondary star fills its Roche lobe during core helium burning and in the later phases. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3159, "title": "Mass Loss and Evolution of Stars and Star Clusters: a Personal Historical Perspective", "abstract": "  The development and progress of the studies of winds and mass loss from hot stars, from about 1965 up to now, is discussed in a personal historical perspective. The present state of knowledge about stellar winds, based on papers presented at this workshop, is described. About ten years ago the mechanisms of the winds were reasonably well understood, the mass loss rates were known, and the predictions of stellar evolution theory with mass loss agreed with observations. However, recent studies especially those based on FUSE observations, have resulted in a significant reduction of the mass loss rates, that disagrees with predictions from radiation driven wind models. The situation is discussed and future studies that can clarify the situation are suggested.   I also discuss what is known about the dissolution of star clusters in different environments. The dissolution time can be derived from the mass and age distributions of cluster samples. The resulting dissolution times of clusters in the solar neighborhood (SN) and in interacting galaxies are shorter than predicted by two-body relaxation of clusters in a tidal field. Encounters with giant molecular clouds can explain the fate of clusters in the SN and are the most likely cause of the short lifetime of clusters in interacting galaxies. ", "machine_abstract": "The evolution of stars is driven by nuclear fusion reactions in their cores, which generate energy that powers the star's luminosity.  The rate at which these reactions occur depends on how much fuel (i.e., helium) there is available to burn.   As the core contracts during its red giant phase, it becomes denser and hotter, increasing the pressure inside the core until the temperature reaches about 100 million degrees Celsius.  At this point, helium begins burning rapidly, releasing large amounts of energy into the surrounding plasma.  This causes the outer layers of the star to expand outward as they are pushed away by the increased pressure generated within the core.  Eventually, the expanding outer layers become so thin that they can no longer support themselves against gravity;  the star then collapses back onto itself, forming a white dwarf or neutron star.   In addition to nuclear fusion reactions occurring in the core, some massive stars also undergo explosive nuclear fusion reactions called supernovae, which eject most of the star's mass out into space.   These explosions release huge amounts of energy and light, making them visible across great distances in the night sky.   During my career studying stellar astrophysics, I have been fortunate enough to witness many spectacular events such as these first hand.   ...", "paraphrased_abstract": "A fusion of nuclear fusions in the core creates enormous amounts of energy, which are then able to be observed in the sky. The nuclear fusion reactions in the core are often called supernovae, and in the course of these explosions enormous amounts of energy are released, and this is reflected in a wide sphere. The evolution of stars is driven by the fusion of nuclear fusions in their cores, which in turn generates the brightness of the star. During the red giant phase the core is expanded, and the pressure increases, and the core cools to a temperature of about 100 million degrees Celsius. At this temperature helium burns rapidly and releases energy into the plasma. The outer layers of the star, however, become so thin that they cannot resist gravity, and fall back to their limbs, forming a white dwarf or neutron star. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1325, "title": "Instabilities in the time-dependent neutrino disc in Gamma-Ray Bursts", "abstract": "  We investigate the properties and evolution of accretion tori formed after the coalescence of two compact objects. At these extreme densities and temperatures, the accreting torus is cooled mainly by neutrino emission produced primarily by electron and positron capture on nucleons (beta reactions). We solve for the disc structure and its time evolution by introducing a detailed treatment of the equation of state which includes photodisintegration of helium, the condition of beta-equilibrium, and neutrino opacities. We self-consistently calculate the chemical equilibrium in the gas consisting of helium, free protons, neutrons and electron-positron pairs and compute the chemical potentials of the species, as well as the electron fraction throughout the disc. We find that, for sufficiently large accretion rates (> 10 solar masses per second), the inner regions of the disk become opaque and develop a viscous and thermal instability. The identification of this instability might be relevant for GRB observations. ", "machine_abstract": "We study instabilities that develop in the accretion flow onto black holes during gamma-ray bursts (GRBs). We use an axisymmetric, general relativistic hydrodynamic code to evolve the equations for mass and momentum conservation with self-gravity included. The initial conditions are taken as those of steady-state discs around Kerr black holes. In order to mimic GRB outflows we add a radial velocity perturbation at large radii which is then advected inward by the fluid. This leads to the development of spiral density waves which grow exponentially on a dynamical timescale. These waves can be identified with the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999) . They also lead to the formation of shocks near the inner edge of the disc where they steepen into strong discontinuities. As these shocks propagate outward through the disc their strength decreases due to dissipation.", "paraphrased_abstract": "The initial conditions are assumed to be the stable state of the Kerr black holes. We use a general hydrodynamic code, an axisymmetrical, general hydrodynamic hydrodynamic model, to develop the equations for mass and momentum conservation, with self-gravity. We study instabilities in the outflow of black holes during gamma-ray bursts. The radial velocity perturbation produced by the fluid is pushed into the fluid, and this is the source of spiral density waves which increase exponentially in a dynamical period. The resulting waves are spiral and elastic, and the rate of the wave is reduced by dissipation. These waves are also caused by a pressure drop at the inner edge of the disc, which becomes a pronounced abyss. This abyss is characterized by Rossby instability (RWI), an analysis of which was carried out by Lovelace et al. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3088, "title": "The 74MHz System on the Very Large Array", "abstract": "  The Naval Research Laboratory and the National Radio Astronomy Observatory completed implementation of a low frequency capability on the VLA at 73.8 MHz in 1998. This frequency band offers unprecedented sensitivity (~25 mJy/beam) and resolution (~25 arcsec) for low-frequency observations. We review the hardware, the calibration and imaging strategies, comparing them to those at higher frequencies, including aspects of interference excision and wide-field imaging. Ionospheric phase fluctuations pose the major difficulty in calibrating the array. Over restricted fields of view or at times of extremely quiescent ionospheric ``weather'', an angle-invariant calibration strategy can be used. In this approach a single phase correction is devised for each antenna, typically via self-calibration. Over larger fields of view or at times of more normal ionospheric ``weather'' when the ionospheric isoplanatic patch size is smaller than the field of view, we adopt a field-based strategy in which the phase correction depends upon location within the field of view. This second calibration strategy was implemented by modeling the ionosphere above the array using Zernike polynomials. Images of 3C sources of moderate strength are provided as examples of routine, angle-invariant calibration and imaging. Flux density measurements indicate that the 74 MHz flux scale at the VLA is stable to a few percent, and tied to the Baars et al. value of Cygnus A at the 5 percent level. We also present an example of a wide-field image, devoid of bright objects and containing hundreds of weaker sources, constructed from the field-based calibration. We close with a summary of lessons the 74 MHz system offers as a model for new and developing low-frequency telescopes. (Abridged) ", "machine_abstract": "The Very Large Array (VLA) is an array of 27 radio telescopes located in New Mexico, USA. The VLA has been upgraded to operate at higher frequencies and with better sensitivity than ever before. In this talk I will describe how we have developed new techniques for operating the system at 74 MHz using digital beamforming technology. This work was done as part of my PhD thesis under the supervision of Prof. David Diamond. My research interests are in developing software systems that can be used by astronomers to analyse data produced by large observational facilities such as the VLA. I am currently working towards a postdoctoral position at Harvard-Smithsonian Center for Astrophysics where I hope to continue my research into advanced signal processing algorithms for astronomy applications. Keywords: Radio Astronomy, Digital Signal Processing, Software Engineering, Data Analysis, Beamforming Algorithms, Very Large Array, 74 MHz", "paraphrased_abstract": "I am currently pursuing a postdoctoral position at Harvard-Smithsonian Astrophysics, where I hope to continue my research in advanced signal processing. I have developed a method for analyzing large-scale data by means of digital signal processing. This research was done under the supervision of Dr. David Diamond. This method has been developed in my thesis under the supervision of Dr. Diamond. The Very Large Array is a collection of 27 radio telescopes in New Mexico. The Very Large Array has been upgraded to operate at a higher frequency and with better sensitivity. The Very Large Array (VLA) is an array of 27 radio telescopes located in New Mexico. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2339, "title": "A Massive Yang-Mills Theory based on the Nonlinearly Realized Gauge Group", "abstract": "  We propose a subtraction scheme for a massive Yang-Mills theory realized via a nonlinear representation of the gauge group (here SU(2)). It is based on the subtraction of the poles in D-4 of the amplitudes, in dimensional regularization, after a suitable normalization has been performed. Perturbation theory is in the number of loops and the procedure is stable under iterative subtraction of the poles. The unphysical Goldstone bosons, the Faddeev-Popov ghosts and the unphysical mode of the gauge field are expected to cancel out in the unitarity equation. The spontaneous symmetry breaking parameter is not a physical variable. We use the tools already tested in the nonlinear sigma model: hierarchy in the number of Goldstone boson legs and weak power-counting property (finite number of independent divergent amplitudes at each order). It is intriguing that the model is naturally based on the symmetry SU(2)_L local times SU(2)_R global. By construction the physical amplitudes depend on the mass and on the self-coupling constant of the gauge particle and moreover on the scale parameter of the radiative corrections. The Feynman rules are in the Landau gauge. ", "machine_abstract": "We present an explicit construction for a massive gauge theory with spontaneous symmetry breaking, which is based on the nonlinearly realized gauge group. The model contains two scalar fields and one vector field in addition to the usual Higgs doublet. We show that this model can be obtained by gauging the global SU(2)LxSU(2)R chiral symmetry of the Nambu-Jona-Lasinio (NJL) model. In order to obtain the correct vacuum structure we introduce additional interactions between the scalars and fermions. These are motivated by the requirement that they should not change the physical spectrum but only modify the effective potential. This leads us to consider four different types of interaction terms. By performing a mean-field analysis at zero temperature we find that our model has three phases depending on the values of the parameters. For small coupling constants there exists a phase where both symmetries remain unbroken. If the couplings become larger then either the left-handed or right-handed symmetry breaks spontaneously while the other remains intact. Finally if all coupling constants are large enough then both symmetries break simultaneously.", "paraphrased_abstract": "A means-field analysis of the mean field at zero temperature reveals that the model has three phases, according to the parameters: for small coupling constants there is a phase where both symmetry breaks off. For large coupling constants, there is a phase in which the two symmetry breaks off both sides. As soon as the couplings reach the maximum, the left and right symmetry will break off in a spontaneous way, and the right will stay in place. The symmetry will break simultaneously when the couplings reach the maximum. We present an explicit theory of the giant gauge theory with a spontaneous breaking of symmetry, based on the nonlinearly discovered gauge group. The result is a unified theory of the chiral symmetry of the Nambu-Jona-Lasinio model. We demonstrate that this model is possible by gauging the chiral symmetry of the NLxS chiral symmetry of the NLxS model. We present an explicit model of the giant gauge theory by combining the chiral symmetry of the n-su and the chiral symmetry of the N-su. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2024, "title": "Generation interval contraction and epidemic data analysis", "abstract": "  The generation interval is the time between the infection time of an infected person and the infection time of his or her infector. Probability density functions for generation intervals have been an important input for epidemic models and epidemic data analysis. In this paper, we specify a general stochastic SIR epidemic model and prove that the mean generation interval decreases when susceptible persons are at risk of infectious contact from multiple sources. The intuition behind this is that when a susceptible person has multiple potential infectors, there is a ``race'' to infect him or her in which only the first infectious contact leads to infection. In an epidemic, the mean generation interval contracts as the prevalence of infection increases. We call this global competition among potential infectors. When there is rapid transmission within clusters of contacts, generation interval contraction can be caused by a high local prevalence of infection even when the global prevalence is low. We call this local competition among potential infectors. Using simulations, we illustrate both types of competition.   Finally, we show that hazards of infectious contact can be used instead of generation intervals to estimate the time course of the effective reproductive number in an epidemic. This approach leads naturally to partial likelihoods for epidemic data that are very similar to those that arise in survival analysis, opening a promising avenue of methodological research in infectious disease epidemiology. ", "machine_abstract": "We consider the problem of estimating the generation interval distribution in an epidemic outbreak, using only information on the number of cases at each time point. We show that this is possible under certain conditions by deriving a formula for the probability generating function (PGF) of the generation interval distribution as a solution to a system of differential equations with boundary conditions corresponding to the observed case counts. The PGF can be used to calculate any moment or other statistic of interest about the generation interval distribution. In particular we derive expressions for the mean and variance of the generation interval distribution which are valid when the total number of cases grows exponentially fast over time. Finally, we apply our results to two real-world datasets, one concerning measles outbreaks in England between 1963 and 1998, and another concerning SARS outbreaks in China during 2002-03. Our method provides estimates of key epidemiological quantities such as the basic reproduction number R0 and the serial interval distribution, without requiring detailed contact tracing data.", "paraphrased_abstract": "The method estimates the basic distribution of the reproduction rate and the serial distribution of the disease, without the need for detailed contact-tracing. In this work, we propose to estimate the generation-interval distribution of an epidemic, and apply it to any event, which is a recurring event. This is possible, in certain cases, by formulating a formula for the probability-generating function (PGF) of the distribution, by a set of differential equations with boundary conditions corresponding to the number of cases at each time. We are particularly interested in the mean and variance of the distribution, which are inapplicable when the number of cases increases exponentially. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4538, "title": "Insulating state and the importance of the spin-orbit coupling in Ca$_3$CoRhO$_6$", "abstract": "  We have carried out a comparative theoretical study of the electronic structure of the novel one-dimensional Ca$_3$CoRhO$_6$ and Ca$_3$FeRhO$_6$ systems. The insulating antiferromagnetic state for the Ca$_3$FeRhO$_6$ can be well explained by band structure calculations with the closed shell high-spin $d^5$ (Fe$^{3+}$) and low-spin $t_{2g}^{6}$ (Rh$^{3+}$) configurations. We found for the Ca$_3$CoRhO$_6$ that the Co has a strong tendency to be $d^7$ (Co$^{2+}$) rather than $d^6$ (Co$^{3+}$), and that there is an orbital degeneracy in the local Co electronic structure. We argue that it is the spin-orbit coupling which will lift this degeneracy thereby enabling local spin density approximation + Hubbard U (LSDA+U) band structure calculations to generate the band gap. We predict that the orbital contribution to the magnetic moment in Ca$_3$CoRhO$_6$ is substantial, i.e. significantly larger than 1 $\\mu_B$ per formula unit. Moreover, we propose a model for the contrasting intra-chain magnetism in both materials. ", "machine_abstract": "We report on the magnetic properties of Ca$_3$Co$RhO$_6$, which is an insulator with a large spin gap ($\\sim$ 100 K) at low temperatures. The magnetization shows no anomaly down to 1.8 K, indicating that there are no magnetic ordering or phase transition below this temperature. We have performed first-principles calculations for Co$_2$RhO$_4$ and Ca$_3$CoRho$_6$ using the generalized gradient approximation plus U method. Our results show that the insulating behavior can be explained by the strong hybridization between Rh-4d orbitals and O-2p orbitals due to the presence of the rhombohedral distortion. In addition, we find that the spin-orbit interaction plays an important role in determining the electronic structure near the Fermi level. These findings suggest that Ca$_3$Co_(Rh,Ir)O$_6$ compounds may also exhibit similar behaviors as those observed in Ca$_3$FeRhO$_6$.     Introduction     Recently, several new materials such as Sr$_2$RuO$_4$ [1] , La$_2$CuO$_4$ [2] , and Ca$_3$FeRho$_6$ [3] were found to possess interesting physical phenomena including unconventional superconductivity [4] . Among these systems, Ca$_3$Fe$RhO$_6$is particularly intriguing because it exhibits both metallic and non-metallic states depending on its crystal structures [5] . It was reported that Ca$_3$Fe_(Rh,Ir,Pt)O$_6$ [6] - [8] has a rhombohedral structure [9] like Ca$_3$Fe_RhO$_6$ [10] but becomes metallic when Ir replaces Rh [11] . This suggests that the replacement of Rh by other elements could lead to novel physics in these compounds [12] .     Herein, we present our study on another member of the family, Ca$_3$Co_RhO$_6 $ [13] . As shown in Figure 1 , Ca$_3$Co_rho$_6", "paraphrased_abstract": "The metallicity of Ca$3_Co_Rho_6_ is the polarity of Ca$3_Co_Rho_6_, which is metallic by the addition of the other elements. We have studied the magnetic properties of Ca$3_Co_Rho_6_, which is an insulator with a large spin (sim> 100 K), and which is at the temperature of 1.8 K, it is also indicated that there is no magnetic ordering at this temperature. We have formulated the polarity of Ca_3_Co_Rho_6_, which is insulator with a high spin (sim> 100 K) and which is also magnetic. We have shown the magnetic properties of Ca_3_Co_Rho_6_, which is insulator with a high spin at low temperatures, and the magnetic flux is quite high at 1.8 K, which means that the magnetic order and phase transitions have been not altered below 1.8 K. These results suggest that the polarity of these compounds may be influenced by spin. We have also found that the spin-orbit interaction plays a major role in the electronic structure near the Fermi level. The magnetic properties of Ca_3_Co_Rho_6_ are especially interesting, because they are of a metallic and non-metallic nature,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.411, "title": "Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists", "abstract": "  We discuss the design of efficient scrip systems and develop tools for empirically analyzing them. For those interested in the empirical study of scrip systems, we demonstrate how characteristics of agents in a system can be inferred from the equilibrium distribution of money. From the perspective of a system designer, we examine the effect of the money supply on social welfare and show that social welfare is maximized by increasing the money supply up to the point that the system experiences a ``monetary crash,'' where money is sufficiently devalued that no agent is willing to perform a service. We also examine the implications of the presence of altruists and hoarders on the performance of the system. While a small number of altruists may improve social welfare, too many can also cause the system to experience a monetary crash, which may be bad for social welfare. Hoarders generally decrease social welfare but, surprisingly, they also promote system stability by helping prevent monetary crashes. In addition, we provide new technical tools for analyzing and computing equilibria by showing that our model exhibits strategic complementarities, which implies that there exist equilibria in pure strategies that can be computed efficiently. ", "machine_abstract": "We study the efficiency of scrip systems in which users can buy goods with their own currency or by exchanging it for other people's currency (scrip). We show that when there are many buyers and sellers who use such a system to trade items, then the system is efficient if most people have an incentive to be fair traders. However, we also find that this may not always happen because some users may try to hoard money while others may act altruistically. In particular, we consider two types of user behavior: selfishness and altruism. Selfish users maximize their utility at each transaction; they will accept only offers that give them more than what they pay out. Altruistic users do not care about maximizing their individual utilities but rather want to make sure that everyone gets paid as much as possible. We analyze these behaviors using game-theoretic models and derive conditions under which both selfish and altruistic users behave fairly.", "paraphrased_abstract": "We analyze this with the game theory, and then show how both selfish and generous people behave in a given way. We investigate the efficiency of the exchange system of scrip, which is a system of exchange in which the buyers and sellers can exchange money in one currency for another. We show that if there are many people in the market and they are willing to exchange money, the system is very efficient if the majority of the customers and sellers have the incentive to be fair. But it is also found that this is not always the case, because some people may have to keep money to be fair, while others might do something to be charitable. We study the efficiency of scrip systems, in which people buy goods in their own currencies or exchange it for the currencies of others (scrip). We analyze the efficiency of scrip systems in which people buy things from the market and exchange it for the currencies of other people. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0576, "title": "Multi-site observations of Delta Scuti stars 7 Aql and 8 Aql (a new Delta Scuti variable): The twelfth STEPHI campaign in 2003", "abstract": "  We present an analysis of the pulsation behaviour of the Delta Scuti stars 7 Aql (HD 174532) and 8 Aql (HD 174589) -- a new variable star -- observed in the framework of STEPHI XII campaign during 2003 June--July. 183 hours of high precision photometry were acquired by using four-channel photometers at three sites on three continents during 21 days. The light curves and amplitude spectra were obtained following a classical scheme of multi-channel photometry. Observations in different filters were also obtained and analyzed. Six and three frequencies have been unambiguously detected above a 99% confidence level in the range 0.090 mHz--0.300 mHz and 0.100 mHz-- 0.145 mHz in 7 Aql and 8 Aql respectively. A comparison of observed and theoretical frequencies shows that 7 Aql and 8 Aql may oscillate with p modes of low radial orders, typical among Delta Scuti stars. In terms of radial oscillations the range of 8 Aql goes from n=1 to n=3 while for 7 Aql the range spans from n=4 to n=7. Non-radial oscillations have to be present in both stars as well. The expected range of excited modes according to a non adiabatic analysis goes from n=1 to n=6 in both stars. ", "machine_abstract": "The 12th STEPHI campaign was carried out at the Roque de los Muchachos Observatory, La Palma, Canaries Islands during August-September 2003. We observed two bright Delta Scuti stars - 7 Aql and 8Aql with the STEPHI photometer equipped with an Andor CCD camera. In this work we present results for these targets obtained by applying different methods to analyse their light curves. For both stars we found pulsation frequencies which are listed in Table 1 .  7 Aql is known as one of the most active Delta Scuti variables showing more than 100 periodicities in its power spectrum. Our analysis revealed that it has several additional modes excited simultaneously. Some of them were not detected before because they have very low amplitudes or are located close to other peaks.  8Aql turned out to be another interesting target. It shows only three significant periods but all of them are quite short -larger than 0.1 d. This star also exhibits low-amplitude variability on longer time scales.    We compared our results with those published previously.", "paraphrased_abstract": ", I am a spectral equilaterality, and the amplitude of the pulsation is not high, but is not very high, and does not appear very large, and it also has low amplitudes. Our experiment focuses on two bright stars of the Delta Scuti group: 7 Aql and 8 Aql. These two stars have been seen in STEPHI in the Roque de los Muchachos Observatory, Canaries, during August and September 2003. We analyzed the pulsation of these two stars by means of different methods, which are presented in Table 1. 7 Aql was known as one of the most active Delta Scuti variables, and there are more than 100 periodicities in its power spectrum, with peaks that are much smaller than 0.1 d, which is more than 0.1 d. We present results of these two stars, which were observed by different methods in a variety of ways. 7 Aql is known as one of the most active Delta Scuti variables, and has more than 100 periodicities in its power spectrum, and the pulsations are only three diffractions; all are short, less than 0.1 d; the star also exhibits short periods in its time scale. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1793, "title": "Intrinsic ripples in graphene", "abstract": "  The stability of two-dimensional (2D) layers and membranes is subject of a long standing theoretical debate. According to the so called Mermin-Wagner theorem, long wavelength fluctuations destroy the long-range order for 2D crystals. Similarly, 2D membranes embedded in a 3D space have a tendency to be crumpled. These dangerous fluctuations can, however, be suppressed by anharmonic coupling between bending and stretching modes making that a two-dimensional membrane can exist but should present strong height fluctuations. The discovery of graphene, the first truly 2D crystal and the recent experimental observation of ripples in freely hanging graphene makes these issues especially important. Beside the academic interest, understanding the mechanisms of stability of graphene is crucial for understanding electronic transport in this material that is attracting so much interest for its unusual Dirac spectrum and electronic properties. Here we address the nature of these height fluctuations by means of straightforward atomistic Monte Carlo simulations based on a very accurate many-body interatomic potential for carbon. We find that ripples spontaneously appear due to thermal fluctuations with a size distribution peaked around 70 \\AA which is compatible with experimental findings (50-100 \\AA) but not with the current understanding of stability of flexible membranes. This unexpected result seems to be due to the multiplicity of chemical bonding in carbon. ", "machine_abstract": "The electronic structure and transport properties of intrinsic ripples on single-layered graphene are investigated by using the tight-binding model with first-principles hopping parameters, which is based on density-functional theory (DFT). The results show that the ripple-induced local strain can lead to an energy gap opening at Dirac point for both armchair and zigzag directions. In addition, it is found that the electron mobility decreases as the amplitude increases due to the increase of scattering centers induced by the ripple structures. These findings may be useful for understanding the physical mechanism behind the experimental observations of rippling effects on the electrical performance of graphene devices. Graphene has attracted great attention because its unique two-dimensional honeycomb lattice structure leads to many novel phenomena such as high carrier mobility [1] , Klein tunneling [2] , valley polarization [3] , etc.. However, recent experiments have shown that the pristine flat monolayer graphene sheets tend to form ripples spontaneously [4] . It was also reported that these ripples could affect the electrical performance of graphene-based devices [5] . In this work we investigate the effect of ripples on the electronic structure and transport properties within the framework of the tight-binding model [6] . We find that the ripple-induced strains can open up an energy gap around the Fermi level for both armchair andzigzag directions. Moreover, the electron mobility decreases as increasing the amplitude of ripples since more scattering centers are introduced into the system.", "paraphrased_abstract": "And the rippling of these wires could have a very detrimental effect on the electrical performance of graphene-based devices. Graphene, a honeycomb-like material with two layers, has attracted great attention because its honeycomb structure affords a very high mobility of its electrons, a Klein tunnelling effect, a valley polarization, and so on. Recently, the results of these experiments have been published and showed that the smooth, flat mono-layer graphene may rippling and spontaneously form wires \u2013 these wires can, of course, rippling the electrostatic field. The result shows that the local strain of the rippling force will open the energy gap around the Fermi point. It is also found that the electron mobility increases with increasing the strength of the rippling force, the more scattering centers added to the system. In this work, we investigated the effect of the rippling forces on the electronic structure and transport properties of single-layer graphene. We found that the rippling forces open the energy gap around the Dirac point for both armchair and zigzag. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.361, "title": "A Logic of Reachable Patterns in Linked Data-Structures", "abstract": "  We define a new decidable logic for expressing and checking invariants of programs that manipulate dynamically-allocated objects via pointers and destructive pointer updates. The main feature of this logic is the ability to limit the neighborhood of a node that is reachable via a regular expression from a designated node. The logic is closed under boolean operations (entailment, negation) and has a finite model property. The key technical result is the proof of decidability. We show how to express precondition, postconditions, and loop invariants for some interesting programs. It is also possible to express properties such as disjointness of data-structures, and low-level heap mutations. Moreover, our logic can express properties of arbitrary data-structures and of an arbitrary number of pointer fields. The latter provides a way to naturally specify postconditions that relate the fields on entry to a procedure to the fields on exit. Therefore, it is possible to use the logic to automatically prove partial correctness of programs performing low-level heap mutations. ", "machine_abstract": "We present a logic for reasoning about reachability properties on linked data structures, which are graphs with labeled nodes and edges that can be traversed using the Web Linking Language (WLL). The WLL is an extension to RDFS that allows one to specify how links between resources should be created or modified by applications. We show how our logic can be used to express important classes of queries over linked data such as finding all paths between two given nodes, computing the distance between them, checking whether there exists at least one path between them, etc., and we prove its decidability. Finally, we provide algorithms for evaluating these queries under certain restrictions. Our results have been implemented within the LOD2 system. This work has been partially supported by the European Commission through the FP7 project LOD2 (Grant Agreement No. 258520) and the Swiss National Science Foundation (SNF) grant number PP00P2-125231/1.", "paraphrased_abstract": "The European Commission is partially supported by the project LOD2 (grant agreement No. 258520) and the Swiss National Science Foundation (SNF) grant no. P21-26-2351/2. We also present a logic for analyzing reachability properties of linked data, which are graphs with labels for nodes and edges, and which can be traversed using the web-linking language (WLL), which is an extension of RDFS and which can be configured to create or alter links between resources. Our results are presented in the LOD2 system. We present an implementation of our method for the analysis of reachability properties of linked data, based on the web-linking language. This language is a extension of RDFS and can be used to define how links should be connected to the resources. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2569, "title": "Modelling the clumping-induced polarimetric variability of hot star winds", "abstract": "  Clumping in the winds of massive stars may significantly reduce empirical mass-loss rates, and which in turn may have a large impact on our understanding of massive star evolution. Here, we investigate wind-clumping through the linear polarization induced by light scattering off the clumps. Through the use of an analytic wind clumping model, we predict the time evolution of the linear polarimetry over a large parameter space. We concentrate on the Luminous Blue Variables, which display the greatest amount of polarimetric variability and for which we recently conducted a spectropolarimetric survey. Our model results indicate that the observed level of polarimetric variability can be reproduced for two regimes of parameter space: one of a small number of massive, optically-thick clumps; and one of a very large number of low-mass clumps. Although a systematic time-resolved monitoring campaign is required to distinguish between the two scenarios, we currently favour the latter, given the short timescale of the observed polarization variability. As the polarization is predicted to scale linearly with mass-loss rate, we anticipate that all hot stars with very large mass-loss rates should display polarimetric variability. This is consistent with recent findings that intrinsic polarization is more common in stars with strong H$\\alpha$ emission. ", "machine_abstract": "We present new results on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures, using Monte Carlo radiative transfer simulations. We find that for stars with high mass-loss rates (\u1e40 > 10-7 M\u2299 yr-1), the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering processes within the wind. For lower mass loss rate objects (\u1e40 < 10-7 M\u2299yr-1) we find that the effect is less pronounced but still significant enough to be detectable at certain wavelengths. The predicted changes are found to depend strongly upon the properties of the individual clumps; specifically, they increase as the number density contrast between the clumps and surrounding medium increases. In addition, we show how these predictions may be used to constrain the physical parameters describing the clumpy structure of the wind.  These findings have important implications for future observations of hot-star winds which will be made possible through the use of next-generation instruments such as SPHERE/VLT and GPI/Gemini Observatory.", "paraphrased_abstract": "The new findings may have important implications for the future research of hot stars, which will be made possible by the next-generation instruments such as SPHERE/VLT and the Gemini Observatory. In addition, we demonstrate how these predictions can be used to determine the physical properties of the clumps in the wind. Moreover, we demonstrate how they can be used to calculate the properties of the wind\u2019s clumps, which in turn depend on the properties of the individual clumps, and their relative density in the medium. Moreover, we show how these predictions can be used to constrain the physical parameters describing the clumpy structure of the wind. We show that in the case of high mass (                    ) the presence of clumps significantly affects the degree and the angle of the wind, which are influenced by scattering. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4266, "title": "El universo de Einstein: 1905 - annus mirabilis - 2005", "abstract": "  Year 2005 was the World Year of Physics, and it was commemorated worldwide as the 100th anniversary of Albert Einstein's legendary works in which the foundations of at least three areas of modern physics were laid: statistical mechanics, quantum mechanics and special relativity. These works turned year 1905 in what deservedly was called the \"miraculous year\" (annus mirabilis) of Einstein. However, the influence of this great mind was not limited to science. As it is well known, his fingerprint remains engraved in many aspects of our daily life. Because of that, in this work we have tried to cover almost all the subjects in which Einstein was involved during the last century and, of course, try and reveal the future projection his actions will have in this new century that has just begun. Among the subjects covered in the volume one can find the possible connection between Einstein and the artistic vanguard movement, his religiosity, his days in Argentina, the Nobel Prize, the Einstein-Freud correspondence, his pacifism and the nuclear energy, science and politics under the Second Empire and the Weimar Republic, and even a description of his patents and experiments, the studies performed to search for \"built-in\" features in his brain and the doubts many had regarding whether Einstein was a genius or a crank. Also included are essays on Einstein and Borges, philosophy and relativity, Mach's principle, the relation among science, arts and humanities, and, closer to Einstein's own theoretical research, the twin paradox, chance and quantum mechanics, the arrow of time, gravitational lenses, string theory, holography and black holes, gravitomagnetism, superconductivity, gravitational waves, the photoelectric effect and the alternative theories for gravitation. ", "machine_abstract": "El a\u00f1o que cambi\u00f3 la historia del Universo (2005) es un documental sobre el descubrimiento por parte de Albert Einstein, en 1915, de las ecuaciones b\u00e1sicas de su teor\u00eda general relativista y los acontecimientos que tuvieron lugar durante ese mismo a\u00f1o. El film se centra especialmente en dos hechos extraordinarios ocurridos en 1905: la publicaci\u00f3n de cinco art\u00edculos cient\u00edficos revolucionarios escritos por Einstein entre marzo y diciembre; y el nacimiento de su hija mayor, Margot, en mayo. La pel\u00edcula est\u00e1 narrada por el actor brit\u00e1nico David Threlfall e incluye entrevistas con expertos mundiales como Sean Carroll o Brian Cox.[...]\u200b    La teor\u00eda general de la relatividad fue publicada por primera vez en 1915 pero sus fundamentos fueron establecidos ya en 1905 cuando Einstein public\u00f3 una serie de trabajos revolucionarios que cambiaron nuestra comprensi\u00f3n del Universo. Este documental cuenta c\u00f3mo ocurri\u00f3 todo eso centr\u00e1ndose en dos hechos extraordinarios sucedidos aquel a\u00f1o: la publicaci\u00f3n de cinco trabajos cient\u00edficos revolucionarios escritos por \u00e9l entre marzo y diciembre; as\u00ed como el nacimiento de su hija Margot en mayo.", "paraphrased_abstract": "El film se centra en dos extraordinarios sucedidos en 1905: la publicaci\u00f3n de cinco trabajos revolucionarios escritos por Einstein entre marzo y diciembre; el nacimiento de su hija mayor, Margot en mayo. El ao que cambi\u00f3 la historia del Universo (2005) es un documental sobre la descubrimiento por parte de Albert Einstein, en 1915, de las ecuaciones b\u00e1sicas de su teora general relativista y los acontecimientos que tuvieron lugar durante ese mismo ao. El ao que cambi\u00f3 la historia del Universo (2005) es un documental sobre la descubrimiento por parte de Albert Einstein, en 1905, de las ecuaciones b\u00e1sicas de su teora general relativista y los acontecimientos que tuvieron lugar durante ese mismo ao.", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1847, "title": "Growing Networks: Limit in-degree distribution for arbitrary out-degree one", "abstract": "  We compute the stationary in-degree probability, $P_{in}(k)$, for a growing network model with directed edges and arbitrary out-degree probability. In particular, under preferential linking, we find that if the nodes have a light tail (finite variance) out-degree distribution, then the corresponding in-degree one behaves as $k^{-3}$. Moreover, for an out-degree distribution with a scale invariant tail, $P_{out}(k)\\sim k^{-\\alpha}$, the corresponding in-degree distribution has exactly the same asymptotic behavior only if $2<\\alpha<3$ (infinite variance). Similar results are obtained when attractiveness is included. We also present some results on descriptive statistics measures %descriptive statistics such as the correlation between the number of in-going links, $D_{in}$, and outgoing links, $D_{out}$, and the conditional expectation of $D_{in}$ given $D_{out}$, and we calculate these measures for the WWW network. Finally, we present an application to the scientific publications network. The results presented here can explain the tail behavior of in/out-degree distribution observed in many real networks. ", "machine_abstract": "We study the limit degree distributions of growing networks with prescribed initial degrees and arbitrary growth rules, where each new node is connected to exactly one existing node chosen uniformly at random among all nodes that are already present in the network. We show that if the number of nodes grows exponentially fast then this choice can be made without loss of generality. In particular we prove that any sequence of such graphs converges weakly as n goes to infinity towards an exponential family of probability measures on [0, 1] parametrized by their mean value. This result allows us to derive explicit expressions for the limiting degree distributions of several classes of growing networks including preferential attachment models, copying models or threshold models. Finally, we use our results to obtain exact asymptotic formulas for the average distance between two randomly selected nodes in these networks. The main tool used throughout the article is the theory of large deviations which provides sharp estimates on the tail behavior of sums of iid Bernoulli variables.", "paraphrased_abstract": "The main tool of this work is the theory of large deviations, which gives a very sharp estimate of the tail behavior of sums of iid Bernoulli variables. We present a method for identifying the limits of the limiting degree distributions of growing networks, with their deterministic initial degrees and arbitrary growth rates. The new nodes are connected to exactly one existing node, chosen from among all nodes that are already present. We show that, when the number of nodes increases exponentially, this choice is still possible. Then we show that, if the number of nodes increases exponentially, we can choose this node without losing the generality of the system. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0269, "title": "Forward stagewise regression and the monotone lasso", "abstract": "  We consider the least angle regression and forward stagewise algorithms for solving penalized least squares regression problems. In Efron, Hastie, Johnstone & Tibshirani (2004) it is proved that the least angle regression algorithm, with a small modification, solves the lasso regression problem. Here we give an analogous result for incremental forward stagewise regression, showing that it solves a version of the lasso problem that enforces monotonicity. One consequence of this is as follows: while lasso makes optimal progress in terms of reducing the residual sum-of-squares per unit increase in $L_1$-norm of the coefficient $\\beta$, forward stage-wise is optimal per unit $L_1$ arc-length traveled along the coefficient path. We also study a condition under which the coefficient paths of the lasso are monotone, and hence the different algorithms coincide. Finally, we compare the lasso and forward stagewise procedures in a simulation study involving a large number of correlated predictors. ", "machine_abstract": "We consider forward stagewise regression (FSR) for linear models with nonnegative coefficients, which is an iterative procedure that adds variables to the model one at a time until some stopping criterion is met.  We show how FSR can be used in conjunction with the monotone Lasso penalty to produce sparse solutions whose support contains all relevant predictors while simultaneously ensuring their signs are correct.  The resulting algorithm has computational complexity similar to standard Lasso algorithms but produces more accurate results on simulated data sets as well as real-world examples involving gene expression microarray data. Forward Stagewise Regression (FSR), introduced by Frank & Friedman [1] , is an iterative procedure where each iteration consists of adding a single variable into the current set of selected features based on its contribution to the objective function. This process continues until some stopping criteria is reached such as reaching a maximum number of iterations or meeting a desired level of accuracy [2] . In this work we focus on using FSR within the context of linear models with non-negative coefficients. For example, if our goal was to find genes associated with breast cancer then it would make sense to only select those genes that have been shown to increase risk rather than decrease risk [3] . The main advantage of FSR over other greedy selection methods like stepwise regression [4] is that it does not require any tuning parameters [5] . However, there are two major drawbacks when applying FSR directly to problems with large numbers of potential covariates [6] :  1) It may take many iterations before the final solution converges; 2) There is no guarantee that the final solution will contain all relevant predictors. To address these issues, Tibshirani et al. [7] proposed the Least Absolute Shrinkage and Selection Operator (Lasso). Lasso solves the following optimization problem:", "paraphrased_abstract": "In this work, we will describe the method in more detail. For example, if the aim of our work is to find genes associated with breast cancer, it would be better to select genes that are shown to increase risk rather than decrease it. Besides, it may be difficult to estimate the underlying variables. Then, after several iterations, it may take many iterations, and it is not certain that the final result will contain all the relevant predictors. The method we propose is the shortest-form method, and it is based on the principle of one-step optimization. We take the following case: Forward-stage-regression, introduced by Frank et al. [236] is a method of increasing the number of variables in the current set, one by one, until the end of the step is reached, that is, the last step is reached, or the number of steps has been reached. In this case, the problem is: what is the importance of the genes associated with breast cancer? To select genes with a higher risk, we have to select only those that have the highest risk and are therefore not harmful, for example, to select genes that have been shown to increase risk rather than decrease it. Then, by means of a monotonous Lasso penalty, it is possible to add one variable to a certain set of parameters, and to converge in a single step,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3218, "title": "Stern-Volmer Modeling of Steady-State Forster Energy Transfer Between Dilute, Freely Diffusing Membrane-Bound Fluorophores", "abstract": "  Two different metrics are used to assess Forster resonance energy transfer (FRET) between fluorophores in the steady state: (1) acceptor-quenching of donor fluorescence, E (a.k.a. transfer efficiency); and (ii) donor-excited acceptor fluorescence, F-A-Dex. While E is still more widely used, F-A-Dex has been gaining in popularity for practical reasons among experimentalists who study biomembranes. Here, for the special case of membrane-bound fluorophores, we present a substantial body of experimental evidence that justifies the use of simple Stern-Volmer expressions when modeling either FRET metric under dilute-probe conditions. We have also discovered a dilute-regime correspondence between our Stern-Volmer expression for E and Wolber and Hudson's series approximation for steady-state Forster quenching in 2D. This novel correspondence allows us to interpret each of our 2D quenching constants in terms of both (i) an effective Forster distance, and (ii) two maximum acceptor-concentration limits, each of which defines its own useful experimental regime. Taken together, our results suggest a three-step strategy toward designing more effective steady-state FRET experiments for the study of biomembranes. ", "machine_abstract": "We present an analytical solution to the steady-state forster energy transfer between two freely diffusing membrane-bound fluorophores in close proximity (<10 nm). The model is based on the assumption that both donor and acceptor molecules are bound to the same lipid bilayer with their transition dipole moments parallel to each other but perpendicular to the plane of the membrane. We show how this simple geometry can be used to derive a closed-form expression for the fluorescence lifetime of the donor molecule as well as its dependence on the concentration of acceptors. This approach allows us to extract information about the distance distribution between donors and acceptors directly from experimental data without any additional assumptions or fitting parameters. In addition we demonstrate how our results can be applied to study the lateral organization of proteins within biological membranes using single-molecule spectroscopy. Single-molecule FRET experiments have been widely used over recent years to investigate protein-protein interactions at the molecular level1-5 . However, despite significant progress made during last decade6-9 , there still remain several challenges associated with extracting quantitative structural information from such measurements10-13 . In particular, it has recently become apparent that many commonly employed methods for analyzing single-molecule FRET data suffer from systematic errors due to various factors including photophysical properties of fluorescent dyes14-16 , heterogeneity of sample17-19 , presence of multiple species20-22 , etc. . To overcome these difficulties, several groups23-26 have developed sophisticated statistical approaches which allow one to obtain reliable estimates of key physical quantities characterizing the system under investigation by performing global fits to large sets of experimental data27-29 . Unfortunately, however, most of these techniques require extensive computational resources and/or involve complicated numerical procedures making them difficult...", "paraphrased_abstract": "In recent years, FRET studies have been used extensively to study the lateral arrangement of proteins and protein interactions at the molecular level. However, despite the great advances made during the last decade, many of the methods for analyzing the data of single molecules have been made unavoidably wrong. These mistakes were due to a variety of factors, including the nature of fluorescent dyes, the heterogeneity of the samples, the presence of many species, etc.. To overcome these obstacles, several groups have developed sophisticated statistical methods that enable them to obtain a quantitative estimate of the physical properties of the system to be analyzed by global fit to large amounts of experimental data. In addition, we show that our method can be applied to the study of the lateral organization of proteins in biological membranes. In the present study, we introduce an analytical solution to the steady transfer of resonant energy between two free fluorophores, in close proximity, of 10 nm. We assume that the donor and acceptor molecules are bound to the same bilayer, and their transitions are parallel to each other, and their dipoles parallel to the plane of the membrane. In this simple geometry, we derive a closed form for the duration of the donor and its dependency on the concentration of acceptors. We show how this simple form is used to calculate the distance between the donors and the acceptors in the same", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3242, "title": "Line Emission in the Brightest Cluster Galaxies of the NOAO Fundamental Plane and Sloan Digital Sky Surveys", "abstract": "  We examine the optical emission line properties of Brightest Cluster Galaxies (BCGs) selected from two large, homogeneous datasets. The first is the X-ray selected National Optical Astronomy Observatory Fundamental Plane Survey (NFPS), and the second is the C4 catalogue of optically selected clusters built from the Sloan Digital Sky Survey Data Release ~3 (SDSS DR3). Our goal is to better understand the optical line emission in BCGs with respect to properties of the galaxy and the host cluster. Throughout the analysis we compare the line emission of the BCGs to that of a control sample made of the other bright galaxies near the cluster centre. Overall, both the NFPS and SDSS show a modest fraction of BCGs with emission lines (~15%). No trend in the fraction of emitting BCGs as a function of galaxy mass or cluster velocity dispersion is found. However we find that, for those BCGs found in cooling flow clusters, 71^{+9}_{-14}% have optical emission. Furthermore, if we consider only BCGs within 50kpc of the X-ray centre of a cooling flow cluster, the emission-line fraction rises further to 100^{+0}_{-15}%. Excluding the cooling flow clusters, only ~10% of BCGs are line emitting, comparable to the control sample of galaxies. We show that the physical origin of the emission line activity varies: in some cases it has LINER-like line ratios, whereas in others it is a composite of star-formation and LINER-like activity. We conclude that the presence of emission lines in BCGs is directly related to the cooling of X-ray gas at the cluster centre. ", "machine_abstract": "We present new measurements of line emission for the brightest cluster galaxies (BCGs) in clusters with z < 0.3, using data obtained by the Chandra X-ray Observatory. We find that BCGs' optical luminosities are correlated strongly with their soft-band X-ray luminosities; this correlation is stronger than previously reported correlations between optical and radio luminosity or between optical and infrared luminosity.  The observed relationship can be explained if we assume that most of the X-rays come from inverse Compton scattering off hot electrons associated with the central supermassive black holes. This result suggests that there may be an evolutionary link between active galactic nuclei and BCGs. In addition to the strong correlation between Lopt and LX , we also observe a weak but significant anti-correlation between Lopt and the temperature Tgas of the intracluster medium surrounding each galaxy. These results suggest that the gas density around these galaxies decreases as they evolve into more massive systems.", "paraphrased_abstract": "It was found that the radiances of X-rays were strongly correlated with the soft band X-rays, which were more than previously reported to be between X-rays and radio rays, and between X-rays and infrared rays. We propose that this correlation can be explained by the fact that the X-rays of BCGs originate from inverse Compton scattering, which is due to the central supermassive black holes. The results of these measurements show that the density of gaseous gas is diminishing as more and more massive systems become. We also present the results of a new X-ray experiment for BCGs in clusters of z  0.3 in which the Chandra X-ray Observatory is operating. We demonstrate that the BCGs' optical luminosity is strongly correlated with their soft X-ray luminosity. The X-ray luminosity is stronger than the previous X-ray luminosity of X-rays or radio luminosity. This relationship, as we have already noticed, can be explained by the fact that the X-rays are produced by the inverse Compton scattering of the central black holes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3801, "title": "An annular gap acceleration model for $\\gamma$-ray emission of pulsars", "abstract": "  If the binding energy of the pulsar's surface is not so high (the case of a neutron star), both the negative and positive charges will flow out freely from the surface of the star. The annular free flow model for $\\gamma$-ray emission of pulsars is suggested in this paper. It is emphasized that: (1). Two kinds of acceleration regions (annular and core) need to be taken into account. The annular acceleration region is defined by the magnetic field lines that cross the null charge surface within the light cylinder. (2). If the potential drop in the annular region of a pulsar is high enough (normally the cases of young pulsars), charges in both the annular and the core regions could be accelerated and produce primary gamma-rays. Secondary pairs are generated in both regions and stream outwards to power the broadband radiations. (3). The potential drop in the annular region grows more rapidly than that in the core region. The annular acceleration process is a key point to produce wide emission beams as observed. (4). The advantages of both the polar cap and outer gap models are retained in this model. The geometric properties of the $\\gamma$-ray emission from the annular flow is analogous to that presented in a previous work by Qiao et al., which match the observations well. (5). Since charges with different signs leave the pulsar through the annular and the core regions, respectively, the current closure problem can be partially solved. ", "machine_abstract": "We present an annular gap acceleration model to explain the observed gamma-ray light curves and spectra of young pulsars with high magnetic fields (B > 10$^{12}$ G). The outer magnetosphere is divided into two regions, i.e., the open field line region and closed field line region. In our model, particles are accelerated in both regions by electric fields parallel to the local magnetic field lines. We find that the particle distribution function can be described as a power law with exponential cutoffs at low energies and high energies respectively. The spectral index below the break energy depends on the inclination angle between the rotation axis and magnetic dipole moment. For small inclination angles, it decreases with decreasing latitude; while for large inclination angles, it increases with decreasing latitude. Our results show good agreement with observations. This work was supported by National Natural Science Foundation of China under grant No. 10273001.     1 Introduction     Pulsar radio emission has been successfully explained by the rotating vector model [1] . However, this model cannot account for the high-energy radiation emitted by pulsars [2] , which may come from the relativistic electrons/positrons produced in the magnetospheric accelerator [3] . Many theoretical models have been proposed to explain the origin of pulsar high-energy emissions [4] - [8] .   In these models, particles are accelerated along the open or closed field lines near the neutron star surface due to some mechanisms such as curvature drift [9] , resonant inverse Compton scattering [10] , first-order Fermi acceleration [11] , second-order Fermi acceleration [12] , etc.. These accelerated particles radiate photons through synchrotron process [13] , inverse Compton scattering [14] , and/or bremsstrahlung [15] . Some authors also suggested that the pair production [16] could play important roles in producing the high-energy emission [17] - [19] .", "paraphrased_abstract": "The inner magnetosphere is divided into two regions, the open-field-line region and the closed-field-line region. In this region, the particles are accelerated by electric fields parallel to the magnetic field lines, and the spectral index of the particles is determined by the angle of rotation of the axes and the magnetic moment of the dipoles, for a small inclination and a large inclination, and for a large inclination, the inclination increases with the distance between the magnetic field and the earth, and the amplitude of the rays is reduced by the bremsstrahlung, and the bremsstrahlung - is observed. The pulsar radiation can be explained by the inverse vector, but there are also some authors who suggest that the pair formation and the pair production can be important in the pulsar's emission. In this study, we have used the theory of the pulsar's high-energy emission to explain the gamma ray spectra of young pulsars, with a high magnetic field. The outer magnetosphere is divided into two zones, and the open and the closed zone are divided. The inner zone is divided by electric fields parallel to the magnetic zone, and the outer zone is divided by a line of electric fields, and the distance between the two regions is", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0033, "title": "Improved constraints on dark energy from Chandra X-ray observations of the largest relaxed galaxy clusters", "abstract": "  We present constraints on the mean matter density, Omega_m, dark energy density, Omega_de, and the dark energy equation of state parameter, w, using Chandra measurements of the X-ray gas mass fraction (fgas) in 42 hot (kT>5keV), X-ray luminous, dynamically relaxed galaxy clusters spanning the redshift range 0.05<z<1.1. Using only the fgas data for the 6 lowest redshift clusters at z<0.15, for which dark energy has a negligible effect on the measurements, we measure Omega_m=0.28+-0.06 (68% confidence, using standard priors on the Hubble Constant, H_0, and mean baryon density, Omega_bh^2). Analyzing the data for all 42 clusters, employing only weak priors on H_0 and Omega_bh^2, we obtain a similar result on Omega_m and detect the effects of dark energy on the distances to the clusters at ~99.99% confidence, with Omega_de=0.86+-0.21 for a non-flat LCDM model. The detection of dark energy is comparable in significance to recent SNIa studies and represents strong, independent evidence for cosmic acceleration. Systematic scatter remains undetected in the fgas data, despite a weighted mean statistical scatter in the distance measurements of only ~5%. For a flat cosmology with constant w, we measure Omega_m=0.28+-0.06 and w=-1.14+-0.31. Combining the fgas data with independent constraints from CMB and SNIa studies removes the need for priors on Omega_bh^2 and H_0 and leads to tighter constraints: Omega_m=0.253+-0.021 and w=-0.98+-0.07 for the same constant-w model. More general analyses in which we relax the assumption of flatness and/or allow evolution in w remain consistent with the cosmological constant paradigm. Our analysis includes conservative allowances for systematic uncertainties. The small systematic scatter and tight constraints bode well for future dark energy studies using the fgas method. (Abridged) ", "machine_abstract": "We present new measurements of the Hubble constant and the equation-of-state parameter w0 using Chandra X-ray Observatory data for the most massive, dynamically relaxed galaxy clusters in the Universe. We use these results to place improved limits on the properties of dark energy. The sample consists of eight galaxy clusters with redshifts between 0.3 and 1.2 that were observed by Chandra as part of our ongoing program to study the evolution of cluster scaling relations out to high redshift. Using hydrostatic equilibrium models we measure the gas mass fraction within r500 (the radius at which the mean density is 500 times the critical density) for each system. These values are combined with independent estimates of the total gravitating mass obtained through weak lensing analysis performed by other groups. This yields an average value of H0 = 70 +/- 6 km s-1 Mpc-1 assuming flat priors on both parameters. If instead we assume Gaussian priors based on previous determinations of the Hubble constant and baryon content of the universe then this measurement becomes H0 = 68 +/-6 km s-1 Mpc-", "paraphrased_abstract": "It turns out that we assume Gaussian priors, which were derived from the previous determinations of the Hubble constant and the baryon content of the universe, then the result is H0 = 70 and 6 km s-1 Mpc-1, which assumes flat priors on both parameters. These results, applied to the properties of dark energy, will be made more precise. We study the radiance of gas in r500 (the radius at which the maximum density is 500 times the critical density), and we obtain the average value of H0 = 70 and 6 km s-1 Mpc-1, which consists of a minimum of flat priors on both parameters. The sample consists of eight synchronously moving clusters, with redshifts between 0.3 and 1.2. These clusters were observed by Chandra as part of our ongoing study of the evolution of clusters from low redshifts to high redshifts. We present a new calculation of the Hubble constant and the equation of state w0 with the Chandra data for the largest and most dynamically relaxed galaxy clusters in the universe. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4622, "title": "A Keck/DEIMOS spectroscopic survey of faint Galactic satellites: searching for the least massive dwarf galaxies", "abstract": "  [abridged] We present the results of a spectroscopic survey of the recently discovered faint Milky Way satellites Boo, UMaI, UMaII and Wil1. Using the DEIMOS spectrograph on Keck, we have obtained samples that contain from 15 to 85 probable members of these satellites for which we derive radial velocities precise to a few km/s down to i~21-22. About half of these stars are observed with a high enough S/N to estimate their metallicity to within \\pm0.2 dex. From this dataset, we show that UMaII is the only object that does not show a clear radial velocity peak. However, the measured systemic radial velocity (v_r=115\\pm5 km/s) is in good agreement with recent simulations in which this object is the progenitor of the recently discovered Orphan Stream. The three other satellites show velocity dispersions that make them highly dark-matter dominated systems. In particular the Willman 1 object is not a globular cluster given its metallicity scatter over -2.0<[Fe/H]<-1.0 and is therefore almost certainly a dwarf galaxy or dwarf galaxy remnant. We measure a radial velocity dispersion of only 4.3_{-1.3}^{+2.3} km/s around a systemic velocity of -12.3\\pm2.3 km/s which implies a mass-to-light ratio of ~700 and a total mass of ~5x10^5 Msun for this satellite, making it the least massive satellite galaxy known to date. Such a low mass could mean that the 10^7 Msun limit that had until now never been crossed for Milky Way and Andromeda satellite galaxies may only be an observational limit and that fainter, less massive systems exist within the Local Group. However, more modeling and an extended search for potential extra-tidal stars are required to rule out the possibility that these systems have not been significantly heated by tidal interaction. ", "machine_abstract": "We present results on a sample of low surface brightness (LSB) dwarfs selected in the southern hemisphere using photometric data obtained with the CFHT Legacy Survey and DEEP2 surveys, respectively. The LSB dwarfs are located within 100 kpc of the Milky Way's center and have absolute magnitudes between M V = \u221210 to \u221216 mag. We observed these targets with the DEIMOS spectrograph at the W.M. Keck Observatory over three observing runs during 2005-2007. Our observations cover wavelengths ranging from 6500\u00c5 to 9200\u00c5 with a spectral resolution R\u223c2000. In total we obtain spectra for 16 out of 23 candidates. From our analysis of the spectra we find that only one galaxy is likely to be an old stellar population system based on its lack of H\u03b1 emission line fluxes. This object has been previously identified as a globular cluster by Karachentsev et al. (2006) . For all other objects we detect strong H\u03b1 emission lines indicating ongoing star formation activity.", "paraphrased_abstract": "The deimos spectrograph was used in the W.M. Keck Observatory for three observing runs during the summer of 2005. The spectrograph was compared with a low-energy LSB dwarf, and the absolute magnitude was 10 mag. All other objects possessed high H fluxes. For all the rest, the deimos spectra were strong and suggestive of ongoing star formation. We analyzed a group of LSB dwarfs selected in the southern hemisphere, based on the observations made by the CFHT, and the 1-DEEP surveys, respectively. We conclude that only one galaxy is probably an old, highly active population of stellar stars, based on the absence of H fluxes. These two objects, a globular cluster, were recently identified by Karachentsev et al. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2839, "title": "The SAURON project - XI. Stellar Populations from Absorption Line Strength Maps of 24 Early-Type Spirals", "abstract": "  We present absorption line strength maps of a sample of 24 representative early-type spiral galaxies, mostly of type Sa, obtained as part of the SAURON survey of nearby galaxies using our custom-built integral-field spectrograph. Using high-quality spectra, spatially binned to a constant signal-to-noise, we measure several key age, metallicity and abundance ratio sensitive indices from the Lick/IDS system over a contiguous two-dimensional field including bulge and inner disc. We present maps of H beta, Fe 5015 and Mg b, for each galaxy   The absorption line maps show that many galaxies contain some younger populations (<= 1 Gyr), distributed in small or large inner discs, or in circumnuclear star forming rings. In many cases these young stars are formed in circumnuclear mini-starbursts, which are dominating the light in the centres of some of the early-type spirals. These mini-starburst cause a considerable scatter in index-index diagrams such as Mg b- H beta and Mg b -Fe 5015, more than is measured for early-type galaxies. We find that the central regions of Sa galaxies display a wide range in ages, even within the galaxies. 50% of the sample show velocity dispersion drops in their centres.   All of the galaxies of our sample lie on or below the Mg b- $\\sigma$ relation for elliptical galaxies in the Coma cluster, and above the H beta absorption line - $\\sigma$ relation for elliptical galaxies. If those relations are considered to be relations for the oldest local galaxies we see that our sample of spirals has a considerable scatter in age, with the largest scatter at the lowest $\\sigma$. This is in disagreement with highly inclined samples, in which generally only old stellar populations are found in the central regions.   All this can be understood if ... (see paper for rest of abstract) ", "machine_abstract": "We present the results of an analysis of absorption line strength maps for 24 early-type spiral galaxies observed with the SAURON integral field spectrograph at the William Herschel Telescope, which are used to derive their stellar populations and chemical abundances. The sample is divided into two groups according to morphological type (Sa-Sab and Sb-Scd), each containing 12 objects. We find that both samples have similar mean ages but different metallicities; Sa-Sab spirals show solar or super-solar metallicity while Sb-Scd spirals tend to be sub-solar in abundance. This difference can be explained by assuming that the former group has experienced more recent star formation than the latter one. In addition we find evidence for radial age gradients within individual galaxies, where younger stars are found towards larger galactocentric radii. Finally, we compare our results with those obtained using photometric data from the Sloan Digital Sky Survey.", "paraphrased_abstract": "As we see, we have a radial age of 2. It is based on the fact that the younger stars tend to have the larger radii of. In addition, we show that there are age-grading variations in the age of the individual galaxies, when the older ones are younger. We compare these observations with those of the Sloan Digital Sky Survey. We present the results of an analysis of the absorption line strength of the 24 spiral galaxies that were observed with the SAURON spectrograph, which we use to calculate their ages and chemical abundances. The results of this analysis were compared with those of the SAURON spectrograph of the William Herschel Telescope, with which we derive the chemistry and abundances of their stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0787, "title": "Construction of Bayesian Deformable Models via Stochastic Approximation Algorithm: A Convergence Study", "abstract": "  The problem of the definition and the estimation of generative models based on deformable templates from raw data is of particular importance for modelling non aligned data affected by various types of geometrical variability. This is especially true in shape modelling in the computer vision community or in probabilistic atlas building for Computational Anatomy (CA). A first coherent statistical framework modelling the geometrical variability as hidden variables has been given by Allassonni\\`ere, Amit and Trouv\\'e (JRSS 2006). Setting the problem in a Bayesian context they proved the consistency of the MAP estimator and provided a simple iterative deterministic algorithm with an EM flavour leading to some reasonable approximations of the MAP estimator under low noise conditions. In this paper we present a stochastic algorithm for approximating the MAP estimator in the spirit of the SAEM algorithm. We prove its convergence to a critical point of the observed likelihood with an illustration on images of handwritten digits. ", "machine_abstract": "In this work, we propose an algorithm for constructing the posterior distribution of deformable models using stochastic approximation (SA). The proposed method is based on the use of Metropolis-Hastings within Gibbs sampling and SA to approximate the intractable likelihood function in order to obtain samples from the posterior distribution. We show that our approach converges under certain conditions. In addition, we demonstrate its performance by applying it to synthetic data as well as real medical images. Finally, we compare our results with those obtained by other methods such as variational Bayes and expectation propagation algorithms.  Keywords: Bayesian inference, deformation model, image segmentation, convergence study, stochastic approximation algorithm. 1 Introduction Image segmentation has been widely studied over the past decades due to its importance in many applications including computer vision, pattern recognition, medical imaging analysis, etc.. Among various approaches developed so far, statistical modeling techniques have attracted much attention because they can provide more accurate solutions than deterministic ones [1] . Statistical modeling techniques are usually divided into parametric and nonparametric categories depending on whether or not there exists any prior knowledge about the underlying distributions [2] . The most popular parametric technique is probably the mixture model which assumes each pixel belongs to one of several classes [3] , where the number of components needs to be specified beforehand [4] . However, the choice of the optimal number of components remains difficult since different numbers may lead to very similar results [5] . To overcome these difficulties, some researchers have recently introduced new types of mixture models [6] - [8] . For example, Banerjee et al. [9] presented a hierarchical Dirichlet process mixture model (HDP-M) which allows the number of clusters to grow automatically during the clustering procedure; while Ishwaran & Zarepour [10] proposed a finite normal mixture model (FNMM), which uses a truncated Gaussian density instead of a Dirac delta function to represent each cluster's probability mass function. Although both HDP-M and FNMM can avoid specifying the number of clusters manually, their computational complexity increases dramatically when dealing with high-dimensional problems [11] .", "paraphrased_abstract": "\u201cImage segmentation has been widely studied over the past decade, owing to its importance in a number of different fields, such as computer vision, pattern recognition, medical imaging, etc. Its application is presently being studied by a number of researchers. The most well-known of these methods is that of mixtures, which assumes each pixel belongs to one of several classes, which, in turn, need to be specified in advance. However, this approach has the disadvantage that it cannot be set at random, and hence its computational complexity is greatly increased. This approach, based on Metropolis-Hastings, in Gibbs sampling, tries to approximate the intractable likelihood function by estimating the posterior distribution. In order to find out how much is the posterior distribution, we use an algorithm of stochastic approximation based on Metropolis-Hastings, to obtain the unassailable likelihood function, in order to extract the posterior distribution. We present a method of constructing the posterior distribution of deformable models by using the method of stochastic approximation, and in particular of constructing the posterior distribution of deformable models, which is based on the principle of invariable proportions, which is used to produce samples of the posterior distribution. However, our method is incompatible with many other methods, such as variational Bayes and expectation", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3054, "title": "And in the Darkness Bind Them: Equatorial Rings, B[e] Supergiants, and the Waists of Bipolar Nebulae", "abstract": "  We report the discovery of two new circumstellar ring nebulae in the western Carina Nebula. The brighter object, SBW1, resembles a lidless staring eye and encircles a B1.5 Iab supergiant. Its size is identical to the inner ring around SN1987A, but SBW1's low N abundance indicates that the star didn't pass through a RSG phase. The fainter object, SBW2, is a more distorted ring, is N-rich, and has a central star that seems to be invisible. We discuss these two new nebulae in context with rings around SN1987A, Sher25, HD168625, RY Scuti, WeBo1, SuWt2, and others. The ring bearers fall into two groups: Five rings surround hot supergiants, and all except for the one known binary are carbon copies of the ring around SN1987A. We propose a link between these rings and B[e] supergiants, where the rings derive from the same material in an earlier B[e] phase. The remaining four rings surround evolved intermediate-mass stars; all members of this ring fellowship are close binaries, hinting that binary interactions govern the forging of such rings. We estimate that there may be several thousand more dark rings in the Galaxy, but we are scarcely aware of their existence due to selection effects. The lower-mass objects might be the equatorial density enhancements often invoked to bind the waists of bipolar PNe. ", "machine_abstract": "We present new near-infrared polarimetric observations of the equatorial ring surrounding the evolved star HD 163296 (HR 5171 A). The data reveal that the ring is highly polarized at infrared wavelengths with an average polarization fraction of ~20%. We find no evidence for significant changes to this value over time scales ranging from years to decades. This result suggests that dust grains are not being destroyed or created on short timescales within the ring. In addition, we detect a small degree of circularly polarized light emerging from the central region of the ring which may be due to scattering off non-spherical particles such as ice crystals.     We also report results from our analysis of archival Hubble Space Telescope images showing that the ring has remained remarkably stable since its discovery more than 20 years ago. Finally, we discuss how these findings can be used to constrain models of bipolar nebulae formation. Keywords: Polarization; Circumstellar matter", "paraphrased_abstract": "In addition, we observe a small amount of circularly polarized light, which may be due to the scattering of spherical particles, such as ice crystals. This result suggests that dust cannot be destroyed or created on short timescales. The polarization, the circumstellar matter, and the bipolar nebulae have all been well established for more than twenty years. We report here the results of our study of the Hubble Space Telescope, which shows that the ring has remained remarkably stable since its discovery, some 20 years ago. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2232, "title": "Spontaneous Symmetry Breaking as a Basis of Particle Mass", "abstract": "  Electroweak theory joins electromagnetism with the weak force in a single quantum field theory, ascribing the two fundamental interactions--so different in their manifestations--to a common symmetry principle. How the electroweak gauge symmetry is hidden is one of the most urgent and challenging questions facing particle physics. The provisional answer incorporated in the \"standard model\" of particle physics was formulated in the 1960s by Higgs, by Brout & Englert, and by Guralnik, Hagen & Kibble: The agent of electroweak symmetry breaking is an elementary scalar field whose self-interactions select a vacuum state in which the full electroweak symmetry is hidden, leaving a residual phase symmetry of electromagnetism. By analogy with the Meissner effect of the superconducting phase transition, the Higgs mechanism, as it is commonly known, confers masses on the weak force carriers W and Z. It also opens the door to masses for the quarks and leptons, and shapes the world around us. It is a good story--though an incomplete story--and we do not know how much of the story is true. Experiments that explore the Fermi scale (the energy regime around 1 TeV) during the next decade will put the electroweak theory to decisive test, and may uncover new elements needed to construct a more satisfying completion of the electroweak theory. The aim of this article is to set the stage by reporting what we know and what we need to know, and to set some \"Big Questions\" that will guide our explorations. ", "machine_abstract": "The Standard Model (SM) is the most successful theory in particle physics, but it has some problems such as hierarchy problem and naturalness problem. In this article we will discuss spontaneous symmetry breaking which can solve these problems. We will also introduce Higgs mechanism to explain how particles get their masses. Finally, we will talk about electroweak precision tests that are used to test SM predictions at high energy experiments. The Standard Model (SM) describes all known elementary particles and interactions between them very well up to now. However there are still some open questions like why do quarks have different masses? Why does the weak interaction become strong when energies increase? These questions lead us to consider new physics beyond the Standard Model.     Spontaneous symmetry breaking occurs if a system with many symmetries becomes unstable against small perturbations. This means that even though the original Lagrangian describing the system looks symmetric under certain transformations, the ground state of the system breaks those symmetries spontaneously. For example, in quantum electrodynamics (QED), photons interact with electrons through the exchange of virtual electron-positron pairs. If you look closely into QED, you find out that the photon-electron vertex violates parity because the electric charge of an electron is negative while its spin is positive. But since the vacuum expectation value of the field operator for the electron vanishes, the ground state of QED preserves parity.", "paraphrased_abstract": "It is a method of enlightening all elementary particles and their interactions. Its primary properties, however, are still to be developed. The symmetry of the system becomes unstable under the influence of a small perturbation, and in a state of instability the symmetry is broken by itself. In this article we shall discuss the phenomenon of spontaneous symmetry breaking, which may overcome these problems. We shall also discuss the Higgs method, and will discuss how it is possible to obtain mass by means of a symmetries in the symmetry of the system. The symmetry of the system has been developed by the most successful theory in particle physics. It is capable of covering all known elementary particles and of all interactions between them. Nevertheless, there are still some open questions to be posed, like why are quarks of different masses, and why do they become strong at higher energies? The symmetry of the system has been broken by sudden symmetries, so that, even though the original Lagrangian formulation of the system appears symmetric, the ground state breaks the symmetries naturally. This phenomenon is known as \u201cspillovering\u201d because it contains the symmetry of the atoms, and the symmetry of the atoms is only known to us by the principle of the higgs, a mechanism by which particles are able to get their", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2418, "title": "Adding Environmental Gas Physics to the Semi-Analytic Method for Galaxy Formation: Gravitational Heating", "abstract": "  We present results of an attempt to include more detailed gas physics motivated from hydrodynamical simulations within semi-analytic models (SAM) of galaxy formation, focusing on the role that environmental effects play. The main difference to previous SAMs is that we include 'gravitational' heating of the intra-cluster medium (ICM) by the net surplus of gravitational potential energy released from gas that has been stripped from infalling satellites. Gravitational heating appears to be an efficient heating source able to prevent cooling in environments corresponding to dark matter halos more massive than $\\sim 10^{13} $M$_{\\odot}$. The energy release by gravitational heating can match that by AGN-feedback in massive galaxies and can exceed it in the most massive ones. However, there is a fundamental difference in the way the two processes operate. Gravitational heating becomes important at late times, when the peak activity of AGNs is already over, and it is very mass dependent. This mass dependency and time behaviour gives the right trend to recover down-sizing in the star-formation rate of massive galaxies. Abridged... ", "machine_abstract": "We present an improved semi-analytical method (SAM) that includes gravitational heating by dark matter halos and gas cooling in galaxy formation, which is essential to reproduce observed properties of galaxies such as luminosity functions at different redshifts.  We show that our SAM can successfully explain the evolution of the stellar mass function over cosmic time with reasonable parameters. In addition, we find that the inclusion of gravitational heating leads to more realistic predictions on the star formation rate density history than previous models without this effect. Finally, we discuss how the model could be further improved by including other physical processes like supernova feedback or AGN activity. The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085.A-0488(A). This work was supported by JSPS KAKENHI Grant Number JP15K05481. Figure 1 . Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature. Red circles represent the predicted number densities using our new SAM code while blue squares indicate those obtained with the original SAM code developed by Nagashima & Yoshii (2004) .", "paraphrased_abstract": "As we have seen, the number of galaxies is compared to the observations of the literature. We have now developed a semi-analytical method, using the fusion of gravitational heating by dark matter and gas cooling, which is necessary to reproduce the observed properties of the galaxies such as the luminosity at different redshifts. We have now introduced the fusion of gravitational heating with the fusion of dark matter, which is necessary to reproduce the luminosity of the galaxies, the star formation rate, and the aeon. We demonstrate that the fusion of fusion of the dark matter has a much better chance of resolving the fusion of the stars, than the other models which are averse to fusion. This work was supported by the grant of the Japanese government under the grant No. JP15K05489. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4475, "title": "Dynamical heterogeneities and the breakdown of the Stokes-Einstein and Stokes-Einstein-Debye relations in simulated water", "abstract": "  We study the Stokes-Einstein (SE) and the Stokes-Einstein-Debye (SED) relations using molecular dynamics simulations of the extended simple point charge model of water. We find that both the SE and SED relations break down at low temperature. To explore the relationship between these breakdowns and dynamical heterogeneities (DH), we also calculate the SE and SED relations for subsets of the 7% ``fastest'' and 7% ``slowest'' molecules. We find that the SE and SED relations break down in both subsets, and that the breakdowns occur on all scales of mobility. Thus these breakdowns appear to be generalized phenomena, in contrast with the view where only the most mobile molecules are the origin of the breakdown of the SE and SED relations, embedded in an inactive background where these relations hold. At low temperature, the SE and SED relations in both subsets of molecules are replaced with ``fractional'' SE and SED relations, $D_t\\sim(\\tau/T)^{-\\xi_t}$ and $D_r\\sim(\\tau/T)^{-\\xi_r}$ where $\\xi_t\\approx0.84<1$ and $\\xi_r\\approx0.75<1$. We also find that there is a decoupling between rotational and translational motion, and that this decoupling occurs in both fastest and slowest subsets of molecules. We also find that when the decoupling increases, upon cooling, the probability of a molecule being classified as both translationally and rotationally fastest also increases. To study the effect of time scale for SE and SED breakdown and decoupling, we introduce a time-dependent version of the SE and SED relations, and a time-dependent function that measures the extent of decoupling. Our results suggest that both the decoupling and SE and SED breakdowns are originated at the time scale corresponding to the end of the cage regime, when diffusion starts. This is also the time scale when the DH are more relevant. ", "machine_abstract": "We study dynamical heterogeneity (DH) in molecular dynamics simulations of supercooled water by analyzing the spatial distribution of relaxation times, as well as their dependence on temperature T and pressure P . We find that DH is present at all temperatures studied here, but becomes more pronounced with decreasing T , leading to an increase in the number of slow particles. The presence of DH leads to violations of both the Stokes-Einstein relation between diffusion constant D and viscosity \u03b7, and the Stokes-Einstein-Debeye relation between D and rotational relaxation time \u03c4R. These results are consistent with recent experiments probing the microscopic structure of glassy water. DOI: 10.1103/PhysRevE.93.020101 I. INTRODUCTORY REMARkS The properties of liquids approaching the glass transition have been extensively investigated over many decades [1] . In particular, it has been shown that the structural relaxation time increases dramatically upon cooling [2] . This behavior can be understood within the framework of mode-coupling theory [3] which predicts that the liquid freezes into a solid-like state when the relaxation time exceeds the experimental observation window [4] . In addition to this dramatic change in relaxation times, there also exists a growing body of evidence for the existence of \"dynamical heterogeneity\" (DH), i.e., spatially localized regions where particle motion slows down significantly compared to the average [5] . Such regions were first observed experimentally using neutron scattering [6] and later confirmed by computer simulation studies [7, 8] . It was found that these regions grow larger and become more numerous upon cooling [9] . Recent work suggests that DH may play an important role in determining the physical properties of glasses [10] . For example, it has been suggested that DH could explain why the shear modulus G decreases faster than predicted by the Debye-Stokes-Einstein equation [11] . Furthermore, it has recently been proposed that DH might provide insight into the origin of fragile-to-strong transitions [12] . However, despite its importance, little is known about how DH evolves during the process of vitrification or what determines its size and lifetime [13] . Here we investigate", "paraphrased_abstract": "However, little is known about how DH develops in the vitrification process, and what determines its size and lifespan. This is a new discovery and is in agreement with recent studies on the structure of glass. We have been studying the properties of liquids approaching the glass transition for many years. In particular, we have found that the time of relaxation varies greatly in the presence of temperature and pressure, and the increase of the temperature and the pressure increase the density of the particles. This phenomenon is characterized by the mode-coupling theory, which explains that the liquid freezes to solid state when the relaxation time is exceeded. This is in the context of mode-coupling theory, which predicts that liquid freezes into a solid state when the relaxation time is exceeded. Moreover, it has been proposed that DH may be the basis of the physical properties of glasses, such as the deformation of the shear-miming shear modulus G and the reduction of the amplitude of the shear-miming shear modulus G, thereby explaining the weak to strong transition. Besides these dramatic changes in the relaxation times, we have also observed a growing number of \u201cdynamical diversity,\u201d i.e., regions in which the motion of the particles is greatly accelerated by the temperature of the supercooled water, and this sort of diversity becomes larger and larger as temperature d increases. We present", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1818, "title": "Ultra-relativistic geometrical shock dynamics and vorticity", "abstract": "  Geometrical shock dynamics, also called CCW theory, yields approximate equations for shock propagation in which only the conditions at the shock appear explicitly; the post-shock flow is presumed approximately uniform and enters implicitly via a Riemann invariant. The nonrelativistic theory, formulated by G. B. Whitham and others, matches many experimental results surprisingly well. Motivated by astrophysical applications, we adapt the theory to ultra-relativistic shocks advancing into an ideal fluid whose pressure is negligible ahead of the shock, but one third of its proper energy density behind the shock. Exact results are recovered for some self-similar cylindrical and spherical shocks with power-law pre-shock density profiles. Comparison is made with numerical solutions of the full hydrodynamic equations. We review relativistic vorticity and circulation. In an ultrarelativistic ideal fluid, circulation can be defined so that it changes only at shocks, notwithstanding entropy gradients in smooth parts of the flow. ", "machine_abstract": "We present an analysis of the relativistic Riemann problem for ideal fluids in two space dimensions, with emphasis on the role played by vortex sheets. We show that the solution to this problem can be constructed as a sequence of self-similar solutions which are determined uniquely up to translations along the x-axis (the direction of propagation). The first step is to construct a family of exact solutions describing the interaction between a planar shock wave and a vortex sheet. These solutions have been obtained previously using different methods but we provide here a new derivation based on the method of characteristics. In particular, we obtain explicit expressions for the density and pressure profiles across the shock front. Next, we consider the case where the initial data consists of a single vortex sheet separating regions of constant density and pressure. This situation corresponds physically to a fluid initially at rest being accelerated impulsively by a piston moving at speed c = 1. We prove that there exists exactly one such solution corresponding to each value of the total mass M . Finally, we study numerically the evolution of more general initial data consisting of several vortex sheets separated by shocks.", "paraphrased_abstract": "In the first place, we consider the case in which the initial data consist of a single vortex, separating the regions of constant density and pressure. We assume the corresponding position of a fluid in a state of rest, accelerated by a piston that moves at a constant speed, c = 1 and then prove that we have exactly one such solution for each value of the mass M. Next, we consider the case in which the first data consist of a single vortex, separating the regions of constant density and pressure. In this case, we consider the physical case of a fluid, initially at rest, being accelerated by a piston that moves at c = 1... Then we study numerically the evolution of a general equilibrium, consisting of several vortexes separated by shocks. The first step is to construct a family of precise solutions, which describe the interaction between a planar shock and a vortexe, which have been developed in different ways, but here we provide a new formulation. The density and pressure of the two vortes are formulated in detail. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0448, "title": "Helicity generation and alpha-effect by Vandakurov-Tayler instability with z-dependent differential rotation", "abstract": "  We investigate in isothermal MHD simulations the instability of toroidal magnetic fields resulting by the action of z-dependent differential rotation on a given axial field B^0 in a cylindrical enclosure where in particular the helicity of the resulting nonaxisymmetric flow is of interest. The idea is probed that helicity H is related to the external field and the differential rotation as H ~ B^0_i B^0_j Omega_i,j. The observed instability leads to a nonaxisymmetric solution with dominating mode m=1. With the onset of instability both kinematic and current helicity are produced which fulfill the suggested relation. Obviously, differential rotation dOmega/dz only needs an external axial field B^0_z to produce remarkable amounts of the helicities. Any regular time-dependency of the helicity could not be found. The resulting axial alpha-effect is mainly due to the current helicity, the characteristic time scale between both the values is of order of the rotation time. If the axial field is switched off then the helicity and the alpha-effect disappear. ", "machine_abstract": "We study the nonlinear evolution of the magnetorotational instability (MRI) in accretion disks, taking into account the vertical dependence of the angular velocity $\\Omega(z)$. We find that MRI can generate magnetic helicity efficiently even when there is no net poloidal field initially present. The generated magnetic helicity leads to an increase of the toroidal component of the magnetic field through the inverse cascade process. This results in the formation of large-scale helical structures which are similar to those observed in numerical simulations of MRI-driven turbulence. In addition, we show that the presence of differential rotation causes the growth rate of MRI to be larger than that obtained for Keplerian rotation at all wavenumbers. As a result, MRI grows faster and generates more magnetic energy compared to previous studies where only Keplerian rotation was considered. Finally, we discuss how our results may affect the alpha effect in accretion disks.", "paraphrased_abstract": "But, since there is no poloidal field, the magnetic field will increase, and a large-scale helical structure will form, a great similarity to those found in numerical simulations of a MRI-induced turbulence. Then, we will present the results of our experiment, describing how the MRI can be induced to grow faster and more rapidly than before, and what the effect of differential rotation is. This is a result of the differential rotation. The results of our experiment are compared with those of Keplerian rotation at all wavenumbers. We find that the MRI can be induced to grow faster than Keplerian rotation, and this is the result of MRI growing faster and more strongly than Keplerian rotation, in contrast to the previous work, where the only physics was based on Keplerian rotation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3173, "title": "Probing the Impact of Stellar Duplicity on Planet Occurrence with Spectroscopic and Imaging Observations", "abstract": "  Although it is commonly agreed that the presence of a close stellar companion is likely to affect planet formation and evolution, the precise effects and their actual impact on planet occurrence and properties are still debated. In particular, observational constraints are sparse, a consequence of the discrimination against close binaries in Doppler planet searches. To bring observational constraints on the occurrence and properties of planets in binaries and multiple stars, we have been conducting two dedicated observing programs using both adaptive optics imaging and radial-velocity monitoring. In this chapter we explain our approach and present preliminary results from these two programs. A simplified statistical analysis of the data from our VLT/NACO imaging survey brings the first observational evidence that the occurrence of planets is reduced in binaries closer than ~120 AU. On the radial-velocity side, current results confirm that the use of two-dimensional correlation allows to search for circumprimary giant planets in many types of spectroscopic binaries. Definitive results from our ongoing planet search in spectroscopic binaries should yield important constraints on the closest binaries susceptible of hosting circumprimary giant planets. ", "machine_abstract": "We present an analysis of planet occurrence rates in binary systems, using spectroscopic observations to identify binaries among stars observed by Kepler for which we have obtained high-resolution imaging follow-up. We find that planets are less likely to be found around close visual doubles than single stars (<2 AU), but more likely to be found around wide visual doubles or triples (>20 AU). The difference between these two populations is statistically significant at greater than 3-sigma confidence level. These results suggest that stellar multiplicity may play some role in shaping planetary system architectures. However, our sample size is small compared to other studies, so further investigation will be needed before firm conclusions can be drawn.     Keywords: Binary star - Planets - Multiplicity - Kepler - Visual double - Triple star - Planet formation - Circumstellar disk - Star formation - Exoplanet - Circumbinary disk - Double-lined spectroscopic binary", "paraphrased_abstract": "We present an analysis of the formation of planets in binary systems with Kepler's spectroscopic observations of the stars we have observed, and we have obtained an infrared scan of them. This study suggests that stellar multiplicity may be a factor in the development of planetary systems. We found that there was a marked difference between the two groups, about one AU in length, between the two-and-two-dimensional visual doubles and the three-and-two-inch-long spectral doubles. This difference, however, was statistically significant at a confidence level of three-sigma. Moreover, we were not the only study to have sample sizes smaller than those of other studies, and so we were not able to make a conclusion about our subject. We then analysed the rates of planet formation in binary systems, utilizing the spectral double-axis to identify binaries in spectral doubles, which Kepler observed with great accuracy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2607, "title": "Selection Against Demographic Stochasticity in Age-Structured Populations", "abstract": "  It has been shown that differences in fecundity variance can influence the probability of invasion of a genotype in a population, i.e. a genotype with lower variance in offspring number can be favored in finite populations even if it has a somewhat lower mean fitness than a competitor. In this paper, Gillespie's results are extended to population genetic systems with explicit age structure, where the demographic variance (variance in growth rate) calculated in the work of Engen and colleagues is used as a generalization of \"variance in offspring number\" to predict the interaction between deterministic and random forces driving change in allele frequency. By calculating the variance from the life history parameters, it is shown that selection against variance in the growth rate will favor a genotypes with lower stochasticity in age specific survival and fertility rates. A diffusion approximation for selection and drift in a population with two genotypes with different life history matrices (and therefore, different growth rates and demographic variances) is derived and shown to be consistent with individual based simulations. It is also argued that for finite populations, perturbation analyses of both the growth rate and demographic variances may be necessary to determine the sensitivity of \"fitness\" (broadly defined) to changes in the life history parameters. ", "machine_abstract": "We study the evolution of age structure and life history traits by using an age-structured population model with stochastic demographic events, where individuals can reproduce at any age. We show that selection against demographic stochasticity leads to increased reproductive effort early in life (i.e., earlier reproduction) and decreased mortality rates later in life. This is because higher reproductive efforts increase offspring numbers but also lead to more deaths due to competition for resources among siblings. In contrast, lower mortality rates decrease sibling competition and thus reduce the number of deaths caused by this source of demographic stochasticity. Our results are robust across different levels of environmental stochasticity and initial conditions. The evolutionary consequences of demographic stochasticity have been studied extensively in recent years [1\u20133] . However, most studies on this topic focus on populations without age structure or assume that all individuals reproduce at the same age [4\u20136] , which may not be realistic [7\u20139] . Here we use an age-structured model with stochastic demographic events [10, 11] to investigate how selection acts on life-history traits when there is variation in individual ages at first reproduction [12] . Specifically, we consider a scenario where individuals can reproduce at various ages and compete for limited resources within their family groups [13] . We find that selection against demographic stochasticities increases reproductive effort early in life and decreases mortality rates late in life. These findings are consistent with previous theoretical work showing that selection favors reduced variance in offspring number [14\u201318] .", "paraphrased_abstract": ", they have a low mortality rate. In the course of this study, we compared the evolution of age structure and the development of life history by means of a stochastic population, a population that is able to reproduce at any age, and we show that selection against population structure increases the reproductive effort in the early part of life and the mortality rate in the late part. In particular, we analyzed a population with different ages and in the competing group for limited resources. In this case, selection against population structure increases the fertility rate in the early part of life and decreases the mortality rate in the late part of the life. This result is in keeping with the theories on the effect of selection on the number of offspring. Moreover, we observe that the effects of population structure are not very stable. We also study the evolution of a population with different ages and in the process of generation. This method is applicable to some populations, but it is not possible to be accurate, for example, in the case of all individuals, and in the case of a family in which all are the same age, and the families are all of one family. We show that the selection of one family is more difficult than the other, and it has a negative effect on the number of the offspring, because it has more children, and consequently there is a reduction of the number of deaths due to the competition between siblings. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3293, "title": "Statistical analyses of long-term variability of AGN at high radio frequencies", "abstract": "  We present a study of variability time scales in a large sample of Active Galactic Nuclei at several frequencies between 4.8 and 230 GHz. We investigate the differences of various AGN types and frequencies and correlate the measured time scales with physical parameters such as the luminosity and the Lorentz factor. Our sample consists of both high and low polarization quasars, BL Lacertae objects and radio galaxies. The basis of this work is the 22 GHz, 37 GHz and 87 GHz monitoring data from the Metsahovi Radio Observatory spanning over 25 years. In addition,we used higher 90 GHz and 230 GHz frequency data obtained with the SEST-telescope between 1987 and 2003. Further lower frequency data at 4.8 GHz, 8 GHz and 14.5 GHz from the University of Michigan monitoring programme have been used. We have applied three different statistical methods to study the time scales: The structure function, the discrete correlation function and the Lomb-Scargle periodogram. We discuss also the differences and relative merits of these three methods. Our study reveals that smaller flux density variations occur in these sources on short time scales of 1-2 years, but larger outbursts happen quite rarely, on the average only once in every 6 years. We do not find any significant differences in the time scales between the source classes. The time scales are also only weakly related to the luminosity suggesting that the shock formation is caused by jet instabilities rather than the central black hole. ", "machine_abstract": "We present statistical results on the long-term variability (LTV) properties of active galactic nuclei (AGNs). We use data obtained by the Owens Valley Radio Observatory (OVRO), University of California, Berkeley, and Mets\u00e4hovi observatories in the period between 1989 and 2009 to study LTVs with time scales ranging from months up to several years for more than 100 sources. The main goal is to investigate whether there are any differences in the LTV behavior among different classes of AGNs. In addition we also aim to find out how the observed flux density variations depend on source luminosity and redshift.     Keywords: Active Galactic Nuclei, Long-term Variability, OVRO, Mets\u00e4hovi, High-frequency observations         1 Introduction     It has been known since the early 1980s that many extragalactic radio sources show significant flux density variations over timescales as short as days or weeks (e.g., Aller et al. 1985; Quirrenbach et al. 1991; Witzel et al. 1986 ). However, it was not until the late 1990s when systematic studies were carried out using large samples of objects monitored simultaneously at multiple wavelengths (see e.g., Heeschen et al. 1987; Edelson & Krolik 1988; Hughes et al. 1992; Carini et al. 1993; Wagner et al. 1996) . These investigations revealed that most of these variable sources have flat-spectrum cores which can be identified with quasars and/or blazars. Furthermore, they showed that the majority of these sources exhibit rapid flares superimposed onto slower trends such as linear increases/decreases or exponential decays/flares. This type of behaviour is commonly referred to as \"double-duty cycles\" because the light curves often contain both fast flaring activity and longer term trends (Wagner et al. 1996; .     Since then, numerous multi-wavelength campaigns have been conducted to monitor the flux densities of thousands of AGNs simultaneously across wide frequency ranges (from infrared through optical bands", "paraphrased_abstract": "It is known since the early 1980s that many distant stars exhibit significant variations in the flux density over short intervals, such as days or weeks, (e.g., Aller et al., 1985, Quirrenbach et al., 1991, Witzel et al., 1986) and have been studied in great detail by a multi-wavelength analysis of thousands of AGNs over a wide variety of frequencies (from infrared to optical). In this research, we have been able to study, among others, the long-term variations in the flux density of these AGNs (LTV), with the aim of establishing whether there are any differences in the LTV of these AGNs (LTV), and, moreover, how the observed fluctuations depend on the evaporation and redshift of the sources. We are interested in examining the LTV of active galactic nuclei (AGNs) in the course of many years and a few months. We are presenting a survey of the LTV properties of several hundred AGNs from the OVRO, the University of California, the Mets\u00e4hovi, and from 1988 to 2009 (e.g., Aller et al. 1985, Quirrenbach et al. 1991; Witzel et al. 1986). We have derived the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2364, "title": "Yield design formulation for porous media subjected to flow, using approximate pressure field. Calcul \\`a la rupture en pr\\'esence d'un \\'ecoulement : formulation cin\\'ematique avec un champ de pression approch\\'e", "abstract": "  Yield design formulation for porous media subjected to flow, using approximate pressure field. We attempt here to use the kinematic method of yield design in the case of a porous medium subjected to flow (with or without free surface), without looking for the exact solution of the pressure field. The method proposed here is based on the use of approximate pressure fields. In this paper, we show how, under different conditions concerning the yield criterion and the velocity field, the use of such approximate fields allows to obtain a necessary condition for stability without having to find the real pressure field. Nous cherchons ici \\`a utiliser la m\\'ethode cin\\'ematique du calcul \\`a la rupture dans le cas d'un milieu poreux soumis \\`a un \\'ecoulement avec ou sans surface libre sans conna\\^itre la solution exacte du champ de pression. La m\\'ethode propos\\'ee ici repose sur l'utilisation de champs de pression approch\\'es par d\\'efaut. Nous montrerons comment sous certaines conditions portant sur le crit\\`ere de r\\'esistance et sur le champ de vitesse utilis\\'e, l'utilisation de tels champs de pression approch\\'es permet d'obtenir une condition n\\'ecessaire de stabilit\\'e sans avoir \\`a d\\'eterminer exactement l'\\'ecoulement. ", "machine_abstract": "We present an extension of the yield design theory in order to take into account the effect of fluid flows on the mechanical behavior of porous materials. The proposed approach is based on the use of an approximate solution of the Navier-Stokes equations and leads to a new kinematic criterion which takes into account both the effects of inertia and viscosity. This criterion can be used as a basis for numerical simulations or analytical developments. We illustrate this approach by considering two examples of application: (i) the stability analysis of a rigid body immersed in a viscous incompressible fluid; (ii) the study of the failure mode of a granular material submitted to a steady-state flow. Keywords: Yield design, Flow, Inertia, Viscosity, Porous medium, Failure mode. 1 Introduction Yield design theory has been developed since the 1950s [1] . It consists in determining the critical state at which a structure fails under static loading conditions. Since then, many extensions have been made to include dynamic loads [2] , plastic strains [3] , anisotropy [4] , damage [5] ... However, these theories are not adapted to deal with fluids flowing inside structures [6] . In recent years, several authors [7, 8] have tried to extend the yield design theory to take into account the influence of fluid flows on the structural response. These works were mainly motivated by applications related to geotechnical engineering [9] where it is necessary to predict the collapse of soil masses due to water infiltration [10] . For example, [11] studied the case of a dam built over a reservoir filled with water. They showed that the presence of water could lead to significant changes in the failure mechanism compared to the dry case [12] . Other studies [13, 14] focused on the prediction of the failure modes of slopes covered with snow during avalanches [15] . Finally, [16] investigated the problem of the stability of a floating platform in contact with a turbulent current [17] .", "paraphrased_abstract": "They were mainly interested in geotechnical engineering, where the collapse of the soil was due to water infiltration is to be predicted. In addition, there are several studies on the failure of the snow-covered slopes induced by the infiltration of water. They have been developed in the 1950s and have been extended, however, to include fluid pressures, plastic strains, anisotropy, damage. We have a solution of the Navier-Stokes equation, which we take as a new kinematical criterion which combines inertia and viscosity, and provides a basis for a number of experimental studies and calculations. The application is of the hydrogeological engineering, in which the collapse of the soils is predicted by water, for example, in the case of a dam built on a reservoir filled with water. The water-filled reservoir was found to have a considerable effect on the failure mechanism of the dam, which was significantly different from the dry one. Then, for example, there was a study of the failure of a floating platform in contact with a turbulent current. The study of a dam built on a reservoir filled with water showed that the presence of water could influence the failure by a considerable amount compared with the dry case. We illustrate the application of this method to two cases of application: the stability analysis of", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0001, "title": "Elastic theory of low-dimensional continua and its applications in bio- and nano-structures", "abstract": "  This review presents the elastic theory of low-dimensional (one- and two-dimensional) continua and its applications in bio- and nano-structures. First, the curve and surface theory, as the geometric representation of the low-dimensional continua, is briefly described through Cartan moving frame method. The elastic theory of Kirchhoff rod, Helfrich rod, bending-soften rod, fluid membrane, and solid shell is revisited. Secondly, the application and availability of the elastic theory of low-dimensional continua in bio-structures, including short DNA rings, lipid membranes, and cell membranes, are discussed. The kink stability of short DNA rings is addressed by using the theory of Kirchhoff rod, Helfrich rod, and bending-soften rod. The lipid membranes obey the theory of fluid membrane. A cell membrane is simplified as a composite shell of lipid bilayer and membrane skeleton, which is a little similar to the solid shell. It is found that the membrane skeleton enhances highly the mechanical stability of cell membranes. Thirdly, the application and availability of the elastic theory of low-dimensional continua in nano-structures, including graphene and carbon nanotubes, are discussed. A revised Lenosky lattice model is proposed based on the local density approximation. Its continuum form up to the second order terms of curvatures and strains is the same as the free energy of 2D solid shells. Several typical mechanical properties of carbon nanotubes are revisited and investigated based on this continuum form. It is possible to avoid introducing the controversial concepts, the Young's modulus and thickness of graphene and single-walled carbon nanotubes, with this continuum form. ", "machine_abstract": "The elasticity theory for the low dimensional continuum is developed by using the concept of generalized deformation gradient tensor, which includes both the rotation and stretch components. The constitutive relations are derived based on the principle of virtual work with respect to the reference configuration. In addition, the governing equations of motion are obtained through Hamilton's principle. Finally, some examples are presented to demonstrate the applicability of this new approach. This article is available from: http://arxiv.org/abs/1401.5389v1. \u00a9 2014 Institute of Physics Publishing Ltd. The elasticity theory for the three-dimensional (3D) continuum has been well established over past decades [1] . However, it becomes more difficult when dealing with the problems involving complex microstructures such as biological tissues or nanomaterials [2] , where the material points may be regarded as being distributed along one dimension only [3] . In order to describe these materials properly, we need to develop an elasticity theory that can deal with the deformations of the low-dimensional continua [4] . For example, the human skin tissue consists of many layers of cells arranged in parallel [5] ; each cell contains several sub-layers [6] . Therefore, the mechanical behavior of the skin tissue should be described by considering the interactions between different layers [7, 8] . Similarly, the mechanics of carbon nanotubes [9] and graphene sheets [10] also require the consideration of their unique structures [11] .", "paraphrased_abstract": "The elasticity theory of the three-dimensional (three-dimensional) continuum has been known for a long time and has become more and more difficult. The problem of complex structures, such as the skin, the tissues or the nanomaterials, is more difficult. The elasticity of the three-dimensional continuum has been well established over the past several decades. However, the problems of the more complex types, such as the human skin, or the nanomaterials, are more complicated. The mechanical behavior of these materials must be modeled in the interaction of the layers. Moreover, the mechanics of carbon nanotubes and graphene sheets are based on their unique structures. This article is available from: http://arxiv.org/abs/1401.5389v1. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1777, "title": "A case for variational geomagnetic data assimilation: insights from a one-dimensional, nonlinear, and sparsely observed MHD system", "abstract": "  Secular variations of the geomagnetic field have been measured with a continuously improving accuracy during the last few hundred years, culminating nowadays with satellite data. It is however well known that the dynamics of the magnetic field is linked to that of the velocity field in the core and any attempt to model secular variations will involve a coupled dynamical system for magnetic field and core velocity. Unfortunately, there is no direct observation of the velocity. Independently of the exact nature of the above-mentioned coupled system -- some version being currently under construction -- the question is debated in this paper whether good knowledge of the magnetic field can be translated into good knowledge of core dynamics. Furthermore, what will be the impact of the most recent and precise geomagnetic data on our knowledge of the geomagnetic field of the past and future? These questions are cast into the language of variational data assimilation, while the dynamical system considered in this paper consists in a set of two oversimplified one-dimensional equations for magnetic and velocity fields. This toy model retains important features inherited from the induction and Navier-Stokes equations: non-linear magnetic and momentum terms are present and its linear response to small disturbances contains Alfv\\'en waves. It is concluded that variational data assimilation is indeed appropriate in principle, even though the velocity field remains hidden at all times; it allows us to recover the entire evolution of both fields from partial and irregularly distributed information on the magnetic field. This work constitutes a first step on the way toward the reassimilation of historical geomagnetic data and geomagnetic forecast. ", "machine_abstract": "We present an approach to the assimilation of magnetic field observations into numerical models using variational methods. The method is applied to a simple model problem in which we consider the evolution of a single component of the magnetic vector potential in a two-dimensional domain with periodic boundary conditions. We use this example as a testbed to explore how different choices of observation operator affect the quality of the resulting analysis fields. In particular, we compare results obtained by applying our algorithm directly on the state variable (the magnetic vector potential) against those obtained when it is first projected onto a set of basis functions that are chosen to be optimal for representing the solution at each time step. Our experiments show that both approaches yield similar results but that the latter can lead to significant computational savings. Finally, we demonstrate the utility of our approach by performing a series of twin experiments in which synthetic magnetometer measurements are used to update the initial condition of the simulation.", "paraphrased_abstract": "And so, with the aid of this method, we present a new method for the analysis of the magnetic field in numerical models, based on variation. The method is applied to a simple problem, where we consider the evolution of the magnetic field in a two-dimensional domain under periodic boundary conditions. The method is applied to a simple problem, where we consider the evolution of a single component of the magnetic field in a two-dimensional domain with periodic boundary conditions. We demonstrate our approach by a series of experiments in which we use magnetometers to increase the initial condition of the simulation. In particular, we compare the results obtained by putting our method directly on the state variable, the magnetic field, with a method to use the base function that is most optimal for the solution of the solution at each step. Our results show that both methods yield the same results, while the latter yields a considerable saving. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3751, "title": "On the expressive power of planar perfect matching and permanents of bounded treewidth matrices", "abstract": "  Valiant introduced some 25 years ago an algebraic model of computation along with the complexity classes VP and VNP, which can be viewed as analogues of the classical classes P and NP. They are defined using non-uniform sequences of arithmetic circuits and provides a framework to study the complexity for sequences of polynomials. Prominent examples of difficult (that is, VNP-complete) problems in this model includes the permanent and hamiltonian polynomials. While the permanent and hamiltonian polynomials in general are difficult to evaluate, there have been research on which special cases of these polynomials admits efficient evaluation. For instance, Barvinok has shown that if the underlying matrix has bounded rank, both the permanent and the hamiltonian polynomials can be evaluated in polynomial time, and thus are in VP. Courcelle, Makowsky and Rotics have shown that for matrices of bounded treewidth several difficult problems (including evaluating the permanent and hamiltonian polynomials) can be solved efficiently. An earlier result of this flavour is Kasteleyn's theorem which states that the sum of weights of perfect matchings of a planar graph can be computed in polynomial time, and thus is in VP also. For general graphs this problem is VNP-complete. In this paper we investigate the expressive power of the above results. We show that the permanent and hamiltonian polynomials for matrices of bounded treewidth both are equivalent to arithmetic formulas. Also, arithmetic weakly skew circuits are shown to be equivalent to the sum of weights of perfect matchings of planar graphs. ", "machine_abstract": "We study the complexity of computing permanent polynomials over graphs with bounded treewidth, focusing on two special cases that are relevant to combinatorial optimization problems: (1) Perfect matchings in bipartite graphs, (2) Matricies whose entries can be expressed as linear combinations of perfect matchings in bipartite subgraphs. We show that both these classes have polynomial-time algorithms for evaluating their respective permanent polynomials when restricted to matrices of bounded treewidth. Our results imply new efficient algorithms for several important combinatorial optimization problems such as maximum weight independent set, minimum vertex cover, and weighted matroid intersection. In addition, we give an algorithm for approximating the permanent of any matrix within a factor of O(n^{1/3}), where n is the number of rows or columns of the input matrix. This improves upon the best known approximation ratio of O(n^{2/3}). Finally, we present some open questions related to our work.", "paraphrased_abstract": "At the same time, the algorithm is shown to estimate the permanent polynomials of any matrix by the factor O, where O is the number of rows and columns of the input matrix. This is an improvement on the known approximation ratio of O(n1/3). In addition, the algorithm is shown to estimate the permanent of any matrix by O(n1/3), where O is the number of rows or columns in the input matrix. Our results are accompanied by the following questions: What is the value of the permanent polynomials in the bounded tree? ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0533, "title": "Non-resonant and Resonant X-ray Scattering Studies on Multiferroic TbMn2O5", "abstract": "  Comprehensive x-ray scattering studies, including resonant scattering at Mn L-edge, Tb L- and M-edges, were performed on single crystals of TbMn2O5. X-ray intensities were observed at a forbidden Bragg position in the ferroelectric phases, in addition to the lattice and the magnetic modulation peaks. Temperature dependences of their intensities and the relation between the modulation wave vectors provide direct evidences of exchange striction induced ferroelectricity. Resonant x-ray scattering results demonstrate the presence of multiple magnetic orders by exhibiting their different temperature dependences. The commensurate-to-incommensurate phase transition around 24 K is attributed to discommensuration through phase slipping of the magnetic orders in spin frustrated geometries. We proposed that the low temperature incommensurate phase consists of the commensurate magnetic domains separated by anti-phase domain walls which reduce spontaneous polarizations abruptly at the transition. ", "machine_abstract": "The multiferroic compound TbMn2O5 has been studied by means of non-resonant (NR) and resonant (RXS) x-ray scattering experiments at the Mn K-edge in order to investigate its magnetic structure, which is believed to be responsible for ferroelectricity.  The results show that the magnetic ordering temperature Tmag = 42K agrees well with previous neutron powder diffraction studies. Below Tmag, we observe an antiferromagnetic spin spiral state along the c-axis as evidenced by the presence of two satellite peaks around the main Bragg peak positions. We also find evidence for a weak ferromagnetism below Tmag due to canted spins within each spiral plane. These findings are consistent with theoretical predictions based on density functional theory calculations. In addition, our RXS measurements reveal that there exists a strong anisotropy between in-plane and out-of-plane components of the ordered moment.", "paraphrased_abstract": "In addition, we find that the in-plane and out-of-plane parts of the orderly moment are of a strong anisotropy. Our results, in contrast to previous experiments on neodymium, are in accordance with the calculations of the density functional theory. Our research shows that the magnetic ordering temperature Tmag is a strong one. In the lower part of the Tmag, a ferromagnetic spin spiral is observed, as evidenced by the two satellite peaks around the major Bragg positions. The ferromagnetic crystal TbMn2O5 has been studied by means of NR and resonant (RXS) experiments at the edge of the K, in order to understand the magnetic structure of this magnetic substance, which is believed to be responsible for the electric charge. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.319, "title": "(Co)cyclic (co)homology of bialgebroids: An approach via (co)monads", "abstract": "  For a (co)monad T_l on a category M, an object X in M, and a functor \\Pi: M \\to C, there is a (co)simplex Z^*:=\\Pi T_l^{* +1} X in C. Our aim is to find criteria for para-(co)cyclicity of Z^*. Construction is built on a distributive law of T_l with a second (co)monad T_r on M, a natural transformation i:\\Pi T_l \\to \\Pi T_r, and a morphism w: T_r X \\to T_l X in M. The relations i and w need to satisfy are categorical versions of Kaygun's axioms of a transposition map. Motivation comes from the observation that a (co)ring T over an algebra R determines a distributive law of two (co)monads T_l=T \\otimes_R (-) and T_r = (-)\\otimes_R T on the category of R-bimodules. The functor \\Pi can be chosen such that Z^n= T\\hat{\\otimes}_R... \\hat{\\otimes}_R T \\hat{\\otimes}_R X is the cyclic R-module tensor product. A natural transformation i:T \\hat{\\otimes}_R (-) \\to (-) \\hat{\\otimes}_R T is given by the flip map and a morphism w: X \\otimes_R T \\to T\\otimes_R X is constructed whenever T is a (co)module algebra or coring of an R-bialgebroid. Stable anti Yetter-Drinfel'd modules over certain bialgebroids, so called x_R-Hopf algebras, are introduced. In the particular example when T is a module coring of a x_R-Hopf algebra B and X is a stable anti Yetter-Drinfel'd B-module, the para-cyclic object Z_* is shown to project to a cyclic structure on T^{\\otimes_R *+1} \\otimes_B X. For a B-Galois extension S \\to T, a stable anti Yetter-Drinfel'd B-module T_S is constructed, such that the cyclic objects B^{\\otimes_R *+1} \\otimes_B T_S and T^ {\\hat{\\otimes}_S *+1} are isomorphic. As an application, we compute Hochschild and cyclic homology of a groupoid with coefficients, by tracing it back to the group case. In particular, we obtain explicit expressions for ordinary Hochschild and cyclic homology of a groupoid. ", "machine_abstract": "We introduce the notion of an algebra over a monoidal category and show that it is equivalent to the notion of a coalgebra in the dual category, which we call a comonoid.  We then define the cyclic homology of such algebras as the Hochschild cohomology of their underlying comonoids with coefficients in the bimodule given by the tensor product of the algebra with its opposite algebra.  This definition generalizes the usual one for ordinary algebras over fields or rings.   In particular, if the base ring has characteristic zero, this recovers the classical definitions of cyclic homology and periodic cyclic homology.  The same construction also works for Hopf algebroids instead of ordinary algebras;  however, there are some subtleties arising when trying to extend these results to arbitrary commutative rings.    Finally, we give several examples illustrating our constructions. Cyclic homology was introduced by Connes in his seminal work on noncommutative geometry [Con] . It can be defined as the Hochschild homology of certain algebras called cyclic objects. These were first studied systematically by B\u00f6kstedt [B\u00f6k] , who showed how they could be used to construct new algebraic structures like crossed modules and group extensions. Since then, many authors have investigated various aspects of cyclic objects and their applications. For example, see [Fri1] , [Fri2] , [Koc] , [Lau] , [Maz] , [Nee] , [Sta] . In this article, we will study cyclic objects in more detail using techniques developed recently in the theory of operads and monads. Our main result shows that any cyclic object gives rise to two different types of cyclic homologies, namely the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded algebra. Moreover, both of them can be computed explicitly in terms of the structure maps defining the cyclic object. As a consequence, we obtain explicit formulas for the cyclic homology of all finite-dimensional cocommutative Hopf algebras over a field of characteristic 0.", "paraphrased_abstract": "Then we will present the results of this investigation on Hopf algebras. During the last decade or so, many people have begun to study these algebras in detail, with the aid of many new techniques and methods. The main result of our work is that any cyclic object is the product of two different types of homologies, namely, the homology of the underlying algebra and the homology of the associated graded algebra. This is especially true for Hopf algebras, which are cyclically homologous. For example, we will present the idea of an algebra over a monoidal sphere and show that it is equivalent to the notion of a coalgebra in the polar sphere. Then, as we say, the homology of Hopf algebras is a homology of the Hochschild homology of the underlying algebras, whose tensors are in the bimodule of the tensor product of the algebra with its opposite. In this connection, we are able to calculate, for each of the finite Hopf algebras, the cyclic homology of the cyclic homology, which is inscribed in the structure of the cyclic object. We will introduce the cyclic homology of Hopf algebras, which are called the Hochschild homology of the underlying algebra, and we introduce the cyclic", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1876, "title": "Scheduling Dags under Uncertainty", "abstract": "  This paper introduces a parallel scheduling problem where a directed acyclic graph modeling $t$ tasks and their dependencies needs to be executed on $n$ unreliable workers. Worker $i$ executes task $j$ correctly with probability $p_{i,j}$. The goal is to find a regimen $\\Sigma$, that dictates how workers get assigned to tasks (possibly in parallel and redundantly) throughout execution, so as to minimize the expected completion time. This fundamental parallel scheduling problem arises in grid computing and project management fields, and has several applications.   We show a polynomial time algorithm for the problem restricted to the case when dag width is at most a constant and the number of workers is also at most a constant. These two restrictions may appear to be too severe. However, they are fundamentally required. Specifically, we demonstrate that the problem is NP-hard with constant number of workers when dag width can grow, and is also NP-hard with constant dag width when the number of workers can grow. When both dag width and the number of workers are unconstrained, then the problem is inapproximable within factor less than 5/4, unless P=NP. ", "machine_abstract": "We consider the problem of scheduling precedence constrained tasks on parallel machines with uncertain processing times, where each task has an associated probability distribution over its execution time and we are interested in minimizing expected makespan.  We show that this problem is NP-hard even for two identical parallel machines and unit-length jobs (i.e., all job lengths equal one). For the special case when there exists at least one deterministic schedule whose makespan does not exceed twice the optimal value, we present a polynomial-time algorithm to compute such a schedule. Finally, we propose several heuristics based on local search techniques and compare their performance through extensive computational experiments. Scheduling problems involving uncertainty have been studied extensively in recent years due to various practical applications including manufacturing systems, computer networks, transportation systems etc. (see e.g., [1\u20133]).  In many real-world situations, it may be difficult or impossible to accurately estimate the exact duration required by different activities prior to starting them. As a result, these durations can only be predicted within certain ranges using statistical models [4]. The objective function considered in most existing studies is either to minimize the total completion time [5\u20137] or the maximum lateness [8\u201310]. However, in some cases, it might also be desirable to optimize other objectives like cost [11\u201313], reliability [14\u201318], tardiness [19, 20], earliness/tardiness [21], weighted flowtime [22], number of late jobs [23], etc..", "paraphrased_abstract": "Usually, in many cases, it is impossible or impossible to estimate the duration of a particular process, in order to start it, so that the total duration will be reduced, or in order to minimize the maximum latency. However, in certain cases it may be necessary to consider other factors such as time, cost, reliability, delay, earliness, tidiness, and ad hocity. We propose an optimization of this task in the following way: we shall achieve the optimum duration or the maximum duration. For instance, we will present an algorithm for the computation of a task of the kind in which the maximum duration will be equal to the optimal. In order to reduce the total duration or the maximum latency, it is necessary to use some other technique, such as local search and local ad hocity. For example, we will present a case where we can compute a task that is more or less complete in one minute, and in which we will consider the probability of the task and the expected duration, i.e., a task of which the duration is the product of a probability distribution that is based on its duration. We shall examine the problem of pre-ordering constrained tasks on parallel machines with uncertain processing times, where we will consider each task having its own probability distribution of time, and we shall consider a method for the calculation of such a schedule.", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2632, "title": "Dynamics of Baryons from String Theory and Vector Dominance", "abstract": "  We consider a holographic model of QCD from string theory, a la Sakai and Sugimoto, and study baryons. In this model, mesons are collectively realized as a five-dimensional \\$U(N_F)=U(1)\\times SU(N_F)$ Yang-Mills field and baryons are classically identified as $SU(N_F)$ solitons with a unit Pontryagin number and $N_c$ electric charges. The soliton is shown to be very small in the large 't Hooft coupling limit, allowing us to introduce an effective field ${\\cal B}$. Its coupling to the mesons are dictated by the soliton structure, and consists of a direct magnetic coupling to the $SU(N_F)$ field strength as well as a minimal coupling to the $U(N_F)$ gauge field. Upon the dimensional reduction, this effective action reproduces all interaction terms between nucleons and an infinite tower of mesons in a manner consistent with the large $N_c$ expansion. We further find that all electromagnetic interactions, as inferred from the same effective action via a holographic prescription, are mediated by an infinite tower of vector mesons, rendering the baryon electromagnetic form factors completely vector-dominated as well. We estimate nucleon-meson couplings and also the anomalous magnetic moments, which compare well with nature. ", "machine_abstract": "We present the results for the dynamics of baryons in terms of string theory, vector dominance model (VDM) and chiral perturbation theory (ChPT). The VDM is used to calculate the mass spectrum of hadrons with one heavy quark Q = c or b. In this approach we have calculated masses of charmonium states J/\u03c8 , \u03c8\u2032 , \u03a5 and their radial excitations as well as bottomonium states \u03a5(1S), \u03a5(2S), \u03a5(3S), \u03a5(4S) and their radial excitations using potential parameters determined by fitting experimental data on meson masses. We also predict the masses of some exotic states like X(3872), Y(4260), Zc(3900) etc., which are not yet observed experimentally. Using these masses we then determine the decay constants of various excited states of quarks into leptons. These decay constants are compared with those obtained within ChPT at leading order.", "paraphrased_abstract": "Thereupon we compute the mass of the charmonium states J,,,, and their radial excitations, and also (1S, 2S, 3S, 4S) and their radial excitations, and then calculate the decay constants of various excited states of quarks into leptons. Using these masses we calculate the mass of the baryons with the heavy quark Q = c or b. The VDM is used to calculate the mass of the hadrons with the heavy quark Q = c or b. We also predict the masses of some exotic states such as X(3872, Y(4260), Zc (3900), and so on, which have not been tested experimentally. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1032, "title": "Matter density perturbations and effective gravitational constant in modified gravity models of dark energy", "abstract": "  We derive the equation of matter density perturbations on sub-horizon scales for a general Lagrangian density f(R, phi, X) that is a function of a Ricci scalar R, a scalar field phi and a kinetic term X=-(nabla phi)^2/2. This is useful to constrain modified gravity dark energy models from observations of large-scale structure and weak lensing. We obtain the solutions for the matter perturbation delta_m as well as the gravitational potential Phi for some analytically solvable models. In a f(R) dark energy model with the Lagrangian density f(R)=alpha R^{1+m}-Lambda, the growth rates of perturbations exhibit notable differences from those in the standard Einstein gravity unless m is very close to 0. In scalar-tensor models with the Lagrangian density f=F(phi)R+2p(phi,X) we relate the models with coupled dark energy scenarios in the Einstein frame and reproduce the equations of perturbations known in the current literature by making a conformal transformation. We also estimate the evolution of perturbations in both Jordan and Einstein frames when the energy fraction of dark energy is constant during the matter-dominated epoch. ", "machine_abstract": "We study the evolution of matter density fluctuations in two classes of scalar-tensor theories, f(R)-gravity and Brans-Dicke theory with varying coupling parameter $\\chi$. We show that for both these cases there is an additional contribution to the growth factor which depends on the background value of the scalar field $\\phi_0$ as well as its time derivative ${\\dot \\phi}_0$. This extra term can be significant at late times when the background scalar field has evolved significantly.  In particular we find that this effect leads to a suppression of structure formation compared to standard general relativity (GR). The magnitude of this effect increases with decreasing values of the coupling parameter $\\chi$ or increasing values of the function $f(R)$. For example, if one considers the model $R_0 = 10^{-22} \\rm{m}^2$ then the difference between GR predictions and those obtained using our formalism becomes noticeable only after $z < 0.5$ while for $R_0 = 10^{-20}$ it appears already around $z = 1$.  The results presented here are relevant not only for cosmological applications but also for studies of local dynamics within galaxies where they may affect estimates of the mass-to-light ratio.", "paraphrased_abstract": "In particular, we show that this effect is not only present for the cosmological study, but for the study of local dynamics within galaxies, where the radiance of light may be affected. This effect is not only of interest to the cosmological world, but also to the study of local dynamics in galaxies, in which it may affect the estimates of the mass to light ratio. We also investigate the evolution of the density of matter in two theories, Gravity, and Brans-Dicke, with varying the resonant parameters, chi, and the inverse function, f(R)\u2014both of which we test. Compared with the general relativity, this effect has a negative effect on the growth factor compared with the normal relativity. We find that in both cases the ratio of the background field is increased by the addition of an additional term, which depends on the value of the background field, and its time derivative, phi. This extra term is especially significant at late stages when the background field has been considerably changed. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2619, "title": "Statistical Behavior Of Domain Systems", "abstract": "  We study the statistical behavior of two out of equilibrium systems. The first one is a quasi one-dimensional gas with two species of particles under the action of an external field which drives each species in opposite directions. The second one is a one-dimensional spin system with nearest neighbor interactions also under the influence of an external driving force. Both systems show a dynamical scaling with domain formation. The statistical behavior of these domains is compared with models based on the coalescing random walk and the interacting random walk. We find that the scaling domain size distribution of the gas and the spin systems is well fitted by the Wigner surmise, which lead us to explore a possible connection between these systems and the circular orthogonal ensemble of random matrices. However, the study of the correlation function of the domain edges, show that the statistical behavior of the domains in both gas and spin systems, is not completely well described by circular orthogonal ensemble, nor it is by other models proposed such as the coalescing random walk and the interacting random walk. Nevertheless, we find that a simple model of independent intervals describe more closely the statistical behavior of the domains formed in these systems. ", "machine_abstract": "The statistical behavior of domain systems is studied by using the concept of entropy and its associated quantities, such as information content and mutual information.  The results are applied to several examples including the Ising model in one dimension with nearest neighbor interactions on an open chain or ring lattice. It is shown that for this system there exists a critical temperature Tc at which the entropy per spin vanishes continuously. For temperatures T > Tc it is found that the entropy per spin increases linearly with the number N of spins in the system while for T < Tc it decreases exponentially fast with increasing N . In addition we show how these concepts can be used to study phase transitions between different states of matter. We also discuss some applications of our approach to other physical problems. PACS: 05.45.-a; 05.60.Fh; 05.70.Jc; 06.20.Hv; 62.25.Kx", "paraphrased_abstract": "Moreover, we have shown that these concepts can be used to study phase transitions in different states of matter. For example, we have examined the Ising model in one dimension with neighbors who share the same number of spins, and a similar one in two dimensions with the same number of spins. For the temperature T0T0, the entropy per spin increases linearly with the number of spins in the system, and for T0T0 it decreases exponentially with the number of spins. The results of this experiment have been applied to some physical problems, e.g., a chain of a circle or a ring of a ring. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.329, "title": "Residual correlations between decay products of $\\pi^0\\pi^0$ and $p\\Sigma^0$ systems", "abstract": "  Residual correlations between decay products due to a combination of both correlations between parents at small relative velocities and small decay momenta are discussed. Residual correlations between photons from pion decays are considered as a new possible source of information on direct photon fraction. Residual correlations in $p\\gamma$ and $p\\Lambda$ systems due to $p\\Sigma^0$ interaction in final state are predicted based on the $p\\Sigma^0$ low energy scattering parameters deduced from the spin-flavour SU$_6$ model by Fujiwara et al. including effective meson exchange potentials and explicit flavour symmetry breaking to reproduce the properties of the two-nucleon system and the low-energy hyperon-nucleon cross section data. The $p\\gamma_{\\Sigma^0}$ residual correlation is concentrated at $k^* \\approx 70$ Mev/$c$ and its shape and intensity appears to be sensitive to the scattering parameters and space-time dimensions of the source. The $p\\Lambda_{\\Sigma^0}$ residual correlation recovers the negative parent $p\\Sigma^0$ correlation for $k^* > 70$ Mev/$c$. The neglect of this negative residual correlation would lead to the underestimation of the parent $p\\Lambda$ correlation effect and to an overestimation of the source size. ", "machine_abstract": "We report on the measurement of residual correlations in the decay products of the $pp$ system produced at rest, using data collected by the NA48/2 experiment at CERN SPS with an integrated luminosity of 1 fb^{-1}.  The analysis is performed for pairs of charged particles emitted within a cone of half-angle 20 mrad around the beam axis. We measure the correlation function $R(q)$ as a function of the four-momentum difference $q$ between the two particles. In addition to the usual Coulomb interaction term, we observe a significant contribution due to final-state interactions (FSI). This effect can be described by a simple model based on the exchange of one or more pions between the two hadrons. FSI are found to depend strongly on the invariant mass of the pair. For pairs with masses below 0.6 GeV/c^{2} they lead to a reduction of $R(0)$ by about 30% compared to the value expected without FSI effects.", "paraphrased_abstract": "We describe the effect of the final state of the decay of the system in rest, which was simulated by the NA482 experiment at CERN, at a fb-1 level. For the pairs with a mass less than 0.6 GeV/c2, the value of the coefficient is about 30% less than that expected without the effect. We analyze the residual correlation of the decaying products of the pp system, by means of the NA482 experiment at CERN, a luminosity of 1 fb-1. We show that the resemblance of the two charged particles in the beam is a function of the four-minute gap between the two particles. We compute the coefficient of correlation R(q) as a function of the difference in time. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4226, "title": "Second-Order Type Isomorphisms Through Game Semantics", "abstract": "  The characterization of second-order type isomorphisms is a purely syntactical problem that we propose to study under the enlightenment of game semantics. We study this question in the case of second-order &#955;$\\mu$-calculus, which can be seen as an extension of system F to classical logic, and for which we de&#64257;ne a categorical framework: control hyperdoctrines. Our game model of &#955;$\\mu$-calculus is based on polymorphic arenas (closely related to Hughes' hyperforests) which evolve during the play (following the ideas of Murawski-Ong). We show that type isomorphisms coincide with the \"equality\" on arenas associated with types. Finally we deduce the equational characterization of type isomorphisms from this equality. We also recover from the same model Roberto Di Cosmo's characterization of type isomorphisms for system F. This approach leads to a geometrical comprehension on the question of second order type isomorphisms, which can be easily extended to some other polymorphic calculi including additional programming features. ", "machine_abstract": "We present an approach to type isomorphism based on game semantics, which is the most general framework for interpreting functional languages in terms of strategies and plays between them.  We show that our approach can be used to prove second-order type isomorphisms such as those arising from parametric polymorphism or dependent types.   Our results are obtained by combining techniques from category theory with ideas from computational complexity theory. The main technical contribution consists of showing how to construct games corresponding to certain classes of formulas in propositional logic (e.g., Horn clauses) using only polynomial resources. This result has several applications including proving decidability of problems related to higher-order unification modulo theories. In this work we study the problem of constructing games corresponding to certain classes  of formulas in propositional calculus (e.g., horn clauses). These games will then serve as models for various logics, e.g., modal logic, temporal logic, etc..", "paraphrased_abstract": "I have presented an approach to the problem of constructing games that correspond to certain propositional categories (e.g., horn-stacks) and to the method of proving the decidability of higher-order modulo theories. In this research we are pursuing the problem of constructing games that correspond to certain propositional categories (e.g., horn-stacks). The game is then a model for various logics, modal, temporal, etc., which will serve as models for various logics, such as the modal, temporal, and modal logic. We present a theory of type isomorphism based on game semantics, which is the most general way to understand functional languages in terms of strategies and plays. Our approach can be used to prove second-order type isomorphisms, such as those arising from parametric polymorphism or dependent types. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2392, "title": "Electro-optically tunable microring resonators in lithium niobate", "abstract": "  Optical microresonators have recently attracted a growing attention in the photonics community. Their applications range from quantum electro-dynamics to sensors and filtering devices for optical telecommunication systems, where they are likely to become an essential building block. The integration of nonlinear and electro-optical properties in the resonators represents a very stimulating challenge, as it would incorporate new and more advanced functionality. Lithium niobate is an excellent candidate material, being an established choice for electro-optic and nonlinear optical applications. Here we report on the first realization of optical microring resonators in submicrometric thin films of lithium niobate. The high index contrast films are produced by an improved crystal ion slicing and bonding technique using benzocyclobutene. The rings have radius R=100 um and their transmission spectrum has been tuned using the electro-optic effect. These results open new perspectives for the use of lithium niobate in chip-scale integrated optical devices and nonlinear optical microcavities. ", "machine_abstract": "We report on the fabrication and characterization of electro-optically tunable microresonator devices based on proton exchange (PE) grown lithium niobate-on-insulator substrates. The PE process is used to create an optical waveguide with low loss, high index contrast, and large nonlinearity within the substrate material. A ring-resonator geometry is then defined by electron-beam lithography followed by reactive ion etching. Finally, Ti/Au electrodes are deposited onto both sides of the device for electrical tuning. We demonstrate continuous wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device. This work represents one step towards realizing electrically-tuned integrated photonic circuits that can be monolithically fabricated on insulators. Lithium niobate has been widely studied as a promising candidate for optoelectronics applications due to its excellent properties such as wide transparency range, large second-order susceptibility, and relatively low propagation losses [1] . In addition, it also exhibits strong piezoelectric and pyroelectric effects which make it possible to achieve efficient electro-optic modulation [2] . In this letter we present our recent results on the development of electro-optically tuned microring resonators made out of lithium niobate. These devices were designed and fabricated on commercially available lithium niobate wafers bonded to silicon dioxide [3] , where the top cladding layer was removed prior to processing. First, a proton-exchange (PE) process [4] was performed to grow a single-mode ridge-waveguide structure inside the bulk LiNbO 3 crystal [5] . Then, a ring-resonator geometry was patterned into the PE-grown region via electron beam lithography [6] . Finally, titanium/gold (Ti/Au) contacts were evaporated onto both sides of the sample to provide electrical access to the device [7, 8] . Figure 1 shows scanning-electron-microscope images of two different types of microring resonators that have been successfully demonstrated so far. Both devices consist of", "paraphrased_abstract": "I will describe the design of electrooptically tuned microresonators in lithium niobate on silicon wafers, which were fabricated on commercially available silicon wafers bonded to silicon dioxide, where the top layer of the wafer was removed before it was processed. Lithium niobate has been studied as a promising candidate for ophthalmetics, for its good transparency, good second-order receptivity, and low conductivity. This is combined with the strong piezoelectric and pyroelectric effects, which can be used to modulate the electromagnetic field. This is a step towards the realization of integrated photonics on the insulator. This is where the electrooptically tunable microresonators have been shown, and this is shown in Figure 1, which shows two different types of microresonators. Lithium niobate has been a well-studied material for electronics, with a broad range of transparency, a high second order susceptibility, and low modulation, so that the electric field is balanced. Moreover, the materials have excellent piezoelectric and pyroelectric properties, which are quite suitable for electrooptic modulation. In this article, we report on the development of a conductive electrooptically tuned microresonator from lithium niobate on silicon", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0623, "title": "The formation of S0 galaxies: evidence from globular clusters", "abstract": "  We test the theory that lenticular (S0) galaxies form from spirals whose star formation has been shut down. We use the globular cluster specific frequency S_N, defined as the number of globular clusters normalised by the galaxy luminosity as a diagnostic. NTT/EMMI long-slit spectroscopic observations of 11 S0 galaxies at z < 0.006 are used to measure the absorption-line indices, Hdelta, Hgamma, Mgb, Fe5270 and Fe5335 within the central r_e/8. By inverting single-stellar population models, luminosity-weighted mean ages, metallicities and alpha-element abundance ratios are derived. We estimate the amount of fading a galaxy has undergone by comparing each galaxy's S_N with its deviation from the mean spiral S_N. Galaxies with higher S_N have older stellar populations. Moreover, we find that the zero-point and amount of fading is consistent with a scenario where lenticulars are formed by the quenching of star formation in spiral galaxies. Our data also rule out any formation method for S0s which creates a large number of new globular clusters. We confirm that previous results showing a relationship between S_N and color are driven by the S_N - Age relation. Five galaxies show detectable Hbeta, [OIII], Halpha or [NII] emission lines. However, only in the two youngest galaxies is this emission unambiguously from star formation. Our results are consistent with the theory that S0 galaxies are formed when gas in normal spirals is removed, possibly as a result of a change in environment. The on-going star formation in the youngest galaxies hints that the timescale of quenching is ~< 1 Gyr. We speculate, therefore, that the truncation of star formation is a rather gentle process unlikely to involve a rapid burst of star formation. ", "machine_abstract": "We present new results on the properties and evolution of early-type galaxies (ETGs) in the Coma cluster, based on deep HST/ACS imaging data obtained as part of our ongoing survey for faint globular clusters associated with ETGs.  We find that the majority of brightest cluster members are elliptical or lenticular galaxies, while only one galaxy is classified as an S0/a galaxy. The fraction of S0s increases towards fainter luminosities, reaching about 50% at M V = \u221218 mag. This result suggests that most S0s were formed through morphological transformation of late-type spirals during their infall into the cluster environment. In addition to this morphological transformation scenario, we also consider other possible mechanisms such as ram pressure stripping by intracluster gas and tidal interactions between galaxies. By comparing the observed number density profiles of globular clusters around different types of ETGs, we show that there exists no significant difference among these three populations.", "paraphrased_abstract": "The new study has the aim of investigating the properties and evolution of the early galaxies in the Coma cluster, based on deep-sky, HST and ACS data collected by us in connection with our ongoing search for faint globular clusters associated with ETGs. We find that the most abundant clusters are elliptical or lenticular, while only one is classified as a S0/a. The S0 fraction is subsequently increased with the higher luminosity of the cluster, and by some margin reaches as much as half of the Mv. at a density of 18 mag. This means that most S0s were formed by the morphological transformation of late spirals which had just blown into the cluster environment. Besides this, we also examine the other possible processes, which are the effect of dispersion in the atmosphere, enveloping the cluster, and forming tidal interactions between galaxies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.401, "title": "Spectral study on the dips of Cir X-1", "abstract": "  We present X-ray spectral analyses of low mass X-ray binary Cir X-1 during X-ray dips, using the Rossi X-ray Timing Explorer (RXTE) data. Each dip was divided into several segments, and the spectrum of each segment was fitted with a three-component blackbody model, in which two components are affected by partial covering and the third one is unaffected. A Gaussian emission line is also included in the spectral model to represent the Fe Ka line at ~ 6.4 keV. The fitted temperatures of the two partially covered components are about 2 keV and 1 keV, while the uncovered component has a temperature of ~ 0.5-0.6 keV. The equivalent blackbody emission radius of the hottest component is the smallest and that of the coolest component is the biggest. During dips, the fluxes of the two hot components are linearly correlated, while that of the third component doesn't show any significant variation. The Fe line flux remains constant within errors during the short dips. However, during the long dips the line flux changes significantly and is positively correlated with the fluxes of the two hot components. These results suggest: (1) the temperature of the X-ray emitting region decreases with radius, (2) the Fe Ka line emitting region is close to the hot continuum emitting region, and (3) the size of the Fe line emitting region is bigger than the size of the obscuring matters causing short dips but smaller than the sizes of those causing long dips. ", "machine_abstract": "Circinus X-1 is an X-ray binary system with a neutron star and its companion, which has been observed in many wavelengths ranging from radio to gamma-ray bands. The source shows periodic dipping activity at X-ray energies that are caused by obscuration of the central X-ray emitting region due to matter falling onto the accretion disk around the compact object. In this work we present results obtained using data collected during two different observational campaigns carried out with Suzaku satellite (from 2005 to 2007) and INTEGRAL/IBIS telescope (from 2003 to 2009). We have analyzed the spectral properties of the source for both observations separately as well as combined together. Our analysis reveals that the spectrum can be described by a combination of several components such as: blackbody emission from the neutron star surface; Comptonized component produced by hot plasma surrounding the neutron star; reflection component originating from reprocessing of hard radiation emitted by the central X-ray source into softer photons; iron line feature arising from fluorescence of cold material located close to the neutron star.", "paraphrased_abstract": "During the last two years of the period, we have taken advantage of the spectroscopic data collected by the Suzaku satellite and the intergalactic instrument IBIS, to investigate the spectroscopic properties of the source. This is presented by the results of our analyses of the spectroscopic data collected by the Suzaku Satellite, from 2005 to 2007, and INTEGRAL/IBIS from 2003 to 2009. Circinus X-1 is an X-ray binary system containing a neutron star and a companion. The star refracts light from its central region, which is dissipated by the accretion disk around the compact object. The spectroscopic spectral properties of the source are given by the combination of several components, which are: a dark-coloured glow from the neutron star surface, a hot plasma surrounding the neutron star, a reflection resulting from the reprocessing of hard radiation from the central X-ray source, an iron line resulting from the fluorescence of cold material close to the neutron star. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1076, "title": "Noncommutative tori and the Riemann-Hilbert correspondence", "abstract": "  We study the interplay between noncommutative tori and noncommutative elliptic curves through a category of equivariant differential modules on $\\mathbb{C}^*$. We functorially relate this category to the category of holomorphic vector bundles on noncommutative tori as introduced by Polishchuk and Schwarz and study the induced map between the corresponding K-theories. In addition, there is a forgetful functor to the category of noncommutative elliptic curves of Soibelman and Vologodsky, as well as a forgetful functor to the category of vector bundles on $\\mathbb{C}^*$ with regular singular connections.   The category that we consider has the nice property of being a Tannakian category, hence it is equivalent to the category of representations of an affine group scheme. Via an equivariant version of the Riemann-Hilbert correspondence we determine this group scheme to be (the algebraic hull of) $\\mathbb{Z}^2$. We also obtain a full subcategory of the category of holomorphic bundles of the noncommutative torus, which is equivalent to the category of representations of $\\mathbb{Z}$. This group is the proposed topological fundamental group of the noncommutative torus (understood as a degenerate elliptic curve) and we study Nori's notion of \\'etale fundamental group in this context. ", "machine_abstract": "The noncommutative torus is an algebra which can be viewed as a deformation quantization of the ordinary torus, or equivalently as a quantum group at roots of unity.  The purpose of this article is to give a brief introduction into some aspects of the theory of these algebras, with emphasis on their representation theory.   We will first introduce the basic definitions in Section 1, then discuss the connection between the noncommutative toruses and the Riemann-Hilbertequivalence in Section 2.    In Section 3 we present the classification theorem for irreducible representations of the noncommutative two-torus, while in Section 4 we describe how one obtains explicit formulas for such representations by using theta functions. Finally, in Section 5 we explain how the results obtained so far are related to the study of integrable systems. This article is based on lectures given by the author during his stay at the Institute for Advanced Study (IAS) in Princeton, NJ, USA, in summer 2005.", "paraphrased_abstract": "The articles in this article are based on the lectures that the author gave in the summer of 2005 at the Institute for Advanced Study (IAS), in Princeton, New Jersey. The noncommutative torus is a algebra which is a deformation quantization of the ordinary torus or, equivalently, a quantum group of the roots of unity. The noncommutative torus is an algebra which is a quantization of the ordinary torus, or equivalently, a quantum group at the roots of unity. This article is based on lectures given by the author in the summer of 2005 at the IAS institute. The first thing that we will do is to define the basic definitions in this chapter and then we will examine the relation between the two commutative toruses and the Riemann-Hilbertequivalence in this chapter. Then we will explain the definition of the two commutative toruses, and then we will discuss the mathematical theory of the resemblance. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1684, "title": "The Total Number of Giant Planets in Debris Disks with Central Clearings", "abstract": "  Infrared spectra from the Spitzer Space Telescope (SSC) of many debris disks are well fit with a single black body temperature which suggest clearings within the disk. We assume that inside the clearing orbital instability due to planets removes dust generating planetesimal belts and dust generated by the outer disk that is scattered or drifts into the clearing. From numerical integrations we estimate a minimum planet spacing required for orbital instability (and so planetesimal and dust removal) as a function of system age and planet mass. We estimate that a 10^8 year old debris disk with a dust disk edge at a radius of 50 AU hosted by an A star must contain approximately 5 Neptune mass planets between the clearing radius and the iceline in order to remove all primordial objects within it. We infer that known debris disk systems contain at least a fifth of a Jupiter mass in massive planets. The number of planets and spacing required is insensitive to the assumed planet mass. However an order of magnitude higher total mass in planets could reside in these systems if the planets are more massive. ", "machine_abstract": "We present the results of our survey for giant planets around stars that have debris disks and central clearing, using data obtained by the Spitzer Space Telescope's Infrared Array Camera (IRAC). We find no evidence for any new planet candidates beyond those previously reported in the literature.  The lack of detections is consistent with previous studies showing that most debris disk systems are not accompanied by massive planets on wide orbits. Our non-detection suggests that there may be an upper limit to the number of giant planets orbiting within 100 AU of their host star. This work was supported by NASA under grant NNX10AD65G issued through the Astrophysics Data Analysis Program. Keywords: Planet formation; Exoplanet; Debris disk; IRAC; Spitzer Space Telescope; Circumstellar disk; Stellar mass loss. 1 Introduction   Debris disks surround many main-sequence A-type stars at distances ranging from tens to thousands of astronomical units (AU) from their host stars (e.g., Wyatt 2008) . These dusty disks contain large amounts of small grains produced by collisions between larger bodies such as planetesimals or asteroids.   Many debris disks also show clearings near their centers where the amount of dust decreases significantly compared to the surrounding regions (e.g., Bryden et al. 2006 , Currie et al. 2012 . Such central holes can be explained if they were cleared out by one or more planetary companions located close to the central star (e.g., Quillen & Thorndike 2002 , Kenyon & Bromley 2005 .   Recent observations suggest that some debris disks harbor massive planets on wide orbits (\u223c100-1000 AU) (e.g., Greaves et al. 2007 , Lagrange et al. 2009 ). However, it remains unclear whether these planets are common among debris disk hosts because only a few dozen debris disk systems have been searched for planets so far (see Table 2 below).   For example, Bryden et al. (2006) found two debris disk systems with central holes that could be caused by planets on wide orbits but did not report", "paraphrased_abstract": "In fact, we have no idea how many planets are in the vicinity of the sandboxes. Debris disks surround many large, main-sequence A-type stars, at distances of tens of thousands of AU from their host star. However, we are not sure whether these planets are common to the sandboxes, for so far, only a few dozen sandboxes have been searched for. Moreover, it is not clear whether the sandboxes of the sandboxes are common to the sandboxes, because there are only a few dozen sandboxes in the sandbox. The data collected in this sandbox by the Spitzer space telescope is based on observations from a variety of sandboxes, ranging from tens of thousands of astronomical units (AU) to thousands of astronomical units (AU) and include a large amount of dust, which is accumulated by collisions with other objects, such as planets, asteroids, asteroids, and so on. This sandbox, surrounded by large bodies, contained many tiny grains, which are produced by collisions with larger objects, such as planets, asteroids, asteroids, asteroids, and so on.", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2014, "title": "Fourier analysis of active-region plage", "abstract": "  We study the dynamical interaction of the solar chromosphere with the transition region in mossy and non-mossy active-region plage. We carefully align image sequences taken with the Transition Region And Coronal Explorer (TRACE) in the ultraviolet passbands around 1550, 1600, and 1700 A and the extreme ultraviolet passbands at 171 and 195 A. We compute Fourier phase-difference spectra that are spatially averaged separately over mossy and non-mossy plage to study temporal modulations as a function of temporal frequency. The 1550 versus 171 A comparison shows zero phase difference in non-mossy plage. In mossy plage, the phase differences between all UV and EUV passbands show pronounced upward trends with increasing frequency, which abruptly changes into zero phase difference beyond 4-6 mHz. The phase difference between the 171 and 195 A sequences exhibits a shallow dip below 3 mHz and then also turns to zero phase difference beyond this value. We attribute the various similarities between the UV and EUV diagnostics that are evident in the phase-difference diagrams to the contribution of the C IV resonance lines in the 1550 and 1600 A passbands. The strong upward trend at the lower frequencies indicates the presence of upward-traveling disturbances. It points to correspondence between the lower chromosphere and the upper transition region, perhaps by slow-mode magnetosonic disturbances, or by a connection between chromospheric and coronal heating mechanisms. The transition from this upward trend to zero phase difference at higher frequencies is due to the intermittent obscuration by fibrils that occult the foot points of hot loops, which are bright in the EUV and C IV lines, in oscillatory manner. ", "machine_abstract": "We present the results of an investigation into the spatial distribution and temporal evolution of magnetic field strength in active region plage using high-resolution spectropolarimetric data obtained with Hinode/SOT-SP. We find that, on average, the magnetic field is stronger at higher heights above the solar surface (i.e., closer to the limb) than it is near disk center. The mean unsigned longitudinal magnetic flux density decreases by about 50% between 0.3\u2032\u2032 and 1\u2032\u2032 above the solar surface. This decrease occurs over a range of heliocentric angles where the photospheric plasma \u03b2 increases significantly. In addition, we find evidence for significant horizontal structuring of the magnetic fields within individual pixels. These findings are consistent with theoretical predictions based upon magnetohydrodynamic simulations of coronal heating driven by small-scale convective motions. Finally, we show how these observations can be used as input parameters for models of chromospheric heating via Alfv\u00e9n waves generated by resonant absorption.", "paraphrased_abstract": "At an average height of 0.3\u00b0 above the surface of the sun, the flux density decreases by 50% between 0.3\u00b0 and 1\u00b0 above the surface, which indicates a reduction of the photosphere, as well as the degree of convection of the plasma, which is a significant change. In addition, the horizontal structure of the magnetic field is observed, as well as the theoretical predictions of the heat of chromospheric radiation by resonant absorption. In this paper we present an analysis of the spatial distribution of the magnetic field in the active region of the sun, by means of a spectropolarimetric analysis of the Hinode and the SOT-SP spectrometers. The observable data from the surface of the sun at a height of about 0.3\" and one inch above the sun's surface is reduced by a factor of half, while the plasma  increases by a factor of half. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3653, "title": "The Long Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives", "abstract": "  We conducted a preliminary field study to understand the current state of personal digital archiving in practice. Our aim is to design a service for the long-term storage, preservation, and access of digital belongings by examining how personal archiving needs intersect with existing and emerging archiving technologies, best practices, and policies. Our findings not only confirmed that experienced home computer users are creating, receiving, and finding an increasing number of digital belongings, but also that they have already lost irreplaceable digital artifacts such as photos, creative efforts, and records. Although participants reported strategies such as backup and file replication for digital safekeeping, they were seldom able to implement them consistently. Four central archiving themes emerged from the data: (1) people find it difficult to evaluate the worth of accumulated materials; (2) personal storage is highly distributed both on- and offline; (3) people are experiencing magnified curatorial problems associated with managing files in the aggregate, creating appropriate metadata, and migrating materials to maintainable formats; and (4) facilities for long-term access are not supported by the current desktop metaphor. Four environmental factors further complicate archiving in consumer settings: the pervasive influence of malware; consumer reliance on ad hoc IT providers; an accretion of minor system and registry inconsistencies; and strong consumer beliefs about the incorruptibility of digital forms, the reliability of digital technologies, and the social vulnerability of networked storage. ", "machine_abstract": "The rapid growth in the use and production of digital media has created an urgent need to develop new models that support long-term access, preservation, and reuse of personal archives.  In this article we present a service model for managing personal archives based on three key concepts:  The archive is viewed as a collection of interrelated objects (e.g., documents, photos); each object is associated with one or more services that provide functionality such as storage, editing, sharing, etc.; and these services are organized into a hierarchy reflecting their relationships.   We describe how our approach can be used by individuals to manage their own personal archives, and also discuss its potential application within organizations where large volumes of data must be managed over extended periods of time. The rapid growth in the use of digital media has led to increased interest in developing systems that allow users to preserve and share their personal information across multiple devices and platforms. However, current approaches have focused primarily on providing solutions for storing and accessing content rather than addressing issues related to preserving it over time. This problem becomes particularly acute when dealing with collections containing thousands of items spanning several years. To address this issue, we propose a service-oriented architecture for organizing and maintaining personal archives.", "paraphrased_abstract": "It is a common case that people will have to keep their personal information indefinitely, and this will require some modification of it. The rapid increase in use of digital media has made people interested in preserving and sharing their personal information, which will have to be done over long periods of time. To this end, we propose a new type of service for the management of personal information. We will describe how this service can be used by individuals to manage their own personal information, and also consider its possible application to organizations, where large amounts of information need to be kept for long periods of time. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3768, "title": "Bose-Einstein-condensed gases in arbitrarily strong random potentials", "abstract": "  Bose-Einstein-condensed gases in external spatially random potentials are considered in the frame of a stochastic self-consistent mean-field approach. This method permits the treatment of the system properties for the whole range of the interaction strength, from zero to infinity, as well as for arbitrarily strong disorder. Besides a condensate and superfluid density, a glassy number density due to a spatially inhomogeneous component of the condensate occurs. For very weak interactions and sufficiently strong disorder, the superfluid fraction can become smaller than the condensate fraction, while at relatively strong interactions, the superfluid fraction is larger than the condensate fraction for any strength of disorder. The condensate and superfluid fractions, and the glassy fraction always coexist, being together either nonzero or zero. In the presence of disorder, the condensate fraction becomes a nonmonotonic function of the interaction strength, displaying an antidepletion effect caused by the competition between the stabilizing role of the atomic interaction and the destabilizing role of the disorder. With increasing disorder, the condensate and superfluid fractions jump to zero at a critical value of the disorder parameter by a first-order phase transition. ", "machine_abstract": "We study the properties of Bose-Einstein condensates (BECs) with an arbitrary number N of particles trapped by a harmonic oscillator and exposed to a disordered external potential. We show that, for any strength of disorder, there is always a finite fraction of atoms localized at each site of the lattice. The localization length decreases as the disorder increases but remains macroscopic even when the disorder becomes very large compared to the interatomic interaction energy. This result holds true both in one dimension and higher dimensions.  In particular we find that the critical disorder above which all states are localized scales like 1/N in 1D and 1/d in 2D and 3D where d is the spatial dimension. Our results provide a microscopic understanding of recent experiments on ultracold atomic gases in optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases of bosons or fermions confined in periodic potentials [1] . These systems can be described theoretically using the framework of the Bose-Hubbard model [2] , which has been extensively studied over the past decade [3] . In this work we consider the case of a gas of interacting bosons in a disordered potential. Disorder leads to Anderson localization [4] : eigenstates become exponentially localized around their initial position if the disorder exceeds some threshold value [5] . It was recently shown experimentally [6] that such a system exhibits a transition between extended Bloch-like states and localized Wannier-Stark ladders [7, 8] . However, these experiments were performed only in the weak-disorder regime, i.e., when the disorder amplitude V0 is much smaller than the characteristic hopping matrix element J. Here we investigate how the presence of interactions affects the physics of strongly disordered systems.", "paraphrased_abstract": "But we have been able to make such systems by means of experimental techniques, as a result of our work in the form of a swarm of Bose-Einstein condensates, with a arbitrary number of particles confined in a harmonic oscillator and a disordered external potential. These are then modeled by the Bose-Hubbard model, which was developed over the last decade and whose basis was the theory of the Bose-Hubbard model, which has been studied extensively. We will now consider the case of a gas of interacting Bosons in a swarm of disordered potentials. Our study shows that, for any disorder, there is always a finite fraction of atoms at each site of the lattice, and this is true both in one dimension and in three dimensions. We show that for all states there is a finite fraction of atoms at each site, and that the concentration of atoms decreases as the disorder increases, but remains macroscopic even when the disturbance becomes so large compared to the energy of the interatomic forces. We present the theory of Bose-Hubbard, which has been thoroughly studied for a decade or more. This paper deals with the properties of the gas of interacting bosons in a cyclic potential. We have recently shown that", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.248, "title": "Recursive calculation of effective resistances in distance-regular networks based on Bose-Mesner algebra and Christoffel-Darboux identity", "abstract": "  Recently in \\cite{jss1}, the authors have given a method for calculation of the effective resistance (resistance distance) on distance-regular networks, where the calculation was based on stratification introduced in \\cite{js} and Stieltjes transform of the spectral distribution (Stieltjes function) associated with the network. Also, in Ref. \\cite{jss1} it has been shown that the resistance distances between a node $\\alpha$ and all nodes $\\beta$ belonging to the same stratum with respect to the $\\alpha$ ($R_{\\alpha\\beta^{(i)}}$, $\\beta$ belonging to the $i$-th stratum with respect to the $\\alpha$) are the same. In this work, an algorithm for recursive calculation of the resistance distances in an arbitrary distance-regular resistor network is provided, where the derivation of the algorithm is based on the Bose-Mesner algebra, stratification of the network, spectral techniques and Christoffel-Darboux identity. It is shown that the effective resistance on a distance-regular network is an strictly increasing function of the shortest path distance defined on the network. In the other words, the two-point resistance $R_{\\alpha\\beta^{(m+1)}}$ is strictly larger than $R_{\\alpha\\beta^{(m)}}$. The link between the resistance distance and random walks on distance-regular networks is discussed, where the average commute time (CT) and its square root (called Euclidean commute time (ECT)) as a distance are related to the effective resistance. Finally, for some important examples of finite distance- regular networks, the resistance distances are calculated.   {\\bf Keywords: resistance distance, association scheme, stratification, distance-regular networks, Christoffel-Darboux identity}   {\\bf PACs Index: 01.55.+b, 02.10.Yn} ", "machine_abstract": "We present an algorithm for calculating the effective resistance between any two nodes in a distance-regular network, which is based on the recursive application of the Christoffel-Darbousu formula to the corresponding adjacency matrix. The proposed method can be applied to both regular graphs with arbitrary degree as well as irregular graphs that are locally tree-like. We demonstrate our approach by applying it to several examples including the complete graph K_n{\\displaystyle K_{n}} , the hypercube Q_n{\\displaystyle Q_{n}} , and the Johnson graph J(n,k){\\displaystyle J(n,k).}  The concept of effective resistance has been widely used in physics, chemistry, biology, computer science, electrical engineering, and other fields. In particular, this quantity plays an important role in analyzing the performance of parallel computing systems such as multiprocessor computers or distributed-memory clusters.     Distance-regular graphs have received considerable attention over the past few decades due to their wide applications in various areas ranging from combinatorics to coding theory. These graphs possess many interesting properties; see [1] for more details. For example, they include all Cayley graphs [2] .     This work was supported by NSFC (No. 61271140), NNSF (No. 60903060), and MOST (No. 2011CB302400).     1 Introduction", "paraphrased_abstract": "I will be brief, but I am a man who is in the habit of putting out some kind of smear in order to be sure that it is not a sign. In addition, he will be a man who does not have a fetus, a bifocal equator, and a compass whose equator is a compass, a compass whose index is a spherical graph and a compass whose equator is a graph, the whole of which is a Cayley graph, the whole of which is a compass. This is a very important feature in the analysis of parallel computer systems, such as multi-processors or distributed memory systems. In our work we introduce an algorithm for calculating the effective resistance of a network, which is based on the Christoffel-Darbousu formula, recursively applied to the corresponding adjacency matrix. In our work we show how to apply it to a certain network of regular graphs, that is, a graph with arbitrary degrees of resolution and a graph of irregularity, such as a tree. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.027, "title": "Keck spectroscopy and Spitzer Space Telescope analysis of the outer disk of the Triangulum Spiral Galaxy M33", "abstract": "  In an earlier study of the spiral galaxy M33, we photometrically identified arcs or outer spiral arms of intermediate age (0.6 Gyr - 2 Gyr) carbon stars precisely at the commencement of the HI-warp. Stars in the arcs were unresolved, but were likely thermally-pulsing asymptotic giant branch carbon stars. Here we present Keck I spectroscopy of seven intrinsically bright and red target stars in the outer, northern arc in M33. The target stars have estimated visual magnitudes as faint as V \\sim 25 mag. Absorption bands of CN are seen in all seven spectra reported here, confirming their carbon star status. In addition, we present Keck II spectra of a small area 0.5 degree away from the centre of M33; the target stars there are also identified as carbon stars. We also study the non-stellar PAH dust morphology of M33 secured using IRAC on board the Spitzer Space Telescope. The Spitzer 8 micron image attests to a change of spiral phase at the start of the HI warp. The Keck spectra confirm that carbon stars may safely be identified on the basis of their red J-K_s colours in the outer, low metallicity disk of M33. We propose that the enhanced number of carbon stars in the outer arms are an indicator of recent star formation, fueled by gas accretion from the HI-warp reservoir. ", "machine_abstract": "We present new near-infrared (NIR) spectroscopic observations with Keck II/DEIMOS, which cover the entire optical extent of the nearby spiral galaxy M33 out to its last measured isophote at 25 mag arcsec-2 in B-band. We also use archival data obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our study. The main goal of this work was to investigate how star formation proceeds beyond the edge of galactic disks into the surrounding intergalactic medium. Our results show that there are two distinct components along the line-of-sight towards M33: an extended component associated with diffuse ionized gas and young stars; and a compact component dominated by old stellar populations. Using these NIR spectra we have derived radial profiles of several physical parameters such as electron density, temperature, extinction coefficient etc., across the face-on view of M33's disk. These profiles reveal interesting trends in the properties of interstellar matter within different regions of the galaxy.", "paraphrased_abstract": "The aim of this research was to determine the extent of the formation of stars in the outer space of galactic disks. We show that the two phases of formation are divided into two layers: the long one, which is composed of ionized gas and young stars, and the compact one, which is composed of old stars. These layers reveal important variations in the nature of the interstellar material. With the help of these NIR images, we have been able to study the relative radii of M33 in the radii of B. The radii show that there are two different dimensions, i.e., electron density, temperature, extinction coefficient, etc., in the face of M33. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1201, "title": "Asymmetric superconductivity in metallic systems", "abstract": "  Different types of superfluid ground states have been investigated in systems of two species of fermions with Fermi surfaces that do not match. This study is relevant for cold atomic systems, condensed matter physics and quark matter. In this paper we consider this problem in the case the fermionic quasi-particles can transmute into one another and only their total number is conserved. We use a BCS approximation to study superconductivity in two-band metallic systems with inter and intra-band interactions. Tuning the hybridization between the bands varies the mismatch of the Fermi surfaces and produces different instabilities. For inter-band attractive interactions we find a first order normal-superconductor and a homogeneous metastable phase with gapless excitations. In the case of intra-band interactions, the transition from the superconductor to the normal state as hybridization increases is continuous and associated with a quantum critical point. The case when both interactions are present is also considered. ", "machine_abstract": "The authors present the results of their study on the properties of two different types of systems, which are both characterized by an asymmetric distribution of charge carriers and exhibit unconventional superconducting behavior.  The first type is represented by layered compounds with alternating layers of conducting and insulating material (e.g., SrTiO3/SrRuO3/LaAlO3). In these materials, the presence of strong electron correlations leads to the formation of two-dimensional electronic states at the interface between the layers that can be described as a doped Mott insulator. These interfaces act like Josephson junctions and give rise to a spontaneous current perpendicular to the layers. This current causes a magnetic field to penetrate into the sample even when no external magnetic fields are applied. The second class consists of quasi-one-dimensional conductors such as carbon nanotubes or semiconductor nanowires. Here, the asymmetry arises because one end of the wire has been terminated by chemical functionalization.", "paraphrased_abstract": "It is a type of layered material composed of alternating layers of conducting and insulating materials (e.g., SiO2/SrRuO3/LaAlO3). The first type is a layered material composed of alternating layers of conducting and insulating material (e.g., SiO2/SrRuO3/LaAlO3). These alternating layers of conducting and insulating materials form two-dimensional electronic peaks at the interfaces, a sort of molecular insulator. Then, at the junctions, a spontaneous current is produced, and the current is directed, as is the case with the other layers, toward the sample, when no magnetic fields are applied. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3009, "title": "Gravitational waves from galaxy encounters", "abstract": "  We discuss the emission of gravitational radiation produced in encounters of dark matter galactic halos. To this aim we perform a number of numerical simulations of typical galaxy mergers, computing the associated gravitational radiation waveforms as well as the energy released in the processes. Our simulations yield dimensionless gravitational wave amplitudes of the order of $10^{-13}$ and gravitational wave frequencies of the order of $10^{-16}$ Hz, when the galaxies are located at a distance of 10 Mpc. These values are of the same order as those arising in the gravitational radiation originated by strong variations of the gravitational field in the early Universe, and therefore, such gravitational waves cannot be directly observed by ground-based detectors. We discuss the feasibility of an indirect detection by means of the B-mode polarization of the Cosmic Microwave Background (CMB) induced by such waves. Our results show that the gravitational waves from encounters of dark matter galactic halos leave much too small an imprint on the CMB polarization to be actually observed with ongoing and future missions. ", "machine_abstract": "The gravitational wave signal GW150914 detected by the LIGO and Virgo detectors on September 14, 2015 was produced during an encounter between two black holes with masses 36  - 29 solar masses (M\u2609) and 29 - 36 M\u2609 respectively.  The event is consistent with theoretical predictions for binary black hole mergers in general relativity.   This article describes how this merger occurred as well as its consequences for astrophysics. Gravitational waves are ripples in space-time that can be generated when massive objects accelerate or decelerate relative to each other. These waves carry energy away from their source at the speed of light and cause tiny changes in the distance between freely falling test particles. In addition to being emitted by merging black holes like those observed in GW150914, gravitational waves may also be created by colliding neutron stars, exploding supernovae, and spinning rapidly-rotating pulsars.", "paraphrased_abstract": "The gravitational waves are created in spacetime by collisions between the neutron stars, explosions of supernovae, and rapidly rotating pulsars. The gravitational waves are produced by collisions between two black holes with mass 36-36 solar masses, and by 39-36 M. The event corresponds to the theories of a quatrain merger in general relativity. Gravitational waves are waves that spread out from a massive object at the speed of light, and they amplify the distance between the free falling particles. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0676, "title": "A very massive runaway star from Cygnus OB2", "abstract": "  Aims: We analyze the available information on the star BD+43 3654 to investigate the possibility that it may have had its origin in the massive OB association Cygnus OB2.   Methods: We present new spectroscopic observations allowing a reliable spectral classification of the star, and discuss existing MSX observations of its associated bow shock and astrometric information not previously studied.   Results: Our observations reveal that BD+43 3654 is a very early and luminous star of spectral type O4If, with an estimated mass of (70 +/- 15) solar masses and an age of about 1.6 Myr. The high spatial resolution of the MSX observations allows us to determine its direction of motion in the plane of the sky by means of the symmetry axis of the well-defined bow shock, which matches well the orientation expected from the proper motion. Tracing back its path across the sky we find that BD+43 3654 was located near the central, densest region of Cygnus OB2 at a time in the past similar to its estimated age.   Conclusions: BD+43 3654 turns out to be one of the three most massive runaway stars known, and it most likely formed in the central region of Cygnus OB2. A runaway formation mechanism by means of dynamical ejection is consistent with our results. ", "machine_abstract": "We report the discovery of an extremely bright and hot (T eff = 300,000 K) blue supergiant in the open cluster NGC 6231 with a mass loss rate of 10^-6 M_sun/yr. The object is located at a distance of 1 kpc from Earth and has a luminosity of 5 x 10^{5} L_Sun. It shows strong emission lines of He II 4686 \u00c5, N III 4641 \u00c5, C IV 5801 \u00c5, O V 7322 \u00c5, and H Balmer series. We suggest that this object may be a member of the Cygnus OB2 association which contains several other high-mass stars. This would make it one of the most luminous known single stars outside our Galaxy. If confirmed by further observations, this object will provide important constraints on stellar evolution models for massive stars. Keywords: Open clusters; Blue supergiants", "paraphrased_abstract": "\"It is a star in the open cluster NGC 6231, which is at a distance of about a kpc from the Earth and has a luminosity of five x five x 10 5. It is known that this star is a member of the Cygnus OB2 group, which contains many high-mass stars. The object is very bright and hot (T = 30,000 K) blue supergiant, a star of a mass of about ten million l. It has an emission of He II 4686, N III 4641, C IV 5801, O IV 7322, and H Balmer. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1866, "title": "On global solution to the Klein-Gordon-Hartree equation below energy space", "abstract": "  In this paper, we consider the Cauchy problem for Klein-Gordon equation with a cubic convolution nonlinearity in $\\R^3$. By making use of Bourgain's method in conjunction with a precise Strichartz estimate of S.Klainerman and D.Tataru, we establish the $H^s (s<1)$ global well-posedness of the Cauchy problem for the cubic convolution defocusing Klein-Gordon-Hartree equation. Before arriving at the previously discussed conclusion, we obtain global solution for this non-scaling equation with small initial data in $H^{s_0}\\times H^{s_0-1}$ where $s_0=\\frac\\gamma 6$ but not $\\frac\\gamma2-1$, for this equation that we consider is a subconformal equation in some sense. In doing so a number of nonlinear prior estimates are already established by using Bony's decomposition, flexibility of Klein-Gordon admissible pairs which are slightly different from that of wave equation and a commutator estimate. We establish this commutator estimate by exploiting cancellation property and utilizing Coifman and Meyer multilinear multiplier theorem. As far as we know, it seems that this is the first result on low regularity for this Klein-Gordon-Hartree equation. ", "machine_abstract": "We consider the following nonlinear Schr\u00f6dinger-Klein-Gordon system in three spatial dimensions with an attractive interaction potential and a repulsive self-interaction potential, i$\\partial_t$u(x,t)=H0(u)(x,t), x\u2208R3, where H0 is given by  $$H_0(u)(x, t) = \\frac{1}{2m} \\Delta u + Vext(x)u - \\frac{1}{\\sigma} |u|^2 u,$$ with $m>0$ being the mass parameter, $\\sigma > 0$ the coupling constant between the two fields $u$ and $v$, $Vext\\in C^{1}$ satisfying $-\u0394Vext<0$ on $\\mathbb{R}^3$, and $|u|^2$ denotes the usual norm-square function for complex-valued functions defined on $\\mathbb{R}^3$.     We prove that if the initial data $(u(0),v(0))$ belongs to $H^1_0$ \u00d7 $L^2$ then there exists $t_\u2217>0$ such that the unique local-in-time solution $(u(t), v(t))$ satisfies         where $C>0$ depends only on $m$, $\\sigma$, and $V$ but not on $(u(0) , v (0) )$ or $t$.", "paraphrased_abstract": "We prove that the first data of 'U' or 'V' is not a \"H' or 'L', and that there is 't' and 't' in 'U' and 'v', and there is 't'. We show that the unique local-in-time solution of 'U' and 'v' satisfies if 'C' equals 'V', which is the usual normal square function for complex-valued functions of 'U'.'The inverse of 'Standard' is ''H'0'; 'x', 'x', 'u';' with 'Meadow u', 'Sigma  u';' where 'C' satisfies 'V' satisfies' on 'R', 'u' satisfies', where 'C' satisfy's 'C', where 'C' satisfying 'C' in 'C' satisfying 'C' in 'C' on 'R'. 'U', 'U', 'U', 'u', 'u' '.'' ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3557, "title": "Optical investigations of quantum-dot spin dynamics", "abstract": "  We have performed all-optical measurements of spin relaxation in single self-assembled InAs/GaAs quantum dots (QD) as a function of static external electric and magnetic fields. To study QD spin dynamics we measure the degree of resonant absorption which results from a competition between optical spin pumping induced by the resonant laser field and spin relaxation induced by reservoirs. Fundamental interactions that determine spin dynamics in QDs are hyperfine coupling to QD nuclear spin ensembles, spin-phonon coupling and exchange-type interactions with a nearby Fermi sea of electrons. We show that the strength of spin relaxation generated by the three fundamental interactions can be changed by up to five orders of magnitude upon varying the applied electric and magnetic fields. We find that the strength of optical spin pumping that we use to study the spin relaxation is determined predominantly by hyperfine-induced mixing of single-electron spin states at low magnetic fields and heavy-light hole mixing at high magnetic fields. Our measurements allow us to determine the rms value of the hyperfine (Overhauser) field to be ~15 mTesla with an electron g-factor of g_e=0.6 and a hole mixing strength of |epsilon|^2 = 0.0005. ", "machine_abstract": "We report on the optical investigation of single self-assembled InAs/GaAs quantum dots (QDs) in an external magnetic field applied along their growth direction. The QD emission line splits into two components with opposite circular polarization when the magnetic field is increased to about 1 T, which corresponds to the Zeeman splitting energy of 0.5 meV at 4 K. We observe that this splitting increases linearly as temperature decreases down to 20 mK and then saturates below 10 mK. This behavior can be explained by taking into account both electron-hole exchange interaction and phonon-assisted relaxation processes between different excitonic states within QDs. Our results show that the spin-flip time for electrons confined inside QDs is longer than 100 ns even under high magnetic fields up to 5 T. Quantum dot (QD), also known as semiconductor nanocrystal or artificial atom, has attracted much attention due to its unique physical properties such as size-tunable band gap [1] , strong confinement effect [2] , and large oscillator strength [3] . These features make it possible to use QDs as building blocks for various optoelectronic devices including light-emitting diodes [4] , lasers [5] , solar cells [6] , photodetectors [7] , and so forth [8] . In recent years, there have been many efforts devoted to investigating the spin dynamics of carriers confined in QDs [9] - [11] . It was found that the carrier spins are very stable against decoherence caused by environmental noise [12] - [14] . However, the spin flip times were reported to vary widely depending on experimental conditions [15] - [17] . For example, the spin lifetimes of holes [18] and electrons [19] confined in QDs were measured to be several nanoseconds using pulsed excitation techniques. On the other hand, the spin lifetime of electrons [20] and holes [21] confined in QDs could reach microsecond level if continuous wave laser was used instead.", "paraphrased_abstract": "QDs are the electronic components for light-emitting diodes, lasers, solar cells, photodetectors, and so on. During the last few years, a great deal of research has been done on the spin dynamics of QDs, especially in the presence of noise, especially when they are bound by the wind and the atoms, so that they are used for a multitude of devices, from the diodes to the light-emitting diodes, to the lasers, to the solar cells, to the photodetectors, to the photodetectors, and so on. QDs are often called semiconductor crystals or artificial atoms, and have been particularly studied for their unique properties, such as the size and the confinement, the strong tin-nuclearity, the strong resonance, the strong resonant, and the strong oscillator. These properties, as a result of the unique physical properties, have been adopted as the basis of various optoelectronic devices, such as light-emitting diodes, lasers, solar cells, and so on. We have already studied the spin dynamics of the QDs with a pulsed excitation technique. We found that the spin dynamics of the QDs was about a few nanoseconds, using pulsed excitation, but a microsecond", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2413, "title": "Radio Spectral Evolution of an X-ray Poor Impulsive Solar Flare: Implications for Plasma Heating and Electron Acceleration", "abstract": "  We present radio and X-ray observations of an impulsive solar flare that was moderately intense in microwaves, yet showed very meager EUV and X-ray emission. The flare occurred on 2001 Oct 24 and was well-observed at radio wavelengths by the Nobeyama Radioheliograph (NoRH), the Nobeyama Radio Polarimeters (NoRP), and by the Owens Valley Solar Array (OVSA). It was also observed in EUV and X-ray wavelength bands by the TRACE, GOES, and Yohkoh satellites. We find that the impulsive onset of the radio emission is progressively delayed with increasing frequency relative to the onset of hard X-ray emission. In contrast, the time of flux density maximum is progressively delayed with decreasing frequency. The decay phase is independent of radio frequency. The simple source morphology and the excellent spectral coverage at radio wavelengths allowed us to employ a nonlinear chi-squared minimization scheme to fit the time series of radio spectra to a source model that accounts for the observed radio emission in terms of gyrosynchrotron radiation from MeV-energy electrons in a relatively dense thermal plasma. We discuss plasma heating and electron acceleration in view of the parametric trends implied by the model fitting. We suggest that stochastic acceleration likely plays a role in accelerating the radio-emitting electrons. ", "machine_abstract": "We report on the radio spectral evolution during a solar flare that was observed by RHESSI in 2002 February 18, which is one of the most energetic impulsive solar flares ever recorded with high energy electrons up to ~100 MeV. The flare started at about 17:45 UT as a GOES class M5.7 event peaking around 18:10 UT. It showed two peaks in its hard X-ray light curve (HXR); the first peak lasted only 3 minutes while the second peak lasted more than 10 minutes. We found that there were three distinct phases in the radio spectrum evolution: pre-flare phase, impulsive phase, and decay phase. In addition, we also found that the radio emission had a clear correlation between the HXR fluxes and microwave flux densities. During the impulsive phase, the radio emission increased rapidly and then decayed slowly after the end of the impulsive phase.", "paraphrased_abstract": "The first peak, of which the first lasted only three minutes, was longer than the second. The first peak lasted only a few minutes, and the second lasted more than ten minutes. We also showed that the radio spectrum is very closely correlated with the temperature of the microwave. The first and second peaks are the most intense. They are mainly found in the middle of the X-ray. The second peak lasts for three minutes, and the third lasts for ten minutes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0981, "title": "Can We Detect the Anisotropic Shapes of Quasar HII Regions During Reionization Through The Small-Scale Redshifted 21cm Power Spectrum?", "abstract": "  Light travel time delays distort the apparent shapes of HII regions surrounding bright quasars during early stages of cosmic reionization. Individual HII regions may remain undetectable in forthcoming redshifted 21 cm experiments. However, the systematic deformation along the line of sight may be detectable statistically, either by stacking tomographic 21cm images of quasars identified, for example, by JWST, or as small-scale anisotropy in the three-dimensional 21cm power spectrum. Here we consider the detectability of this effect. The anisotropy is largest when HII regions are large and expand rapidly, and we find that if bright quasars contributed to the early stages of reionization, then they can produce significant anisotropy, on scales comparable to the typical sizes of HII regions of the bright quasars (approx. 30 Mpc and below). The effect therefore cannot be ignored when analyzing future 21cm power spectra on small scales. If 10 percent of the volume of the IGM at redshift z=10 is ionized by quasars with typical ionizing luminosity of S= 5 x 10^{56} photons/second, the distortions can enhance by more than 10 percent the 21cm power spectrum in the radial (redshift) direction, relative to the transverse directions. The level of this anisotropy exceeds that due to redshift-space distortion, and has the opposite sign. We show that on-going experiments such as MWA should be able to detect this effect. A detection would reveal the presence of bright quasars, and shed light on the ionizing yield and age of the ionizing sources, and the distribution and small-scale clumping of neutral intergalactic gas in their vicinity. ", "machine_abstract": "We investigate whether we can detect anisotropy in quasar H II regions during reionization through their small-scale redshifted 21 cm power spectrum (21-cm PS). In our model, quasars are assumed to be located at peaks of dark matter density fluctuations and ionize surrounding gas with an anisotropic Str\u00f6mgren sphere whose shape is determined by the local tidal field. By performing numerical simulations for different values of the spin temperature T S , we find that the 21-cm PS has a characteristic peak structure which reflects the shapes of individual H II regions. This peak structure becomes more prominent as T S decreases because the number of neutral hydrogen atoms increases due to the decrease in the brightness temperature difference between the CMB and the 21-cm emission line. Our results suggest that it may be possible to use this peak structure to constrain the value of T S . However, since there exist many other factors affecting the 21-cm PS besides T S , further studies will be needed before drawing any conclusions on its detectability.", "paraphrased_abstract": "Then we applied the same principle to the 21 cm PS in a numerical simulation. The results showed that the 21 cm PS had a characteristic peak structure, and that it resembled individual H II regions in a way that reflected the individual shapes of the individual H II regions. But we needed to investigate the other factors which influenced the 21 cm PS, which, we found, are not the only factors that influence the 21 cm PS. Therefore, it is not clear whether the 21 cm PS can be detected by reionization. The model assumes that the quasars are located at the heights of the density and the surrounding gas is ionized with the anisotropic Str\u00f6mgren sphere, whose shape is determined by the local tidal field. In this study, we simulated the structure of the 21 cm PS in the presence of various temperatures. The structure of the 21 cm PS became more noticeable as the temperature of the sun fell, and the number of neutral hydrogen atoms increased in proportion to the brightness of the '21 cm' line. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1939, "title": "Towards Informative Statistical Flow Inversion", "abstract": "  A problem which has recently attracted research attention is that of estimating the distribution of flow sizes in internet traffic. On high traffic links it is sometimes impossible to record every packet. Researchers have approached the problem of estimating flow lengths from sampled packet data in two separate ways. Firstly, different sampling methodologies can be tried to more accurately measure the desired system parameters. One such method is the sample-and-hold method where, if a packet is sampled, all subsequent packets in that flow are sampled. Secondly, statistical methods can be used to ``invert'' the sampled data and produce an estimate of flow lengths from a sample.   In this paper we propose, implement and test two variants on the sample-and-hold method. In addition we show how the sample-and-hold method can be inverted to get an estimation of the genuine distribution of flow sizes. Experiments are carried out on real network traces to compare standard packet sampling with three variants of sample-and-hold. The methods are compared for their ability to reconstruct the genuine distribution of flow sizes in the traffic. ", "machine_abstract": "We present an algorithm for the statistical flow inversion problem that is based on variational Bayesian inference and uses Gaussian processes to model the forward operator. The proposed method allows us to obtain posterior distributions over the unknown source parameters, which can be used as prior information in subsequent inverse problems or uncertainty quantification studies. We demonstrate our approach by applying it to two different test cases with synthetic data generated using the finite element method (FEM). Our results show that we are able to recover the true source distribution accurately even if only few measurements are available. Furthermore, we compare our method against state-of-the-art algorithms and find that our approach outperforms them significantly when the number of measurement locations is small. Finally, we apply our method to real-world data obtained during a field experiment at the University of Houston's Space Research Center. This dataset consists of temperature measurements taken inside a building after a fire was started near one of its walls.", "paraphrased_abstract": "When the data is in the finite element (FEM) domain, the calculation of the distribution is much more accurate than the recurrence of the measurement location. In this paper, we introduce an inversion-based approach to the statistical flow problem, which is based on the Bayesian Bayesian inference and on Gaussian processes. We show our method in two different cases, which were generated with a numerical method, with the synthesis of two different data. The test cases we consider are a case study of the inversion problem and an experiment conducted by the Space Research Center of the University of Houston. The data we take is from a fire which had been set near one of the walls of the building. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2985, "title": "Electronic structure of BaFeO3: an abinitio DFT study", "abstract": "  First principles calculations were performed to study the ground state electronic properties of BaFeO3 (BFO) within the density functional theory (DFT). Adopting generalized gradient approximation (GGA) exchange and correlation functional and Vosko-Wilk-Nusair correlation energy functional interpolation, we have systematically conducted the band structure, density of states and electronic distribution along different crystalline planes. Calculating results show that band gap in the majority spin band structure and band gap in the minority spin band structure were found to be 2.7012 eV and 0.6867 eV respectively. Up-spin Fe t2g were fully occupied and down-spin Fe eg were empty. Moreover, the up-spin Fe eg and down-spin Fe t2g were partially occupied near the Fermi energy, leading to a finite density of states. The Fe4+-O-Fe4+ plane superexchange coupling should rearrange the magnetic order to make the ferromagnetic characteristic being possible, moreover the tetragonal displacement along the c axis could induce the perovskites materials to acquire ferroelectric property. These reasons could lead to the fact that the tetragonal phase BFO could be a potential multiferroics while it was produced under the very experimental conditions. The charge density along different crystalline planes were illustrated to show that strong covalent bonding between O and Fe can be used to investigate the exchange coupling, and this strong hybridization may further increase the superexchange coupling to enhance the magnetic ordering. ", "machine_abstract": "BaFeO3 is one of the most promising cathode materials for solid oxide fuel cells (SOFCs). In this work, we have performed first-principles density functional theory calculations to investigate its electronic and magnetic properties in detail. We find that it has a metallic ground state with a large spin polarization at the Fermi level. The calculated band gap between the valence bands and conduction bands is 0.2 eV which agrees well with previous experimental results. Our calculation shows that there are two Fe-3d orbitals crossing the Fermi level, indicating that both Fe3+ ions and Fe4+ ions exist in the compound simultaneously. This result can be explained by the fact that the crystal field splitting energy is much smaller than the electron-electron interaction energies. Furthermore, our calculation indicates that the ferromagnetic ordering is mainly due to superexchange interactions mediated through oxygen atoms.  Finally, we also calculate the phonon dispersion relations along high symmetry lines within the framework of density functional perturbation theory.", "paraphrased_abstract": "The calculation of the phonon fluxes at the high-symmetry axis of the formula is given. Besides, we substantiate the ferromagnetic order, by a complex of ferromagnetic exchanges mediated by oxygen atoms. This result was given by the fact that the polarization of the atoms at the Fermi level was considerably larger than the electron-electron energy. In addition, the calculated band gap between the valence and conduction bands was a mere 0.2 eV, in accordance with the results obtained from the experimental experiment. BaFeO3 was considered to be a promising conductive material for SOFCs. The conductive material was fabricated in Bao3 and is a metallic, metallurgy-metallic material, with a very large spin at the Fermi level. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0563, "title": "The Spitzer c2d Survey of Weak-line T Tauri Stars II: New Constraints on the Timescale for Planet Building", "abstract": "  One of the central goals of the Spitzer Legacy Project ``From Molecular Cores to Planet-forming Disks'' (c2d) is to determine the frequency of remnant circumstellar disks around weak-line T Tauri stars (wTTs) and to study the properties and evolutionary status of these disks. Here we present a census of disks for a sample of over 230 spectroscopically identified wTTs located in the c2d IRAC (3.6, 4.5, 4.8, and 8.0 um) and MIPS (24 um) maps of the Ophiuchus, Lupus, and Perseus Molecular Clouds. We find that ~20% of the wTTs in a magnitude limited subsample have noticeable IR-excesses at IRAC wavelengths indicating the presence of a circumstellar disk. The disk frequencies we find in these 3 regions are ~3-6 times larger than that recently found for a sample of 83 relatively isolated wTTs located, for the most part, outside the highest extinction regions covered by the c2d IRAC and MIPS maps. The disk fractions we find are more consistent with those obtained in recent Spitzer studies of wTTs in young clusters such as IC 348 and Tr 37. From their location in the H-R diagram, we find that, in our sample, the wTTs with excesses are among the younger part of the age distribution. Still, up to ~50% of the apparently youngest stars in the sample show no evidence of IR excess, suggesting that the circumstellar disks of a sizable fraction of pre-main-sequence stars dissipate in a timescale of ~1 Myr. We also find that none of the stars in our sample apparently older than ~10 Myrs have detectable circumstellar disks at wavelengths < 24 um. Also, we find that the wTTs disks in our sample exhibit a wide range of properties (SED morphology, inner radius, L_DISK/L*, etc) which bridge the gaps observed between the cTTs and the debris disk regimes. ", "machine_abstract": "We present new constraints on planet formation timescales by combining infrared photometry and spectroscopy with optical data to study the circumstellar environments of young stars in nearby open clusters. We find that most weak-lined T Tauri stars (WTTS) have disks, but only half as many show evidence for accretion onto their central star. The fraction of WTTSs showing signs of accretion decreases rapidly between ages of 1 Myr and 3 Myr. This suggests that disk dispersal is an important process during this period. In addition, we find that the distribution of dust temperatures in these systems evolves significantly over time. These results are consistent with models where planets form quickly after the dissipation of the protoplanetary disk. However, our sample size is small and additional observations will be needed before firm conclusions can be drawn about how planet formation proceeds.     Keywords: Open cluster, Circumstellar disk", "paraphrased_abstract": "It is necessary to take into account the present circumstances, which, however, do not allow for the production of planets, and therefore, we propose to apply these principles to the formation of planets, as they are in reality. Using the methods of infrared spectroscopy and spectroscopic observation, we are able to investigate the conditions of young stars in the vicinity of the open clusters. We have identified the most weak-lined T-Tauris (WTTs) with disks, but only half the WTTs have evidence of accretion, which indicates that dispersal of the disks is an important process during this period. In addition, we find that the dust temperature of the region of dust grows significantly in the region of dust. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3422, "title": "Equivalence principle and electromagnetic field: no birefringence, no dilaton, and no axion", "abstract": "  The coupling of the electromagnetic field to gravity is discussed. In the premetric axiomatic approach based on the experimentally well established conservation laws of electric charge and magnetic flux, the Maxwell equations are the same irrespective of the presence or absence of gravity. In this sense, one can say that the charge \"substratum\" and the flux \"substratum\" are not influenced by the gravitational field directly. However, the interrelation between these fundamental substrata, formalized as the {\\it spacetime relation} H=H(F) between the 2-forms of the electromagnetic excitation H and the electromagnetic field strength F, is affected by gravity. Thus the validity of the equivalence principle for electromagnetism depends on the form of the spacetime relation. We discuss the nonlocal and local linear constitutive relations and demonstrate that the spacetime metric can be accompanied also by skewon, dilaton, and axion fields. All these premetric companions of the metric may eventually lead to a violation of the equivalence principle. ", "machine_abstract": "We show that the equivalence principle (EP) is violated in quantum gravity if there are dilatons or axions coupled to photons. The violation can be tested by measuring the polarization rotation angle of light passing through gravitational fields. We find that the EP-violating effect is suppressed for macroscopic distances but enhanced for microscopic ones such as those inside atoms. This suggests that the measurement of atomic clocks may provide an opportunity to test the EP at high precision.     In this work we study how the equivalence principle (EP), which states that all local physical laws should take their same form in any freely falling frame, is modified when one considers quantum gravity effects. It has been shown previously that the EP is violated in general relativity with massive gravitons [1] . Here we consider whether it remains valid in theories beyond Einstein's theory where new degrees of freedom exist. Specifically, we focus on two types of models: scalar-tensor theories [2] , including Brans-Dicke theory [3] , and string-theory inspired models [4] .   In these theories, dilatons and/or axions appear as additional degrees of freedom besides graviton(s). Dilatons couple directly to photons while axions do so indirectly via coupling to photons and gluons [5] . These couplings lead to violations of the EP [6] . For example, in scalar-tensor theories, the photon acquires a mass term proportional to the strength of the gravitational field [7, 8] . As a result, the speed of light depends on its direction relative to the gravitational field [9] . If the gravitational field varies along the path of propagation, then the speed of light also changes accordingly [10] . Since different polarizations travel at slightly different speeds, they acquire different phases during propagation [11] . Therefore, the polarization state of light will rotate after traveling through a gravitational potential gradient [12] .", "paraphrased_abstract": "But the light irradiation is only limited by the direction of the gravitational field, and the light is therefore refractory to the gravity field. The equivalence principle, which states that all physical laws are the same in a free falling frame, is modified in the case of quantum gravity. This work considers the equivalence principle, which states that all local laws should be the same in a free falling frame, and, consequently, changes in quantum gravity. The equivalence principle, which is the statement that all local laws should be the same in a free falling frame, is in violation of the equivalence principle in general relativity with huge gravitons. We have already shown that the equivalence principle is in violation of general relativity with massive gravitons. In scalar-tensor theories the photon acquires a mass proportional to the strength of the gravitational field, and so the speed of light changes accordingly. For example, in the scalar-tensor theory the mass of the photon is proportional to the strength of the gravitational field. Hence, the polarization of light is altered by a change in the gravitational potential. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1686, "title": "Forced accretion in stochastically fed AGN and quasars", "abstract": "  Steady state accretion discs larger than ~ 0.01-0.1 pc are known to be gravitationally unstable for the accretion rates needed to explain super-massive black hole (SMBH) activity. We propose that SMBH are fed by a succession of mass deposition events with randomly directed angular momenta. Because of incomplete angular momentum cancellation a warped accretion disc forms in the inner few parsec. The orientation of the disc performs a random walk. Deposition of new material promotes SMBH accretion at rates much faster than viscous. Observational implications of this picture include: (i) lighter accretion discs that can fuel AGN and quasars and yet avoid star formation at R >> 0.1 pc; (ii) star formation inside the disc is not a function of mass accretion rate only. It can take place at high or low accretion rates, e.g., when too few clouds arrive in the inner region. An example of this might be the central parsec of our Galaxy. (iii) The discs can form Compton-thick obscuring structures of ~ parsec size as required in AGN unification models; (iv) faster black hole growth resulting from misalignment of the disc and the black hole spin in the early Universe; (v) Isotropic deposition of SMBH energy and momentum feedback in the galaxy bulge. This may help explain the high efficiency with which it seems to be operating in the Universe. (vi) No correlation between SMBH activity and the presence of kiloparsec scale bars or gaseous discs in galactic bulges; (vii) Bodily collisions between gaseous components of merging galaxies facilitate production of gas streams feeding the centre of the combined galaxy. Mergers should thus be catalysts of SMBH growth. (viii) Conversely, galaxies experiencing fewer mergers are more likely to form massive nuclear star clusters than feed their SMBHs. ", "machine_abstract": "We present the results of cosmological simulations that follow the growth of supermassive black holes (SMBHs) by stochastic gas inflow, including radiative feedback effects on their surroundings. We find that SMBHs grow primarily through mergers with other BHs rather than gas accretion at high redshifts z > 6. At lower redshift we observe an increase in the fraction of mass gained via gas accretion relative to merger events. The resulting luminosity function is consistent with observations for both active galactic nuclei (AGNs) and quasars out to z = 7.5. Our model predicts a large number of low-luminosity AGNs which are not observed yet but may be detectable with future surveys such as LSST or Euclid. In addition, our model produces a population of obscured quasars whose properties agree well with recent observational constraints. Finally, we show that the predicted quasar lifetime distribution agrees very well with current estimates based on SDSS data.", "paraphrased_abstract": "The corresponding luminosity is well consistent with previous measurements of the quasar. Our model predicts a great many low-luminosity AGNs that are not yet detected but might be detected by future surveys, such as LSST or Euclid. The cosmological models analyzed here, which were run in the space of the galaxy, show that SMBHs grow chiefly by a merger of other BHs, rather than by the action of gas accretion. The higher the redshift, the more the growth of gas accretion. At higher redshifts, the growth of gas accretion is accompanied by a decrease in the mass gained by merger. At lower redshifts, the proportion of mass gained by gas accretion increases in proportion to the mass gained by merger. We present an analysis of the cosmological simulations of the formation of supermassive black holes (SMBHs) by stochastic gas inflow, with radiative feedbacks from the surrounding environment. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1799, "title": "Super Low Traction under EHD and Mixed Lubrication Regimes", "abstract": "  After the pioneered experimental works on superlubricity by Martin et al. on MoS2 [1], Hirano et al. on tungsten and silicon [2] and the further confirmation by Dienwiebel et al. on graphite [3], many groups around the word investigated the occurrence of near frictionless sliding contacts. This large mobilization of tribologists, material sciences specialists and physicists has lead to emerging solutions involving new materials and coatings, the most promising being carbon based like graphite, diamond, carbon composites or diamond-like-carbons. Some of them are currently used in practical applications. The situation is different especially in EHL: the highest friction coefficients are close to 10% when traction fluids are involved, i.e. fluids that have especially designed to transmit the highest friction, and they vary within 3-6% for the rest of lubricants. The range of variation is consequently very narrow and these typical values are really low compared to those obtained in dry contacts: as a consequence the gain expected from a super low traction regime in lubrication will be probably more limited, especially in the case of experiments conducted at the meso or macro scales. This weak perspective could be one explanation on the relatively low number of articles in recent literature dealing with lubricated superlubricity in the above conditions. ", "machine_abstract": "The effect of the lubricant viscosity on friction in elastohydrodynamic (EHD) regime is investigated by using numerical simulations based on Reynolds equation coupled with elasticity equations for an axisymmetric slider bearing system. The results show that, at low sliding speeds, the traction decreases as the lubricant viscosity increases due to the increase of pressure gradient along the film thickness direction. At high sliding speeds, however, the opposite trend occurs because the viscous shear stress becomes dominant over the pressure gradient term. In mixed lubrication regimes where both hydrodynamic and boundary layers exist simultaneously, it was found that the minimum traction can be reduced significantly when the lubricant viscosity is increased. This reduction is attributed mainly to the decrease of the maximum pressure in the contact region.     Keywords: Elastohydrodynamics; Slider Bearing; Minimum Film Thickness; Viscosity; Pressure Gradient Term", "paraphrased_abstract": "And the maximum pressure in the contact region is reduced by the decreasing of the lubricant viscosity. The study of the effect of the viscosity on the friction in the elastohydrodynamic (EHD) regime was carried out by means of numerical simulations on the Reynolds equation, and the elasticity equation. In the low-sliding speed range the friction decreases as the viscosity increases, and the pressure gradient is increased. The results show that the traction decreases with the increase of the viscosity, and that the viscous shear strain dominates the term pressure gradient. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0969, "title": "An Optimal Algorithm to Generate Pointed Trivalent Diagrams and Pointed Triangular Maps", "abstract": "  A trivalent diagram is a connected, two-colored bipartite graph (parallel edges allowed but not loops) such that every black vertex is of degree 1 or 3 and every white vertex is of degree 1 or 2, with a cyclic order imposed on every set of edges incident to to a same vertex. A rooted trivalent diagram is a trivalent diagram with a distinguished edge, its root. We shall describe and analyze an algorithm giving an exhaustive list of rooted trivalent diagrams of a given size (number of edges), the list being non-redundant in that no two diagrams of the list are isomorphic. The algorithm will be shown to have optimal performance in that the time necessary to generate a diagram will be seen to be bounded in the amortized sense, the bound being independent of the size of the diagrams. That's what we call the CAT property. One objective of the paper is to provide a reusable theoretical framework for algorithms generating exhaustive lists of complex combinatorial structures with attention paid to the case of unlabeled structures and to those generators having the CAT property. ", "machine_abstract": "We present an optimal algorithm for generating pointed trivalent diagrams, which are combinatorial objects that encode the topology of 3-manifolds. The algorithm is based on a new concept called \"triangulation by flipping\", in which we flip edges of a triangulated surface until it becomes minimal with respect to some criterion. We show how this can be used to generate all possible pointed trivalent diagrams associated to any given triangulated surface. As applications, we give algorithms for computing the Heegaard genus of closed orientable 3-manifolds and for enumerating all prime factorizations of integers into primes less than or equal to n{\\displaystyle n}. In addition, we provide an efficient method for counting the number of pointed triangular maps (a subclass of pointed trivalent diagrams) up to isotopy. This work was supported by NSF grant DMS-0805040. 1 Introduction Let M be a compact connected oriented 3-manifold whose boundary consists of tori. A Heegaard splitting of M is a decomposition of M into two handlebodies V and W such that their common boundary is homeomorphic to M. It follows immediately from the definition that every closed orientable 3-manfiold admits at least one Heegaard splitting. However, there exist manifolds that admit more than one Heegaard splitting; these manifolds are said to have multiple Heegaard splittings. For example, if S is a sphere with three holes then S\u00d7S has four different Heegaard splitttings [O] .", "paraphrased_abstract": ", n., n., n., n., and n. We describe an algorithm for the calculation of the Heegaard genus of closed orientable 3-manifolds, a genus of orientable 3-manifolds containing a finite number of arithmetic objects and a genus of arithmetic objects of a dimension of three, which we call the Heegaard genus. It is possible to find the Heegaard genus of orientable 3-manifolds, and to count the number of arithmetic elements as low as ndisplaystyle n. We introduce a method for the creation of a triangular tessellation, that is, a collection of triangular tessellations of three-dimensional objects. We introduce the concept of triangulation, which is to say, a re-decomposition of the surfaces of a triangular tessellation, so that, for example, S has four different tessellations of Heegaard. In the course of this, we show how the method can be employed to generate all possible triangular tessellations connected with a triangular surface. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0672, "title": "Waves and instabilities in dissipative rotating superfluid neutron stars", "abstract": "  We discuss wave propagation in rotating superfluid neutron star cores, taking into account the vortex mediated mutual friction force. For models where the two fluids co-rotate in the unperturbed state, our analysis clarifies the role of chemical coupling and entrainment for sound and inertial waves. We also investigate the mutual friction damping, providing results that demonstrate the well-known fact that sound waves propagating along a vortex array are undamped. We show that the same is not true for inertial waves, which are damped by the mutual friction regardless of the propagation direction. We then include the vortex tension, which arises due to local vortex curvature. Focussing on purely transverse inertial waves, we derive the small correction that the tension induces in the wave frequency. Finally, we allow for a relative linear flow in the background (along the rotation axis). In this case we show how the mutual friction coupling may induce a dynamical instability in the inertial waves. We discuss the critical flow required for the instability to be present, its physical interpretation and the possible relevance it may have for neutron star physics. ", "machine_abstract": "We study the dynamics of superfluids with rotation, viscosity and mutual friction using numerical simulations based on the two-fluid model for superfluids developed by Tsubota et al. (Phys Rev Lett 106, 053001 (2011)). We show that the system is unstable to axisymmetric perturbations when the angular velocity exceeds some critical value $\\Omega_c$. The instability leads to the formation of vortex rings which are then advected outward by the flow. In addition we find that there exists another type of non-axisymmetric instability at higher values of $\\Omega_c$. This instability results in the generation of spiral waves which can be seen as an analogue of Rossby waves observed in planetary atmospheres. Finally, we discuss how these results may affect the spin evolution of young pulsars. Superfluidity plays an important role in many astrophysical phenomena such as the cooling of white dwarfs or the structure of neutron stars. It has been shown recently that superfluid turbulence could explain the rapid spindown of young pulsars [1] . However, despite its importance, our understanding of superfluid hydrodynamics remains incomplete due to the lack of direct experimental observations.", "paraphrased_abstract": "Superfluidity plays an important role in many astronomical phenomena, including the cooling of white dwarfs or the formation of neutron stars. We have recently shown that superfluid turbulence may explain the rapid spin of young pulsars. Moreover, we have found another type of instability at higher values of Omega; the spiral waves, which are akin to Rossby waves, are a type of wave, which we call Rossby waves in planetary atmospheres. Besides, we have observed a non-axisymmetric instability at higher values of Omega; this non-axisymmetric instability is called spiral waves, and they form a type of Rossby waves in planetary atmospheres. This work, published in the journal of the International Association for Science, has a rich literature, and is also referred to in the literature as \u201cSuperfluid Hydrodynamics\u201d, and the authors have drawn up a mathematical model, in which the two fluids are rotated, the viscosity is proportional to the pressure and the friction is mutual, so that the system is unstable at an axisymmetrical moment, and the velocity is so high that the flow is ejected from the top of the axis, and a spiral is akin to Rossby waves. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1069, "title": "Meta-nematic transitions in a bilayer system: Application to the bilayer ruthenate", "abstract": "  It was suggested that the two consecutive metamagnetic transitions and the large residual resistivity discovered in Sr$_3$Ru$_2$O$_7$ can be understood via the nematic order and its domains in a single layer system. However, a recently reported anisotropy between two longitudinal resistivities induced by tilting the magnetic field away from the c-axis cannot be explained within the single layer nematic picture. To fill the gap in our understanding within the nematic order scenario, we investigate the effects of bilayer coupling and in-plane magnetic field on the electronic nematic phases in a bilayer system. We propose that the in-plane magnetic field in the bilayer system modifies the energetics of the domain formation, since it breaks the degeneracy of two different nematic orientations. Thus the system reveals a pure nematic phase with a resistivity anisotropy in the presence of an in-plane magnetic field. In addition to the nematic phase, the bilayer coupling opens a novel route to a hidden nematic phase that preserves the x-y symmetry of the Fermi surfaces. ", "machine_abstract": "We study the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory (DFT) calculations, which show that this material is close to an insulator-metal transition driven by charge transfer between layers. We find that the Fermi surface topology changes dramatically across the metal-insulator boundary, with the appearance of new hole pockets at the Brillouin zone center. The calculated band gap agrees well with experiments on single crystals. In addition, we predict that there are two competing nematic phases near the metal-insulator boundary. One has in-plane anisotropy along the Ru-O-Ru bond direction while another one has out-of-plane anisotropy perpendicular to it. These results provide insights into the origin of the observed structural distortion in bilayer ruthenates. Bilayer ruthenates have attracted considerable attention recently due to their rich physical properties including unconventional superconductivity [1] , quantum criticality [2] , and multiferroicity [3] . Among these materials, Sr3Ru2O7 shows particularly interesting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying pressure [4] . In recent years, several experimental studies have been performed to investigate the nature of the metal-insulator transition (MIT). For example, angle resolved photoemission spectroscopy measurements [5] found that the Fermi surface topology changed significantly when crossing the MIT line. X-ray scattering [6] showed that the crystal symmetry was lowered from tetragonal to orthorhombic below TMI = 160 K. Neutron scattering [7] revealed that the lattice parameters were different for the ab plane and c axis below TMIT ~ 150 K. However, despite extensive investigations, the microscopic mechanism behind the MIT remains unclear [8] .", "paraphrased_abstract": "The equilateral boundary of the metal-insulator boundary is very close to that of the metal-insulator boundary, with a hole at the center of the Brillouin zone. The crystal symmetry is lowered by the X-rays from the tetragonal to the orthorhombic, and the c-axis is shifted to the opposite. But this is not the same for the ab plane and the c-axis. The latter is more than 150 K. In recent years, many experiments have been carried out to find out the nature of the MIT, which is a very strange phenomenon, as it can be tuned from metal to metal and by chemical doping. However, the mechanism of the MIT is still unknown. In this regard, it is known that the bilayer ruthenates are highly interested in the fact that they are so rich in physical properties as to be extremely superconductive, a multivalent material, a stoichiometric tidal atom. We have obtained the DFT analysis, which shows that this material is close to an insulator-metal transition, driven by a charge-transferring mechanism between layers. Moreover, we are sure that there are two competing phases close to the metal-insulator boundary, one with in-plane anisotropy along the Ru-O-Ru", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1111, "title": "QCD in One Dimension at Nonzero Chemical Potential", "abstract": "  Using an integration formula recently derived by Conrey, Farmer and Zirnbauer, we calculate the expectation value of the phase factor of the fermion determinant for the staggered lattice QCD action in one dimension. We show that the chemical potential can be absorbed into the quark masses; the theory is in the same chiral symmetry class as QCD in three dimensions at zero chemical potential. In the limit of a large number of colors and fixed number of lattice points, chiral symmetry is broken spontaneously, and our results are in agreement with expressions based on a chiral Lagrangian. In this limit, the eigenvalues of the Dirac operator are correlated according to random matrix theory for QCD in three dimensions. The discontinuity of the chiral condensate is due to an alternative to the Banks-Casher formula recently discovered for QCD in four dimensions at nonzero chemical potential. The effect of temperature on the average phase factor is discussed in a schematic random matrix model. ", "machine_abstract": "We study the properties of QCD with one flavor and nonzero chemical potential $\\mu$ by using the Schwinger-Dyson equations (SDEs) for quarks, gluons, and ghosts. We solve these SDEs numerically to obtain the quark propagator $S(p)$ as well as the dressed-gluon and ghost propagators $D_G(q)$ and $D_F(q)$. The numerical results show that there is no phase transition up to $\\mu = 1 GeV$.  In particular, we find that the chiral condensate $<\\sigma>$ decreases monotonically as $\\mu$ increases. This result indicates that the system becomes more dilute when $\\mu$ increases. Furthermore, we observe that the mass function $M(p^2)$ has an inflection point around $p^2$ = 0.5 GeV$^2$ which corresponds to the constituent-quark mass.", "paraphrased_abstract": "Moreover, the mass function M(p2) has an inflection of 0.5 GeV222 which corresponds to the constituent quadrupole mass. We also consider that the chiral condensate \"Sigma\" decreases monotonically as mu increases. This implies that mu increases. We study the properties of a QCD with a one-dimensional, nonzero chemical potential, mu. We are based on the Schwinger-Dyson equations of quarks, gluons, and ghosts. The numerical results show that there is no phase transition as long as mu = 1 GeV. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0197, "title": "Analysis of random Boolean networks using the average sensitivity", "abstract": "  In this work we consider random Boolean networks that provide a general model for genetic regulatory networks. We extend the analysis of James Lynch who was able to proof Kauffman's conjecture that in the ordered phase of random networks, the number of ineffective and freezing gates is large, where as in the disordered phase their number is small. Lynch proved the conjecture only for networks with connectivity two and non-uniform probabilities for the Boolean functions. We show how to apply the proof to networks with arbitrary connectivity $K$ and to random networks with biased Boolean functions. It turns out that in these cases Lynch's parameter $\\lambda$ is equivalent to the expectation of average sensitivity of the Boolean functions used to construct the network. Hence we can apply a known theorem for the expectation of the average sensitivity. In order to prove the results for networks with biased functions, we deduct the expectation of the average sensitivity when only functions with specific connectivity and specific bias are chosen at random. ", "machine_abstract": "We study the dynamics of random Boolean networks (RBNs) by analyzing their attractors and basins, which are determined by the network's state transition matrix. We show that the number of states in an RBN is bounded above by its average sensitivity (AS), defined as the expected Hamming distance between two randomly chosen nodes at time t+1 given they differed on one node at time t. This bound can be used to estimate the size of the basin of attraction for any fixed point or cycle. In addition, we prove that if AS = 1 then there exists only one attractor with probability one. Finally, we present simulation results showing how our bounds compare against exact values obtained through exhaustive search over small networks. Random Boolean Networks (RBNs) have been widely studied since Kauffman introduced them in 1969 [1] . They consist of N binary-valued nodes connected together into a directed graph where each edge has a weight equal to either 0 or 1. The value of each node i at time step t + 1 depends on the values of all other nodes j at time step t according to some function fji(xj). For example, in Figure 1 , node 2 takes input from both nodes 1 and 3 while node 4 takes no inputs. A sequence of states x1, x2, ..., xt, ... is called an orbit when xi+1 = fi(xi) for every i > 0. An orbit is said to converge to a fixed point or cycle if it eventually repeats itself after a finite number of steps [2] . The behavior of an RBN is completely specified by its adjacency matrix Mij, whose elements represent the weights of edges connecting node i to node j. If Mij = 1, then node i receives input from node j; otherwise, node i does not receive input from node j. Since the output of each node is determined solely by its current state and the states of its neighbors,...", "paraphrased_abstract": "Then we show how our method can calculate the number of states in an RBN and the probability of its attraction. In the present paper we show how the number of states in an RBN is regulated by its advection Matrix, whose elements are the weights of the edges connecting nodes i and j, and the weights of nodes i and j. The value of each node i at the time of step t is proportioned to the value of all nodes j, according to a certain function fji. We show how the number of states in an RBN is regulated by the average sensitivity Matrix, which is the expected Hamming distance between two random nodes, if t+1 was different from the one t., t, t, t, t, t, t, t, t, t, t, t, t, t is called an orbit. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1054, "title": "Geometry of Time, Axiom of Choice and Neuro-Biological Quantum Zeno Effect", "abstract": "  Role of axiom of choice in quantum measurement is highlighted by suggesting that the conscious observer chooses the outcome from a mixed state. Further, in a periodically repeating universe, these outcomes must be pre-recorded within the non-physical conscious observers, which precludes free will. Free will however exists in a universe with open time, It is suggested that psychology's binding problem is connected with Cantor's original definition of set. Influence of consciousness on material outcome through quantum processes is discussed and interesting constraints derived. For example, it is predicted that quantum mechanical brain states should get frozen if monitored at sufficiently small space-time intervals - a neuro-biological version of the so called quantum zeno effect, which has been verified in domain of micro-physics. Existence of a very small micro-mini-black-hole in brain is predicted as a space-time structural interface between consciousness and brain, whose vaporization explains mass-loss reported in weighing experiments, conducting during the moments of death. ", "machine_abstract": "The geometry of time is the study of how space-time evolves in time.  The axiom of choice states that for any collection of non-empty sets there exists at least one set which contains exactly one element from each set.   In this article we show that if the universe has an underlying quantum structure then it follows that the geometry of time can be described by using the axiom of choice to select a single point on every trajectory through spacetime.   We also discuss some possible implications of our results for neurobiology where the quantum Zeno effect may play a role in explaining certain aspects of brain function such as consciousness. The geometry of time is the mathematical description of how space-time evolutes over time [1] . It was first introduced into physics by Hermann Minkowski [2] who showed that the geometry of space-time could be represented by four numbers (x,y,z,t) called coordinates or co-ordinates. These are related by the following equation:  x2+y2-z2-t2=(c^2)(1-(v/c))1/2   Where c represents the speed of light and v represents the velocity of the object being observed. This equation describes the relationship between distance travelled along the x-axis, y-axis, z-axis and t-axis respectively. For example, if you were observing someone walking across your living room floor with their back towards you they would have a positive value for the x-axis but no values for the other three axes because they are not moving in those directions. If however you were watching them walk away from you they would have negative values for all three axes except the x-axis since they are still travelling forward in that direction. As another example consider two objects traveling side-by-side down a roadway. They will both travel at the same speed so their velocities will be equal. However, if one car travels north while the other travels south they will appear to move faster than each other even though they are traveling at the same speed.", "paraphrased_abstract": "As a consequence, there are two objects traveling at the same speed, so the distances are equal. To show this, we introduce the mathematical theory of the geometry of time. It is a mathematical description of the evolution of time, as we have said. The geometry of time is a mathematical description of how time is evolving. We have derived it from the theory of Hermann Minkowski, who introduced it to the world by introducing four numbers, x, y, t, called coordinates, and whose four numbers are the coordinates of the co-ordinates, i.e. the coordinates of the axis. The equation of ac x = ac v t = ac v t = c  v t = ac c  t = ac c v t = ac v t = ac v t = c - v - v v v - v - v. The geometry of time is the study of how time evolves in time. We shall see in this article that the universe has an underlying quantum structure, and it is possible to describe it by means of this axiom of choice, for all unempty sets there is at least one point, which is exactly one, from all", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1412, "title": "The Alignments of the Galaxy Spins with the Real-Space Tidal Field Reconstructed from the Two Mass Redshift Survey", "abstract": "  We report a direct observational evidence for the existence of the galaxy spin alignments with the real space tidal field. We calculate the real space tidal field from the real space density field reconstructed recently from the Two Mass Redshift Survey (2MRS) by Erdogdu et al. in 2006. Using a total of 12122 nearby spiral galaxies from the Tully Galaxy Catalog, we calculate the orientations of their spin axes relative to the 2MRS tidal field. We find a clear signal of the intrinsic correlations between the galaxy spins and the intermediate principal axes of the tidal shears. The null hypothesis of no correlation is rejected at 99.99 % confidence level. We also investigate the dependence of the intrinsic correlations on the galaxy morphological type and the environment. It is found that (i) the intrinsic correlation depends weakly on the morphological type of the spiral galaxies but tends to decrease slightly as the type increases; (ii) it is stronger in the high-density regions than in the low-density regions. The observational result is quantitatively consistent with analytic prediction based on the tidal torque theory. It is concluded that the galaxy spin orientations may provide in principle a new complimentary probe of the dark matter distribution. ", "machine_abstract": "We present an analysis of the alignments between galaxy spins and tidal fields in real space, using data from the Two Mass Redshfit Survey (TMRS). We find that galaxies are preferentially aligned perpendicular to their local tidal field on scales larger than 1 Mpc/h. This alignment is stronger for more massive galaxies at higher redshifts. The observed spin-tide correlation can be explained by the effect of gravitational torques exerted by large-scale structures during the formation process of these galaxies. Our results suggest that this mechanism may play an important role in shaping galactic angular momenta. These findings have implications for understanding how dark matter halos acquire their angular momentum as well as for interpreting observations of cosmic shear statistics. Introduction: Galaxies form within overdense regions of the universe where they experience strong gravitational interactions with other objects such as neighboring galaxies or clusters of galaxies. During the formation process, these interactions induce gravitational torques which affect the orientation of the galactic angular momentum vector. In turn, the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction processes. Therefore, it has been suggested that the shape distribution of galaxies could provide information about the origin of galactic angular momentums (e.g., Catelan & Theuns 1996; Lee et al. 2008) . However, observational studies show conflicting results regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors' positions (see e.g., Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 , for recent works). In order to understand the physical mechanisms responsible for determining the directions of galactic angular momentas, we need to study the statistical properties of galaxy spin distributions over large volumes of the universe. Recent surveys like Sloan Digital Sky Survey (SDSS) allow us to measure galaxy orientations accurately enough to perform such analyses. For example, Lee et al. (2008) used SDSS DR4 data to investigate the alignments between galaxy spin vectors and their nearest neighbor's position angles. They found no", "paraphrased_abstract": "Then the angular momentum was found to be determined by the gravitational axis of the angular momentum, which is shaped by the angular momentum of the galaxy. We propose that the distribution of the angular momentum is obtained by the movement of galaxies, by influencing the formation of large scale structures, and influencing the shape of the galaxies by dynamical friction. In this study, we will examine the relation between the angular momentum of galaxies and their local tidal fields. Moreover, the study of the angular momentum of galaxies, with the help of the Sloan Digital Sky Survey (SDSS), is able to estimate the angle of the spin vector of each galaxy, which is of a very high order, based on the information obtained from the measurements of the two masses, and from the observations of cosmic shear. We are particularly interested in the relation of the angular momentum of galaxies with the angular momentum of other galaxies, or between galaxies and clusters. These observations indicate that, due to the presence of large structures, angular momentum is a dynamical force. In the genesis of galaxies, there are gravitational forces which impose the orientation of the angular momentum. Consequently, we have been able to explain", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.158, "title": "Composite Interstellar Grains", "abstract": "  The composite grain is made up of a host silicate spheroid and graphite inclusions. The extinction efficiencies of the composite spheroidal grains for three axial ratios are computed using the discrete dipole approximation (DDA). The interstellar extinction curve is evaluated in the spectral region 3.40--0.10$\\mu m$ using the extinction efficiencies of the composite spheroidal grains. The model extinction curves are then compared with the average observed interstellar extinction curve. We also calculate the linear polarization for the spheroidal composite grains at three orientation angles and find the wavelength of maximum polarization. Further, we estimate the volume extinction factor, an important parameter from the point of view of cosmic abundance, for the composite grain models that reproduce the average observed interstellar extinction. The estimated abundances derived from the composite grain models for both carbon and silicon are found to be lower than that are predicted by the bare silicate/graphite grain models but these values are still higher than that are implied from the recent ISM values. ", "machine_abstract": "We present the results of laboratory measurements on composite interstellar grains, which are composed of amorphous silicate and carbonaceous materials with various compositions. The samples were irradiated by energetic protons in order to simulate cosmic ray bombardment under conditions similar to those found in dense clouds where dust is formed. We have measured infrared (IR) emission spectra before and after proton irradiation at energies ranging from 1 MeV/nucleon up to 100 MeV/nucleon for different sample temperatures between 10 K and 300 K. In addition we performed IR transmission spectroscopy experiments using synchrotron radiation as well as electron energy loss spectroscopy (EELS). Our experimental data show that the composition of the grain material has an important influence on its response towards proton irradiation. For example, the intensity ratio of the 3.4 micron feature over the 11 micron feature increases significantly when the amount of aromatic hydrocarbons relative to silicates decreases.", "paraphrased_abstract": "We have studied the spectroscopic properties of composite interstellar grain materials, which are composed of amorphous silica and carbonaceous materials, with the following compositions: in the light of the radiated protons we measured the intensity of the radiated light from the infrared, from the 1 kHz to the 1 kHz. We have measured the absorption of the radiated light at the different temperatures from the 10 kHz to the 300 kHz. We have also tested the irradiation of the light from the radiated grains with synchrotron radiation, the elution of the atoms, the transmission of electrons, and the elution of the electrons. We present the results of laboratory measurements of the composite interstellar grains, composed of amorphous silicates and carbonaceous material. We have also studied the transmission of the irradiated grains by elution by protons in the same manner as the dense clouds where dust is accumulated. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3301, "title": "Positivity and almost positivity of biharmonic Green's functions under Dirichlet boundary conditions", "abstract": "  In general, for higher order elliptic equations and boundary value problems like the biharmonic equation and the linear clamped plate boundary value problem neither a maximum principle nor a comparison principle or -- equivalently -- a positivity preserving property is available. The problem is rather involved since the clamped boundary conditions prevent the boundary value problem {from} being reasonably written as a system of second order boundary value problems.   It is shown that, on the other hand, for bounded smooth domains $\\Omega \\subset\\mathbb{R}^n$, the negative part of the corresponding Green's function is \"small\" when compared with its singular positive part, provided $n\\ge 3$.   Moreover, the biharmonic Green's function in balls $B\\subset\\mathbb{R}^n$ under Dirichlet (i.e. clamped) boundary conditions is known explicitly and is positive. It has been known for some time that positivity is preserved under small regular perturbations of the domain, if $n=2$. In the present paper, such a stability result is proved for $n\\ge 3$. Keywords: Biharmonic Green's functions, positivity, almost positivity, blow-up procedure. ", "machine_abstract": "In this article, we study the positivity and almost-positivity properties of biharmonic Green\u2019s function in an open bounded domain $\\Omega$ with smooth boundary $\\partial \\Omega$. We prove that if $(\\Omega, \\partial \\Omega)$ is strictly convex then the biharmonic Green\u2019s functions are positive on $\\Omega \\setminus \\partial \\Omega$. If $(\\Omega, \\partial\\Omega )$ has non-negative curvature but not necessarily strictly convex, then the biharmonic green\u2019s functions may be negative somewhere inside $\\Omega$. However, they can never change sign on $\\Omega \\setminus\\partial \\Omega$. Finally, for any given point $x_0 \\in \\overline{\\Omega}$, there exists a neighborhood $U_x_0$ such that the biharmonic Green's function is positive or zero everywhere outside $U_{x_0}\\cap \\partial \\Omega$. The results obtained here generalize those known previously only when $\\Omega$ is strictly convex.", "paraphrased_abstract": "When it comes to a particular point in the spectrum of the total spectrum, there is a locality of Ux0, in which the function is positive or zero, but it is never changeable. In the following, we present the results of this study. Assuming the graph is bounded, the domain is bounded by an obliquely inclined line, and the line is not curved, the function is positive or zero within the line of the oblique line. We demonstrate that, if the line of the oblique line is curved, the function of the oblique line is positive or zero outside the line of oblique line. It is true that if the line of the oblique line is not convex, then the function of the oblique line is positive, and it is possible to vary the sign of the oblique line in a negative way. However, the sign cannot be changed by oblique line in a negative line. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1428, "title": "A unified projection formalism for the Al-Pd-Mn quasicrystal Xi-approximants and their metadislocations", "abstract": "  The approximants xi, xi' and xi'_n of the quasicrystal Al-Mn-Pd display most interesting plastic properties as for example phason-induced deformation processes (Klein, H., Audier, M., Boudard, M., de Boissieu, M., Beraha, L., and Duneau, M., 1996, Phil. Mag. A, 73, 309.) or metadislocations (Klein, H., Feuerbacher, M., Schall, P., and Urban, K., 1999, Phys. Rev. Lett., 82, 3468.). Here we demonstrate that the phases and their deformed or defected states can be described by a simple projection formalism in three-dimensional space - not as usual in four to six dimensions. With the method we can interpret microstructures observed with electron microscopy as phasonic phase boundaries. Furthermore we determine the metadislocations of lowest energy and relate them uniquely to experimentally observed ones. Since moving metadislocations in the xi'-phase can create new phason-planes, we suggest a dislocation induced phase transition from xi' to xi'_n. The methods developed in this paper can as well be used for various other complex metallic alloys. ", "machine_abstract": "We present an analysis of dislocation structures in icosahedral approximant phases based on a new approach to describing dislocation networks, which is applicable both to periodic crystals and aperiodic solids with any kind of local order. The method relies on projecting the Burgers vectors onto a set of basis vectors that are determined by the underlying lattice structure. We show how this can be used to describe the dislocation network in the decagonal phase of the AlPdMn system as well as its parent cubic phase. In particular we find that the dislocation network in these two phases has very similar characteristics despite the fact that they have different symmetries. This suggests that the dislocation network may play an important role in determining the physical properties of these materials.     Introduction     Dislocations are line defects in crystalline materials where there is a discontinuity in the atomic arrangement along some direction. They occur naturally during plastic deformation processes such as bending or stretching but also arise spontaneously when certain conditions are satisfied [1] . For example, it was recently shown that dislocations form at grain boundaries between grains of differing orientations [2] , and that they can even appear within single grains [3] .   Dislocations are classified according to their Burgers vector b = mu + nv (where u and v are primitive lattice vectors) into edge dislocations if m+n=0, screw dislocations if n=m=1, mixed dislocations otherwise [4] . Edge dislocations correspond to a displacement field perpendicular to the slip plane while screw dislocations give rise to a displacement parallel to the slip plane [5] . Mixed dislocations combine features of both types [6] .    The presence of dislocations leads to elastic strain fields around them [7, 8] . These strains can be calculated using the Peach-Koehler force acting on each individual dislocation [9] . If all dislocations were isolated then the total energy would simply be given by the sum over all contributions from individual dislocations [10] . However, in real systems dislocations interact strongly with one another through elastic interactions [11] . As a result, the total energy depends not only on the number density", "paraphrased_abstract": "And since the elasticity of these dislocations is very sensitive, it is possible to calculate the mass in a single dislocation. The analysis of dislocations in icosahedral phases is based on the Burgers theory of dislocations, which can be applied to periodic crystals and to aperiodic solids with any local order. In a word, the dislocations are line defects that have a discontinuity in the atomic arrangement in one direction. The dislocations are naturally formed at the boundary between the grain-finger and the grain-finger, but also occur spontaneously, when certain conditions are satisfied. In a given system, dislocations form forces of elasticity, which are based on the Peach-Koehler force of each dislocation. The resulting strains are calculated by the Peach-Koehler force imposed on each dislocation. It was recently shown that dislocations form at the boundary between grain-finger and grain-finger, and that they even exist in a single grain. The dislocations are divided into dislocations according to their Burgers' vector B, mu + nv (where u and v are primitive lattice vectors) into dislocations if m=n, screws if m=n, mixed dislocations if", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.334, "title": "Renormalized Quantum Yang-Mills Fields in Curved Spacetime", "abstract": "  We present a proof that quantum Yang-Mills theory can be consistently defined as a renormalized, perturbative quantum field theory on an arbitrary globally hyperbolic curved, Lorentzian spacetime. To this end, we construct the non-commutative algebra of observables, in the sense of formal power series, as well as a space of corresponding quantum states. The algebra contains all gauge invariant, renormalized, interacting quantum field operators (polynomials in the field strength and its derivatives), and all their relations such as commutation relations or operator product expansion. It can be viewed as a deformation quantization of the Poisson algebra of classical Yang-Mills theory equipped with the Peierls bracket. The algebra is constructed as the cohomology of an auxiliary algebra describing a gauge fixed theory with ghosts and anti-fields. A key technical difficulty is to establish a suitable hierarchy of Ward identities at the renormalized level that ensure conservation of the interacting BRST-current, and that the interacting BRST-charge is nilpotent. The algebra of physical interacting field observables is obtained as the cohomology of this charge. As a consequence of our constructions, we can prove that the operator product expansion closes on the space of gauge invariant operators. Similarly, the renormalization group flow is proved not to leave the space of gauge invariant operators. ", "machine_abstract": "We present the renormalization group flow equations for quantum gauge fields on curved spacetime, including fermions and scalars as well as their interactions with gravity. We show that these flows are governed by an exact functional differential equation which is derived using the background field method. The solution to this equation yields all possible counterterms needed to render the theory finite at any order in perturbation theory. This result generalizes previous results obtained within the context of flat space-time. In particular we find that the beta functions for the gravitational couplings do not vanish even if one considers only pure Yang-Mills theories without matter fields. Furthermore, our analysis shows that the running of the gravitational coupling constants can be determined completely in terms of the beta functionals associated with the non-gravitational sectors. Finally, we discuss how our formalism may be used to study the effects of radiative corrections on black hole entropy. Introduction: Gauge theories play a central role in modern particle physics. They provide a description of fundamental forces such as electromagnetism or weak nuclear force. However, it has been known since the early days of QED (quantum electrodynamics) that perturbatively quantizing gauge theories leads to ultraviolet divergences. These divergences have to be removed by adding appropriate local counterterms to the classical action. It turns out that there exist infinitely many different ways to add these counterterms so that the resulting effective action remains invariant under the original gauge symmetry transformations. Therefore, the choice of the correct set of counterterms depends crucially on the regularization scheme chosen to regulate the infinities appearing during the calculation of Feynman diagrams. For example, in dimensional regularization [1] , where the number of dimensions is taken to be d = 4 \u2212 2\u03b5 instead of four, the most general form of the counterterm Lagrangian reads [2]  where F \u00b5\u03bd denotes the electromagnetic field strength tensor and D \u00b5 \u2261 \u2202 \u00b5 + ieA \u00b5 . Here e denotes the electric charge while c 1 , c 2 , . . . denote arbitrary coefficients whose values depend on the specific regularization scheme employed.", "paraphrased_abstract": "... if we look at a regularization of the Feynman diagram, we will find the Lagrangian expression for a dimensional regularization of the sphere, which, as we know, is four dimensions, the most general form of the 'Lagrangian' is a Lagrangian expression, where d is the electromagnetic force, and d is the electrical charge, while c is the electric charge, c 2, c..., c... a arbitrary arbitrary coefficients whose values depend on the 'Lagrangian'. Here, e is the electric charge, and e is the electric charge, and c 1, c 2..., c 2... are the arbitrary coefficients that depend on the 'local' entropy of the black hole. The flow of the renormalized group on curved space-time is governed by a function differential to the background, a field of background field, and the solution to the problem. The result is general and extends previous results in a flat space-time. This result generalizes the result of previous results for the flat space-time. Besides, we find that the beta functions of the gravitational axis are not abolished even in the pure Yang-Mills theory, which", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3941, "title": "Bimodal AGNs in Bimodal Galaxies", "abstract": "  By their star content, the galaxies split out into a red and a blue population; their color index peaked around u-r=2.5 or u-r=1, respectively, quantifies the ratio of the blue stars newly formed from cold galactic gas, to the redder ones left over by past generations. On the other hand, upon accreting substantial gas amounts the central massive black holes energize active galactic nuclei (AGNs); here we investigate whether these show a similar, and possibly related, bimodal partition as for current accretion activity relative to the past. To this aim we use an updated semianalytic model; based on Monte Carlo simulations, this follows with a large statistics the galaxy assemblage, the star generations and the black hole accretions in the cosmological framework over the redshift span from z=10 to z=0. We test our simulations for yielding in close detail the observed split of galaxies into a red, early and a blue, late population. We find that the black hole accretion activities likewise give rise to two source populations: early, bright quasars and later, dimmer AGNs. We predict for their Eddington parameter $\\lambda_E$ -- the ratio of the current to the past black hole accretions -- a bimodal distribution; the two branches sit now under $\\lambda_E \\approx 0.01$ (mainly contributed by low-luminosity AGNs) and around $\\lambda_E \\approx 0.3-1$. These not only mark out the two populations of AGNs, but also will turn out to correlate strongly with the red or blue color of their host galaxies. ", "machine_abstract": "We present the results of our study on bimodality in galaxies and active galactic nuclei (AGN). We find that there is no significant difference between the fraction of AGNs hosted by red or blue galaxies, but we do see an excess of AGNs with respect to normal galaxies at intermediate colors. This suggests that AGNs are not preferentially found in either red or blue galaxies, as previously thought; instead they appear to be more common among galaxies with intermediate color. The lack of correlation between galaxy color and AGN activity may indicate that AGNs play only a minor role in quenching star formation in massive galaxies. Alternatively, it could suggest that AGNs have different effects depending on their luminosity and/or accretion rate. In addition, we find that the majority of AGNs reside in galaxies with bulges, regardless of whether these galaxies are classified as early-type or late-type systems.", "paraphrased_abstract": "We further study the distribution of AGNs in the galaxy and the activity of Galactic nuclei. This is a remarkable finding, for there is no significant difference between the number of AGNs in red and in blue galaxies, but there is an excess of AGNs in normal galaxies. AGNs may have little or no effect on star formation in large galaxies, whereas AGNs may be an important part of star formation in giant galaxies. We further examine the phenomenon of bimodality between the two galaxies and the active galactic nuclei. In our study, we have found no significant difference between the number of AGNs in red and in blue galaxies, and there is no significant difference between the number of AGNs in normal galaxies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0165, "title": "Is Modified Gravity Required by Observations? An Empirical Consistency Test of Dark Energy Models", "abstract": "  We apply the technique of parameter-splitting to existing cosmological data sets, to check for a generic failure of dark energy models. Given a dark energy parameter, such as the energy density Omega_Lambda or equation of state w, we split it into two meta-parameters with one controlling geometrical distances, and the other controlling the growth of structure. Observational data spanning Type Ia Supernovae, the cosmic microwave background (CMB), galaxy clustering, and weak gravitational lensing statistics are fit without requiring the two meta-parameters to be equal. This technique checks for inconsistency between different data sets, as well as for internal inconsistency within any one data set (e.g., CMB or lensing statistics) that is sensitive to both geometry and growth. We find that the cosmological constant model is consistent with current data. Theories of modified gravity generally predict a relation between growth and geometry that is different from that of general relativity. Parameter-splitting can be viewed as a crude way to parametrize the space of such theories. Our analysis of current data already appears to put sharp limits on these theories: assuming a flat universe, current data constrain the difference Omega_Lambda(geom) - Omega_Lambda(grow) to be -0.0044 +/- 0.0058 (68% C.L.); allowing the equation of state w to vary, the difference w(geom) - w(grow) is constrained to be 0.37 +/- 0.37 (68% C.L.). Interestingly, the region w(grow) > w(geom), which should be generically favored by theories that slow structure formation relative to general relativity, is quite restricted by data already. We find w(grow) < -0.80 at 2 sigma. As an example, the best-fit flat Dvali-Gabadadze-Porrati (DGP) model approximated by our parametrization lies beyond the 3 sigma contour for constraints from all the data sets. ", "machine_abstract": "We present an empirical consistency test for dark energy models based on the growth rate of large-scale structure and its dependence on cosmological parameters, including the equation-of-state parameter w{\\displaystyle w}. We use data from the Sloan Digital Sky Survey to measure the evolution of clustering in galaxy samples with different luminosities at redshifts z=0.1\u20130.7. The results are compared against predictions made using two classes of dark energy model: (i) quintessence scalar fields that evolve slowly over time and (ii) modified gravity theories where the gravitational force is mediated by additional degrees of freedom beyond those predicted by general relativity. Our analysis shows no evidence for deviations from standard gravity or quintessence-like behavior within current observational uncertainties. This result provides further support for the concordance model of cosmology as well as constraining power for future experiments such as Euclid. Introduction     In recent years there has been considerable interest in testing whether modifications to Einstein's theory of general relativity can explain observations of cosmic acceleration [1\u20133] . A number of theoretical frameworks have been proposed which modify gravity on large scales while remaining consistent with local tests of gravity [4\u20136] , but it remains unclear if these theories will be able to reproduce all observed phenomena [7\u20139] .     Here we consider one class of modified gravity theories known as f(R)-gravity [10\u201312] , where the gravitational action contains higher-order curvature terms. These theories predict new degrees of freedom associated with the extra dimensions of space-time [13, 14] , leading to observable effects on the expansion history of the universe [15\u201318] and the growth of large-scale structure [19\u201321] .  In this work we perform an empirical consistency test between measurements of the growth of large-scale structure and predictions made using two classes", "paraphrased_abstract": "The theories of gravity are very general, but in general, the observable phenomena of the universe have been well studied. There are a number of theories, mainly based on the general theory of gravity, which, while preserving local tests of gravity, are still quite a long way from being realized. Many theories of gravity have been proposed, but they are not universally applicable to all phenomena. There is a growing interest in the study of the effects of the cosmic expansion of the universe and the development of large structures. We study the occurrence of large structures in the world in a way that resembles the emergence of the ethereal spheres, and compare this with the prediction of the growth of large structures in the southwestern sky. This is done with data from the Sloan Digital Sky Survey and compared it with predictions from two classes of dark energy, namely, the quintessence scalars which grow slowly over time, and the modified gravity theories, where the gravitational force is mediated by new dimensions that are not predicted by the general gravity, and the epoch of the growth of large structures. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1778, "title": "Minimal size of a barchan dune", "abstract": "  Barchans are dunes of high mobility which have a crescent shape and propagate under conditions of unidirectional wind. However, sand dunes only appear above a critical size, which scales with the saturation distance of the sand flux [P. Hersen, S. Douady, and B. Andreotti, Phys. Rev. Lett. {\\bf{89,}} 264301 (2002); B. Andreotti, P. Claudin, and S. Douady, Eur. Phys. J. B {\\bf{28,}} 321 (2002); G. Sauermann, K. Kroy, and H. J. Herrmann, Phys. Rev. E {\\bf{64,}} 31305 (2001)]. It has been suggested by P. Hersen, S. Douady, and B. Andreotti, Phys. Rev. Lett. {\\bf{89,}} 264301 (2002) that this flux fetch distance is itself constant. Indeed, this could not explain the proto size of barchan dunes, which often occur in coastal areas of high litoral drift, and the scale of dunes on Mars. In the present work, we show from three dimensional calculations of sand transport that the size and the shape of the minimal barchan dune depend on the wind friction speed and the sand flux on the area between dunes in a field. Our results explain the common appearance of barchans a few tens of centimeter high which are observed along coasts. Furthermore, we find that the rate at which grains enter saltation on Mars is one order of magnitude higher than on Earth, and is relevant to correctly obtain the minimal dune size on Mars. ", "machine_abstract": "We study the minimal size of a barchan, which is one of the most common types of sand dunes in nature. We show that this problem can be formulated as an optimal control problem for a nonlinear partial differential equation with nonlocal boundary conditions and prove existence of solutions by using variational methods. The numerical results are obtained by applying finite element method to discretize the state equations and then solving them by Newton's iteration scheme. Finally we present some examples illustrating our theoretical findings. Sand dunes are among the most beautiful natural phenomena on Earth. They have been studied extensively since the 19th century [1] . One of the simplest forms of sand dunes is called barchan [2] , see Figure 1 (a). It has a crescent shape with its horns pointing away from the wind direction. Barchans occur naturally over large areas around the world such as Australia [3] , Namibia [4] , Saudi Arabia [5] , China [6] , Japan [7] , etc.. In recent years there has been growing interest in studying mathematical models describing formation of sand dunes [8, 9, 10] . In this work we consider the following model proposed by Kroy et al [11] :  where u(x) denotes the height of the sand bed at position x \u2208 \u2126 = [0, L] \u00d7 R + ; f > 0 represents the rate of deposition; g \u2265 0 stands for the erosion coefficient; h(u) describes the effect of surface tension; p(x), q(x) represent the pressure terms due to gravity and friction respectively; \u03b1 > 0 measures the strength of the wind blowing along x-axis; \u03b2 > 0 characterizes the resistance against the flow of air; \u03b3 > 0 is related to the cohesion between grains of sand; \u03b8 is the angle of repose of sand particles; c > 0 is the constant volume fraction of sand per unit area; finally, n is the outward normal vector to the boundary \u0393 = {0 < x < L} \u00d7 {0} \u222a {L} \u00d7 R + . For more details about physical meaning of parameters involved in system (1) , please refer to [12] .", "paraphrased_abstract": "As we have mentioned, the most common forms of sand dunes are the sand-dunes of Australia, Namibia, Saudi Arabia, China, Japan, etc. In general, the sand-dunes of Australia have been studied in the last few years and have been studied for many years. In recent years, however, much interest has been devoted to the study of mathematical models for the formation of sand dunes. In this work, we have used the following mathematical model, which has been suggested by Kroy and others,: where u (x) is the height of the sand-dunes at a certain position, i.e. where x, L, i.e. 0, 0, L, 0, 0, L, 0, 0, 0, 0, L, 0, 0, 0, 0, 0, 0, 0, L, 0, L, 0, L, 0, L, 0, 2,.,,,,,,,,,,, ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2718, "title": "Coulomb gauge Gribov copies and the confining potential", "abstract": "  We study the approach, initiated by Marinari et al., to the static inter-quark potential based on Polyakov lines of finite temporal extent, evaluated in Coulomb gauge. We show that, at small spatial separations, the potential can be understood as being between two separately gauge invariant colour charges. At larger separations Gribov copies obstruct the non-perturbative identification of individually gauge invariant colour states. We demonstrate, for the first time, how gauge invariance can be maintained quite generally by averaging over Gribov copies. This allows us to extend the analysis of the Polyakov lines and the corresponding, gauge invariant quark-antiquark state to all distance scales. Using large scale lattice simulations, we show that this interpolating state possesses a good overlap with the ground state in the quark-antiquark sector and yields the full static inter-quark potential at all distances. A visual representation of the Gribov copies on the lattice is also presented. ", "machine_abstract": "We study the effect of Coulomb gauge fixing on the gluon propagator in Landau gauge, using lattice simulations with two different actions for SU(2) Yang-Mills theory. We find that the inclusion of Gribov copies leads to an increase in the infrared strength of the gluon propagator; this is consistent with earlier results obtained by other groups.     In addition we show how the presence of Gribov copies affects the static quark-antiquark potential at large distances. The latter quantity can be extracted from the correlation function of Polyakov loops which are defined as closed lines winding around the periodic spatial directions of the lattice. This allows us to compare our results directly with those obtained previously within perturbation theory. Our findings suggest that the perturbative approach breaks down when one considers the full non-perturbative effects associated with Gribov copies. Finally, we discuss possible implications of these results for phenomenological studies of QCD.", "paraphrased_abstract": "And finally, we discuss the implications of these results for phenomenological studies of QCD. Coulomb gauge is used for the gluon propagator at Landau, and two different combinations are used in SU and Yang-Mills theory. In the analysis of this, we show that Gribov copies produce a positive effect on the gluon propagator at a large distance. We have obtained, in fact, the results of a correspondence between the correlation coefficients of Polyakov loops, which are closed loops winding round the periodic spatial directions of the lattice. The result shows that Gribov copies, when present, have the effect of the gluon propagator at an extremely large distance. We demonstrate, in the following, the effect of Gribov copies on the static gluon-antiquark potential at long distances. We conclude that this effect is not accompanied by perturbation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3456, "title": "Spectral methods for orthogonal rational functions", "abstract": "  An operator theoretic approach to orthogonal rational functions on the unit circle with poles in its exterior is presented in this paper. This approach is based on the identification of a suitable matrix representation of the multiplication operator associated with the corresponding orthogonality measure. Two different alternatives are discussed, depending whether we use for the matrix representation the standard basis of orthogonal rational functions, or a new one with poles alternatively located in the exterior and the interior of the unit circle. The corresponding representations are linear fractional transformations with matrix coefficients acting respectively on Hessenberg and five-diagonal unitary matrices.   In consequence, the orthogonality measure can be recovered from the spectral measure of an infinite unitary matrix depending uniquely on the poles and the parameters of the recurrence relation for the orthogonal rational functions. Besides, the zeros of the orthogonal and para-orthogonal rational functions are identified as the eigenvalues of matrix linear fractional transformations of finite Hessenberg and five-diagonal matrices.   As an application of this operator approach, we obtain new relations between the support of the orthogonality measure and the location of the poles and parameters of the recurrence relation, generalizing to the rational case known results for orthogonal polynomials on the unit circle.   Finally, we extend these results to orthogonal polynomials on the real line with poles in the lower half plane. ", "machine_abstract": "We consider the problem of approximating an unknown function f in L^2(0,1) by a linear combination of orthonormal polynomials with respect to some weight function w on (0,1). We show that if we choose the weights so as to minimize the error between the approximation and the true solution then this leads to a system of equations which can be solved using spectral methods. The resulting algorithm is shown to have optimal convergence rates under certain conditions. In particular it has been proved recently that if the coefficients of the expansion are chosen optimally then the rate of convergence is O(N^{-2}), where N denotes the number of terms used in the expansion.  The main purpose of this thesis was to implement these algorithms numerically and compare their performance against other existing techniques such as Chebyshev expansions or Legendre expansions. This work will also allow us to investigate whether there exist any advantages associated with choosing different types of basis functions when solving differential equations.", "paraphrased_abstract": "This thesis is a numerical study, comparing it with other approaches, Chebyshev and Legendre. This study will enable us to investigate the advantages of selecting different basis functions in differential equations. We will prove in this work that it is quite possible to choose different basis functions for solving differential equations. We will show that the weights chosen for the optimization will reduce the error between the approximation and the solution, and this will result in an equation that can be solved by spectral methods. We consider the problem of approximating an unknown function f in L2(0,1) by the application of orthonormal polynomials to a weight function w in (0,1) which is a purely orthonormal function. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0685, "title": "Non-Parametric Field Estimation using Randomly Deployed, Noisy, Binary Sensors", "abstract": "  The reconstruction of a deterministic data field from binary-quantized noisy observations of sensors which are randomly deployed over the field domain is studied. The study focuses on the extremes of lack of deterministic control in the sensor deployment, lack of knowledge of the noise distribution, and lack of sensing precision and reliability. Such adverse conditions are motivated by possible real-world scenarios where a large collection of low-cost, crudely manufactured sensors are mass-deployed in an environment where little can be assumed about the ambient noise. A simple estimator that reconstructs the entire data field from these unreliable, binary-quantized, noisy observations is proposed. Technical conditions for the almost sure and integrated mean squared error (MSE) convergence of the estimate to the data field, as the number of sensors tends to infinity, are derived and their implications are discussed. For finite-dimensional, bounded-variation, and Sobolev-differentiable function classes, specific integrated MSE decay rates are derived. For the first and third function classes these rates are found to be minimax order optimal with respect to infinite precision sensing and known noise distribution. ", "machine_abstract": "We consider the problem of estimating an unknown field in R^d based on noisy measurements taken by randomly deployed binary sensors. The measurement process is modeled as follows: each sensor takes one bit of information about its local environment and transmits it to a fusion center via a wireless link; at the fusion center, these bits are combined into estimates for the underlying field. We assume that there exists some prior knowledge regarding the structure of this field (e.g., smoothness), which can be used to design efficient estimators. In particular, we propose two classes of estimators: non-parametric estimators with fixed bandwidths, and adaptive estimators whose bandwidths vary according to the local properties of the field being estimated.  For both types of estimators, we provide performance guarantees under various assumptions on the noise model and the spatial distribution of the sensors. Finally, we present numerical results illustrating our theoretical findings.", "paraphrased_abstract": "In general, we present two types of estimation, nonparametric, with fixed bandwidths, and adaptive, whose bandwidths vary according to the local properties of the field. Specifically, we propose two classes of estimation: nonparametric with fixed bandwidths, and adaptive with fixed bandwidths, depending on the local properties of the field. The estimation is based on noisy observations made by random binary sensors. We assume that the information about the surrounding environment is known, and that the underlying field is well known (e.g., smoothness), which can be used to design efficient estimation methods. The process is described in this way: each sensor takes a little bit of information about its environment and transmits it to a fusion center, where it is estimated for the underlying field. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0299, "title": "Parametrized Post-Newtonian Expansion of Chern-Simons Gravity", "abstract": "  We investigate the weak-field, post-Newtonian expansion to the solution of the field equations in Chern-Simons gravity with a perfect fluid source. In particular, we study the mapping of this solution to the parameterized post-Newtonian formalism to 1 PN order in the metric. We find that the PPN parameters of Chern-Simons gravity are identical to those of general relativity, with the exception of the inclusion of a new term that is proportional to the Chern-Simons coupling parameter and the curl of the PPN vector potentials. We also find that the new term is naturally enhanced by the non-linearity of spacetime and we provide a physical interpretation for it. By mapping this correction to the gravito-electro-magnetic framework, we study the corrections that this new term introduces to the acceleration of point particles and the frame-dragging effect in gyroscopic precession. We find that the Chern-Simons correction to these classical predictions could be used by current and future experiments to place bounds on intrinsic parameters of Chern-Simons gravity and, thus, string theory. ", "machine_abstract": "We present the first post-newtonian expansion for Chern-Simons gravity, which is an alternative to general relativity that has been proposed as a candidate theory of quantum gravity.  We show how this can be done by using the method developed in Ref. [1] . The resulting expression agrees with previous results obtained within the framework of effective field theories [2] , and it also reproduces the leading-order corrections to Newton's law predicted by Einstein-Cartan gravity [3] . The work presented here was supported by the National Science Foundation under Grant No. PHY-0555310. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. This document is available at http://arxiv.org/abs/1106.5389v1. In recent years there have been several attempts to develop new gravitational theories beyond general relativity (GR) [4] - [8] . One such proposal is ChernSimons gravity [9] , where the action contains higher order curvature terms but no matter fields [10] . It has been shown [11] that Chern-Simons gravity reduces to GR when expanded around flat space-time, while it gives rise to novel effects on cosmological scales [12] . Moreover, Chern-Simons gravity appears naturally in string theory [13] , and it may provide a description of black hole entropy [14] .", "paraphrased_abstract": "Moreover, Chern-Simons gravity can be described by string theory as a natural description of black hole entropy. The work presented here has been supported by the National Science Foundation under grant no. PHY 0555310, and the authors\u2019 opinions and conclusions are the own and not necessarily those of the National Science Foundation. In recent years, several attempts have been made to develop new gravity theories, not limited to general relativity (GR). These theories are called Chern-Simons gravity, and are based on the theory of Chern-Simons gravity, which is the theory of quantum gravity, and has been proposed as a candidate theory for quantum gravity. We have recently found the first, post-newtonian expansion of Chern-Simons gravity, which is an alternative to GR in the sense that it has no matter-of-gravity. In addition, Chern-Simons gravity is incorporated in string theory, and it may also be the theory of black hole entropy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0655, "title": "Exploring the Variable Sky with the Sloan Digital Sky Survey", "abstract": "  We quantify the variability of faint unresolved optical sources using a catalog based on multiple SDSS imaging observations. The catalog covers SDSS Stripe 82, and contains 58 million photometric observations in the SDSS ugriz system for 1.4 million unresolved sources. In each photometric bandpass we compute various low-order lightcurve statistics and use them to select and study variable sources. We find that 2% of unresolved optical sources brighter than g=20.5 appear variable at the 0.05 mag level (rms) simultaneously in the g and r bands. The majority (2/3) of these variable sources are low-redshift (<2) quasars, although they represent only 2% of all sources in the adopted flux-limited sample. We find that at least 90% of quasars are variable at the 0.03 mag level (rms) and confirm that variability is as good a method for finding low-redshift quasars as is the UV excess color selection (at high Galactic latitudes). We analyze the distribution of lightcurve skewness for quasars and find that is centered on zero. We find that about 1/4 of the variable stars are RR Lyrae stars, and that only 0.5% of stars from the main stellar locus are variable at the 0.05 mag level. The distribution of lightcurve skewness in the g-r vs. u-g color-color diagram on the main stellar locus is found to be bimodal (with one mode consistent with Algol-like behavior). Using over six hundred RR Lyrae stars, we demonstrate rich halo substructure out to distances of 100 kpc. We extrapolate these results to expected performance by the Large Synoptic Survey Telescope and estimate that it will obtain well-sampled 2% accurate, multi-color lightcurves for ~2 million low-redshift quasars, and will discover at least 50 million variable stars. ", "machine_abstract": "The Sloan Digital Sky Survey (SDSS) is an ambitious project to map one quarter of the sky in five photometric bands, and obtain spectra for over 100 million galaxies, quasars, stars, and other celestial objects.  The survey has been completed on two thirds of its original footprint; it will be complete by 2008. This talk describes some of the science that can be done using this data set. It also discusses how astronomers are working together to make use of these vast resources. In particular I describe my own work on studying galaxy clusters as dark matter halos, measuring their masses through gravitational lensing effects, and determining their evolution across cosmic time. Finally, I discuss plans for future surveys which will build upon SDSS's successes. The Sloan Digital Sky Survey is an ambitious project to measure the positions and redshifts of more than 100 million galaxies, quasARS, stars, and other objects in our Universe.", "paraphrased_abstract": "This talk explains the history of the Sloan Sky Survey, the most ambitious and ambitious survey of our galaxy. The survey will be carried out in a quarter of a century and will cover more than a quarter of the sky. It will cover a quarter of the sky in five dimensions, and will provide data for over 100 million galaxies, quasars, stars, and the like. I will also discuss the plans for future surveys that will take advantage of the data from this survey. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3813, "title": "The Curious Case of NGC6908", "abstract": "  The object NGC6908 was once thought to be simply a surface-brightness enhancement in the eastern spiral arm of the nearby spiral galaxy NGC6907. Based on an examination of near-infrared imaging, the object is shown in fact to be a lenticular S0(6/7) galaxy hidden in the optical glare of the disk and spiral structure of the larger galaxy. New radial velocities of NGC6908 (3,060+/-16 (emission); 3,113+/-73 km/s (absorption)) have been obtained at the Baade 6.5m and the duPont 2.5m telescopes at Las Campanas, Chile placing NGC6908 at the same expansion-velocity distance as NGC6907 (3,190+/-5 km/s), eliminating the possibility of a purely chance line-of-sight coincidence. The once-enigmatic asymmetries in the disk and outer spiral structure of NGC6907 are now explained as being due to an advanced merger event. Newly discovered tails and debris in the outer reaches of this galaxy further support the merger scenario for this system. This pair of galaxies is a rather striking example of two objects discovered over 100 years ago, whose true nature was lost until modern detectors operating at infrared wavelengths gave us a new (high-contrast) look. Other examples of embedded merger remnants may also reveal themselves in the growing samples of near-infrared imaging of nearby galaxies; and a pilot study does reveal several other promising candidates for follow-up observations. ", "machine_abstract": "The Hubble Space Telescope has revealed the most detailed view yet of an active galactic nucleus (AGN) in the galaxy NGC6908, located about 300 million light years away.  The AGN is powered by supermassive black holes that are surrounded by bright clouds of gas and dust called torii.   This image shows how these torii appear when they are illuminated by powerful radiation coming out of the central engine of the AGN.   ... Full text here .     Image credits: NASA, ESA, STScI, A. Simionescu et al. (University of Leicester), DSS2, Digitized Sky Survey 2.0, Aladin sky atlas developed at CDS, Strasbourg Observatory; NRAO/AUI/NSF, National Radio Astronomy Observatory, which is operated by Associated Universities Inc., under cooperative agreement with the National Science Foundation. This work was supported by NASA grant NNX10AD65G to University of Leicester.", "paraphrased_abstract": "This work was funded by NASA grant NNX10AD65G to the University of Leicester. The Hubble Space Telescope has produced the most detailed view yet of an active galactic nucleus in NGC6908, located about 300 million light years away. The AGN consists of supermassive black holes which are surrounded by clouds of gas and dust. This image shows how these clouds are illuminated by the powerful radiation of the central engine of the AGN. This ikon is a heliostat. The Hubble Space Telescope has made the first comprehensive view of an active galactic nucleus in NGC6908, which is located about 300 million light years away. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2296, "title": "Extracting the ${\\bar q}q$ condensate for light quarks beyond the chiral limit in models of QCD", "abstract": "  It has recently been suggested \\cite{Chang:2006bm} that a reliable and unambiguous definition of the non-perturbative massive quark condensate could be provided by considering a non positive-definite class of solutions to the Schwinger Dyson Equation for the quark propagator. In this paper we show that this definition is incomplete without considering a third class of solutions. Indeed, studying these three classes reveals a degeneracy of possible condensate definitions leading to a whole range of values. However, we show that the {\\it physical} condensate may in fact be extracted by simple fitting to the Operator Product Expansion, a procedure which is stabilised by considering the three classes of solution together. We find that for current quark masses in the range from zero to 25 MeV or so (defined at a scale of 2 GeV in the $\\bar{MS}$ scheme), the dynamically generated condensate increases from the chiral limit in a wide range of phenomenologically successful models of the confining QCD interaction. Lastly, the role of a fourth class of noded solutions is briefly discussed. ", "machine_abstract": "We present an extension to the method developed by Shuryak and Verbaarschot, which allows one to extract the quark condensates $\\bar{q}q$ with $q\\ne u,d$ directly from lattice data on hadronic correlation functions. The method is applied to quenched SU(3) gauge configurations generated at several values of the bare strange-quark mass $m$S. We find that the results are consistent with those obtained using the standard procedure based on the operator product expansion (OPE). In particular, we obtain the following numerical values:     ms/m0 = 0.1 :  < \\bar s s > / < \\bar u d >= 1.00 \u00b1 0.01 , < \\bar c c > / < \\bar ud >= 0.10 \u00b1 0.02 ;   ms/m0 = -0.2 :  < \\bar u d > / < \\bar s s >= 1.03 \u00b1 0.04 , < \\bar c s > / < \\ud >= 0.11 \u00b1 0.06 .", "paraphrased_abstract": ", d== 0.1 ; d==  bar s s, d== 1.03  0.04 ; d== bar c c, d== 0.09  0.02 ; d== -0.2 : d== bar u d, d== 1.03  0.04, d== bar c c, d== -0.09 ; d==  bar c c, d==  ud,   ud,         ud,   ud,                  ,     ,        ,    ,        ,    ,     ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3759, "title": "Identifying network communities with a high resolution", "abstract": "  Community structure is an important property of complex networks. An automatic discovery of such structure is a fundamental task in many disciplines, including sociology, biology, engineering, and computer science. Recently, several community discovery algorithms have been proposed based on the optimization of a quantity called modularity (Q). However, the problem of modularity optimization is NP-hard, and the existing approaches often suffer from prohibitively long running time or poor quality. Furthermore, it has been recently pointed out that algorithms based on optimizing Q will have a resolution limit, i.e., communities below a certain scale may not be detected. In this research, we first propose an efficient heuristic algorithm, Qcut, which combines spectral graph partitioning and local search to optimize Q. Using both synthetic and real networks, we show that Qcut can find higher modularities and is more scalable than the existing algorithms. Furthermore, using Qcut as an essential component, we propose a recursive algorithm, HQcut, to solve the resolution limit problem. We show that HQcut can successfully detect communities at a much finer scale and with a higher accuracy than the existing algorithms. Finally, we apply Qcut and HQcut to study a protein-protein interaction network, and show that the combination of the two algorithms can reveal interesting biological results that may be otherwise undetectable. ", "machine_abstract": "We present an algorithm for identifying the community structure in networks, which is based on optimizing modularity locally and globally at once. The local optimization step uses a fast greedy agglomeration method to find good starting configurations for global optimization using simulated annealing. We show that our approach outperforms other state-of-the-art methods by finding higher quality solutions faster than competing algorithms. Our results are obtained on both synthetic benchmark graphs as well as real-world social networks. Identifying the community structure in complex networks has been one of the most active areas of research over recent years [1] . In this context, many different approaches have been proposed ranging from purely statistical techniques [2] , spectral clustering [3] or hierarchical clustering [4] to more sophisticated heuristics such as evolutionary algorithms [5] . In particular, Newman introduced the concept of modularity [6] , which measures how much better a given partitioning into communities fits the data compared to a random graph model. This measure has become very popular since it allows to compare partitions of different sizes and shapes [7, 8] . However, maximizing modularity is known to be NP-hard [9] and therefore requires efficient heuristic search strategies [10] . While there exist several exact [11] and approximate [12] solution methods for modularity maximization problems, they often suffer from scalability issues when applied to large networks [13] .", "paraphrased_abstract": "We have a new approach to identifying the structure of complex networks: it uses the deterministic method, a fast greedy agglomeration method to search for a good starting point for the global optimization, which is simulated annealing. We have shown that our method is superior to other existing methods, and it has the advantages of being fast enough to find the best solutions. We have recently been developing several different methods for defining the structure of complex networks: the simplest is statistical, spectral clustering, hierarchical clustering, etc., and the more sophisticated are the evolutionary algorithms, such as the evolutionary algorithms. However, the maximizing of the modularity is difficult and requires an efficient heuristic search. We have here a method for identifying the community structure in complex networks, and it is based on a method that, by focusing on local and global optimization, finds optimal configurations and results, and that, when we consider annealing, our method can find the highest quality solutions faster than others. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1311, "title": "Galaxy clustering constraints on deviations from Newtonian gravity at cosmological scales II: Perturbative and numerical analyses of power spectrum and bispectrum", "abstract": "  We explore observational constraints on possible deviations from Newtonian gravity by means of large-scale clustering of galaxies. We measure the power spectrum and the bispectrum of Sloan Digital Sky Survey galaxies and compare the result with predictions in an empirical model of modified gravity. Our model assumes an additional Yukawa-like term with two parameters that characterize the amplitude and the length scale of the modified gravity. The model predictions are calculated using two methods; the second-order perturbation theory and direct N-body simulations. These methods allow us to study non-linear evolution of large-scale structure. Using the simulation results, we find that perturbation theory provides reliable estimates for the power spectrum and the bispectrum in the modified Newtonian model. We also construct mock galaxy catalogues from the simulations, and derive constraints on the amplitude and the length scale of deviations from Newtonian gravity. The resulting constraints from power spectrum are consistent with those obtained in our earlier work, indicating the validity of the previous empirical modeling of gravitational nonlinearity in the modified Newtonian model. If linear biasing is adopted, the bispectrum of the SDSS galaxies yields constraints very similar to those from the power spectrum. If we allow for the nonlinear biasing instead, we find that the ratio of the quadratic to linear biasing coefficients, b_2/b_1, should satisfy -0.4 < b_2/b_1<0.3 in the modified Newtonian model. ", "machine_abstract": "We present the results of our analysis of galaxy clustering data in order to constrain possible deviations from general relativity (GR) at large distances, using both perturbative and fully nonlinear approaches. We consider two classes of models for modified gravity: f(R) theories with an additional scalar degree of freedom and Dvali-Gabadadze-Porrati brane-world scenarios. In particular we focus on the effects that these modifications have on the matter power spectrum and bispectra. The main goal is to determine whether current observations are able to rule out or put strong bounds on such alternative gravitational theories.     For this purpose we use the most recent measurements of the galaxy correlation function by the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). These include the projected correlation functions wp(rp), measured up to rp = 20h-1Mpc/h, as well as the three-point correlation functions w3p(rp, r\u03c0), where rp and r\u03c0 denote separations parallel and perpendicular to the line-of-sight respectively. Our theoretical predictions are obtained within perturbation theory and through N-body simulations.", "paraphrased_abstract": "The latter is the first of the three-phase correlations for the sky of the Southern Ocean, which were computed at the southwestern boundary of the sky at rp = 20\u00b0-1Mpc. and a third-phase correlation of rp and r, whose intervals are parallel and perpendicular to the line of sight. We apply to the results of the astronomical analysis, in order to find the limits of certain alternative theories of gravity at large distances. In particular, we examine the effects of this modification of gravity on the spectral spectrum and the spectra of matter. We consider two classes of modified gravity: f(R) theories, with a higher degree of freedom, and Dvali-Gabadadze-Porrati brane world. Our goal is to know whether the present observations can be relied upon to substantiate or constrain these new theories. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1309, "title": "Robust Multi-Cellular Developmental Design", "abstract": "  This paper introduces a continuous model for Multi-cellular Developmental Design. The cells are fixed on a 2D grid and exchange \"chemicals\" with their neighbors during the growth process. The quantity of chemicals that a cell produces, as well as the differentiation value of the cell in the phenotype, are controlled by a Neural Network (the genotype) that takes as inputs the chemicals produced by the neighboring cells at the previous time step. In the proposed model, the number of iterations of the growth process is not pre-determined, but emerges during evolution: only organisms for which the growth process stabilizes give a phenotype (the stable state), others are declared nonviable. The optimization of the controller is done using the NEAT algorithm, that optimizes both the topology and the weights of the Neural Networks. Though each cell only receives local information from its neighbors, the experimental results of the proposed approach on the 'flags' problems (the phenotype must match a given 2D pattern) are almost as good as those of a direct regression approach using the same model with global information. Moreover, the resulting multi-cellular organisms exhibit almost perfect self-healing characteristics. ", "machine_abstract": "We present an approach to robust multi-cellular developmental design that combines the advantages of both bottom-up and top-down approaches, while avoiding their respective disadvantages. The proposed method is based on a novel concept called \"embryonic development space (EDS)\" which represents all possible designs in terms of cell types and connections between them. We show how EDS can be used for efficient exploration of different designs by applying evolutionary algorithms. Finally we demonstrate our approach using two case studies - one synthetic and another biological. Robustness has been recognized as one of the most important features of living systems [1] . In particular, it plays crucial role during embryogenesis when cells differentiate into various tissues and organs [2] , but also later in life [3] . In this work we propose a new computational framework for designing robust multicellular systems. Our approach combines the advantages of both \"bottom-up\" [4] and \"top-down\" [5] methods, while overcoming some of their limitations. Bottom-up methods are typically applied to model cellular differentiation [6] or morphogenetic processes [7] . They usually start with a single cell type and then evolve towards more complex structures through successive divisions and/or mutations [8] . Top-down methods use genetic programming [9] or other optimization techniques [10] to search for optimal solutions within pre-defined constraints [11] . However, these methods often require extensive tuning of parameters [12] and may get stuck at local optima [13] . Our approach uses a novel concept called \"embryo-", "paraphrased_abstract": "\u201cIt is known that the structure of cells is a key factor in the development of tissues and organs, but also in the later development of human life. We have developed a new computational framework for the development of complex systems, and in that framework we combine the advantages of both the bottom-up and the top-down methods without the limitations of the other two methods. In our system we use a novel concept called embryo-genesis, which represents all possible cell types and connections between them. It is the result that we show how embryo-genesis can be used to explore the various designs of cells by using evolutionary methods. Our method is based on the novel concept of embryo-development, which represents all the possible cell types and connections. It is used to explore and discover different designs by applying evolutionary algorithms. Our method has the advantages of both the bottom-up and top-down methods, but with a few limitations. In our work we propose an approach for the construction of multicellular systems, combining the advantages of both the bottom-up and top-down methods, but without overcoming their shortcomings. In the previous work we have applied a synthesis of the top-down method for the modeling of cellular differentiation or morphogenetics, in which we begin with a single cell type and grow from it to a complex one, by dividing and/or mut", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.002, "title": "The Mass Function of Active Black Holes in the Local Universe", "abstract": "  We present the first measurement of the black hole (BH) mass function for broad-line active galaxies in the local Universe. Using the ~9000 broad-line active galaxies from the Fourth Data Release of the Sloan Digital Sky Survey, we construct a broad-line luminosity function that agrees very well with the local soft X-ray luminosity function. Using standard virial relations, we then convert observed broad-line luminosities and widths into BH masses. A mass function constructed in this way has the unique capability to probe the mass region <10^6 M_sun, which, while insignificant in terms of total BH mass density, nevertheless may place important constraints on the mass distribution of seed BHs in the early Universe. The characteristic local active BH has a mass of ~10^7 M_sun radiating at 10% of the Eddington rate. The active fraction is a strong function of BH mass; at both higher and lower masses the active mass function falls more steeply than one would infer from the distribution of bulge luminosity. The deficit of local massive radiating BHs is a well-known phenomenon, while we present the first robust measurement of a decline in the space density of active BHs at low mass. ", "machine_abstract": "We present an analysis of the mass function (MF) of active black holes (BHs), based on the sample of quasars with redshifts z < 0.5 and luminosities L > 10^44 erg/s, selected by Shen et al. (2007). We use two different methods to estimate BH masses for this sample - virial method and continuum-fitting method. The MF is constructed using both these estimates separately as well as their combination. Our results are compared against previous studies which used similar samples but different techniques to determine BH masses. We find that our best fit Schechter parameters agree within errors with those obtained previously. However, we also find evidence for a possible excess at low-mass end when we combine all three data sets together. This excess could be due to incompleteness or biases in the selection criteria adopted here. In addition, we compare our results with theoretical predictions made by Hopkins et al. (2006a) and Shankar et al. (2009b) .", "paraphrased_abstract": "It was found that our best estimate of the BH mass was within the range of those calculated in previous studies, based on similar cases but with different methods. The results of this study were compared with theoretical predictions of Hopkins and Shankar, 2006a and 2009b. The result of this study was that our results were consistent with those of the previous studies that used similar samples, but a different approach. Our results were compared with previous studies that used similar samples but different methods to calculate the BH mass. We present a numerical analysis of the mass of active black holes, using quasars with redshifts of z0.5 and luminosities of 1044 erg/s. Our calculations used both the virial method and the continuum method, and subsequently applied these to the MF. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3431, "title": "Late-Type Red Supergiants: Too Cool for the Magellanic Clouds?", "abstract": "  We have identified seven red supergiants (RSGs) in the Large Magellanic Cloud (LMC) and four RSGs in the Small Magellanic Cloud (SMC), all of which have spectral types that are considerably later than the average type observed in their parent galaxy. Using moderate-resolution optical spectrophotometry and the MARCS stellar atmosphere models, we determine their physical properties and place them on the H-R diagram for comparison with the predictions of current stellar evolutionary tracks. The radial velocities of these stars suggest that they are likely all members of the Clouds rather than foreground dwarfs or halo giants. Their locations in the H-R diagram also show us that those stars are cooler than the current evolutionary tracks allow, appearing to the right of the Hayashi limit, a region in which stars are no longer in hydrodynamic equilibrium. These stars exhibit considerable variability in their V magnitudes, and three of these stars also show changes in their effective temperatures (and spectral types) on the time-scales of months. One of these stars, [M2002] SMC 055188, was caught in an M4.5 I state, as late as that seen in HV 11423 at its recent extreme: considerable later, and cooler, than any other supergiant in the SMC. In addition, we find evidence of variable extinction due to circumstellar dust and changes in the stars' luminosities, also consistent with our recent findings for HV 11423 - when these stars are hotter they are also dustier and more luminous. We suggest that these stars have unusual properties because they are in an unstable (and short-lived) evolutionary phase. ", "machine_abstract": "We have obtained near-infrared (NIR) spectroscopy and photometry of two late-type red supergiants in the Small Magellanic Cloud, SMC-RSGC1 and SMC-RSGC2. We find that these stars are cooler than previously thought; their effective temperatures range between 3200 K and 3400 K. The NIR spectra show strong CO absorption bands characteristic of cool supergiants. These results suggest that there is an additional population of cool RSGs in the LMC/SMC system beyond those found by Massey & Olsen (2003) . This new sample may be useful to study the evolution of massive stars at low metallicity.     Keywords: Red supergiant, Near-infrared spectrum, Small Magellanic Cloud. 1 Introduction     Red supergiants (RSGs; M > 8M\u2299) play important roles as progenitors of type II supernovae and as sources of dust formation in galaxies. However, our understanding of RSGs has been hampered by difficulties associated with obtaining accurate distances to individual objects due to their large intrinsic luminosities and high extinctions. In addition, it was not until recently that systematic surveys were conducted to search for RSGs outside the Milky Way Galaxy. As a result, only about 100 Galactic RSGs are known today compared to thousands of OB stars. To date, most studies on extragalactic RSGs have focused on the Large Magellanic Cloud (LMC), where several hundred RSGs have been identified using optical color-magnitude diagrams (CMDs). However, this method cannot identify RSGs fainter than V = 16 mag or more obscured by interstellar extinction. Therefore, many faint and/or heavily extincted RSGs remain undiscovered even in the LMC. Recently, Massey et al. (2006a) discovered two extremely bright RSGs in the Small Magellanics Cloud (SMC) based on infrared (IR) CMDs constructed from 2MASS data. They suggested that such bright RSGs could also exist in other nearby dwarf irregular galaxies like NGC", "paraphrased_abstract": "Moreover, we have not yet been able to find a single globular RSG outside the Milky Way, and in the LMC it is difficult to find a single globular RSG, which is far from a globular star, but is in the LMC, where we can only find some hundred globular RSGs. These are stars of the late type, and they are the genitors of type II supernovae, and the sources of dust in the galaxies. We have recently been able to find a pair of globular RSGs in the SMC, based on a pixel-like CMD of 2MASS, and they have shown that they are more globular than usual, and so they are not known in the LMC. Consequently, we have found a few more globular RSGs in the SMC, namely, SMC-RSGC and SMC-RSGC2. These globular stars are cooler than the average star; their effective temperature is 3200 K, and the NIR spectra have the CO-absorption of the stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2066, "title": "SDSS J121811.0+465501.2: a new Low Surface Brightness Galaxy with low metallicity", "abstract": "  We serendipitously find a new nearby Low Surface Brightness (LSB) galaxy from SDSS database. We estimate oxygen abundance of its H II region SDSS J121811.0+465501.2 from electron temperature, as well as for another H II region, SDSS J135440.5+535309.6, located in irregular LSB galaxy UGC 8837. These two extragalactic H II regions were classified as stars in the SDSS-DR4 database, and were found occasionally by us in the automatic recognition and classification on stellar spectra.Their optical spectra show obvious emission lines, i.e., strong [O III]4959, 5007, Balmer emission lines, but very weak [N II]6548,6583 and [S II]6317,6731, which could indicate that they are metal-poor star-forming regions. The derived oxygen abundances of the two objects are 12+log(O/H) ~ 7.88+-0.30 and 7.70+-0.30, respectively. The host of the H II region SDSS J121811.0+465501.2 is identified as a new nearly edge-on LSB disc galaxy (almost without bulge) with the B-band central surface brightness mu_0(B) as 23.68 mag arcsec^{-2} and inclination angle as ~75 degree by using the GIM2D software to analyze its g- and r-band images independently. It is a nearby dwarf galaxy with redshift z~0.00157, disk scale-length ~0.40 kpc and B-band absolute magnitude M_B ~ -13.51 mag. The very low oxygen abundances of these two objects confirm the low metallicities of LSB galaxies. ", "machine_abstract": "SDSS J121811+465501 is an extremely faint galaxy discovered in the Sloan Digital Sky Survey (SDSS). It has been classified as a dwarf elliptical galaxy, but its surface brightness profile shows that it is actually a very extended and diffuse object. We have obtained deep optical spectroscopy for this galaxy using the Gemini Multi-Object Spectrograph on Gemini North telescope to study its physical properties. The spectrum reveals strong Balmer absorption lines indicating young stellar populations. Its oxygen abundance is found to be one third solar or lower, which makes SDSS J121811+ 465501 the lowest-metallicity galaxy known at z=0.1. This galaxy may represent a population of galaxies formed early during cosmic time when star formation was more intense than today. Keywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness", "paraphrased_abstract": "The atoms are very faint and the surface brightness is very thin. It is a young, extended, and dense galaxy. The author has a very faint and light atom, and it was discovered in the Sloan Digital Sky Survey by the Gemini Observatory. It was categorized as a dwarf elliptical galaxy, but its surface brightness indicates that it is really an extended, diffuse object. The atoms of the atoms were three times as high as the atoms of the atoms, and their oxygen abundance was one third of the solar system, which means that this galaxy is the lowest metallicity in the known universe. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0309, "title": "The Crossover from Impurity to Valence Band in Diluted Magnetic Semiconductors: The Role of the Coulomb Attraction by Acceptor", "abstract": "  The crossover between an impurity band (IB) and a valence band (VB) regime as a function of the magnetic impurity concentration in models for diluted magnetic semiconductors (DMS) is studied systematically by taking into consideration the Coulomb attraction between the carriers and the magnetic impurities. The density of states and the ferromagnetic transition temperature of a Spin-Fermion model applied to DMS are evaluated using Dynamical Mean-Field Theory (DMFT) and Monte Carlo (MC) calculations. It is shown that the addition of a square-well-like attractive potential can generate an IB at small enough Mn doping $x$ for values of the $p-d$ exchange $J$ that are not strong enough to generate one by themselves. We observe that the IB merges with the VB when $x >= x_c$ where $x_c$ is a function of $J$ and the Coulomb attraction strength $V$. Using MC calculations, we demonstrate that the range of the Coulomb attraction plays an important role. While the on-site attraction, that has been used in previous numerical simulations, effectively renormalizes $J$ for all values of $x$, an unphysical result, a nearest-neighbor range attraction renormalizes $J$ only at very low dopings, i.e., until the bound holes wave functions start to overlap. Thus, our results indicate that the Coulomb attraction can be neglected to study Mn doped GaSb, GaAs, and GaP in the relevant doping regimes, but it should be included in the case of Mn doped GaN that is expected to be in the IB regime. ", "machine_abstract": "We study theoretically the crossover between impurity and valence band states for diluted magnetic semiconductors (DMS). We show that this crossover is determined by the competition between the exchange interaction with conduction electrons, which favors impurity-like behavior at low carrier concentration, and the Coulomb attraction by acceptor ions, which leads to formation of bound hole pairs around each ion and thus promotes valence-band like behavior.  In particular we find that the crossover occurs when the density of holes becomes comparable to the density of acceptor ions. This result explains why the crossover has been observed only recently in experiments on GaAs-based DMSs but not earlier in Si-based ones where the density of acceptor atoms was much higher than the density of doped carriers. Our results also explain recent experimental observations of an unusual temperature dependence of the magnetization near the crossover point. Finally our theory predicts that the crossover should be observable even if there are no free carriers present in the system due to compensation effects.", "paraphrased_abstract": "Hence, we assume that this cross-section is observed, even if there is no free electron in the system, because of the compensation. The theory is that the cross-section is obtained by the collision of the equilateral atoms with the conduction electrons, which favors the impurity-like behavior of low carriers, and by the Coulomb attraction of the ions, which produces bound hole pairs around each ion, thus promoting the equilateral atoms. This result, together with our experimental results, is the reason why it has not been observed until recently in experiments on GaAs-based DMSs, but only in the Si-based ones, where the density of the acceptor atoms is much higher than that of the doped carriers. The separation of the impurity and the valence band is explained by the differential strength of the two electrons, the collision of the two electrons and the valence band, and the repulsion of the valence band. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2159, "title": "Accretion Disk Illumination in Schwarzschild and Kerr Geometries: Fitting Formulae", "abstract": "  We describe the methodology and compute the illumination of geometrically thin accretion disks around black holes of arbitrary spin parameter $a$ exposed to the radiation of a point-like, isotropic source at arbitrary height above the disk on its symmetry axis. We then provide analytic fitting formulae for the illumination as a function of the source height $h$ and the black hole angular momentum $a$. We find that for a source on the disk symmetry axis and $h/M > 3$, the main effect of the parameter $a$ is allowing the disk to extend to smaller radii (approaching $r/M \\to 1$ as $a/M \\to 1$) and thus allow the illumination of regions of much higher rotational velocity and redshift. We also compute the illumination profiles for anisotropic emission associated with the motion of the source relative to the accretion disk and present the fractions of photons absorbed by the black hole, intercepted by the disk or escaping to infinity for both isotropic and anisotropic emission for $a/M=0$ and $a/M=0.99$. As the anisotropy (of a source approaching the disk) increases the illumination profile reduces (approximately) to a single power-law, whose index, $q$, because of absorption of the beamed photons by the black hole, saturates to a value no higher than $q \\gtrsim 3$. Finally, we compute the fluorescence Fe line profiles associated with the specific illumination and compare them among various cases. ", "machine_abstract": "We present fitting formulae for the illumination of accretion disks by hot spots, as seen in Schwarzschild and rotating black holes (Kerr). The formulae are derived using ray tracing through the disk atmosphere with an approximate treatment of Compton scattering. We find that the dependence on the spin parameter is weak when the spot size is small compared to the radius at which photons decouple from matter. For larger spots we find that the effect increases strongly towards prograde spins. Our results can be used to estimate the effects of relativistic Doppler boosting and gravitational lensing on observed spectra. They may also provide useful input into models of X-ray reflection spectroscopy.  Introduction  Accreting black holes produce bright emission lines in their X-ray spectrum due to reprocessing of hard X-rays emitted near the event horizon by cold material orbiting close to the equatorial plane. These features have been studied extensively over many years both observationally and theoretically (see Reynolds & Nowak 2003 , Done et al 2004 . In particular, they show strong red-shifts indicating that the emitting gas orbits rapidly around the black hole. This rapid rotation causes additional shifts in energy due to relativistic Doppler boosts and gravitational lensing. Relativistic effects become more important if the emitting region has a high degree of rotational support or is viewed nearly face-on. It is therefore necessary to take these effects into account when interpreting observations of such systems.   In this work we consider the case where the illuminating source is located above the disk surface but below its photosphere. Such sources include magnetic flares produced within the disk itself or active regions associated with the inner edge of the disk. We assume that the disk is optically thick so that all radiation reaching it is absorbed and re-emitted locally. We use Monte Carlo simulations to calculate the emergent flux from the disk under various assumptions about the geometry of the system.  The main goal of our study was to develop simple analytical expressions describing how the shape of the line profile depends on the properties of the system. To do this we performed extensive numerical calculations covering a wide range", "paraphrased_abstract": "We have developed the most precise formulation for the emanation of the emitted light from the evaporator, based on the theory of Reynolds & Nowak, and Done et al., 2004. We have also derived a general formula for the emanation of the evaporator based on the ray-tracing from the atmosphere of the disk and on the emanation of Compton scattering. In our study, we have considered the case where the source is not above the surface but below the photosphere. Such sources include magnetic flares generated within the disk itself or active areas connected with the inner edge of the disk. The evaporation of the gas is rapid, causing red shifts in the radiating gas, which, due to the rapid rotation of the evaporation gas, produces an additional ejection of energy from relativistic Doppler and gravitational lensing. Our calculations show that the effects of relativistic Doppler and gravitational lensing are weaker when the density is small compared with the radius of refraction of the matter; we find that it becomes stronger with larger sizes, towards the prograde spins. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3715, "title": "Efficient model chemistries for peptides. I. Split-valence Gaussian basis sets and the heterolevel approximation in RHF and MP2", "abstract": "  We present an exhaustive study of more than 250 ab initio potential energy surfaces (PESs) of the model dipeptide HCO-L-Ala-NH2. The model chemistries (MCs) used are constructed as homo- and heterolevels involving possibly different RHF and MP2 calculations for the geometry and the energy. The basis sets used belong to a sample of 39 selected representants from Pople's split-valence families, ranging from the small 3-21G to the large 6-311++G(2df,2pd). The reference PES to which the rest are compared is the MP2/6-311++G(2df,2pd) homolevel, which, as far as we are aware, is the more accurate PES of a dipeptide in the literature. The aim of the study presented is twofold: On the one hand, the evaluation of the influence of polarization and diffuse functions in the basis set, distinguishing between those placed at 1st-row atoms and those placed at hydrogens, as well as the effect of different contraction and valence splitting schemes. On the other hand, the investigation of the heterolevel assumption, which is defined here to be that which states that heterolevel MCs are more efficient than homolevel MCs. The heterolevel approximation is very commonly used in the literature, but it is seldom checked. As far as we know, the only tests for peptides or related systems, have been performed using a small number of conformers, and this is the first time that this potentially very economical approximation is tested in full PESs. In order to achieve these goals, all data sets have been compared and analyzed in a way which captures the nearness concept in the space of MCs. ", "machine_abstract": "The present work is concerned with efficient methods to calculate molecular properties, such as vibrational frequencies or electronic excitation energies, using ab initio quantum chemical techniques. The main focus lies on the calculation of these quantities for large systems containing many atoms (e.g., proteins). In this context we have developed an approach which allows us to reduce computational costs significantly by combining two different approximations. First, we use split-valence Gaussian basis functions instead of conventional Cartesian Gaussians. Second, we apply the so-called \"heterolevel\" approximation within restricted Hartree-Fock theory and second-order Moller-Plesset perturbation theory. This combination leads to very accurate results at low computational cost even if applied to relatively large molecules like polypeptides. We demonstrate that our method can be used successfully to study the influence of solvent effects on the structure and stability of small peptides. Finally, we show how it may also be employed to investigate excited-state processes occurring during photochemical reactions.", "paraphrased_abstract": "This method is particularly well suited for the study of small peptides and their structure. It is also well suited to the study of electrochemical reactions in the presence of solvents. In the present work, we have developed a method to reduce the computational cost of our calculations by using two different approximations: the first is to use a symmetrical Gaussian basis function, which is based on the so-called \u201cHenry-Fock\u201d model and the second the second-order Moller-Plesset model. We show that this method is capable of predicting the structure and stability of small proteins in the presence of a solvent, and then to analyze the phenomenon of a photochemical reaction. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.151, "title": "Aromatic emission from the ionised mane of the Horsehead nebula", "abstract": "  We study the evolution of the Aromatic Infrared Bands (AIBs) emitters across the illuminated edge of the Horsehead nebula and especially their survival and properties in the HII region. We present spectral mapping observations taken with the Infrared Spectrograph (IRS) at wavelengths 5.2-38 microns. A strong AIB at 11.3 microns is detected in the HII region, relative to the other AIBs at 6.2, 7.7 and 8.6 microns. The intensity of this band appears to be correlated with the intensity of the [NeII] at 12.8 microns and of Halpha, which shows that the emitters of the 11.3 microns band are located in the ionised gas. The survival of PAHs in the HII region could be due to the moderate intensity of the radiation field (G0 about 100) and the lack of photons with energy above about 25eV. The enhancement of the intensity of the 11.3 microns band in the HII region, relative to the other AIBs can be explained by the presence of neutral PAHs. Our observations highlight a transition region between ionised and neutral PAHs observed with ideal conditions in our Galaxy. A scenario where PAHs can survive in HII regions and be significantly neutral could explain the detection of a prominent 11.3 microns band in other Spitzer observations. ", "machine_abstract": "We report on observations made with the Spitzer Space Telescope and Hubble Space Telescope (HST) that reveal aromatic infrared bands in the ionized gas surrounding the brightest star in the Horsehead Nebula, NGC 2264. The observed PAH features are consistent with those seen toward other H II regions excited by hot stars. We find no evidence for an enhanced abundance or unusual excitation conditions of these molecules compared to Galactic H II regions. This suggests that the physical processes responsible for the formation of PAHs may be similar in both environments. These results provide new insights into how PAHs form under extreme radiation fields.     Keywords: Infrared, PAH, Horsehead Nebula, Starburst galaxies     The Horsehead Nebula is one of several spectacular bipolar reflection nebulae associated with young open clusters in Orion's molecular cloud complex. It contains a central O-type star surrounded by a compact cluster of B- and A-type stars embedded within a dense shell of dusty material. Previous studies have shown that this region harbors large amounts of warm dust and cold molecular gas, as well as numerous Herbig-Haro objects indicative of ongoing mass ejection events. However, little has been known about its chemical composition until now because it lies behind thick clouds of interstellar extinction at optical wavelengths. Here we present mid-infrared spectroscopic data obtained using the Infrared Spectrograph aboard the Spitzer Space Telescope which show strong absorption features due to polycyclic aromatic hydrocarbons (PAHs). These features arise from vibrational modes of carbonaceous materials containing 50-200 atoms arranged in planar rings and chains. They are commonly found in photodissociation regions (PDRs), where ultraviolet photons emitted by nearby massive stars break apart hydrogen molecules, creating a layer of partially ionized gas between the illuminated surface of the molecular cloud and the neutral interior. Our analysis shows that the PAH features detected here are remarkably similar to those seen towards Galactic H II regions such as M17 SWex, suggesting that they originate from the same type of photoionization process.", "paraphrased_abstract": "The Horsehead Nebula is one of the few dazzling bipolar nebulae associated with young open clusters in Orion's cloud. The central O-type star surrounded by a cluster of B- and A-type stars, forming a thick cloud of dust, was investigated by previous studies, and showed that this region contained a great deal of warm dust and cold gas, and was populated with a great many Herbig-Haro objects, indicative of a mass ejection. The following spectral data of the Spitzer space telescope and the Hubble Space Telescope showed the presence of a pronounced aromatic band in the gas around the brightest star, NGC 2264, which was the brightest in the horse-head nebula. These spectra are based on the vibrational features of carbonaceous materials, containing from 50-200 atoms and a chain of ring and ring of molecules, and the spectra of which are remarkably similar to those of the H II stars, i.e., M17, 17, and 17, respectively, by the same photoionization. We have obtained, using Spitzer, the infrared spectroscopic spectroscopic spectrograph, observations from the mid-infrared region, and the results of the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0824, "title": "A Microscpoic Model for the Neutron Dynamic Structure Factor of Solid Methane in phase II", "abstract": "  We have constructed an approximate microscopic model for the neutron dynamic structure factor of solid methane in phase II. We expect our model to apply for neutron energies below 1\\textit{eV} at pressures near 1 bar and temperatures below 20K, where solid methane exists in a phase, called phase II, in which the tetrahedral hydrogens exist in a mixed phase of free rotors and hindered rotors. Certain matrix elements needed for the dynamic structure factor $S(Q, \\omega)$ are adapted from a calculation by Ozaki \\textit{et al.}\\cite{ozaki:3442}\\cite{ozaki2}. The model treats the effects of molecular translations, intramolecular vibrations and the free and hindered rotational degrees of freedom as uncoupled ones. Total scattering cross sections calculated from the model agree with the cross section measurements of Grieger\\cite{grieger:3161} and Whittemore\\cite{Wittemore} for the incident neutron energies of 0.5 \\textit{meV} $\\sim$ 1\\textit{eV}. The effective density of states extracted from the model was compared with the Harker $&$ Brugger frequency spectrum extracted from neutron scattering measurements conducted at 22K\\cite{HARKER1967}. ", "machine_abstract": "The neutron dynamic structure factor (NDSF) is calculated by using molecular dynamics simulation and the density functional theory with generalized gradient approximation(GGA). The NDSF shows two peaks at low wave number region, which are assigned to the translational motion of methane molecules. At high wave number region, one peak appears around 2 \u00c5-1 ,which corresponds to the libration mode of CH4 molecule. In addition, we find that there exists another peak near 3 \u00c5-1 . This new peak can be explained as the result of the rotation of CH3 group against the C-H bond direction. We also calculate the partial structure factors Sij(Q), where i and j represent different atoms or groups. From these results, it is found that the main contribution comes from the correlation between carbon atom and hydrogen atom. Finally, we compare our theoretical results with experimental data obtained by Hwang et al..", "paraphrased_abstract": "Then we computed the partial structure factor Sij(Q) by means of the tessellation of atoms or groups. It was calculated by a density functional theory and by the GGA. The NDSF was obtained from a combination of the dynamics of the methane molecules and the density functional theory of the density functional theory. In addition, the NDSF showed two peaks, which correspond to the translational motion of methane molecules. At the higher end, one peaks was seen, which resembled the libation mode of the CH4 molecule. Moreover, the NDSF showed a second peak near 3, which corresponds to the libation mode of the CH4 molecule. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4514, "title": "Hagedorn Strings and Correspondence Principle in AdS(3)", "abstract": "  Motivated by the possibility of formulating a strings/black hole correspondence in AdS space, we extract the Hagedorn behavior of thermal AdS_3 bosonic string from 1-loop partition function of SL(2,R) WZW model. We find that the Hagedorn temperature is monotonically increasing as the AdS radius shrinks, reaches a maximum of order of string scale set by the unitarity bound of the CFT for internal space. The resulting density of states near the Hagedorn temperature resembles the form as for strings in flat space and is dominated by the space-like long string configurations. We then argue a conjectured strings/black hole correspondence in AdS space by applying the Hagedorn thermodynamics. We find the size of the corresponding black hole is a function of the AdS radius. For large AdS radius a black hole far bigger than the string scale will form. On the contrary, when the AdS and string scales are comparable a string size black hole will form. We also examine strings on BTZ background obtained through SL(2,Z) transformation. We find a tachyonic divergence for a BTZ black hole of string scale size. ", "machine_abstract": "We study the correspondence principle between string theory on anti-de Sitter space-time (AdS(3)) and field theories at finite temperature, by using Hagedorn strings as probes. We show that the number density of Hagedorn strings is proportional to the free energy density of the thermal gas of massless particles in the dual field theory. This result implies that the entropy density of the thermal gas agrees with the Bekenstein-Hawking entropy density of black holes in AdS(3). In addition we find that the pressure of the thermal gas agrees exactly with the negative cosmological constant in AdS(3), which corresponds to the vacuum expectation value of the dilaton field in the dual conformal field theory. These results are consistent with the holographic principle proposed recently for strongly coupled gauge theories. The present work may be regarded as an extension of our previous works [Phys Rev D71:035010] and [JHEP 0804:0703] .", "paraphrased_abstract": "\u201cThese findings are in keeping with the Holographic Principle recently proposed for the theory of strongly coupled gauge theories. Our present work is therefore a continuation of our previous work: Phys Rev D71:035010 and JHEP 0804:0703. In addition, we find that the pressure of the gas is exactly in keeping with the negative cosmological constant in AdS(3), which corresponds to the vacuum expectation of the dilaton field in the dual conformal field theory. This result is consistent with the Holographic Principle proposed recently for the theory of strongly coupled gauge theories. The number density of the Hagedorn strings corresponds to the density of the thermal gas of massless particles in the Dual Conformal Field Theory. This means that the entropy density of the gas is equal to the Bekenstein-Hawking entropy density of black holes in the Dual Conformal Field Theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1772, "title": "Acceleration of the Universe driven by the Casimir force", "abstract": "  We investigate an evolutional scenario of the FRW universe with the Casimir energy scaling like $(-)(1+z)^4$. The Casimir effect is used to explain the vacuum energy differences (its value measured from astrophysics is so small compared to value obtained from quantum field theory calculations). The dynamics of the FRW model is represented in terms of a two-dimensional dynamical system to show all evolutional paths of this model in the phase space for all admissible initial conditions. We find also an exact solution for non flat evolutional paths of Universe driven by the Casimir effect. The main difference between the FRW model with the Casimir force and the $\\Lambda$CDM model is that their generic solutions are a set of evolutional paths with a bounce solution and an initial singularity, respectively. The evolutional scenario are tested by using the SNIa data, FRIIb radiogalaxies, baryon oscillation peak and CMB observation. We compare the power of explanation of the model considered and the $\\Lambda$CDM model using the Bayesian information criterion and Bayesian factor. Our investigation of the information criteria of model selection showed the preference of the $\\Lambda$CDM model over the model considered. However the presence of negative like the radiation term can remove a tension between the theoretical and observed primordial ${}^4$He and D abundance. ", "machine_abstract": "We show that the acceleration of the universe is due to the repulsive effect of vacuum fluctuations, which are responsible for the so-called Casimir force between parallel plates. We find that this force can be expressed in terms of the energy density and pressure associated with these fluctuations as follows:         where  is the distance between two parallel plates separated by a dielectric medium.  The first term on the right-hand side represents the usual attractive van der Waals interaction while the second one corresponds to the repulsive contribution arising from quantum fluctuations. This expression shows explicitly how the repulsive Casimir force depends on the separation between the plates through the dependence of the energy density and the pressure on . In order to obtain an estimate of the magnitude of the repulsive Casimir effect we consider a model consisting of a pair of parallel plates immersed in a plasma-like medium. For such system it turns out that the repulsive Casimir contribution dominates over the attractive van der Waals part at distances larger than about 1 mm.", "paraphrased_abstract": "The first term in the righthand column is the characteristic van der Waals interaction, the second term the characteristic contribution of quantum fluctuations. In this way, the resemblance is quite clear: in the space of a few hundred meters the resemblance is apparent. We then consider a model of the resemblance of two parallel plates immersed in a plasma medium. We observe that this resemblance consists in the discomfiture of the two plates by a converging pressure, and consists in the absorption of the energy, in the direction of the discomfiture of the plasma. We show that the acceleration of the universe is due to the discomfiture of the vacuum, the so-called Casimir force. This force can be expressed by defining the distance between two parallel plates with the corresponding pressure and the energy density. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.26, "title": "On the cycling operation in braid groups", "abstract": "  The cycling operation is a special kind of conjugation that can be applied to elements in Artin's braid groups, in order to reduce their length. It is a key ingredient of the usual solutions to the conjugacy problem in braid groups. In their seminal paper on braid-cryptography, Ko, Lee et al. proposed the {\\it cycling problem} as a hard problem in braid groups that could be interesting for cryptography. In this paper we give a polynomial solution to that problem, mainly by showing that cycling is surjective, and using a result by Maffre which shows that pre-images under cycling can be computed fast. This result also holds in every Artin-Tits group of spherical type.   On the other hand, the conjugacy search problem in braid groups is usually solved by computing some finite sets called (left) ultra summit sets (left-USS), using left normal forms of braids. But one can equally use right normal forms and compute right-USS's. Hard instances of the conjugacy search problem correspond to elements having big (left and right) USS's. One may think that even if some element has a big left-USS, it could possibly have a small right-USS. We show that this is not the case in the important particular case of rigid braids. More precisely, we show that the left-USS and the right-USS of a given rigid braid determine isomorphic graphs, with the arrows reversed, the isomorphism being defined using iterated cycling. We conjecture that the same is true for every element, not necessarily rigid, in braid groups and Artin-Tits groups of spherical type. ", "machine_abstract": "We study the cycle structure of elements in braid groups, and prove that any element can be written as a product of cycles whose lengths are bounded by some constant depending only on the number of strands. This is an extension to higher dimensions of results obtained for free groups by Magnus and Karrass-Solitar.  Theorem 1 (Cycle decomposition). Let G = B_n(K) be a braid group with n strands over K. Then every g \u2208 G has a unique expression as a product of disjoint cycles c_1, ..., c_k such that:  1. The length of each cycle ci satisfies |ci| \u2264 2^{log_2 n} + 3. 2. Each cycle ci contains at most one generator t_i which does not commute with its inverse. 3. If there exists no generator t_i satisfying condition 2 then k = 0. 4. No two generators appear simultaneously in different cycles. 5. Every generator appears exactly once in each cycle it belongs to. 6. For all i, |c_1 \u00b7 ... \u00b7 c_(i-1)| < |c_i|.", "paraphrased_abstract": "Theorem 1 (Cycle Decomposition). Let G be a braid with n strands over K, and we have one unique form in a set of triads of conjoint, disjointed cycles: 1 The length of each conjoint cycle is equal to the triad of its inverse, and if the conjoint is not equal to the triad, then k = 0. Theorem 1 (Cycle Decomposition) is extended to the upper reaches of the results obtained for free groups by Magnus and Karrass-Solitar. We can study the structure of the elements in the braids, and we show that any element can be written as a product of the triads, whose lengths are proportioned by some constant, and which are equal to each other in length. Whenever there is no triad, then triad is the only generator. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.301, "title": "Ground-state magneto-optical resonances in Cesium vapour confined in an extremely thin cell", "abstract": "  Experimental and theoretical studies are presented related to the ground-state magneto-optical resonance prepared in Cesium vapour confined in an Extremely Thin Cell (ETC, with thickness equal to the wavelength of the irradiating light). It is shown that the utilization of the ETC allows one to examine the formation of a magneto-optical resonance on the individual hyperfine transitions, thus distinguishing processes resulting in dark (reduced absorption) or bright (enhanced absorption) resonance formation. We report on an experimental evidence of the bright magneto-optical resonance sign reversal in Cs atoms confined in the ETC. A theoretical model is proposed based on the optical Bloch equations that involves the elastic interaction processes of atoms in the ETC with its walls resulting in depolarization of the Cs excited state which is polarized by the exciting radiation. This depolarization leads to the sign reversal of the bright resonance. Using the proposed model, the magneto-optical resonance amplitude and width as a function of laser power are calculated and compared with the experimental ones. The numerical results are in good agreement with the experiment. ", "machine_abstract": "We report on the observation of ground state magneto optical resonance (GMOR) in cesium vapor confined to a sub-micron thickness layer inside a glass micro-cell. The GMOR is observed by measuring the transmission spectrum through the cell as it is rotated about its normal axis with respect to the direction of propagation of circularly polarized light. We show that this effect can be explained using simple classical electrodynamics and we present experimental results which demonstrate the dependence of the GMOR signal strength on various parameters such as the intensity, frequency detuning and polarization angle of the incident laser beam. This work opens up new possibilities for studying quantum optics phenomena at the single atom level.     In recent years there has been considerable interest in developing techniques for trapping atoms or molecules within microscopic volumes [1] . Such confinement offers several advantages over conventional atomic beams experiments including increased interaction times between the trapped particles and the applied fields [2] , improved spatial resolution [3] and reduced Doppler broadening [4] . These features are particularly important when considering applications involving high precision measurements [5] . In addition to these practical benefits, confining neutral matter to small dimensions also provides opportunities for exploring fundamental physics [6] . For example, the study of Bose-Einstein condensates [7, 8] requires cooling and trapping of large numbers of atoms into very tight traps [9] . Similarly, investigations into the properties of individual atoms [10] require their isolation from other sources of decoherence [11] . Finally, studies of macroscopic quantum effects [12] may benefit from the ability to control the number of particles involved [13] .     Here we describe our efforts towards achieving controlled confinement of neutral matter to extremely small dimensions. Specifically, we have developed a technique for producing a thin film of cesium gas inside a glass micro-cell [14] . By exploiting the strong magnetic dipole moment associated with the cesium ground state [15] , we observe a novel form of magneto-optical resonance [16] known as ground state magneto-optical resonance [17] . Our observations suggest that this phenomenon could provide a useful tool for investigating quantum optics processes occurring at the single atom level [18] .", "paraphrased_abstract": "In recent years, great interest has been given to developing techniques for the confined confinement of atoms or molecules within a single atom-sized space. In this way, the researchers are able to study the properties of atoms in the smallest possible space. The confined confinement of particles to the smallest possible space offers several advantages, namely: increased interaction between the trapped particles and the environment, increased spatial resolution, increased spatial resolution, and reduced Doppler enlargement. This work is of particular use to investigate the physics of atoms in small, sub-atomic dimensions. We have developed a technique for forming a thin film of cesium vapor in a glass microcell. We exploited the strong magnetic dipole moment of the ground state of cesium to obtain a new type of magneto-optical resonance, known as ground-state magneto-optical resonance. This effect was observed by means of the electrodynamics of the cell, the intensity, the frequency and the angle of the laser beam, and the resonant angle of the circular light. The present work opens new possibilities for the study of the quantum optics at the single atom level. The study of Bose-Einstein condensates, for example, requires the vaporization of large numbers of atoms, which is impossible with a condensate, a", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1402, "title": "Analyzing Design Process and Experiments on the AnITA Generic Tutoring System", "abstract": "  In the field of tutoring systems, investigations have shown that there are many tutoring systems specific to a specific domain that, because of their static architecture, cannot be adapted to other domains. As consequence, often neither methods nor knowledge can be reused. In addition, the knowledge engineer must have programming skills in order to enhance and evaluate the system. One particular challenge is to tackle these problems with the development of a generic tutoring system. AnITA, as a stand-alone application, has been developed and implemented particularly for this purpose. However, in the testing phase, we discovered that this architecture did not fully match the user's intuitive understanding of the use of a learning tool. Therefore, AnITA has been redesigned to exclusively work as a client/server application and renamed to AnITA2. This paper discusses the evolvements made on the AnITA tutoring system, the goal of which is to use generic principles for system re-use in any domain. Two experiments were conducted, and the results are presented in this paper. ", "machine_abstract": "In this work, we present an analysis of design process and experiments conducted in developing a generic tutoring system for intelligent tutoring applications (ITAs). The main goal is to provide insights into how such systems can be developed with minimal effort by reusing existing components. We describe our experiences in building the AnITA system that provides adaptive feedbacks based on student's actions during problem solving activities. In addition, we discuss some issues related to evaluation of ITAs using different types of students as subjects. Finally, we conclude with lessons learned throughout the development cycle. This article is available from: http://arxiv.org/abs/1207.5995. Keywords: Intelligent tutoring application; Adaptive feedbacks; Problem-solving activity; Student modeling; Evaluation experimentation. 1 Introduction Intelligent tutoring applications (ITAS) are computer-based learning environments designed to help students learn specific skills or concepts through interactive problem-solving activities [1] . They have been shown to improve students' performance when compared to traditional teaching methods [2] . The development of ITA requires significant efforts due to their complexity [3] , which includes several components including domain knowledge representation [4] , student modeling [5] , pedagogical strategies [6] , user interface [7] , and adaptation [8] . To reduce these efforts, researchers have proposed frameworks [9] - [11] and tools [12] - [14] that allow developers to build new ITS without having to start from scratch. However, most of them focus only on one aspect of ITS development [15] , e.g., authoring tool [16] , content management [17] , or student modeling [18] .", "paraphrased_abstract": "During the development of an IT, a considerable amount of work has been devoted to the synthesis of several elements: a domain knowledge, student modeling, pedagogical strategies, user interfaces, and adaptation. There are several techniques for achieving this. In the past, researchers have proposed various frameworks and tools, which allow them to build new ITS without having to start from scratch. In this work, we have presented the case for the development of an IT and have thus far evinced that the development of an IT has been achieved with minimal effort, and the main object of our work is to illustrate the benefits of such a system in reducing students\u2019 stress. The main aim of this research is to describe and present an experimental system for an IT implementing intelligent tutoring. It consists of several elements: an explanation of knowledge of the subject, an explanation of the problem solving, an analysis of the student\u2019s behavior, and an evaluation of the effect of the system on the student. Besides, the present paper describes the development of an IT for students with specific problems or skills. The IT is a computer-based learning system that helps students to learn and to develop skills, or to learn a specific subject. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.402, "title": "Statistical Evidence for Three classes of Gamma-ray Bursts", "abstract": "  Two different multivariate clustering techniques, the K-means partitioning method and the Dirichlet process of mixture modeling, have been applied to the BATSE Gamma-ray burst (GRB) catalog, to obtain the optimum number of coherent groups. In the standard paradigm, GRB are classified in only two groups, the long and short bursts. However, for both the clustering techniques, the optimal number of classes was found to be three, a result which is consistent with previous statistical analysis. In this classification, the long bursts are further divided into two groups which are primarily differentiated by their total fluence and duration and hence are named low and high fluence GRB. Analysis of GRB with known red-shifts and spectral parameters suggests that low fluence GRB have nearly constant isotropic energy output of 10^{52} ergs while for the high fluence ones, the energy output ranges from 10^{52} to 10^{54} ergs. It is speculated that the three kinds of GRBs reflect three different origins: mergers of neutron star systems, mergers between white dwarfs and neutron stars, and collapse of massive stars. ", "machine_abstract": "We present the results of an analysis of BATSE data on gamma ray bursts (GRBs) in which we have used three different statistical methods to search for evidence that GRB durations are distributed into at least two distinct populations, as suggested by previous authors.  We find no compelling evidence for such a bimodal distribution using any one method alone; however, when all three methods are combined there is strong evidence for this hypothesis.  The best-fit parameters obtained with each method individually differ significantly and suggest that the true underlying distribution may be more complex than previously thought. In particular, our results indicate that the shortest duration events form their own class of GRBs whose properties are quite different from those of longer-duration GRBs. This result has important implications both for understanding the nature of these phenomena and for planning future missions dedicated to studying them. Keywords: Gamma Ray Bursts, Statistical Analysis", "paraphrased_abstract": "Statistical analysis of gamma-ray bursts, which is a gamma-ray bursts, and we present the results of an analysis of the data from the BATSE, in which we have used three different methods to investigate whether GRB durations are distributed in two distinct groups, as was previously noted. We find no convincing evidence for such a bimodal distribution, but if we combine the three methods together, we find that the true distribution is quite complex. We also present a hypothesis that the shortest durations of the bursts form a special class of bursts, with different properties from those of the long durations. The results of this study have important implications both for our understanding of these phenomena and for future researches. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1129, "title": "General Properties of Quantum Zero-Knowledge Proofs", "abstract": "  This paper studies the complexity classes QZK and HVQZK of problems having a quantum computational zero-knowledge proof system and an honest-verifier quantum computational zero-knowledge proof system, respectively. The results proved in this paper include: (a) HVQZK = QZK, (b) any problem in QZK has a public-coin quantum computational zero-knowledge proof system, (c) any problem in QZK has a quantum computational zero-knowledge proof system of perfect completeness, and (d) any problem in QZK has a three-message public-coin quantum computational zero-knowledge proof system of perfect completeness with arbitrarily small constant error in soundness. All the results above are unconditional and do not rely any computational assumptions. For the classes QPZK, HVQPZK, and QSZK of problems having a quantum perfect zero-knowledge proof system, an honest-verifier quantum perfect zero-knowledge proof system, and a quantum statistical zero-knowledge proof system, respectively, the following new properties are proved: (e) HVQPZK = QPZK, (f) any problem in QPZK has a public-coin quantum perfect zero-knowledge proof system, (g) any problem in QSZK has a quantum statistical zero-knowledge proof system of perfect completeness, and (h) any problem in QSZK has a three-message public-coin quantum statistical zero-knowledge proof system of perfect completeness with arbitrarily small constant error in soundness. It is stressed that our proofs are direct and do not use complete promise problems or those equivalents. This gives a unified framework that works well for all of quantum perfect, statistical, and computational zero-knowledge proofs, and enables us to prove properties even on the computational and perfect zero-knowledge proofs for which no complete promise problems are known. ", "machine_abstract": "Quantum zero-knowledge proofs are quantum protocols that allow one party to prove knowledge of some secret information without revealing any other information about it.  In this work, we study the general properties of these proofs and show how they can be used in order to construct efficient quantum authentication schemes for classical messages. We also present several applications of our results including new constructions of quantum key distribution (QKD) systems with unconditional security guarantees against collective attacks as well as improved QKD systems based on weak coherent pulses. Finally, we discuss possible extensions of our results towards more practical scenarios where the communication between Alice and Bob is not perfect but rather noisy or lossy. Quantum zero-knowledge proofs were introduced by Wiesner [1] , Brassard et al [2] and Cleve [3] . These proofs are quantum protocols that enable one party, called Alice, to convince another party, called Bob, that she knows certain secret information while keeping her secrets hidden. More formally, let us consider an interactive protocol between Alice and Bob consisting of two phases:  The first phase consists of a series of rounds during which Alice sends qubits to Bob who performs measurements on them according to his choice of measurement bases. At the end of each round, Bob announces publicly whether he accepts or rejects the received state. If Bob accepts at least half of all states sent by Alice then the second phase begins; otherwise, the protocol ends after the current round. During the second phase, Alice reveals her secrets to Bob if he accepted enough states in the previous phase. Otherwise, both parties abort the protocol. It was shown in [4] that such a proof system exists if and only if there exist one-way functions. However, no explicit construction of such proofs has been known until now.", "paraphrased_abstract": "However, to our knowledge there is no specific construction of such proofs. In this work, we examine the general properties of such proofs and demonstrate how they can be used to construct efficient quantum authentication systems for classical messages. In this work, we show how to construct quantum-level-knowledge proofs in order to provide a reliable, reliable authentication for messages of classical origin. We examine the general properties of these proofs and show how they can be adapted to various types of communication, for instance, a quantum-level-knowledge proof of Alice's knowledge of some secret, without revealing any secret. In short, there are quantum-level-knowledge proofs, which can be used in a way that is efficient in the case of messages of classical origin, and also in the case that the messages of Alice and Bob are not perfect and are not always completely clear. In the first place, Alice sends qubits to Bob, who measures them according to the metric basis he chooses, and at the end of each round, Alice announces publicly that she has accepted or rejected the given state. If Bob accepts half of the states received by Alice, then the second phase begins. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2908, "title": "Negative Matter, Repulsion Force, Dark Matter, Phantom and Theoretical Test ----Their Relations with Inflation Cosmos and Higgs Mechanism", "abstract": "  First, dark matter is introduced. Next, the Dirac negative energy state is rediscussed. It is a negative matter with some new characteristics, which are mainly the gravitation each other, but the repulsion with all positive matter. Such the positive and negative matters are two regions of topological separation in general case, and the negative matter is invisible. It is the simplest candidate of dark matter, and can explain some characteristics of the dark matter and dark energy. Recent phantom on dark energy is namely a negative matter. We propose that in quantum fluctuations the positive matter and negative matter are created at the same time, and derive an inflation cosmos, which is created from nothing. The Higgs mechanism is possibly a product of positive and negative matter. Based on a basic axiom and the two foundational principles of the negative matter, we research its predictions and possible theoretical tests, in particular, the season effect. The negative matter should be a necessary development of Dirac theory. Finally, we propose the three basic laws of the negative matter. The existence of four matters on positive, opposite, and negative, negative-opposite particles will form the most perfect symmetrical world. ", "machine_abstract": "The present work is devoted to the study of some fundamental problems in physics such as negative matter, repulsion force, dark matter, phantom and theoretical test----their relations with inflation cosmos and higgs mechanism. In this context we have studied the following topics:  1) Negative energy density and its relation with cosmological constant.  2) Energy-momentum tensor for quantum fields and their connection with Einstein field equations.  3) Positive-energy theorem and its application on gravitational waves.  4) Energy conditions and their violation by quantum fields.  5) Energy conditions and their violations by classical fields.  6) Energy conditions and their connections with repulsive gravity.  7) Energy conditions and their applications on black holes.  8) Energy conditions and their implications on wormholes 9) Energy conditions and their consequences on time machines 10) Energy conditions and their effects on singularities 11) Energy conditions and their relationships with Higgs' mechanism 12) Energy conditions and their influences on inflation cosmos 13) Energy conditions and their impacts on cosmic strings 14) Energy conditions and their roles in particle creation 15) Energy conditions and their contributions to vacuum fluctuations 16) Energy conditions and their influence on Casimir effect 17) Energy conditions and their role in Hawking radiation 18) Energy conditions and their contribution to Unruh effect 19) Energy conditions and their relationship with entropy 20) Energy conditions and their impact on information loss 21) Energy conditions and their implication on thermodynamics 22) Energy conditions and their...", "paraphrased_abstract": "He has investigated the following problems: 1) Negative energy density and the cosmological constant; 2) Energy-momentum tensor for quantum fields and the equations of Einstein; 3) Energy-momentum tensor for quantum fields and the equations of Einstein; 3) Energy-momentum tensor for quantum fields and the equations of Einstein; 3) Energy-momentum tensor for quantum fields and the equations of Einstein; 3) Energy-momentum tensor for quantum fields and the equations of Einstein; 4) Energy-momentum tensor for quantum fields and its relation to the equations of Einstein; 4) Energy-momentum tensor for quantum fields and its connection with the equations of Einstein; 4) Energy-momentum tensor for quantum fields and its connection with Einstein\u2019s field equations; 4) Energy-momentum tensor for quantum fields and its relation to the Einstein equations; 5) Energy-momentum tensor for quantum fields and its relation to Einstein\u2019s field equations; 5) Energy-momentum tensor for quantum fields and its relation with Einstein\u2019s field equations; 6) Energy-momentum ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.155, "title": "Zgamma production and limits on anomalous ZZgamma and Zgammagamma couplings in ppbar collisions at sqrt(s) = 1.96 TeV", "abstract": "  We present a study of eegamma and mumugamma events using 1109 (1009) pb-1 of data in the electron (muon) channel, respectively. These data were collected with the D0 detector at the Fermilab Tevatron ppbar Collider at sqrt(s) = 1.96 TeV. Having observed 453 (515) candidates in the eegamma (mumugamma) final state, we measure the Zgamma production cross section for a photon with transverse energy ET > 7 GeV, separation between the photon and leptons Delta R(lgamma) > 0.7, and invariant mass of the di-lepton pair M(ll) > 30 GeV/c^2, to be 4.96 +/- 0.30(stat. + syst.) +/- 0.30(lumi.) pb, in agreement with the standard model prediction of 4.74 +/- 0.22 pb. This is the most precise Zgamma cross section measurement at a hadron collider. We set limits on anomalous trilinear Zgammagamma and ZZgamma gauge boson couplings of -0.085 < h(30)^(gamma) < 0.084, -0.0053 < h(40)^(gamma) < 0.0054 and -0.083 < h(30)^(Z) < 0.082, -0.0053 < h(40)^(Z) < 0.0054 at the 95% C.L. for the form-factor scale Lambda = 1.2 TeV. ", "machine_abstract": "We report the first measurement of the cross section for the process ppbar -> Zgamma + X, where X is any number of additional particles produced along with the Zgamma boson.  The data were collected by the D0 experiment during Run II of Fermilab's Tevatron Collider between 2002 and 2007 using an integrated luminosity of 5.4 fb-1 . We measure the cross section to be 0.84 +/- 0.11 (stat.) +/- 0.10 (syst.) pb, which agrees well with next-to-leading-order perturbative QCD predictions. Using this result we set upper limits on possible anomalous trilinear gauge-boson coupling parameters. These results are also used to derive constraints on models that predict new heavy neutral vector bosons decaying into pairs of photons or gluons. In addition, these measurements provide important input for future searches for Higgs bosons decaying into two photons.  PACS numbers: 11.30.Er, 12.60.Jv", "paraphrased_abstract": "The data are collected in the run-in of the Tevatron Collider during the first half of 2002 in the X-ray experiment of Fermilab. The data are measured by a spectral ratio of 0.84 +/- 0.11 (stat.) pb, which agrees well with the best-order perturbative estimates of the QCD. We therefore give a new set of parameters for the theory of the decay of light-free, radial-bound bosons and gluons. We also give the generalized constraints on models of the decay of radial-bound bosons into two photons and gluons. The cross section is 0.84 +/- 0.11 (stat.) +/- 0.10 (syst.) pb, which corresponds to a pre-established standard of quantum mechanics. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1231, "title": "The outburst of the eruptive young star OO Serpentis between 1995 and 2006", "abstract": "  OO Serpentis is a deeply embedded pre-main sequence star that went into outburst in 1995 and gradually faded afterwards. Its eruption resembled the well-known FU Orionis-type or EX Lupi-type outbursts. Since very few such events have ever been documented at infrared wavelengths, our aim is to study the temporal evolution of OO Ser in the infrared. OO Ser was monitored with the Infrared Space Observatory starting 4 months after peak brightness and covering 20 months. In 2004-2006 we again observed OO Ser from the ground and complemented this dataset with archival Spitzer obsevations also from 2004. We analysed these data with special attention to source confusion and constructed light curves at 10 different wavelengths as well as spectral energy distributions. The outburst caused brightening in the whole infrared regime. According to the infrared light curves, OO Ser started a wavelength-independent fading after peak brightness. Later the flux decay became slower but stayed wavelength-independent. The fading is still ongoing, and current fading rates indicate that OO Ser will not return to quiescent state before 2011. The outburst timescale of OO Ser seems to be shorter than that of FUors, but longer than that of EXors. The outburst timescale and the moderate luminosity suggest that OO Ser is different from both FUors and EXors, and shows similarities to the recently erupted young star V1647 Ori. Based on its spectral energy distribution and bolometric temperature, OO Ser seems to be an early class I object, with an age of < 10^5 yr. The object is probably surrounded by an accretion disc and a dense envelope. Due to the shorter outburst timescales, the viscosity in the circumstellar disc of OO Ser is probably an order of magnitude higher than usual for FUors. ", "machine_abstract": "We present new photometric data on the eruptive young stellar object OO Serpentis (OO Sgr) obtained with the 1 m telescope at Mt. Wilson Observatory in California, USA during the period 1995-2006. The light curve shows two major flares peaking around 1997 and 2002. We also report spectroscopic observations made by us using the 2.3 m Himalayan Chandra Telescope (HCT), Indian Astronomical Observatory (IAO), Hanle, India, which show that the spectrum is dominated by emission lines of H$\\alpha$ , He I $(5876)$ , He II $(4686)$ . These results are compared to those reported earlier for this source.     Keywords: Erupting Young Star, Light Curve, Spectroscopy, Outbursts     Introduction     A number of studies have been carried out recently on the eruptions of young stars. In particular, several authors have studied the eruption of T Tauri stars (TTS). However, there has not yet been any systematic study of eruptions among Herbig Ae/Be stars (HAEBE).     Herbig Ae/Be (HAEBE) stars are intermediate mass pre-main sequence objects located near the end of their formation process. They are surrounded by an accretion disk and exhibit strong infrared excesses due to dusty material surrounding them. Their spectral types range from F0 to B3. They are characterized by high luminosities ranging from 10 to 100 Lsun and effective temperatures ranging from 8,000 K to 20,000 K. Most of these sources are known to be variable both photometrically as well as spectroscopically. Some of them undergo large amplitude variations while others do not. It was suggested by Herbst et al. (1994) that some of these sources may experience episodic accretion events similar to classical TTS. Recent theoretical models suggest that such eruptions can occur when the inner edge of the circumstellar disk moves inward towards the central star because of viscous evolution or gravitational instability (Vorobyov & Bas", "paraphrased_abstract": "However, the most recent studies have not yet been able to verify the existence of such objects. In the newest models of accretion, such a phenomenon may occur from the inner edge of the circumstellar disk abrasion or from a disturbance of the underlying magnetic field. There are various theories about such eruptions, which, like TTS, may be the case. As a result, some of these eruptions may occur as episodic episodes of their formation. The erupting stars of the class of Herbig ae, the second class of the epoch, have a sphere and a disk surrounded by an accretion disk. They are composed of a disk and a dusty atmosphere. Their spectral types are from F0 to F3. We report the observations obtained by our instrument of 2.3 metre-long Himalayan Chandra Telescope at the Hanle Observatory. The spectral type is as follows: Ao Alpha, Ao He I, Ao II, - 486. The results are compared with those which we have already reported for this source. This object has been studied in several experiments and is very variable. It was suggested by Herbst and Bashorov et al. (1998) that some of these objects may be caused by abrupt accretion. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2274, "title": "Consistent reasoning about a continuum of hypotheses on the basis of finite evidence", "abstract": "  In the modern Bayesian view classical probability theory is simply an extension of conventional logic, i.e., a quantitative tool that allows for consistent reasoning in the presence of uncertainty. Classical theory presupposes, however, that--at least in principle--the amount of evidence that an experimenter can accumulate always matches the size of the hypothesis space. I investigate how the framework for consistent reasoning must be modified in non-classical situations where hypotheses form a continuum, yet the maximum evidence accessible through experiment is not allowed to exceed some finite upper bound. Invoking basic consistency requirements pertaining to the preparation and composition of systems, as well as to the continuity of probabilities, I show that the modified theory must have an internal symmetry isomorphic to the unitary group. It thus appears that the only consistent algorithm for plausible reasoning about a continuum of hypotheses on the basis of finite evidence is furnished by quantum theory in complex Hilbert space. ", "machine_abstract": "We present an algorithm for consistent hypothesis testing in which we consider all possible hypotheses that are compatible with some given set of observations, and select those that maximize their posterior probability according to Bayes' theorem.  We show how this can be done efficiently by using dynamic programming techniques. The resulting algorithm is optimal up to constant factors under certain conditions. Our approach also allows us to reason consistently over multiple experiments performed sequentially or simultaneously. This problem has been studied extensively in statistics but only recently in artificial intelligence (AI). In AI it was first considered as part of the PAC learning framework where one seeks algorithms that learn concepts from examples while making few mistakes. However, these approaches do not provide any guarantees when there exists more than one concept that fits the data equally well. In contrast our method provides provable guarantees even if several hypotheses fit the data equally well. Finally, we demonstrate the practicality of our approach through two applications:  1) A new algorithm for finding explanations in probabilistic databases; 2) An improved algorithm for identifying protein families based on sequence alignment.", "paraphrased_abstract": "We propose an algorithm for consistent hypothesis testing that considers all possible hypotheses that are compatible with a given set of observations and selects those which maximize their posterior probability according to Bayes\u2019 theorem. Our approach is applied to two particular examples: 1) a new algorithm for finding explanations in probabilistic databases; 2) a new algorithm for identifying protein families by sequence alignment. We demonstrate the practicality of our method through two applications: 1) a new method for finding explanations in probabilistic databases; 2) a new algorithm for identifying protein families by sequence alignment. This problem has been studied extensively in statistics, but only recently in artificial intelligence. In this respect, we have only recently begun to apply it to the AI framework of PAC, which seeks to learn concepts from examples while making few mistakes. However, our method can provide proof even if many hypotheses fit the data. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2457, "title": "Analysis of Spitzer Spectra of Irradiated Planets: Evidence for Water Vapor?", "abstract": "  Published mid infrared spectra of transiting planets HD 209458b and HD 189733b, obtained during secondary eclipse by the InfraRed Spectrograph (IRS) aboard the Spitzer Space Telescope, are predominantly featureless. In particular these flux ratio spectra do not exhibit an expected feature arising from water vapor absorption short-ward of 10 um. Here we suggest that, in the absence of flux variability, the spectral data for HD 189733b are inconsistent with 8 um-photometry obtained with Spitzer's InfraRed Array Camera (IRAC), perhaps an indication of problems with the challenging reduction of the IRS spectra. The IRAC point, along with previously published secondary eclipse photometry for HD 189733b, are in good agreement with a one-dimensional model of HD 189733b that clearly shows absorption due to water vapor in the emergent spectrum. We are not able to draw firm conclusions regarding the IRS data for HD 209458b, but spectra predicted by 1D and 3D atmosphere models fit the data adequately, without adjustment of the water abundance or reliance on cloud opacity. We argue that the generally good agreement between model spectra and IRS spectra of brown dwarfs with atmospheric temperatures similar to these highly irradiated planets lends confidence in the modeling procedure. ", "machine_abstract": "We present the first analysis of water vapor in irradiated planets using infrared (IR) spectroscopy with the Infrared Spectrograph on board the Spitzer Space Telescope. We have analyzed four transiting exoplanet systems, HD 189733b, HD 209458b, WASP-12b and XO-1b, which are known to be strongly irradiated by their host stars. The IR spectra were obtained during secondary eclipse events when the planet passes behind its star as seen from Earth. Our results show that all these planets exhibit strong absorption features at wavelengths longer than 5 microns due to water vapor in their atmospheres. These observations provide direct evidence for the presence of water vapor in highly-irradiated planetary atmospheres.  Keywords: Exoplanet, Transmission spectrum, Secondary eclipse, Water vapor, Infrared spectrophotometry, Spitzer Space Telescope. 1 Introduction   Water is one of the most important molecules in our Solar System because it plays an essential role in life processes. It has been detected in many different environments ranging from comets to icy satellites such as Europa or Enceladus. However, despite numerous efforts over several decades, no unambiguous detection of water had yet been reported outside our Solar System until recently. This situation changed dramatically thanks to space-based observatories like Hubble Space Telescope (HST), Chandra X-ray Observatory, and especially Spitzer Space Telescope (Werner et al., 2004) .  Since its launch in 2003, Spitzer has observed thousands of targets including hundreds of extrasolar planets. Among them, there are some very interesting cases where the planet orbits close to its parent star so that the intense stellar radiation heats up the atmosphere of the planet significantly. As a result, the atmospheric composition can change drastically compared to what we know about terrestrial planets in our Solar System. For example, if the temperature becomes high enough, hydrogen could escape from the planet's upper atmosphere into space leaving only helium behind (Lammer et al., 2003; Baraffe et al., 2004; Yelle et al., 2006) , while other species may condense out onto", "paraphrased_abstract": "It has also been found in several solar worlds, among which are the very few terrestrials, such as Jupiter and Europa. The water, however, has also been detected in many different environments, from comets to icy satellites, such as Europa or Enceladus. The water is one of the most important molecules in the Solar System, for it plays an essential role in the life of man. Yet until now no clear identification of water in the Solar System has ever been made, and even so much has been achieved by means of space-based observatories, such as Hubble\u2019s HST, Chandra\u2019s X-ray Observatory and Spitzer\u2019s Spectroscopic Telescope. This has now been dramatically changed by the arrival of new instruments, such as the Hubble X-ray Observatory, the Chandra X-ray Observatory, and the Spitzer Telescope, which was launched in 2003, has observed thousands of targets, including several extrasolar planets. This work was carried out by the Spitzer Telescope in 2003, and it has shown that the atmosphere of these extrasolar planets has changed significantly from that of terrestrial planets in our Solar System. The results are inferred from the infrared spectrum, when the planet passes in front of its star as if it were approaching Earth. The secondary phase is measured with the Spitzer Telescope, and the IR spectrum", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1374, "title": "A Close Look at Star Formation around Active Galactic Nuclei", "abstract": "  We analyse star formation in the nuclei of 9 Seyfert galaxies at spatial resolutions down to 0.085arcsec, corresponding to length scales of less than 10pc in some objects. Our data were taken mostly with the near infrared adaptive optics integral field spectrograph SINFONI. The stellar light profiles typically have size scales of a few tens of parsecs. In two cases there is unambiguous kinematic evidence for stellar disks on these scales. In the nuclear regions there appear to have been recent - but no longer active - starbursts in the last 10-300Myr. The stellar luminosity is less than a few percent of the AGN in the central 10pc, whereas on kiloparsec scales the luminosities are comparable. The surface stellar luminosity density follows a similar trend in all the objects, increasing steadily at smaller radii up to 10^{13}L_sun/kpc^2 in the central few parsecs, where the mass surface density exceeds 10^4M_sun/pc^2. The intense starbursts were probably Eddington limited and hence inevitably short-lived, implying that the starbursts occur in multiple short bursts. The data hint at a delay of 50--100Myr between the onset of star formation and subsequent fuelling of the black hole. We discuss whether this may be a consequence of the role that stellar ejecta could play in fuelling the black hole. While a significant mass is ejected by OB winds and supernovae, their high velocity means that very little of it can be accreted. On the other hand winds from AGB stars ultimately dominate the total mass loss, and they can also be accreted very efficiently because of their slow speeds. ", "machine_abstract": "We present new results on the star formation rate (SFR) in galaxies with active galactic nuclei (AGN). We use deep Spitzer Space Telescope observations to study the infrared luminosity function and SFR density evolution for AGNs selected by their mid-infrared colors, which are known to be dominated by dust heated by young stars. The observed infrared luminosities range over four orders of magnitude, from 10^42 erg/sec to more than 10^45 erg/sec. Our sample includes both obscured quasars and unobscured type-1 Seyferts out to redshifts z = 3.5. We find that the infrared luminosity functions of these two populations evolve similarly as a function of redshift, but differ significantly when compared locally. This suggests that the physical processes responsible for heating the dust may not change dramatically between high-z and low-z systems, while the fraction of AGNs surrounded by massive amounts of dust does decrease rapidly with time.", "paraphrased_abstract": "The light from these two species is similar, but it is different in their spatial structure. The dust absorbed by the young stars is, of course, hotter. This suggests that the physical processes for heating dust are not changing significantly, and the encirclement of massive masses of dust can change little by little. In this paper, we will discuss the SFR of the AGNs in active galactic nuclei (AGNs). The SFR in the upper part of the AGNs was studied with deep Spitzer telescopes. The observations were taken in the mid-infrared, and the SFR was measured from the very top down. This resulted in the onset of SFR in a galaxy of active galactic nuclei (AGs). We examined the infrared luminosity of the AGNs of the twilight, and found that the SFR increased as the redshift increased. The SFR, the light fraction, and the temperature of the dust were of a similar order to the redshift, but there were differences in the temperature of the AGNs. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0982, "title": "Daemons and DAMA: Their Celestial-Mechanics Interrelations", "abstract": "  The assumption of the capture by the Solar System of the electrically charged Planckian DM objects (daemons) from the galactic disk is confirmed not only by the St.Petersburg (SPb) experiments detecting particles with V<30 km/s. Here the daemon approach is analyzed considering the positive model independent result of the DAMA/NaI experiment. We explain the maximum in DAMA signals observed in the May-June period to be associated with the formation behind the Sun of a trail of daemons that the Sun captures into elongated orbits as it moves to the apex. The range of significant 2-6-keV DAMA signals fits well the iodine nuclei elastically knocked out of the NaI(Tl) scintillator by particles falling on the Earth with V=30-50 km/s from strongly elongated heliocentric orbits. The half-year periodicity of the slower daemons observed in SPb originates from the transfer of particles that are deflected through ~90 deg into near-Earth orbits each time the particles cross the outer reaches of the Sun which had captured them. Their multi-loop (cross-like) trajectories traverse many times the Earth's orbit in March and September, which increases the probability for the particles to enter near-Earth orbits during this time. Corroboration of celestial mechanics calculations with observations yields ~1e-19 cm2 for the cross section of daemon interaction with the solar matter. ", "machine_abstract": "The article is devoted to the problem of possible existence of dark matter particles in our Galaxy, which are not detected by other methods than their gravitational effects on visible objects (stars). The author considers the possibility that these hypothetical particles can be described as celestial mechanics daemons with certain properties. In particular, it is shown how such daemons could explain some features observed recently for the DAMA experiment at Gran Sasso National Laboratory. It should be noted that this explanation does not contradict any known experimental data. However, there are also serious difficulties associated with the proposed model. These problems will require further study. This work was supported by Russian Science Foundation grant No 14-50-00040. URL: http://arxiv.org/abs/1409.5189 .  I. INTRODUCTORY REMARK . Dark Matter (DM) is one of the most important mysteries of modern physics [1] - [4] . Its presence has been established only indirectly through its gravitational influence on visible stars [5] , galaxies [6] , clusters [7] etc., but direct detection experiments have so far failed [8] - [10] . There exist many theoretical models describing DM [11] - [13] ; however, none of them has yet been confirmed experimentally [14] . One of the possibilities is that DM consists of new elementary particles [15] - [17] . If they interact weakly or electromagnetically with ordinary matter then they would escape detection even if they were produced in large quantities [18] . On the other hand, if they interact strongly enough with normal matter, then they may be detectable directly [19] - [21] . A number of experiments searching for DM particles have been carried out [22] - [26] . Recently, the results obtained by the DAMA collaboration [27] attracted considerable attention [28] - [30] . According to these results, the annual modulation effect [31] - [33] caused by the motion of Earth around Sun [34] - [36] leads to an increase in the rate of nuclear recoils registered by detectors during June-October period [37] compared to December-February period. Such behavior cannot be explained within Standard Model of particle interactions [38] - [41] . Several authors suggested different explanations based on", "paraphrased_abstract": "The results of the study were made known to a great many people, but they were not confirmed experimentally. The most important questions in modern physics were the DMs. They could be detected only indirectly by their gravitational influence on visible objects (stars), but these tests have so far been unable to detect them. This work was supported by a grant from the Russian Science Foundation No. 14-50-00040. The author of the paper considered the possibility that DMs existed in our galaxy and were not detected by any other means than by gravitational influence on visible objects. Some of the most important theories of DM exist, but none of them have been confirmed experimentally. There are many theories about DM, but none of them have been confirmed experimentally. The author considers the possibility that these particles may be described as celestial swarms, and that they might explain certain features observed recently in the DAMA experiment. In particular, he discusses how these swarms may explain certain features observed recently in the DAMA experiment. The authors present herein offer a different interpretation of these results. The author proposes, for instance, that the DM of the Galaxy might be composed of new elementary particles, [11] i.e. of new elementary particles [14], which are unable to be detected, and which, if they are unable to interact with the ordinary material,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2659, "title": "Minimum Expected Distortion in Gaussian Layered Broadcast Coding with Successive Refinement", "abstract": "  A transmitter without channel state information (CSI) wishes to send a delay-limited Gaussian source over a slowly fading channel. The source is coded in superimposed layers, with each layer successively refining the description in the previous one. The receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. In the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. In the optimal power distribution, as SNR increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. On the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. While expected distortion can be improved by acquiring CSI at the transmitter (CSIT) or by increasing diversity from the realization of independent fading paths, at high SNR the performance benefit from diversity exceeds that from CSIT, especially when b is large. ", "machine_abstract": "We consider the problem of successive refinement coding for layered broadcast systems, where each receiver is interested only in one out of several layers and has access to all previous layers. We propose an optimal scheme that minimizes expected distortion at any given receiver by using rate-distortion optimized quantizers. The proposed scheme can be implemented efficiently as it requires no feedback between receivers or encoders. Our results are illustrated through numerical examples. Index Terms-Broadcasting, Data compression, Quantization, Rate-distortion theory, Successive refinement coding. I. INTRODUCTIO N In this work we study the problem of successive refinem ent coding (SRC) [1] , which consists on transmitting information over multiple stages such that the quality of reconstruction improves progressively. SRC is used extensively in video broadcasting applications [2] - [4] . For example, in digital television transmission, the base station sends a coarse description of the video sequence to mobile users via satellite links. Then, when these users get closer to their destination they request additional descriptions of higher resolution. This process continues until the user receives enough data to reconstruct the original signal without error [5] . In general, there exist two different approaches to solve the SRC problem: 1) Joint source-channel coding: Here, the encoder jointly optimizes both source coding and channel coding [6]- [8] ; 2) Separate source-channel coding: Here, separate source coders and channel codes are employed [9] - [11] . In this case, the source code must provide some form of side-information so that the decoder can perform successive decoding [12] .", "paraphrased_abstract": "\u201cWhat is it about the successive refinement of a signal? \u201c Hence, the idea of the successive refinement of a signal, a concept commonly used in television transmission. In this regard, a SRC is an idea which consists of a transmission of information over several stages, so that the quality of reconstruction is gradually improved. In our work, we examine the problem of successive refinement of a signal, which is a complex and complex problem, for which the transmitter and receiver are divided in two groups: a joint source-channel coding, where the transmitter and receiver work together in a separate source-channel coding. b. In this way the source-channel coding is based on some side-information, which the transmitter must give as an intermediate stage of the transmission. Then the transmitter and receiver are grouped into separate groups. The latter have to be accompanied by an adequate quantity of data, and they must be sufficiently precise to reproduce the original signal without a blip. We consider the problem of successive refinement coding, which involves the transmission of information in several stages, and that the quality of reconstruction is gradually improved by each successive decoding. In our example, the broadcasting system uses a technique called successive refinement, which is based on the encoding of information in several stages. In this approach, the encoders and the receivers are provided with", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.227, "title": "The Infrared Cloud Monitor for the MAGNUM Robotic Telescope at Haleakala", "abstract": "  We present the most successful infrared cloud monitor for a robotic telescope. This system was originally developed for the MAGNUM 2-m telescope, which has been achieving unmanned and automated monitoring observation of active galactic nuclei at Haleakala on the Hawaiian island of Maui since 2001. Using a thermal imager and two aspherical mirrors, it at once sees almost the whole sky at a wavelength of $\\lambda\\sim 10\\mu{\\rm m}$. Its outdoor part is weather-proof and is totally maintenance-free. The images obtained every one or two minutes are analysed immediately into several ranks of weather condition, from which our automated observing system not only decides to open or close the dome, but also selects what types of observations should be done. The whole-sky data accumulated over four years show that 50$-$60 % of all nights are photometric, and about 75 % are observable with respect to cloud condition at Haleakala. Many copies of this system are now used all over the world such as Mauna Kea in Hawaii, Atacama in Chile, and Okayama and Kiso in Japan. ", "machine_abstract": "We describe an infrared cloud monitor that has been developed to detect clouds in the atmosphere above the Magellan Observatory on Mt. Haleakala, Hawaii. The system uses two near-infrared cameras and is designed to operate continuously during nighttime hours when astronomical observations are made with the telescope. It provides information about the sky conditions over the entire field-of-view (FOV) of the telescope's primary mirror. This information can be used by observers to select targets or to modify their observing strategies accordingly. We present results obtained using this instrumentation since its installation in August 2005. These data show that the atmospheric transparency varies significantly across the FOV of the telescope. For example, we find that the median value of the total integrated water vapor column density measured within one hour of each observation was 0.5 mm H2O for the northern part of the FOV but 1.1 mm H2O for the southern portion. We also demonstrate how these measurements have helped us optimize our observational strategy.", "paraphrased_abstract": "In addition, we describe the results obtained by measuring the transparency of the atmosphere in all directions within the telescope. This instrument was installed in August 2005 and its instrumentation was used to determine the atmosphere over the Magellan Observatory in Haleakala, Hawaii. We describe the instrumentation that was developed and patented, and it is equipped with two cameras, which are constantly activated during the night, when the astronomical observations are made with the telescope. It is capable of providing the total transparency of the sky over the entire field of view of the primary mirror. It is possible to use this information to select targets and to adjust the astronomical observational strategy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2009, "title": "Bit-Interleaved Coded Multiple Beamforming with Imperfect CSIT", "abstract": "  This paper addresses the performance of bit-interleaved coded multiple beamforming (BICMB) [1], [2] with imperfect knowledge of beamforming vectors. Most studies for limited-rate channel state information at the transmitter (CSIT) assume that the precoding matrix has an invariance property under an arbitrary unitary transform. In BICMB, this property does not hold. On the other hand, the optimum precoder and detector for BICMB are invariant under a diagonal unitary transform. In order to design a limited-rate CSIT system for BICMB, we propose a new distortion measure optimum under this invariance. Based on this new distortion measure, we introduce a new set of centroids and employ the generalized Lloyd algorithm for codebook design. We provide simulation results demonstrating the performance improvement achieved with the proposed distortion measure and the codebook design for various receivers with linear detectors. We show that although these receivers have the same performance for perfect CSIT, their performance varies under imperfect CSIT. ", "machine_abstract": "In this work, we consider the problem of downlink transmission in multi-user MIMO systems where each user is equipped with multiple antennas and the base station has imperfect channel state information (CSI). We propose bit-interleaved coded beamforming schemes for both single-user and multiuser scenarios to improve system performance under imperfect CSI at the transmitter side. In particular, we first develop an optimal linear precoding scheme based on minimum mean square error criterion by exploiting the statistical properties of the estimated channels. Then, we present two practical coding schemes that can be implemented efficiently using low-density parity-check codes or polar codes. Finally, numerical results are provided to demonstrate the effectiveness of our proposed schemes over existing ones. The main contributions of this thesis include:  1) Optimal Linear Precoding Scheme: We derive closed-form expressions for the ergodic capacity achieved by the optimal linear precoder when the number of transmit antennas goes to infinity.  2) Practical Code Designs: We design practical coding schemes which can be implemented efficiently using LDPC codes or polar codes.  3) Numerical Results: We provide simulation results to show the advantages of our proposed schemes compared to existing ones.", "paraphrased_abstract": "We present a preliminary study of the problem of the downlink transmission of multiuser MIMO systems in which each user has multiple antennas, but the transmitter does not have a CSI. Then we develop a simple linear precoding scheme, based on the minimum square error, which exploits the statistical properties of the channel, and then we propose two practical coding schemes, which are either LDPC or polar. The numerical results show the advantages of our proposed systems over existing ones. In the first place, we develop a linear precoding scheme based on the minimum square error of the minimum value of the channel, and exploit the statistical properties of the channels, and then we present two practical coding schemes that are easily implemented with low-density parity-check codes or polar codes, and lastly, numerical results are provided to show the advantages of our proposed systems. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.349, "title": "New scaling relations in cluster radio halos and the re-acceleration model", "abstract": "  In this paper we derive new expected scaling relations for clusters with giant radio halos in the framework of the re-acceleration scenario in a simplified, but physically motivated, form, namely: radio power (P_R) vs size of the radio emitting region (R_H), and P_R vs total cluster mass (M_H) contained in the emitting region and cluster velocity dispersion (sigma_H) in this region.   We search for these correlations by analyzing the most recent radio and X-ray data available in the literature for a well known sample of clusters with giant radio halos. In particular we find a good correlation between P_R and R_H and a very tight ``geometrical'' scaling between M_H and R_H. From these correlations P_R is also expected to scale with M_H and sigma_H and this is confirmed by our analysis. We show that all the observed trends can be well reconciled with expectations in the case of a slight variation of the mean magnetic field strength in the radio halo volume with M_H. A byproduct correlation between R_H and sigma_H is also found, and can be further tested by optical studies. In addition, we find that observationally R_H scales non-linearly with the virial radius of the host cluster, and this immediately means that the fraction of the cluster volume which is radio emitting increases with cluster mass and thus that the non-thermal component in clusters is not self-similar. ", "machine_abstract": "We present new results on the properties of diffuse radio emission associated with galaxy clusters, based on observations carried out at 1.4 GHz using the Giant Metrewave Radio Telescope (GMRT). We have studied the correlation between the total power of the diffuse radio emission and other physical parameters such as X-ray luminosity, temperature and mass for a sample of 25 galaxy clusters observed by GMRT. The main result is that we find evidence for two different populations of radio halo sources which are separated by their radio powers. In particular, our analysis shows that there exists an upper limit to the radio power of radio halos hosted by massive galaxy clusters. This suggests that the energy density of relativistic particles responsible for the synchrotron emission decreases rapidly beyond this threshold value. Our findings also suggest that the magnetic field strength increases with increasing radio power. These results can be explained within the framework of the re-acceleration model where secondary electrons produced through hadronic interactions are reaccelerated by turbulence generated during mergers or accretion events.", "paraphrased_abstract": "They are interpreted in the context of a re-acceleration model, where secondary electrons from haste are increased by the disturbances of mergers or accretion events. The results indicate that radio-frequency emission reaches an upper limit when it comes to enormous galaxy clusters. We find evidence for two separate populations of radio-frequency emitting sources that are separated by their radio-frequency output. The study of the radiating power of a diffused radio radiance in a galaxy cluster, which is measured with the GMRT, showed that the radiating power of the radiances is reduced in a very short time by a decrease in radio-frequency output. Our work is based on a radiance derived from a GMRT at a wavelength of 1.4 GHz. We found that the density of relativistic particles, which are responsible for synchrotron emission, is rapidly decreasing. We find that the strength of the magnetic field increases with increasing radio radiance. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4251, "title": "What are the Best Hierarchical Descriptors for Complex Networks?", "abstract": "  This work reviews several hierarchical measurements of the topology of complex networks and then applies feature selection concepts and methods in order to quantify the relative importance of each measurement with respect to the discrimination between four representative theoretical network models, namely Erd\\\"{o}s-R\\'enyi, Barab\\'asi-Albert, Watts-Strogatz as well as a geographical type of network. The obtained results confirmed that the four models can be well-separated by using a combination of measurements. In addition, the relative contribution of each considered feature for the overall discrimination of the models was quantified in terms of the respective weights in the canonical projection into two dimensions, with the traditional clustering coefficient, hierarchical clustering coefficient and neighborhood clustering coefficient resulting particularly effective. Interestingly, the average shortest path length and hierarchical node degrees contributed little for the separation of the four network models. ", "machine_abstract": "We propose an algorithm to find optimal hierarchical descriptors (OHDs) that can be used as fingerprints in complex networks, such as social and biological systems. The OHDs represent network structures at different levels of granularity by using a tree-like structure with nodes representing subnetworks or clusters. We show how these trees can be constructed efficiently based on modularity maximization algorithms. In addition, we introduce a novel concept called \"clustering coefficient\" which is useful when constructing OHDs. Finally, we demonstrate our method through several examples including synthetic benchmark data sets and real-world networks. Our results suggest that the proposed approach provides more accurate descriptions than existing methods. This work was supported by JSPS KAKENHI Grant Number JP26287040. Keywords: Hierarchical descriptor, Clustering coefficient, Modularity Maximization Algorithm, Tree Structure, Fingerprint, Network Description", "paraphrased_abstract": "This work was supported by the Grant Number JP26287040 from the JSPS KAKENHI Research Center. The proposed method can be used to describe complicated networks, such as social and biological systems. The proposed method can be applied to several examples, such as benchmark data and real networks. The hierarchical descriptor is the most suitable, which can be used for defining network structure, in the form of a tree. There are nodes for each layer, and each layer has a different color, and the hierarchy of the network is described. The hierarchical descriptor is used to describe the network, and each layer of it has a unique color. The hierarchy of the network can be represented in various degrees of detail, and the tree is formed into a tree. The structure is arranged in the form of a tree with nodes, representing subnetworks, and a cluster. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0671, "title": "BEC-BCS Crossover of a Trapped Two-Component Fermi Gas with Unequal Masses", "abstract": "  We determine the energetically lowest lying states in the BEC-BCS crossover regime of s-wave interacting two-component Fermi gases under harmonic confinement by solving the many-body Schrodinger equation using two distinct approaches. Essentially exact basis set expansion techniques are applied to determine the energy spectrum of systems with N=4 fermions. Fixed-node diffusion Monte Carlo methods are applied to systems with up to N=20 fermions, and a discussion of different guiding functions used in the Monte Carlo approach to impose the proper symmetry of the fermionic system is presented. The energies are calculated as a function of the s-wave scattering length a_s for N=2-20 fermions and different mass ratios \\kappa of the two species. On the BEC and BCS sides, our energies agree with analytically-determined first-order correction terms. We extract the scattering length and the effective range of the dimer-dimer system up to \\kappa = 20. Our energies for the strongly-interacting trapped system in the unitarity regime show no shell structure, and are well described by a simple expression, whose functional form can be derived using the local density approximation, with one or two parameters. The universal parameter \\xi for the trapped system for various \\kappa is determined, and comparisons with results for the homogeneous system are presented. ", "machine_abstract": "We study the crossover between Bose-Einstein condensation (BEC) and Bardeen-Cooper-Schrieffer superfluidity in an ultracold trapped two-component fermionic gas with unequal masses by using the mean-field theory based on the Bogoliubov-de Gennes equations. We find that, for equal interaction strengths, the condensate fraction is larger than that of the single component case when one mass is much lighter than the other; while it becomes smaller if both masses are comparable to each other.  The critical temperature T_c decreases as the difference between the masses increases. For large enough mass ratio, we observe a first-order phase transition at low temperatures instead of second order. In addition, we show that there exists a tricritical point where the phase transition changes from second-to-first order. Finally, we calculate the density profiles of the system under different conditions numerically.  I. INTRODUCTIO N Ultracold atomic gases have been widely used to simulate many-body quantum systems [1] . Recently, great progress has been made in studying strongly correlated fermions [2] , especially those with spin degrees of freedom [3] . In particular, the recent experimental realization of BoseEinstein condensation (BEC), i.e., macroscopic occupation of the lowest energy state [4] , in a dilute gas of bosonic atoms opens up new possibilities to explore novel phenomena such as Cooper pairing [5] or supersolidity [6] in fermion systems [7, 8] . It also provides us with a unique opportunity to investigate the interplay between these two kinds of quantum phases [9] . On the theoretical side, this problem can be studied within the framework of the so-called Bose-Fermi mixture model [10] which describes a mixture of interacting bosons and noninteracting fermions [11] . This approach was originally developed to describe nuclear matter [12] but later found applications in various fields including cold atom physics [13] . However, most previous studies were focused on the cases where all particles have identical masses [14] . In real experiments, however, the masses of the constituent particles may not always be exactly the same [15] . Therefore", "paraphrased_abstract": "It is an experiment in the simplest way, based on the Bose-Einstein equation, which describes a finite mixture of bosons and non-bosons. This is a theory originally developed for nuclear matter, but now it is applicable to cold atoms as well. However, in actual experiments the mass of the particles is not always the same. There is a unique opportunity to study the interactions between these two kinds of phases in an atom-like medium, and here we introduce a new theory, the Bose-Einstein equation, a macroscopic occupation of the lowest energy of the gas with the bosonic atoms, in a gas with many atoms, is a new technique for the study of the interactions between Bose-Einstein and Bardeen-Cooper-Schrieffer superfluidity, and the effect of the two-partition transition at low temperatures, and finally the phase transition from second to first. Finally, we calculate the density of the system under various conditions. In our experiments, the density of the gases has been studied with the use of the Bose-Einstein equations, and it is a principle for the study of the Bose-Einstein condensation, which is the macroscopic occupation of the lowest energy state in a gas of bosonic atoms, which gives new", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1393, "title": "A Panchromatic Study of the Globular Cluster NGC 1904. I: The Blue Straggler Population", "abstract": "  By combining high-resolution (HST-WFPC2) and wide-field ground based (2.2m ESO-WFI) and space (GALEX) observations, we have collected a multi-wavelength photometric data base (ranging from the far UV to the near infrared) of the galactic globular cluster NGC1904 (M79). The sample covers the entire cluster extension, from the very central regions up to the tidal radius. In the present paper such a data set is used to study the BSS population and its radial distribution. A total number of 39 bright ($m_{218}\\le 19.5$) BSS has been detected, and they have been found to be highly segregated in the cluster core. No significant upturn in the BSS frequency has been observed in the outskirts of NGC 1904, in contrast to other clusters (M 3, 47 Tuc, NGC 6752, M 5) studied with the same technique. Such evidences, coupled with the large radius of avoidance estimated for NGC 1904 ($r_{avoid}\\sim 30$ core radii), indicate that the vast majority of the cluster heavy stars (binaries) has already sunk to the core. Accordingly, extensive dynamical simulations suggest that BSS formed by mass transfer activity in primordial binaries evolving in isolation in the cluster outskirts represent only a negligible (0--10%) fraction of the overall population. ", "machine_abstract": "We present new photometry for the globular cluster NGC 1904, obtained with the Wide Field Camera 3 (WFC3) on board HST in four filters spanning the optical and near-infrared wavelength range. We find that this cluster has an extended blue horizontal branch (BHB), which is populated by both hot BHBs and blue stragglers (BSs). In order to study these populations separately we use two different methods. First, we select stars based on their location along the red giant branch (RGB); secondly, we perform artificial star tests using our best-fit model CMD as input. Both approaches yield consistent results. Our analysis shows that the fraction of BSs among all evolved stars amounts to f = 0.11 \u00b1 0.01. This value agrees well with previous studies of other clusters. Using theoretical models we estimate the age of the cluster at t = 12 Gyr.", "paraphrased_abstract": "In order to examine these populations separately, we utilise two different methods. First, we pick out stars from the rGB, and secondly, we simulate stars by using our best fit CMD. Our analysis indicates that the fraction of BSs in all the evolved stars is f = 0.2  0.01. The aforementioned theoretical calculations show that the ages of this globular cluster are t=12 gyr. The dyad is a large, blue horizontal branch, which is populated with hot, hot BSs. The dyads are hot, while the BSs are cold. We find that there is an extended blue horizontal branch, a branch that is blue with hot, cold BSs, which is the same as previous studies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2756, "title": "Black Hole Motion as Catalyst of Orbital Resonances", "abstract": "  The motion of a black hole about the centre of gravity of its host galaxy induces a strong response from the surrounding stellar population. We treat the case of a harmonic potential analytically and show that half of the stars on circular orbits in that potential shift to an orbit of lower energy, while the other half receive a positive boost and recede to a larger radius. The black hole itself remains on an orbit of fixed amplitude and merely acts as a catalyst for the evolution of the stellar energy distribution function f(E). We show that this effect is operative out to a radius of approx 3 to 4 times the hole's influence radius, R_bh. We use numerical integration to explore more fully the response of a stellar distribution to black hole motion. We consider orbits in a logarithmic potential and compare the response of stars on circular orbits, to the situation of a `warm' and `hot' (isotropic) stellar velocity field. While features seen in density maps are now wiped out, the kinematic signature of black hole motion still imprints the stellar line-of-sight mean velocity to a magnitude ~18% the local root mean-square velocity dispersion sigma. ", "machine_abstract": "We study the orbital evolution in galactic nuclei with supermassive black holes (SMBHs) and show that SMBH motion can lead to strong resonant interactions between stars, which may be responsible for some observed phenomena such as nuclear star clusters or tidal disruption events.  We use N-body simulations to demonstrate how the presence of an eccentric SMBH orbit leads to the formation of multiple families of stable orbits around it. The number of these families depends on the mass ratio between the SMBH and its host galaxy's bulge. For small mass ratios we find only one family of stable orbits; however, when this ratio is increased by several orders of magnitude, additional families appear. These new families are characterized by their high eccentricities and inclinations relative to the equatorial plane of the system. In addition, they have very different periods compared to those found previously in studies where the SMBH was assumed to move along circular orbits. Finally, we discuss possible observational signatures associated with each family of orbits.", "paraphrased_abstract": "We describe the evolution of orbits in a galactic nucleus with supermassive black holes. We show that the SMBH motion can lead to very sharp interactions between stars, which may result in a number of phenomena, such as the formation of a cluster of stars or the disruption of tidal disturbances. The number of these families is a function of the density of the SMBH and the size of its star. For the small mass of a SMBH, only one family of stable orbits is observed; but when the density of the SMBH is increased by several orders of magnitude, new families appear. The number of stable orbits is based on the equator's mass and the density of the SMBH; when the SMBH is increased by several orders of magnitude, new families are found, and their period is very different from that of previous studies, where the SMBH is assumed to move in circular orbits. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0378, "title": "Geodesics for Efficient Creation and Propagation of Order along Ising Spin Chains", "abstract": "  Experiments in coherent nuclear and electron magnetic resonance, and optical spectroscopy correspond to control of quantum mechanical ensembles, guiding them from initial to final target states by unitary transformations. The control inputs (pulse sequences) that accomplish these unitary transformations should take as little time as possible so as to minimize the effects of relaxation and decoherence and to optimize the sensitivity of the experiments. Here we give efficient syntheses of various unitary transformations on Ising spin chains of arbitrary length. The efficient realization of the unitary transformations presented here is obtained by computing geodesics on a sphere under a special metric. We show that contrary to the conventional belief, it is possible to propagate a spin order along an Ising spin chain with coupling strength J (in units of Hz), significantly faster than 1/(2J) per step. The methods presented here are expected to be useful for immediate and future applications involving control of spin dynamics in coherent spectroscopy and quantum information processing. ", "machine_abstract": "We present an algorithm that efficiently creates and propagates order in spin chains with the help of geodesic paths on graphs representing the spins. The algorithm is based on the observation that, if we consider two neighboring sites as nodes of a graph, then the probability distribution over all possible states of these two sites can be represented by a weighted edge between them. We show how to use this representation to create ordered regions within the chain using only local information about the state of each site. In particular, our method allows us to find optimal configurations of the system at low temperatures (where thermal fluctuations are small) without having to explore the entire configuration space. This makes it possible to study systems whose size would otherwise make exact calculations intractable. Our results demonstrate that the proposed approach provides accurate predictions even when applied to relatively short chains. Finally, we discuss several extensions of the presented ideas which may lead to further improvements in efficiency.     Introduction     Many physical phenomena such as magnetism or phase transitions occur due to cooperative behavior among many interacting particles. For example, magnetic ordering in solids occurs because individual atoms interact strongly via their magnetic moments. Similarly, liquid helium undergoes superfluidity below its critical temperature T_c = 2.17 K because pairs of helium-4 atoms form tightly bound bosons known as Cooper pairs. These examples illustrate that understanding collective behavior requires studying large ensembles of interacting particles rather than single isolated ones. However, simulating macroscopic properties of complex systems composed of many interacting elements remains one of the most challenging problems in computational physics today. Indeed, while microscopic interactions between individual particles can often be described accurately by quantum mechanics, describing macroscopic properties of large collections of particles typically involves approximations that cannot capture subtle effects arising from correlations between different parts of the system. As a result, numerical simulations of large-scale models of real-world systems are usually performed using approximate methods such as Monte Carlo sampling [1] . Unfortunately, these approaches become computationally expensive when used to simulate systems containing millions...", "paraphrased_abstract": "The results demonstrate that the proposed method is very accurate, even for relatively short chains. The present work reveals that the proposed method is a very accurate and efficient method, if it is applied to the small-sized chains. We have therefore, as an example, developed a method to find optimal conditions for the system at low temperatures (where thermal variations are small) and for large quantities of particles. However, the study of the macrophysical properties of a system with many elements is still a very difficult task for the mathematical physics. It is not only the microscopic interactions between individual particles, but also the macroscopic interactions of large quantities, such as atoms, molecules, and atoms, which are not yet fully understood. In this case, the inverse method can be employed, which aims at obtaining optimal configurations of the system at low temperatures (in which the thermal variations are small), and at a low temperature, in which the temperature is hardly affected. This technique allows us to study systems with very small sizes, which are inaccessible for the calculations. Moreover, our method, based on the observation that if two adjacent places are considered nodes of the graph, the probability of each of the possible states is represented by a weighting of the edge. Then we show how to create ordered regions in the chain, without the need for an exhaustive analysis of the whole configuration space. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0479, "title": "How to Compile Some NAND Formula Evaluators", "abstract": "  We say a unitary operator acting on a set of qubits has been compiled if it has been expressed as a SEO (sequence of elementary operations, like CNOTs and single-qubit operations). SEO's are often represented as quantum circuits. arXiv:quant-ph/0702144 by Farhi-Goldstone-Gutmann has inspired a recent flurry of papers, that propose quantum algorithms for evaluating NAND formulas via quantum walks over tree graphs. These algorithms use two types of unitary evolution: oracle and non-oracle. Non-oracle evolutions are independent of the NAND formula input, whereas oracle evolutions depend on this input. In this paper we compile (i.e., give explicit SEOs and their associated quantum circuits for) the oracle and non-oracle evolution operators used in some of these NAND formula evaluators. We consider here only the case of balanced binary NAND trees. Our compilation methods are based on the CSD (Cosine Sine Decomposition), a matrix decomposition from Linear Algebra. The CS decomposition has been used very successfully in the past to compile unstructured unitary matrices exactly. ", "machine_abstract": "The following is an excerpt from the author's thesis, which can be downloaded at http://arxiv.org/abs/1306.5189.     The formula evaluation problem for propositional logic formulas in conjunctive normal form (CNF) has been studied extensively and many efficient algorithms have been developed. However, most existing approaches are not suitable for evaluating large CNFs with millions or even billions of clauses due to their high computational complexity. In this work we present two novel techniques that significantly improve the efficiency of CNF formula evaluators: one based on parallelization using GPUs and another based on data compression by representing each clause as a bit vector instead of a set of literals. We implemented these techniques into our new software package called CLEVER-CNF, which outperforms state-of-the-art solvers such as SATzilla and Lingeling when evaluated against several benchmark sets including those used in recent SAT competitions.", "paraphrased_abstract": "It is also possible to include an algorithm of data compression that will reduce the number of lines of a formula by as much as ten thousand or a billion. The problem of formula evaluative formulas in the CNF has been studied extensively, and many efficient methods have been developed. However, most of the existing algorithms are not suitable for evaluative formulas of CNFs, millions or even billions of them, because of their high computational complexity. This work is a first attempt to introduce two new techniques to the problem of formula evaluative formulas, which are based on parallelization of the GPU, and on a new method of data compression, which represent each clause as a bit vector instead of a literal. The following is an extract from the thesis which you can download at http://arxiv.org/abstract/1306.5189. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0227, "title": "Zero-temperature phase of the XY spin glass in two dimensions: Genetic embedded matching heuristic", "abstract": "  For many real spin-glass materials, the Edwards-Anderson model with continuous-symmetry spins is more realistic than the rather better understood Ising variant. In principle, the nature of an occurring spin-glass phase in such systems might be inferred from an analysis of the zero-temperature properties. Unfortunately, with few exceptions, the problem of finding ground-state configurations is a non-polynomial problem computationally, such that efficient approximation algorithms are called for. Here, we employ the recently developed genetic embedded matching (GEM) heuristic to investigate the nature of the zero-temperature phase of the bimodal XY spin glass in two dimensions. We analyze bulk properties such as the asymptotic ground-state energy and the phase diagram of disorder strength vs. disorder concentration. For the case of a symmetric distribution of ferromagnetic and antiferromagnetic bonds, we find that the ground state of the model is unique up to a global O(2) rotation of the spins. In particular, there are no extensive degeneracies in this model. The main focus of this work is on an investigation of the excitation spectrum as probed by changing the boundary conditions. Using appropriate finite-size scaling techniques, we consistently determine the stiffness of spin and chiral domain walls and the corresponding fractal dimensions. Most noteworthy, we find that the spin and chiral channels are characterized by two distinct stiffness exponents and, consequently, the system displays spin-chirality decoupling at large length scales. Results for the overlap distribution do not support the possibility of a multitude of thermodynamic pure states. ", "machine_abstract": "We study the zero temperature (ground state) properties of the two-dimensional spin-glass model with nearest-neighbor interactions using an evolutionary algorithm, called genetic embedded matching heuristic (GEMH). We find that GEMH is able to reproduce the ground states obtained by simulated annealing and Monte Carlo simulations for different system sizes upto L=40. The energy distribution function shows a power law behavior at low energies indicating the presence of many metastable states. In addition we also observe a peak near E=0 which corresponds to the ground state configurations. Finally, we show that the average overlap between successive generations decreases exponentially as one goes away from the ground state configuration. This indicates that there are no other low-energy states apart from the ground state.     1 Introduction     Spin glasses have been studied extensively over last few decades both theoretically [1]-[3] and experimentally [4] . They exhibit interesting features like frustration [5] , slow relaxation [6] - [8] etc., which make them very difficult to solve exactly even on small lattices [9] . However, it has been shown recently [10] that these systems can be solved efficiently if they are allowed to evolve under certain conditions [11] - [13] . Evolutionary algorithms [14] - [16] provide us with powerful tools to tackle such problems [17] - [20] .   In this work we consider the following Hamiltonian [21] :", "paraphrased_abstract": "At present, it has been found that this type of system can be simulated in the form of a simulated annealing, a Monte Carlo simulation, and a GEMH algorithm can be used to reproduce the ground states obtained by simulations of annealing and Monte Carlo simulations, up to L=40. The energy distribution of the system shows the power law of the low-energy system, which reveals many metastable states. In addition, we find the epoch close to zero at the lowest energy level, which corresponds to the ground state, and finally we show that the average overlap between generations increases exponentially as we go away from the ground state, indicating that there is no other state that is lower than the ground state. We have been studying spin glasses for many years now, both theoretically and experimentally, and have seen a number of interesting properties such as frustration, slow relaxation, and evanescence, and that is, extremely difficult to solve, even in the smallest lattices. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.236, "title": "Magnetorotational collapse of very massive stars to black holes in full general relativity", "abstract": "  We perform axisymmetric simulations of the magnetorotational collapse of very massive stars in full general relativity. Our simulations are applicable to the collapse of supermassive stars (M > 10^3M_sun) and to very massive Pop III stars. We model our initial configurations by n=3 polytropes. The ratio of magnetic to rotational kinetic energy in these configurations is chosen to be small (1% and 10%). We find that such magnetic fields do not affect the initial collapse significantly. The core collapses to a black hole, after which black hole excision is employed to continue the evolution long enough for the hole to reach a quasi-stationary state. We find that the black hole mass is M_h = 0.95M and its spin parameter is J_h/M_h^2 = 0.7, with the remaining matter forming a torus around the black hole. We freeze the spacetime metric (\"Cowling approximation\") and continue to follow the evolution of the torus after the black hole has relaxed to quasi-stationary equilibrium. In the absence of magnetic fields, the torus settles down following ejection of a small amount of matter due to shock heating. When magnetic fields are present, the field lines gradually collimate along the hole's rotation axis. MHD shocks and the MRI generate MHD turbulence in the torus and stochastic accretion onto the central black hole. When the magnetic field is strong, a wind is generated in the torus, and the torus undergoes radial oscillations that drive episodic accretion onto the hole. These oscillations produce long-wavelength gravitational waves potentially detectable by LISA. The final state of the magnetorotational collapse always consists of a central black hole surrounded by a collimated magnetic field and a hot, thick accretion torus. This system is a viable candidate for the central engine of a long-soft gamma-ray burst. ", "machine_abstract": "We present the first fully general-relativistic magnetohydrodynamic (MHD) simulations of the gravitational collapse of rotating, magnetic white dwarfs and neutron stars with initial masses up to 100 solar masses. We find that for all models considered here, the central density increases by at least eight orders of magnitude during the collapse process. The final state is always a Kerr black hole surrounded by an accretion disk. For most cases we consider, the mass of the black hole exceeds the Chandrasekhar limit by more than 10%. This suggests that there may be no maximum stable mass for nonrotating stellar cores. In addition, our results show that the rotation rate of the newly formed black hole depends on its progenitor's spin parameter as well as its total angular momentum. Finally, we discuss how these findings can be used to explain some observed phenomena such as gamma-ray bursts and superluminous supernovae.", "paraphrased_abstract": "The density of the black hole, in this case, is dependent on its spin, and on its total angular momentum. We have also observed that the rotation rate of the black hole is regulated by the spin parameter of the planetary spin and the total angular momentum. In this case, the mass of the black hole, in most cases, is at least 10% above the Chandrasekhar limit, which means that the maximum mass of the void core is not stable. Then we present the first MHD simulations of the gravitational collapse of the magnetic white dwarfs and the neutron stars. They contain up to 100 solar masses and a mass of up to 100 solar masses. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1306, "title": "Observational Constraints on Interstellar Grain Alignment", "abstract": "  We present new multicolor photo-polarimetry of stars behind the Southern Coalsack. Analyzed together with multiband polarization data from the literature, probing the Chamaeleon I, Musca, rho Opiuchus, R CrA and Taurus clouds, we show that the wavelength of maximum polarization (lambda_max) is linearly correlated with the radiation environment of the grains. Using Far-Infrared emission data, we show that the large scatter seen in previous studies of lambda_max as a function of A_V is primarily due to line of sight effects causing some A_V measurements to not be a good tracer of the extinction (radiation field strength) seen by the grains being probed. The derived slopes in lambda_max vs. A_V, for the individual clouds, are consistent with a common value, while the zero intercepts scale with the average values of the ratios of total-to-selective extinction (R_V) for the individual clouds. Within each cloud we do not find direct correlations between lambda_max and R_V. The positive slope in consistent with recent developments in theory and indicating alignment driven by the radiation field. The present data cannot conclusively differentiate between direct radiative torques and alignment driven by H_2 formation. However, the small values of lambda_max(A_V=0), seen in several clouds, suggest a role for the latter, at least at the cloud surfaces. The scatter in the lambda_max vs. A_V relation is found to be associated with the characteristics of the embedded Young Stellar Objects (YSO) in the clouds. We propose that this is partially due to locally increased plasma damping of the grain rotation caused by X-rays from the YSOs. ", "machine_abstract": "We present new observational constraints on the alignment efficiency and degree of polarization for interstellar dust grains in protoplanetary disks, using polarized thermal emission at millimeter wavelengths (230 GHz) observed with ALMA. We find that grain alignment is efficient enough to produce significant levels of linear polarization even when the disk inclination angle is as low as 20 degrees. The inferred magnetic field strengths are consistent with those expected from theoretical models of magnetorotationally driven turbulence within these systems.     Keywords: Dust, Polarization, Magnetism, Planets, Disk, Millimeter-wave Astronomy, Astrophysics, Astrochemistry     1. Introduction     In recent years there has been growing interest in understanding how dust particles grow into planetesimals during the early stages of planet formation. This process involves collisions between dust particles which can be destructive if they occur at high velocities. However, it was shown by Goldreich & Ward (1973) that this problem could be overcome if the dust particles were aligned with respect to their local magnetic fields such that their angular momenta pointed along the direction of the magnetic field lines. If so, then the collisional cross sections would increase dramatically because the relative velocity vectors would tend to align themselves perpendicularly to each other rather than head-on. Thus, the growth timescale for dust particles would decrease significantly compared to what one would expect without any alignment mechanism.     It follows that an important step towards understanding the physics behind planet formation is to determine whether or not dust particles are indeed efficiently aligned with respect to their surrounding magnetic fields. To date, several studies have attempted to address this question through observations of scattered light from circumstellar disks around young stars (e.g., Kataoka et al. (2015), Yang et al. (2016)). These authors found evidence suggesting that dust particles may be partially aligned but only over limited regions near the star where the gas density is relatively high. On larger scales, however, the results appear inconclusive due to insufficient spatial resolution and/or sensitivity. Moreover, since scattering occurs preferentially parallel to the plane-of-sky projection of the magnetic field", "paraphrased_abstract": "This process, which is a collision of many particles, is accompanied by collisions of high velocity, which are destructive if they are not aligned correctly. In recent years, much research has been done on the subject of the polarization of the dust particles, which occurs in the early stages of planet formation. Moreover, some researchers have demonstrated, based on the scattered light of circumstellar disks around young stars (Takashi et al. (2017), Yang et al.). The results of these experiments are not very satisfactory, because they are insufficiently precise and precise. Hence, it is necessary to find out whether the dust particles are properly arranged in the magnetic field. The dust particles are aligned in the direction of the magnetic field, which is in line with the magnetic field, and their polarity is not in the same direction as in the plane of the sky. Thus, the particle density is reduced, and the time it takes to form a planet is reduced by much less than expected. We propose new methods for calculating the polarization, polarization, and polarization of the dust grains in the protoplanetary disk, using polarized thermal emission of 230 GHz, with ALMA, for which we have observed a large polarization, but which is still a little below 20 degrees, and the magnetic field strength is in agreement with the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2528, "title": "The Millennium Galaxy Catalogue: The connection between close pairs and asymmetry; implications for the galaxy merger rate", "abstract": "  We compare the use of galaxy asymmetry and pair proximity for measuring galaxy merger fractions and rates for a volume limited sample of 3184 galaxies with -21 < M(B) -5 log h < -18 mag. and 0.010 < z < 0.123 drawn from the Millennium Galaxy Catalogue. Our findings are that:   (i) Galaxies in close pairs are generally more asymmetric than isolated galaxies and the degree of asymmetry increases for closer pairs. At least 35% of close pairs (with projected separation of less than 20 h^{-1} kpc and velocity difference of less than 500 km s^{-1}) show significant asymmetry and are therefore likely to be physically bound.   (ii) Among asymmetric galaxies, we find that at least 80% are either interacting systems or merger remnants. However, a significant fraction of galaxies initially identified as asymmetric are contaminated by nearby stars or are fragmented by the source extraction algorithm. Merger rates calculated via asymmetry indices need careful attention in order to remove the above sources of contamination, but are very reliable once this is carried out.   (iii) Close pairs and asymmetries represent two complementary methods of measuring the merger rate. Galaxies in close pairs identify future mergers, occurring within the dynamical friction timescale, while asymmetries are sensitive to the immediate pre-merger phase and identify remnants.   (iv) The merger fraction derived via the close pair fraction and asymmetries is about 2% for a merger rate of (5.2 +- 1.0) 10^{-4} h^3 Mpc^{-3} Gyr^{-1}. These results are marginally consistent with theoretical simulations (depending on the merger time-scale), but imply a flat evolution of the merger rate with redshift up to z ~1. ", "machine_abstract": "We present an analysis of the relationship between morphological distortions in galaxies, as measured by their asymmetries (A), and their projected separations into close pairs (rp). We use data drawn from the Millennium Galaxy Catalogue to examine this relation over three orders of magnitude in rp. Our results show that A increases with decreasing rp at fixed luminosity, colour or stellar mass. This trend is strongest among redder galaxies, which are also more likely to be found in close pairs than bluer ones. These trends can be explained if we assume that mergers drive both the evolution of galaxy morphology and the formation of close pairs. Using these relations together with our knowledge of how close pair fractions evolve with redshift allows us to estimate the fraction of galaxies undergoing major mergers out to z = 1.5. We find that the majority of massive galaxies have undergone one or more major mergers since z=1.", "paraphrased_abstract": "The most massive galaxies, however, have already experienced a great merger, since z=1. This relationship is explained by the fact that, assuming that the mergers are the result of the morphological change of the galaxy and the formation of close pairs, we can conclude that, if we assume the assumption that the evolution of galaxy morphology is driven by mergers, it is also the case that the morphological distortion of the galaxy is caused by the resemblance to A and by the resemblance of rp. Using this information, we calculate the fraction of the galaxies that have been merged at z = 1.5. Our data, from the Millennium Galaxy Catalogue, are compared with this ratio for three orders of magnitude. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0405, "title": "Lattice Boltzmann Approach to High-Speed Compressible Flows", "abstract": "  We present an improved lattice Boltzmann model for high-speed compressible flows. The model is composed of a discrete-velocity model by Kataoka and Tsutahara [Phys. Rev. E \\textbf{69}, 056702 (2004)] and an appropriate finite-difference scheme combined with an additional dissipation term. With the dissipation term parameters in the model can be flexibly chosen so that the von Neumann stability condition is satisfied. The influence of the various model parameters on the numerical stability is analyzed and some reference values of parameter are suggested. The new scheme works for both subsonic and supersonic flows with a Mach number up to 30 (or higher), which is validated by well-known benchmark tests. Simulations on Riemann problems with very high ratios ($1000:1$) of pressure and density also show good accuracy and stability. Successful recovering of regular and double Mach shock reflections shows the potential application of the lattice Boltzmann model to fluid systems where non-equilibrium processes are intrinsic. The new scheme for stability can be easily extended to other lattice Boltzmann models. ", "machine_abstract": "The lattice Boltzmann method (LBM) is an alternative approach for solving the Navier-Stokes equations in fluid dynamics, which has been widely used due to its advantages over traditional numerical methods such as finite difference and finite element approaches.  In this work we present a new LBM scheme that can be applied to high-speed compressible flows with high Reynolds number by using multiple relaxation times (MRT). The MRT-LBM solves the discrete velocity model (DVM), where each distribution function represents one component of the macroscopic variables at different velocities on a regular grid. We use the D2Q9 DVM to solve the two-dimensional incompressible flow problems. To validate our proposed algorithm, several benchmark tests are performed including lid-driven cavity flow, Couette flow, Poiseuille flow, Taylor-Green vortex flow, and shock wave propagation through a channel. Our results show good agreement between the numerical solutions obtained by the MRT-LBM and those reported previously in literature.", "paraphrased_abstract": "This method of Solving Navier-Stokes Equations is an advanced technique for solving fluid dynamics problems. It has the advantage of being indisputable and a mathematically simple one. We present a novel LBM which can be used for compressible flows of a high Reynolds, in order to get rid of the abyss. We present the D2Q9 DVM, which deals with the two-dimensional, two-dimensional, incompressible flow. We show that the solution of LBM is a good one, in a manner that is more exact than the traditional numerical method of finite difference and finite element. We present a novel LBM that can be applied to compressible flows of a high Reynolds, i.e., those with high Reynolds, the two-dimensional eddy equations are solved. The method of LBM, known as LBM, is a mathematical method for solving the Navier-Stokes equations in fluid dynamics, which is widely used, because it is more effective than the classical numerical methods, i.e., faubourg-\u00e0-sac, blrge-sequence, and blde. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2114, "title": "Universal spectrum for DNA base CG frequency distribution in Takifugu rubripes (Puffer fish) genome", "abstract": "  The frequency distribution of DNA bases A, C, G, T exhibit fractal fluctuations ubiquitous to dynamical systems in nature. The power spectra of fractal fluctuations exhibit inverse power law form signifying long-range correlations between local (small-scale) and global (large-scale) perturbations. The author has developed a general systems theory based on classical statistical physics for fractal fluctuations which predicts that the probability distribution of eddy amplitudes and the variance (square of eddy amplitude)spectrum of fractal fluctuations follow the universal Boltzmann inverse power law expressed as a function of the golden mean. The model predicted distribution is very close to statistical normal distribution for fluctuations within two standard deviations from the mean and exhibits a fat long tail. In this paper it is shown that DNA base CG frequency distribution in Takifugu rubripes (Puffer fish) Genome Release 4 exhibit universal inverse power law form consistent with model prediction. The observed long-range correlations in the DNA bases implies that the non-coding 'junk' or 'selfish' DNA which appear to be redundant, may also contribute to the efficient functioning of the protein coding DNA, a result supported by recent studies. ", "machine_abstract": "The universal spectrum is the most important concept in physics and mathematics, which has been widely used to study various physical phenomena such as energy transfer processes in biological systems.  In this work we present an analysis on the universal spectrum of the DNA base CG frequency distribution along the chromosomes of Takifugu rubripe s(pufferfish). We find that there are two different types of universal spectra corresponding to the coding regions and non-coding regions respectively. The universal spectrum of the coding region shows a power law behavior with exponent 1.5 while that of the noncoding region exhibits a fractal structure. Our results show that the universal spectrum can be used to characterize the complexity of the DNA sequence. It may also provide new insights into the understanding of the evolution process of the genomes. Keywords: Universal Spectrum; Fractal; Power Law; Puffer Fish Genome; Energy Transfer Processes. Introduction:  The universal spectrum is one of the most important concepts in physics and mathematics, it was first introduced by Hertz [1] . Since then many scientists have studied its applications in various fields including biology [2] , geology [3] , medicine [4] etc.. Recently, some researchers found that the universal spectrum could be applied to analyze the gene expression data [5] - [8] . In recent years, more and more attention has been paid to the relationship between the universal spectrum and the energy transfer processes in biological system [9] - [11] . For example, Li et al. [12] investigated the universal spectrum of the human heart rate variability and found that the universal spectrum showed a fractal structure. They suggested that the universal spectrum might be useful in characterizing the complexity of the physiological time series. Wang et al. [13] analyzed the universal spectrum of the protein folding dynamics and they found that the universal spectrum exhibited a power-law behavior with exponent 2.0. They proposed that the universal spectrum could reflect the degree of disorderedness of the protein folding dynamics.", "paraphrased_abstract": "A universal spectrum, which is the most important, was introduced by Hertz in 1867. Then it was introduced into biology and geology by Hertz. Since then, many researchers have been working on the universal spectrum in a variety of fields. For example, Li and colleagues have investigated the universal spectrum of the human heart, and they found it to be a fractal sphere, suggesting that the universal sphere could be useful for observing the complexity of the physiological sequence. This fractal sphere is the most important spectral range, and it has been applied to the study of energy transfer in the biology of animals. The universal spectral sphere is one of the most important concepts in physics and mathematics, and was first introduced by Hertz in the 1700s. From this, many researches have been made to study the interaction of the spectral sphere with the energy of biological processes, and more recently there has been a new research in the field of the energy of biological processes. For example, Li et al. investigated the universal spectral sphere of the human heart in a manner which is fractal. The spectral sphere of the coding region is a power-law, and the spectral sphere of the coding region is a fractal structure. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3626, "title": "Mode stability in delta Scuti stars: linear analysis versus observations in open clusters", "abstract": "  A comparison between linear stability analysis and observations of pulsation modes in five delta Scuti stars, belonging to the same cluster, is presented. The study is based on the work by Michel et al. (1999), in which such a comparison was performed for a representative set of model solutions obtained independently for each individual star considered. In this paper we revisit the work by Michel et al. (1999) following, however, a new approach which consists in the search for a single, complete, and coherent solution for all the selected stars, in order to constrain and test the assumed physics describing these objects. To do so, refined descriptions for the effects of rotation on the determination of the global stellar parameters and on the adiabatic oscillation frequency computations are used. In addition, a crude attempt is made to study the role of rotation on the prediction of mode instabilities.The present results are found to be comparable with those reported by Michel et al. (1999). Within the temperature range log T_eff = 3.87-3.88 agreement between observations and model computations of unstable modes is restricted to values for the mixing-length parameter alpha_nl less or equal to 1.50. This indicates that for these stars a smaller value for alpha_nl is required than suggested from a calibrated solar model. We stress the point that the linear stability analysis used in this work still assumes stellar models without rotation and that further developments are required for a proper description of the interaction between rotation and pulsation dynamics. ", "machine_abstract": "We present an extensive study on the mode stability properties of delta Scuti (\u03b4 Sct) pulsators, based on both theoretical and observational results obtained for open clusters with ages between 1 Myr and 2 Gyr. We have performed detailed non-radial stellar oscillation calculations using state-of-the-art evolutionary models that include overshooting at convective boundaries as well as microscopic diffusion processes. The main goal is to investigate how the observed frequency pattern changes during evolution due to the effects of rotation-induced mixing and chemical composition gradients. In particular we focus our attention on the so-called mixed modes which are trapped in the region where the hydrogen burning shell overlaps with the helium core. These modes show very characteristic features such as large amplitudes and high degree of nonlinearity. Our results indicate that these modes can be excited by turbulent pressure fluctuations associated with the convection zone located near the surface layers of the star. Moreover, they also suggest that the excitation mechanism may change significantly when the star evolves off the ZAMS towards higher luminosities.", "paraphrased_abstract": "Moreover, our results show that the excitation mechanism may be changed radically, in particular as the star advances from the ZAMS into the higher luminosity range. The aim of this research is to determine the change in the frequency pattern caused by the changes in the chemical composition of the star and the chemical composition of its atmosphere. We have investigated in detail the nature of the stable and resolving properties of delta-Scuti (-scuti) pulsations, and we have made extensive observations of open clusters ranging from a few thousand years to two thousand years. In particular, we examine the so-called mixed mode, which is confined to the region where the hydrogen-burning shell and the helium core are fused, and which has characteristic features, including a high amplitude and high degree of nonlinearity. We report on a detailed analysis of the frequency of delta-Scuti (-scuti) pulsators, and on the results of observation and experiments, for open clusters with age between one and two years. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1159, "title": "Characterization of open cluster remnants", "abstract": "  Despite progress in the theoretical knowledge of open cluster remnants and the growing search for observational identifications in recent years, open questions still remain. The methods used to analyze open cluster remnants and criteria to define them as physical systems are not homogeneous. In this work we present a systematic method for studying these objects that provides a view of their properties and allows their characterization. Eighteen remnant candidates are analyzed by means of photometric and proper motion data. These data provide information on objects and their fields. We establish criteria for characterizing open cluster remnants, taking observational uncertainties into account. 2MASS J and H photometry is employed (i) to study structural properties of the objects by means of radial stellar density profiles, (ii) to test for any similarity between objects and fields with a statistical comparison method applied to the distributions of stars in the CMDs, and (iii) to obtain ages, reddening values, and distances from the CMD, taking an index of isochrone fit into account. The UCAC2 proper motions allowed an objective comparison between objects and large solid angle offset fields. The objective analysis based on the present methods indicates 13 open-cluster remnants in the sample. Evidence of the presence of binary stars is found, as expected for dynamically evolved systems. Finally, we infer possible evolutionary stages among remnants from the structure, proper motion, and CMD distributions. The low stellar statistics for individual objects is overcome by means of the construction of composite proper motion and CMD diagrams. The distributions of remnants in the composite diagrams resemble the single-star and unresolved binary star distributions of open clusters. ", "machine_abstract": "We present the results of an analysis of archival Hubble Space Telescope (HST) data for two open clusters, NGC 6705 and NGC 6819. We use these observations to determine ages, masses, radii, and distances for each star in both clusters using isochrone fitting techniques. The resulting mass functions are then compared with those predicted by theoretical models. Our main conclusions are:  1. Both clusters have similar age distributions but different mass distributions.  2. The observed mass function slopes agree well with predictions made by standard IMF theory when we assume that all stars more massive than 8 Msun were destroyed during supernovae explosions.  3. There appears to be no significant difference between the number of low-mass stars found within the central regions of the clusters as opposed to their outer regions. 4. The total number of high-mass stars in the clusters agrees very well with the expected number based on our knowledge of the initial mass function.", "paraphrased_abstract": "The resulting mass function is very well predicted, based on our knowledge of the first mass function. The ages of the two clusters are the same, but they are different. We have examined the Hubble space-tracking data of the two open clusters, namely, 6705 and 6819, and we use these data to determine the age, mass, distance, and age of the stars in each cluster. The calculated mass is in agreement with the predictions of the Standard IMF, that is, all stars more than eight thousand are destroyed in supernovae explosions. The total number of high masses is very large, as predicted by the first mass function. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1584, "title": "The standard model on a domain-wall brane?", "abstract": "  We propose a 4+1-dimensional action that is a candidate for realising a standard-model-like effective theory for fields dynamically localised to a domain-wall brane. Our construction is based in part on the conjecture that the Dvali-Shifman mechanism for dynamically localising gauge bosons works correctly in 4+1-d. Assuming this to be so, we require the gauge symmetry to be SU(5)in the bulk, spontaneously breaking to SU(3)xSU(2)xU(1) inside the domain wall, thus dynamically localising the standard model gauge bosons provided that the SU(5) theory in the bulk exhibits confinement. The wall is created jointly by a real singlet-Higgs field eta configured as a kink, and an SU(5) adjoint-Higgs field chi that takes nonzero values inside the wall. Chiral 3+1-dimensional quarks and leptons are confined and split along the bulk direction via their Yukawa couplings to eta and chi. The Higgs doublet and its colour triplet SU(5) partner are similarly localised and split. The splittings suppress coloured-Higgs-induced proton decay and, because of the different localisation profiles, the usual SU(5) mass relation m_e = m_d does not arise. Localised gravity is generated via the Randall-Sundrum alternative to compactification. ", "machine_abstract": "We consider the Standard Model (SM) in 5 dimensions, where one extra dimension is compactified to an orbifold S 1 /Z 2 . The SM fields are assumed to be localized at different fixed points along this extra dimension. We show that such models can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings. In addition we find that these models provide new ways for understanding some other issues related to the SM like neutrino mass generation or flavor changing neutral currents. Finally we discuss how our results could be tested experimentally. Introduction: One of the most important open questions in particle physics today concerns the origin of fermion families and their mixing angles. It has been known since the work by Pati & Salam [1] , that if quarks and leptons were unified into larger multiplets then it would be possible to understand the pattern of quark-lepton masses and mixings within Grand Unified Theories (GUTs). However, despite many attempts over more than 30 years no realistic GUT has yet been constructed which incorporates all the features of the Standard Model (SM). In recent years another possibility was suggested [2] - [4] : If the SM fields live in higher dimensional space-time, they may have Kaluza-Klein excitations corresponding to additional states with masses of order 1/R, where R denotes the size of the extra dimensions. These states might correspond to heavy particles beyond those present in the SM spectrum. This idea leads to interesting phenomenological consequences [5] . The simplest way to realize this scenario is to assume that only gravity propagates in the bulk while the SM fields are confined to a four-dimensional \"brane\" [6] . Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by:  where M P l = 1/ \u221a 8\u03c0G N \u2248 10 19 GeV is the reduced Planck scale and n i counts the number of extra spatial dimensions accessible to field i. For distances smaller than about 0.1 mm deviations from the inverse square law predicted by general relativity will become", "paraphrased_abstract": "\u201cOne of the most important questions in particle physics today concerns the origin of fermions and their mixing angles. For the last thirty years, although a lot of efforts have been made to build such a model as a GUT, no realistic model has been established. This is due to the fact that, although it has been proposed by Pati & Salam, to enclose quarks and leptons into larger multiplets, it is possible to understand the behavior of quarks and leptons in the unified form of GUTs. Moreover, we found that this type of model can explain the origin of three generations of fermions and gauge bosons with their observed masses and mixings. For a distance of a few mm, the inverse square law of general relativity will be observed. The inverse square law of general relativity is based on the assumption that the Newtonian potential will be corrected in the inverse square law of general relativity. We consider the SM in five dimensions, where one of the extra dimensions is compressed to an oblong S1 / Z2; the SM fields are located at different points along this extra dimension. This explains why the three generations of fermions and gauges have their mass and their mixings. We also describe how this synthesis can be carried out by analyzing the behavior of some other phenomena, such as the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0019, "title": "Nonlinear Perturbations and Conservation Laws on Curved Backgrounds in GR and Other Metric Theories", "abstract": "  The field-theoretical approach is reviewed. Perturbations in general relativity as well as in an arbitrary $D$-dimensional metric theory are studied on a background, which is a solution (arbitrary) of the theory. Lagrangian for perturbations is defined, and field equations for perturbations are derived from the variational principle. These equations are exact and equivalent to the equations in the standard formulation, but can be approximate also. The field-theoretical description is invariant under gauge (inner) transformations, which can be presented both in exact and approximate forms. Following the usual field-theoretical prescription, conserved quantities for perturbations are constructed. Conserved currents are expressed through divergences of superpotentials -- antisymmetric tensor densities. This form allows to relate a necessity to consider local properties of perturbations with a theoretical representation of the quasi-local nature of conserved quantities in metric theories. Applications of the formalism in general relativity are discussed. Generalized formulae for an arbitrary metric $D$-dimensional theory are tested in the Einstein-Gauss-Bonnet gravity. ", "machine_abstract": "We study the nonlinear perturbations of general relativity (GR) and other metric theories of gravity, focusing on their effects on conserved quantities such as energy-momentum tensors. We show that these perturbations can be decomposed into two parts: one is associated with the background geometry while another is related to the perturbation itself. In particular, we find that for any given background solution there exists an infinite number of solutions corresponding to different values of the conserved quantity. This implies that the conservation laws are not preserved under small perturbations. Furthermore, we demonstrate how this effect may lead to violations of the weak equivalence principle. Finally, we discuss possible implications of our results for cosmology and black hole physics. General relativity (GR), which describes gravitational interactions at large scales, has been tested extensively against observations over many decades [1] . However, it remains unclear whether or not GR also holds true at smaller length scales where quantum mechanics becomes important [2] . In order to address this question, several alternative theories have been proposed [3] , including scalar-tensor theories [4] , f(R)-gravity [5] , massive gravity [6] , and so forth [7, 8] . These theories typically involve additional degrees of freedom beyond those present in GR [9] . For example, in scalar-tensor theories, the graviton acquires a mass through its coupling to a scalar field [10] . Similarly, in f(R)-theories [11] , the Einstein-Hilbert action contains higher-order curvature terms [12] . It turns out that both types of theories admit self-accelerating solutions [13] , i.e., de Sitter-like solutions without requiring dark energy [14] .", "paraphrased_abstract": "For instance, in scalar tensor theories, the gravitation acquires mass by means of the gravity axis, whereas in f(R) \u2013 the tensor of Einstein-Hilbert, the law of gravity is also referred to as a higher-order curvature. This is to say that the entanglement between GR and other metric theories of gravity is based on a disturbance of conserved quantities, such as the energy-momentum tensors. This, however, has not been tested in the field of quantum mechanics for many years. In order to test this question, several alternative theories have been developed, such as scalar-tensor theories, f-R-gravity, colossal gravity, and so on. In this study we are concentrating on the nonlinear perturbations of general relativity and other metric theories of gravity. In a scalar-tensor approach, the graviton acquires mass through the attraction of a scalar field. In f-R-gravity, for instance, there is a higher order of curvature terms in the equation, so that the conservation laws are not preserved in the small perturbations. We are also concerned with the consequences of the nonlinear perturbations that general relativity and other", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0154, "title": "An Analysis of the Shapes of Interstellar Extinction Curves. V. The IR-Through-UV Curve Morphology", "abstract": "  We study the IR-through-UV interstellar extinction curves towards 328 Galactic B and late-O stars. We use a new technique which employs stellar atmosphere models in lieu of unreddened \"standard\" stars. This technique is capable of virtually eliminating spectral mismatch errors in the curves. It also allows a quantitative assessment of the errors and enables a rigorous testing of the significance of relationships between various curve parameters, regardless of whether their uncertainties are correlated. Analysis of the curves gives the following results: (1) In accord with our previous findings, the central position of the 2175 A extinction bump is mildly variable, its width is highly variable, and the two variations are unrelated. (2) Strong correlations are found among some extinction properties within the UV region, and within the IR region. (3) With the exception of a few curves with extreme (i.e., large) values of R(V), the UV and IR portions of Galactic extinction curves are not correlated with each other. (4) The large sightline-to-sightline variation seen in our sample implies that any average Galactic extinction curve will always reflect the biases of its parent sample. (5) The use of an average curve to deredden a spectral energy distribution (SED) will result in significant errors, and a realistic error budget for the dereddened SED must include the observed variance of Galactic curves. While the observed large sightline-to-sightline variations, and the lack of correlation among the various features of the curves, make it difficult to meaningfully characterize average extinction properties, they demonstrate that extinction curves respond sensitively to local conditions. Thus, each curve contains potentially unique information about the grains along its sightline. ", "machine_abstract": "We have analyzed the shapes of interstellar extinction curves in the infrared through ultraviolet wavelength range using data for more than 100 sight lines with known distances and reddenings, including those obtained by us at Kitt Peak National Observatory (KPNO) and Cerro Tololo Inter-American Observatory (CTIO). We find that all observed extinction curves can be fitted well by a single power law function A(lambda) = lambda -alpha , where alpha is an index ranging between 1.5 to 2.0. This result suggests that there are no significant differences among various types of interstellar dust grains as far as their optical properties are concerned. In addition, we show that the value of alpha correlates strongly with the total-to-selective extinction ratio Rv . These results suggest that the shape of interstellar extinction curve may provide important information on the physical conditions of interstellar matter along individual sight lines. Keywords: Interstellar extinction; Power-law index; Reddening; Dust grain", "paraphrased_abstract": "It was analyzed by the X-ray analysis of a total of more than 100 sight-lines with known distances and reddenings, including the ones we have taken from Kitt Peak National Observatory and the Tsotu Observatory. The data we have gathered have been obtained at KPNO and at CTIO. We have shown that the value of alpha correlates with the ratio of the total to the selectivity of the extinction trough. These results show that the shape of the extinction trough can be interpreted as an indication of the physical condition of the planetary matter. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0172, "title": "The geometrical quantity in damped wave equations on a square", "abstract": "  The energy in a square membrane $\\Omega$ subject to constant viscous damping on a subset $\\omega\\subset \\Omega$ decays exponentially in time as soon as $\\omega$ satisfies a geometrical condition known as the \"Bardos-Lebeau-Rauch\" condition. The rate $\\tau(\\omega)$ of this decay satisfies $\\tau(\\omega)= 2 \\min(-\\mu(\\omega), g(\\omega))$ (see Lebeau [Math. Phys. Stud. 19 (1996) 73-109]). Here $\\mu(\\omega)$ denotes the spectral abscissa of the damped wave equation operator and $g(\\omega)$ is a number called the geometrical quantity of $\\omega$ and defined as follows. A ray in $\\Omega$ is the trajectory generated by the free motion of a mass-point in $\\Omega$ subject to elastic reflections on the boundary. These reflections obey the law of geometrical optics. The geometrical quantity $g(\\omega)$ is then defined as the upper limit (large time asymptotics) of the average trajectory length. We give here an algorithm to compute explicitly $g(\\omega)$ when $\\omega$ is a finite union of squares. ", "machine_abstract": "We consider the following initial value problem for the linearized Korteweg-de Vries equation with periodic boundary conditions and zero mean value:  $$u_t+uu_x-uv_{xx}=0, \\quad u(x,0)=f(x), \\quad x \\in [0,\\pi], \\tag{1}$$ where $f\\in H^1_0([0,\\pi]).$ We prove that if $|f|\\le C$ then there exists $C_1>0$ such that for any $t\\ge 0$ we have $$\\|u(t)\\|_{H^2([0,\\pi])}\\le C_1 t^{1/2}$$ where $u(t)$ is the solution to (1). The proof relies on an explicit formula for the solution obtained by separation of variables. The key point is to show that the first term in this expression decays exponentially fast as time goes to infinity while the second one grows at most polynomially.  This result can be extended to more general nonlinear dispersive models like the Benjamin-Ono or the modified Korteweg-de Vires equations.", "paraphrased_abstract": "This result is extended to other, more general, nonlinear dispersive models, such as Benjamin-Ono or the modified Korteweg-de-Vries equations. The first term in this equation decays exponentially as time passes, while the second one grows polynomially. We have shown that if f = C, then there exists f = C, such that for any t-node 0 there exists f = C \u2013 t-node t-node t-node t-node t \u2013 t \u20131/2, where f = H \u2013 t-node t-node t-node t-node t-node t-node t-node t-node t-node t-node t-node t-node t-node t-node t-node t-node \u2013 which t-node t-node t-node t-node t-node \u2013 t-node t-node \u2013 which t-node t-no", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4429, "title": "A successive sub-grouping method for multiple sequence alignments analysis", "abstract": "  A novel approach to protein multiple sequence alignment is discussed: substantially this method counterparts with substitution matrix based methods (like Blosum or PAM based methods), and implies a more deterministic approach to chemical/physical sub-grouping of amino acids . Amino acids (aa) are divided into sub-groups with successive derivations, that result in a clustering based on the considered property. The properties can be user defined or chosen between default schemes, like those used in the analysis described here. Starting from an initial set of the 20 naturally occurring amino acids, they are successively divided on the basis of their polarity/hydrophobic index, with increasing resolution up to four level of subdivision. Other schemes of subdivision are possible: in this thesis work it was employed also a scheme based on physical/structural properties (solvent exposure, lateral chain mobility and secondary structure tendency), that have been compared to the chemical scheme with testing purposes. In the method described in this chapter, the total score for each position in the alignment accounts for different degree of similarity between amino acids. The scoring value result form the contribution of each level of selectivity for every individual property considered. Simply the method (called M_Al) analyse the n sequence alignment position per position and assigns a score which have contributes by aa identity plus a composed valuation of the chemical or of the structural affinity between the n aligned amino acids. This method has been implemented in a series of programs written in python language; these programs have been tested in some biological cases, with benchmark purposes. ", "machine_abstract": "The problem of finding the optimal alignment between two or more sequences is one of the most important problems in computational biology and bioinformatics, with applications to many areas such as protein structure prediction, phylogenetic tree reconstruction, genome assembly, etc.. The main difficulty lies in the fact that there are exponentially many possible alignments between any given set of input sequences. In this work we present an algorithm which solves exactly the same optimization problem as the popular progressive multiple sequence alignment (MSA) methods but has several advantages over them. First, our approach does not require the user to specify the number of clusters beforehand; instead it automatically determines how many clusters should be formed by analyzing the data at hand. Second, unlike MSA algorithms, ours can handle both gapless and gapped alignments simultaneously without requiring additional parameters. Finally, our algorithm runs faster than existing state-of-the-art approaches on all tested datasets while producing results of similar quality.     We also show that our algorithm can be used to solve other related problems including clustering and classification of biological sequences based on their pairwise similarity scores.", "paraphrased_abstract": "Moreover, it is possible to find the best possible match between two sequences. The most important problem of the problem is the number of sequences. There are infinitely many possibilities. In our work we propose a method to solve the problem of finding the optimal match between two sequences, which is a major problem in the field of bioinformatics. The problem of finding the right match between two sequences is a major problem in computational biology, especially in phylogenetics, in the reconstruction of phylogenetic trees, in the analysis of genomes. Our method can solve this problem even faster than the existing methods, and is also able to solve a few other related problems, such as the clustering of biological sequences and the classification of biological sequences. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2333, "title": "Discrete Choices under Social Influence: Generic Properties", "abstract": "  We consider a model of socially interacting individuals that make a binary choice in a context of positive additive endogenous externalities. It encompasses as particular cases several models from the sociology and economics literature. We extend previous results to the case of a general distribution of idiosyncratic preferences, called here Idiosyncratic Willingnesses to Pay (IWP). Positive additive externalities yield a family of inverse demand curves that include the classical downward sloping ones but also new ones with non constant convexity. When j, the ratio of the social influence strength to the standard deviation of the IWP distribution, is small enough, the inverse demand is a classical monotonic (decreasing) function of the adoption rate. Even if the IWP distribution is mono-modal, there is a critical value of j above which the inverse demand is non monotonic, decreasing for small and high adoption rates, but increasing within some intermediate range. Depending on the price there are thus either one or two equilibria. Beyond this first result, we exhibit the generic properties of the boundaries limiting the regions where the system presents different types of equilibria (unique or multiple). These properties are shown to depend only on qualitative features of the IWP distribution: modality (number of maxima), smoothness and type of support (compact or infinite). The main results are summarized as phase diagrams in the space of the model parameters, on which the regions of multiple equilibria are precisely delimited. ", "machine_abstract": "We study the generic properties of discrete choice models with social influence, where agents make decisions based on their own private information and the choices made by others in their neighborhood.  We show that if there is no correlation between individuals' private signals then any equilibrium must be symmetric across individuals (i.e., all equilibria are identical up to relabeling). If instead there exists some positive correlation among individuals' private signals, we prove that there always exist asymmetric equilibria. Finally, for intermediate levels of correlation, we provide examples showing both types of equilibria can co-exist. These results hold regardless of whether or not agents have common knowledge about each other's payoffs.  Our analysis relies heavily on recent advances in the theory of global games. The main idea behind our approach is to view an agent's decision problem as a two-stage game played against nature. At stage one, Nature chooses her signal realization; at stage two, she makes her choice conditional on this realized signal.", "paraphrased_abstract": "\u201cOur approach is based on the recent discoveries in the field of global games, which is based on the principle that a decision is a two-stage game, which is played against nature: one stage, nature chooses her signals; the other stage, she makes her choice depending on the realization of the signal. This result is true, even if there is a common understanding about the payoff of other players. We study the generic properties of discrete choices with social influence, whereby the agents are forced to choose from their own private information and from other people\u2019s choices. We show that if there is no correlation between private information and private information, then the equilibrium must be symmetrical between individuals (i.e., the equilibrium is identical up to the moment of relabeling). If there is a positive correlation among the private information of each individual, then we prove that there is always asymmetrical equilibrium. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0902, "title": "The role of elastic stresses on leaf venation morphogenesis", "abstract": "  We explore the possible role of elastic mismatch between epidermis and mesophyll as a driving force for the development of leaf venation. The current prevalent 'canalization' hypothesis for the formation of veins claims that the transport of the hormone auxin out of the leaves triggers cell differentiation to form veins. Although there is evidence that auxin plays a fundamental role in vein formation, the simple canalization mechanism may not be enough to explain some features observed in the vascular system of leaves, in particular, the abundance of vein loops. We present a model based on the existence of mechanical instabilities that leads very naturally to hierarchical patterns with a large number of closed loops. When applied to the structure of high order veins, the numerical results show the same qualitative features as actual venation patterns and, furthermore, have the same statistical properties. We argue that the agreement between actual and simulated patterns provides strong evidence for the role of mechanical effects on venation development. ", "machine_abstract": "The development and evolution of vascular networks in leaves is driven by the need to transport water efficiently, but how this process occurs remains poorly understood. Here we show that mechanical forces play an important role during vein formation through their effects on cell division orientation. We use computational models based on experimental data to demonstrate that veins form perpendicularly with respect to the direction of maximum principal stress (MPS), which coincides with the direction of growth at early stages of leaf development. The MPS also determines where new veins will emerge laterally along existing ones. Our results suggest that veins are formed as a result of anisotropic cell divisions induced by localised tensile stresses generated by differential growth between neighbouring cells. This mechanism provides a simple explanation for why veins tend to be oriented perpendicularly to the direction of growth. Vein patterning has been studied extensively over recent years because it plays such an important role in plant function [1] . In particular, the arrangement of veins within a leaf influences its ability to transport water [2] , so understanding how they develop may help us improve crop yields [3] . Veins can be classified into two main types: major veins, which run longitudinally down the centre of the lamina; and minor veins, which branch out from them towards the edges [4] . Major veins typically have larger diameters than minor veins [5] , and are composed of several layers of xylem vessels surrounded by phloem [6] . Minor veins consist only of one or two layers of parenchyma cells [7, 8] . Both types of vein contain living tissue called mesophyll [9] .", "paraphrased_abstract": "It has been well studied that veins develop in plants. In particular, the growth of the vascular system is determined by the nature of the veins in the plant, and by the nature of the formation of veins in the leaf, it is necessary to transport water, and it is important to know how to understand this process. Veins are classified into two categories: major veins, which extend along the whole length of the lamina, and minor veins, which grow in one or two layers. The major veins are usually larger than the minor veins, and they are composed of several layers of xylem, and are surrounded by phloem, while the minor veins consist of only one or two layers of the parenchyma. Our results show that the formation of veins is affected by the entropical movement of cells, which is induced by local tensile stresses, produced by the different growth processes of adjacent cells. This is an easy explanation for why veins are inclined perpendicular to the direction of growth. Our result also proves that veins are inclined to the direction of growth, as a result of the entropical effects of the forces which have a strong influence on the growth of the vascular system. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0282, "title": "Single Top Results from CDF", "abstract": "  The CDF Collaboration has analyzed 955/pb of CDF II data to search for electroweak single top quark production at the Tevatron. We employ three different analysis techniques to search for a single top signal: multivariate likelihood functions; neural networks; the matrix element analysis technique. The sensitivities to a single top signal at the rate predicted by the Standard Model are 2.1 - 2.6 sigma. The first two analyses observe a deficit of single top-like events and set upper limits on the production cross section. The matrix element analysis observes a 2.3 sigma single top excess and measures a combined t-channel and s-channel cross section of 2.7 +1.5-1.3 pb. Using the same dataset, we have searched for non-Standard Model production of single top quarks through a heavy W' boson resonance. No evidence for a signal is observed. We exclude at the 95 % C.L. W' boson production with masses of 760 GeV/c^2 (790 GeV/c^2) in case the right handed neutrino is smaller (larger) than the mass of the W' boson. ", "machine_abstract": "The Compact Muon Solenoid (CMS) experiment at the Large Hadron Collider has recently reported evidence for a new particle with mass around 125 GeV, consistent with Standard Model expectations for the Higgs boson.  The D0 collaboration at Fermilab is also searching for this signal in its data set and has presented results on the search for single top quarks produced via t-channel exchange of a virtual W-boson as well as s-channel production through gluon fusion.   In both cases we find no significant excess over background predictions. We present our results here along with those from other experiments that have searched for similar signals. The CMS experiment at the LHC has recently reported evidence for an unexpectedly light scalar resonance decaying to pairs of photons or leptons [1] . This observation is compatible with the Standard Model expectation for the Higgs boson [2] , which would be expected to weigh about 126 GeV [3] . In addition to the standard model Higgs boson searches performed by ATLAS [4] and CMS [5] , there are many extensions of the SM [6] that predict additional scalars [7, 8] . These models can lead to deviations from the SM prediction for the Higgs boson properties [9] such as spin [10] , parity [11] , CP [12] , coupling strengths [13] , branching ratios [14] , etc.. Many of these scenarios involve heavy particles that may be pair-produced at hadron colliders [15] . However, some theories [16] suggest that the Higgs-like state could be singlet under SU(2), U(1). Such states cannot be directly produced in pairs but only appear in association with another quark [17] . For example, in supersymmetric models [18] , the Higgs-like state appears in association with b-quarks [19] . Other examples include composite [20] and Little-Higgs [21] models where the Higgs-like state couples preferentially to third generation fermions [22] .", "paraphrased_abstract": "The synthesis of the Higgs Boson has been carried out by ATLAS and CMS, and some of these synthesises have led to deviations in the properties of the Higgs Boson, such as the spin, the parity, the CP, the branching strengths, and so on. The results of these experiments are compared with the predictions of the standard SM, in that the Higgs Boson does not have to be singled out in a pair, but merely a single state appears. Moreover, there are many extensions of the Higgs Boson, such as the spin, the parity, the CP, the coupling strengths, the branching ratios, etc. Many of these models, besides the standard model, are composed of heavy particles, which can be pair-produced in hadron colliders. The CMS experiment at the LHC recently detected a new, light scalar resonance, which is in agreement with the expectations of the Standard Model, and which is expected to weigh about 125 GeV. In addition, the D0 experiment at Fermilab has also been searching for such a signal in its data set and has shown that the top quarks produced by t-channels of the virtual W-boson and gluon fusion have both been detected in the same manner. However, it is possible that the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0634, "title": "A Finite Element framework for computation of protein normal modes and mechanical response", "abstract": "  A coarse-grained computational procedure based on the Finite Element Method is proposed to calculate the normal modes and mechanical response of proteins and their supramolecular assemblies. Motivated by the elastic network model, proteins are modeled as homogeneous isotropic elastic solids with volume defined by their solvent-excluded surface. The discretized Finite Element representation is obtained using a surface simplification algorithm that facilitates the generation of models of arbitrary prescribed spatial resolution. The procedure is applied to compute the normal modes of a mutant of T4 phage lysozyme and of filamentous actin, as well as the critical Euler buckling load of the latter when subject to axial compression. Results compare favorably with all-atom normal mode analysis, the Rotation Translation Blocks procedure, and experiment. The proposed methodology establishes a computational framework for the calculation of protein mechanical response that facilitates the incorporation of specific atomic-level interactions into the model, including aqueous-electrolyte-mediated electrostatic effects. The procedure is equally applicable to proteins with known atomic coordinates as it is to electron density maps of proteins, protein complexes, and supramolecular assemblies of unknown atomic structure. ", "machine_abstract": "We present an efficient computational method to compute the elastic properties, vibrational frequencies and normal modes of proteins using finite element analysis (FEA). The proposed approach is based on representing each amino acid residue as a single node in a 3D tetrahedral mesh that covers the entire structure of interest. We show how this representation can be used within FEA to efficiently calculate the stiffness matrix associated with the system under study. In addition we demonstrate how the same representation allows us to obtain accurate results when computing the eigenfrequencies and corresponding mode shapes of the system by solving the generalized eigenvalue problem arising from the discretized form of the governing equations. Finally, we illustrate our methodology through several numerical examples involving different types of proteins. Our results indicate that the proposed approach provides highly accurate predictions compared to those obtained via other state-of-the-art methods such as molecular dynamics simulations or coarse-grained models. This work was supported by NSERC Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189 .", "paraphrased_abstract": "Moreover, the present system, with the use of the same representation, is capable of computing the stiffness matrix of the system, and thereby, in this way, is able to obtain accurate eigenvalues and corresponding mode forms, solving the generalized eigenvalue problem, which arises from the discretized form of the governing equations. We are supported by the National Science Foundation Grants NSERC RGPIN-2014-06576, and NSERC Grants NSERC-07907. Moreover, we demonstrate how the representation is capable of solving the generalized eigenvalue problem, a generalized eigenvalue problem, arising from the discretized form of the governing equations. We show how the proposed method is able to predict elastic properties, the vibrational frequencies and the normal mode of proteins by using FEA. We show how we can apply the same method to the elastic properties, the vibrational frequencies and the normal mode of the protein. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0009, "title": "The Spitzer c2d Survey of Large, Nearby, Insterstellar Clouds. IX. The Serpens YSO Population As Observed With IRAC and MIPS", "abstract": "  We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacy observations of the Serpens star-forming region. In particular we present a set of criteria for isolating bona fide young stellar objects, YSO's, from the extensive background contamination by extra-galactic objects. We then discuss the properties of the resulting high confidence set of YSO's. We find 235 such objects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. An additional set of 51 lower confidence YSO's outside this area is identified from the MIPS data combined with 2MASS photometry. We describe two sets of results, color-color diagrams to compare our observed source properties with those of theoretical models for star/disk/envelope systems and our own modeling of the subset of our objects that appear to be star+disks. These objects exhibit a very wide range of disk properties, from many that can be fit with actively accreting disks to some with both passive disks and even possibly debris disks. We find that the luminosity function of YSO's in Serpens extends down to at least a few x .001 Lsun or lower for an assumed distance of 260 pc. The lower limit may be set by our inability to distinguish YSO's from extra-galactic sources more than by the lack of YSO's at very low luminosities. A spatial clustering analysis shows that the nominally less-evolved YSO's are more highly clustered than the later stages and that the background extra-galactic population can be fit by the same two-point correlation function as seen in other extra-galactic studies. We also present a table of matches between several previous infrared and X-ray studies of the Serpens YSO population and our Spitzer data set. ", "machine_abstract": "We present the results of our analysis of the infrared properties (3.6-24 microns) of young stellar objects in the nearby star forming region Serpens South using data obtained with the Infrared Array Camera (IRAC; Fazio et al., 2004 ) on board the Spitzer Space Telescope . We have identified a total of 1,082 candidate young stars based upon their mid-infrared colors and luminosities , including 759 previously known members of this cluster . Of these newly discovered sources , we find that only about half are associated with outflows as traced by shocked H 2 emission or radio continuum emission . The majority of the remaining new candidates appear to be deeply embedded protostars which lack significant amounts of circumstellar material . These findings suggest that many of the new candidates may represent an earlier evolutionary stage than those already known within this cloud complex . This is consistent with previous studies suggesting that there exists a large population of low-mass pre-main sequence stars in Serpens South which remain undiscovered due to their youthful ages and/or relatively high levels of extinction .", "paraphrased_abstract": "This is in agreement with the previous results, which suggested that there was a great population of pts-disease-stars in the vicinity of Serpens-South, which were not discovered until relatively recently, and which were of high extinction. We present the results of our study on the infrared properties of the newly discovered young stars in the region of the south of Serpens-South, as well as a number of previously known stars. We have found that, for the most part, the outfalls of these young stars are quite faint, as we have seen with H 2 and radio continuum emission. The remaining candidates are deep and well buried, and there are not many circumstellar layers. We have therefore found that some of the new discoveries may represent an earlier evolutionary stage than those already known in this cloud complex. We have examined the mid-infrared properties of the stars in the vicinity of the South Pole, and have found a total of 1,824 candidate stars with the colour and luminosity of 3.6 m. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0136, "title": "The largest eigenvalues of finite rank deformation of large Wigner matrices: convergence and nonuniversality of the fluctuations", "abstract": "  In this paper, we investigate the asymptotic spectrum of complex or real Deformed Wigner matrices $(M_N)_N$ defined by $M_N=W_N/\\sqrt{N}+A_N$ where $W_N$ is an $N\\times N$ Hermitian (resp., symmetric) Wigner matrix whose entries have a symmetric law satisfying a Poincar\\'{e} inequality. The matrix $A_N$ is Hermitian (resp., symmetric) and deterministic with all but finitely many eigenvalues equal to zero. We first show that, as soon as the first largest or last smallest eigenvalues of $A_N$ are sufficiently far from zero, the corresponding eigenvalues of $M_N$ almost surely exit the limiting semicircle compact support as the size $N$ becomes large. The corresponding limits are universal in the sense that they only involve the variance of the entries of $W_N$. On the other hand, when $A_N$ is diagonal with a sole simple nonnull eigenvalue large enough, we prove that the fluctuations of the largest eigenvalue are not universal and vary with the particular distribution of the entries of $W_N$. ", "machine_abstract": "We study the spectrum statistics of random matrices with entries given by the sum over all paths in an infinite tree, which is obtained as a limit of finite rank deformations of large Wigner matrices. We show that for any fixed number of particles there exists a sequence of such deformations converging to the limiting matrix whose spectral properties are described by the Tracy-Widom distribution. In particular we prove universality of this distribution for the case when the number of particles tends to infinity. The results presented here generalize those known previously only for the Gaussian ensembles. This work was supported by Russian Science Foundation grant 14-50-00040. 1 Introduction. The problem of statistical description of the energy levels of complex quantum systems has been studied extensively during last decades (see e.g., [1] ). It turns out that many important features of these spectra can be understood within Random Matrix Theory (RMT) [2] . RMT deals with the statistical analysis of the eigenvalue distributions of various families of random matrices. One of the most popular models considered in RMT is the so-called Wigner ensemble [3] , where one considers N \u00d7N Hermitian matrices H = H \u2020 with independent identically distributed elements h ij having mean zero and variance 1/N . For example, if the probability density function p(h) of each element h ij decays fast enough at |h| \u2192 \u221e then it follows from the standard arguments [4] that the empirical measure \u00b5 N of the normalized eigenvectors of H converges weakly almost surely to some deterministic probability measure \u00b5 on the unit circle T = {z \u2208 C : |z| = 1} called circular law. Moreover, under additional assumptions about the decay rate of p(h), the Stieltjes transform m N (z) := E TrH \u2212 z \u22121 of \u00b5 N converges pointwise almost surely to the Stieltjes transform M (z) of \u00b5 [5] . In recent years much attention has been paid to the investigation of the local behavior of the spectrum near its edge [6] - [8] . In particular, it turned out [9] that the fluctuation statistics of the largest eigenvalues \u03bb max i", "paraphrased_abstract": "But in a case where the number of particles reaches infinitely many, we demonstrate that there is a sequence of deformations, converging upon the limiting matrix of a sphere which is the Tracy-Widom distribution. The study of this distribution is carried out by the principles of RMT, which are applied to various families of random matrices. A particular class of models has been used for the study of the local fluctuations of the sphere at its edge. It is found that the fluctuations of the sphere of the maximum value of i converges on the Stieltjes transform, which is in fact the Schr\u00f6de transform, which is in the form of a circle of circles whose radius is about two tenths of the radius of i. Moreover, for additional assumptions about the decay of i, the Stieltjes transform, ii. v, has the inverse relationship to iiii. In the course of recent years, a great deal of attention has been devoted to the statistical analysis of the energy level of complex systems, especially of the so-called Wigner ensembles. In particular, we find that the eigenvalues of the largest elements iii are, as a rule, eigenvectors of the higher order, the largest number of elements ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2238, "title": "A unified model for AGN feedback in cosmological simulations of structure formation", "abstract": "  We discuss a numerical model for black hole growth and its associated feedback processes that for the first time allows cosmological simulations of structure formation to self-consistently follow the build up of the cosmic population of galaxies and active galactic nuclei. Our model assumes that seed black holes are present at early cosmic epochs at the centres of forming halos. We then track their growth from gas accretion and mergers with other black holes in the course of cosmic time. For black holes that are active, we distinguish between two distinct modes of feedback, depending on the black hole accretion rate itself. Black holes that accrete at high rates are assumed to be in a `quasar regime', where we model their feedback by thermally coupling a small fraction of their bolometric luminosity to the surrounding gas. For black holes with low accretion rates, we conjecture that most of their feedback occurs in mechanical form, where AGN-driven bubbles are injected into a gaseous environment. Using our new model, we carry out TreeSPH cosmological simulations on the scales of individual galaxies to those of massive galaxy clusters, both for isolated systems and for cosmological boxes. We demonstrate that our model produces results for the black hole and stellar mass densities in broad agreement with observational constraints. We find that the black holes significantly influence the evolution of their host galaxies, changing their star formation history, their amount of cold gas, and their colours. Also, the properties of intracluster gas are affected strongly by the presence of massive black holes in the cores of galaxy clusters, leading to shallower metallicity and entropy profiles, and to a suppression of strong cooling flows. [Abridged] ", "machine_abstract": "We present a new method to implement the effect of Active Galactic Nuclei (AGNs) on their host galaxies and surrounding intergalactic medium into hydrodynamical cosmological simulations, which is based on an analytical description of the physical processes involved. The main idea behind our approach is that we use the observed scaling relations between black hole mass, luminosity and velocity dispersion as well as the correlation between radio loudness and Eddington ratio to predict the properties of individual AGNs at each time step during the simulation run. We then inject thermal energy or kinetic momentum into the gas particles around these objects according to their predicted luminosities and velocities. Our implementation allows us to reproduce several observational results such as the M-\u03c3 relation, the fundamental plane of elliptical galaxies, the fraction of red sequence galaxies and the evolution of galaxy clustering with redshift. In addition, it also leads to realistic predictions about the number density distribution of quasars and the cosmic X-ray background spectrum.", "paraphrased_abstract": "And the swarms of quasars are measured, and we estimate the occurrence of quasars in the X-ray background. In addition, we estimate the number density of quasars and the background X-rays. In this work, we propose to incorporate the effect of active Galactic nuclei on their host galaxies and their surrounding atmospheres. In this work, we present a method for interpreting the behavior of the black hole masses, their mass, their luminosity, their velocity, and their proportion of elliptical galaxies, their chromospheres, and their redshift. The system is based on the analytical analysis of the physical processes. The idea is that we apply the observed relationship between mass, luminosity and velocity, and the correlation between the radio frequency and the Eddington ratio, in order to predict the properties of individual AGNs. We then inject energy or kinetic momentum into the gases around these objects, depending on their predicted luminosities and velocities. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2417, "title": "The Spectral Index Distribution of EGRET Blazars: Prospects for GLAST", "abstract": "  The intrinsic distribution of spectral indices in GeV energies of gamma-ray--loud blazars is a critical input in determining the spectral shape of the unresolved blazar contribution to the diffuse extragalactic gamma-ray background, as well as an important test of blazar emission theories. We present a maximum-likelihood method of determining the intrinsic spectral index distribution (ISID) of a population of gamma-ray emitters which accounts for error in measurement of individual spectral indices, and we apply it to EGRET blazars. We find that the most likely Gaussian ISID for EGRET blazars has a mean of 2.27 and a standard deviation of 0.20. We additionally find some indication that FSRQs and BL Lacs may have different ISIDs (with BL Lacs being harder). We also test for spectral index hardening associated with blazar variability for which we find no evidence. Finally, we produce simulated GLAST spectral index datasets and perform the same analyses. With improved statistics due to the much larger number of resolvable blazars, GLAST data will help us determine the ISIDs with much improved accuracy. Should any difference exist between the ISIDs of BL Lacs and FSRQs or between the ISIDs of blazars in the quiescent and flaring states, GLAST data will be adequate to separate these ISIDs at a significance better than 3 sigma. ", "machine_abstract": "We present the spectral index distribution (SID) of blazars detected by the Energetic Gamma Ray Experiment Telescope (EGRET). The SID is derived using two different methods, and we find that both give consistent results within statistical errors. We compare our results with those obtained in previous studies based on smaller samples or limited energy ranges. Our sample contains all sources listed as \"blazar candidates\" in the Third EGRET Catalog (3EG), which includes 157 objects. In addition to these 3EG blazars, we include four more BL Lacertae-type objects whose radio properties are similar to those of known blazars but have not been included in any previous study due to their lack of detection at high energies. These additional sources were found through cross-correlation between the V\u00e9ron-Cetty & V\u00e9ron catalog of active galactic nuclei and the NRAO VLA Sky Survey. Using this combined sample consisting of 161 blazars, we calculate the fraction of flat-spectrum (FSRQ-like) and steep-spectrum (BL Lac-like) blazars among the total number of blazars observed by EGRET. This fraction depends strongly on the assumed value of the high-energy cutoff E_cutof f . For example, if we assume E_cutof f = 10 GeV, then FSRQ-like blazars constitute about 80% of the entire population; however, if we adopt an extremely low value such as 1 MeV, only 20% of the blazars can be classified as FSRQ-like.", "paraphrased_abstract": "It is a matter of fact that we are comparing our results with that of other studies, in which the sample is smaller and the energy is less. We find that our sample contains all the sources that were named \"prospects\" in the Third EGRET Catalogue, which contains 157 objects. This collection of prospects includes the objects in the third EGRET Catalogue, which contains 157 objects. Our sample also contains four other objects, which are in similar radio properties as the prodigals of known blazars, but of which we did not include any previous study, due to their detection at high energies. This proportion is strongly dependent on the assumed value of Ecutof F, which is 10 GeV. The prodigal blazars are only about 80% of the population. However, if Ecutof F is only 10 GeV, then prodigal blazars are only about 20% of the population. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2557, "title": "New systematics in charmless strange $B^+ \\to VP$ decays", "abstract": "  Latest data on charmless strange vector-pseudoscalar $B^+$ decays now including $B^+\\to \\rho^+ K^o$ confirm a simple penguin model in which the gluon $G$ in an initial $\\bar s u G$ state fragments equally into $u \\bar u$, $d \\bar d$ and $s \\bar s$ and all form factors are equal. A search for possible additional contributions shows only a few signals not obscured by experimental errors whose implications are discussed. The experimental value of $0.25 \\pm 0.11$ for the ratio of the branching ratios $BR(B^+ \\to K^{*+}\\eta)$ to $BR(B^+\\to K^{*+}\\eta')$ confirms the parity selection rule prediction 0.32. Large violations arise in a new sum rule for the sum of these branching ratios, analogous to the similar pseudoscalar sum rule including $K^+\\eta$ and $K^+\\eta'$. Indications for either an electroweak penguin contribution or additional admixtures like instrinsic charm in the $\\eta-\\eta'$ system remain to be clarified. An alternative symmetry description with new predictive power clarifies the simple penguin approximation and presents new predictions which can be tested experimentally. The fragmentation of the $\\bar s u G$ state into two mesons is described by a strong interaction S-matrix dominated by nonexotic hadron resonances in multiparticle intermediate states. ", "machine_abstract": "We present the first measurement of the branching fractions for all charmless hadronic decay modes of the B + meson to final states with one vector and one pseudoscalar particle, where V is either a J/$\\psi$ or $\\eta$ meson.  The data sample used corresponds to an integrated luminosity of 1 fb-1 collected by the LHCb experiment at centre-of-mass energies between 7 and 8 TeV during 2011 and 2012.  The results are obtained using a maximum likelihood fit to the invariant mass distributions of the selected candidates. We find that the measured branching fraction values agree well with theoretical predictions based on factorisation approaches but disagree significantly with those predicted within the framework of perturbative QCD calculations including corrections up to next-to-leading order (NLO).  These measurements provide important constraints on models describing non-factorisable contributions to these processes.   Introduction  Charmless hadronic decays of the B + mesons have been studied extensively over many years both experimentally and theoretically [1] . These decays proceed through loop diagrams involving virtual particles such as charm quarks which can be calculated using perturbation theory [2] , while non-perturbative effects enter via matrix elements of four-quark operators [3] . In this Letter we report new measurements of the branching fractions for the decays B + \u2192 K * 0 P + , B + \u2192 K * + P0 , B + \u2192 K + \u03c0 \u2212 \u03b7\u2032 and B + \u2192 K + K \u2212 \u03c0 + [4] . In addition, we also measure the ratio of branching fractions R = B(B + \u2192 K * 0 \u03c0 + )/B(B + \u2192 K + \u03c0 0 ). All these decays involve only light quarks and therefore their amplitudes should be dominated by short-distance physics. However, they differ in the number of spectator quarks involved; hence it is interesting to study how the observed rates depend on this variable.", "paraphrased_abstract": "We therefore calculate the ratio of the branching fraction B (B+  K + 0 P) and B (B+  K + 0 P), namely B (B +  K + 0 P), where B is a vector and a pseudoscalar particle V. These particles are called charm quarks, and can be calculated from a kinetic theory, or from the matrix elements of four-quark operators. The decays in this letter are very simple and a physico-chemical phenomenon, so that the amplitudes are high, and the lengths of the amplitudes are short, and therefore it is interesting to study how these phenomena affect the observed time. Moreover, we are measuring the ratio of branching fractions R = B(B(B(B))0)0 0. This result corresponds to an integrated luminosity of one fb- 1 at the LHCb experiment, between seven and eight TeV, during 2011 and 2012. The experiment was performed at the centre of mass at the apse of 7 and 8 TeV during 2011 and 2012. The data obtained were the combined luminosity of a one-dimensional particle, where V is either a J/psi or a Eta meson. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2029, "title": "The Chemical Compositions of the Type II Cepheids -- The BL Her and W Vir Variables", "abstract": "  Abundance analyses from high-resolution optical spectra are presented for 19 Type II Cepheids in the Galactic field. The sample includes both short-period (BL Her) and long-period (W Vir) stars. This is the first extensive abundance analysis of these variables. The C, N, and O abundances with similar spreads for the BL Her and W Vir show evidence for an atmosphere contaminated with $3\\alpha$-process and CN-cycling products. A notable anomaly of the BL Her stars is an overabundance of Na by a factor of about five relative to their presumed initial abundances. This overabundance is not seen in the W Vir stars. The abundance anomalies running from mild to extreme in W Vir stars but not seen in the BL Her stars are attributed to dust-gas separation that provides an atmosphere deficient in elements of high condensation temperature, notably Al, Ca, Sc, Ti, and $s$-process elements. Such anomalies have previously been seen among RV Tau stars which represent a long-period extension of the variability enjoyed by the Type II Cepheids. Comments are offered on how the contrasting abundance anomalies of BL Her and W Vir stars may be explained in terms of the stars' evolution from the blue horizontal branch. ", "machine_abstract": "The chemical compositions of the type II Cepheid variables are investigated by using high-resolution spectra obtained with Subaru/HDS, Keck/HIRES, VLT/UVES, and McDonald/DEIMOS spectrographs.  We find that all of our sample stars have solar or super-solar metallicities ([Fe/H]>+0.2). In addition to Fe I lines, we also use Mg I triplet lines in order to determine their atmospheric parameters more accurately. For some of these stars, we detect significant overabundances for CNO elements as well as s-process elements; however, there is no clear correlation between the abundance patterns and pulsation periods. These results suggest that the observed abundance anomalies may be caused by mass transfer during binary evolution rather than internal mixing processes associated with stellar pulsations. This work was supported by JSPS Grant-in-Aid for Scientific Research on Priority Areas No. 16071203.", "paraphrased_abstract": "As a result of the new synthesis, a new crystal was derived, which was not fabricated by any other method, and that the ephemeral and the crystalline phases were not produced, the radiances of the stars were as high as 0.5\u00b0, whereas the peaks of the rays were not matched by the pulsation. The composition of the three types of Cepheid was investigated by the use of Subaru, Keck, HiRES, VLT, UVES and McDonald spectroscopies. We report that all the stars in our sample have a solar or supersolar metallicity of 0.2 - 0.3. In some of the stars, we found that the abundances of the X-rays and the s-rays were augmented by considerable amounts of clad elements, but there was no clear correlation between the abundances and the pulsation periods. These results show that the observed abundances are the result of a process of fusion, rather than the result of a pulsation in the form of a pulsating pulsation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1767, "title": "The multiplicity of planet host stars - New low-mass companions to planet host stars", "abstract": "  We present new results from our ongoing multiplicity study of exoplanet host stars, carried out with the infrared camera SofI at ESO-NTT. We have identified new low mass companions to the planet host stars HD101930 and HD65216. HD101930AB is a wide binary systems composed of the planet host star HD101930A and its companion HD101930B which is a M0 to M1 dwarf with a mass of about 0.7Msun separated from the primary by ~73arcsec (2200AU projected separation). HD65216 forms a hierarchical triple system, with a projected separation of 253AU (angular separation of about 7arcsec) between the planet host star HD65216A and its close binary companion HD65216BC, whose two components are separated by only ~0.17arcsec (6AU of projected separation). Two VLT-NACO images separated by 3 years confirm that this system is co-moving to the planet host star. The infrared photometry of HD65216B and C is consistent with a M7 to M8 (0.089Msun), and a L2 to L3 dwarf (0.078Msun), respectively, both close to the sub-stellar limit. An infrared spectrum with VLT-ISAAC of the pair HD65216BC, even though not resolved spatially, confirms this late spectral type. Furthermore, we present H- and K-band ISAAC infrared spectra of HD16141B, the recently detected co-moving companion of the planet host star HD16141A. The infrared spectroscopy as well as the apparent infrared photometry of HD16141B are both fully consistent with a M2 to M3 dwarf located at the distance of the planet host star. ", "machine_abstract": "We report the discovery of three new planets orbiting nearby M dwarfs using high contrast imaging with NACO at VLT, and one additional companion candidate around GJ 436. The newly discovered objects are all very close-in (<2 AU) and have masses between 0.1 and 1 MJup. We also present an analysis of the mass distribution for known exoplanets in orbits shorter than 2 days as well as their orbital eccentricities. This study shows that there is no significant difference between the properties of short period planets found by radial velocity surveys or direct detection techniques. These results suggest that most short-period planets form through gravitational instability rather than core accretion. Keywords: Extrasolar Planet, Direct Imaging, Multiplicity, Formation, Mass Distribution, Eccentricity, Gravitational Instability, Core Accretion, Low-Mass Star, Nearby Stars", "paraphrased_abstract": "The results show that the most frequent and short-lived planets are formed by gravitational instability rather than by core accretion. This study finds that the properties of planetary spheres, which were observed by radial velocity studies and based on direct observations, show that they formed by gravitational instability rather than by accretion. The new objects are very close, 2 AU, with masses of 0.1 MJup. We show that the mass distributions of exoplanets, which have orbits shorter than 2 days, are not significant, but the equilateral eccentricities are not as large as those of known exoplanets of a time of less than 2 days. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3155, "title": "Geometrical properties of local dynamics in Hamiltonian systems: the Generalized Alignment Index (GALI) method", "abstract": "  We investigate the detailed dynamics of multidimensional Hamiltonian systems by studying the evolution of volume elements formed by unit deviation vectors about their orbits. The behavior of these volumes is strongly influenced by the regular or chaotic nature of the motion, the number of deviation vectors, their linear (in)dependence and the spectrum of Lyapunov exponents. The different time evolution of these volumes can be used to identify rapidly and efficiently the nature of the dynamics, leading to the introduction of quantities that clearly distinguish between chaotic behavior and quasiperiodic motion on $N$-dimensional tori. More specifically we introduce the Generalized Alignment Index of order $k$ (GALI$_k$) as the volume of a generalized parallelepiped, whose edges are $k$ initially linearly independent unit deviation vectors from the studied orbit whose magnitude is normalized to unity at every time step. The GALI$_k$ is a generalization of the Smaller Alignment Index (SALI) (GALI$_2$ $\\propto$ SALI). However, GALI$_k$ provides significantly more detailed information on the local dynamics, allows for a faster and clearer distinction between order and chaos than SALI and works even in cases where the SALI method is inconclusive. ", "machine_abstract": "We present an algorithm for computing geometrical characteristics of chaotic trajectories in dynamical systems, which is based on the concept of the alignment index introduced by S.A. Afraimovich and A.V. Bykov.  The proposed approach allows one to study the geometry of invariant manifolds associated with unstable periodic orbits embedded into the phase space of a given dynamical system. We demonstrate that this new technique can be successfully applied to investigate the structure of strange attractors arising in dissipative systems as well as in conservative ones. In particular, we show how it works for the Lorenz model and the Henon-Heiles potential.  The presented results are obtained within the framework of the Russian Science Foundation project 14-50-00040. Geometrical properties of local dynamic in hamiltonian systems: the generalized alignment index (gali) method  V.I. Klimenko1 , D.N. Kozlov2 1 Institute of Mathematics RAS, Moscow, Russia 2 Lomonosov Moscow State University, Moscow, Russia", "paraphrased_abstract": ".. Geometrical properties of local dynamical systems in hamiltonian systems. V.I. Klimenko, D.N. Kozlov, and V.I. Klimenko, a professor of mathematics, Moscow, Russia; Lomonosov Moscow State University, Moscow, Russia; and the National Research Council of Russia, Moscow. We present an algorithm for the calculation of the geometrical characteristics of the equilateral manifolds in dynamical systems, based on the concept of the equilateral equilateral method introduced by S.A. Afraimovich and A.V. Bykov. They will have to calculate the geometrical properties of the irregular manifolds with unstable periodic orbits, embedded in the phase space of a dynamical system. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3068, "title": "Ages for illustrative field stars using gyrochronology: viability, limitations and errors", "abstract": "  We here develop an improved way of using a rotating star as a clock, set it using the Sun, and demonstrate that it keeps time well. This technique, called gyrochronology, permits the derivation of ages for solar- and late-type main sequence stars using only their rotation periods and colors. The technique is clarified and developed here, and used to derive ages for illustrative groups of nearby, late-type field stars with measured rotation periods. We first demonstrate the reality of the interface sequence, the unifying feature of the rotational observations of cluster and field stars that makes the technique possible, and extends it beyond the proposal of Skumanich by specifying the mass dependence of rotation for these stars. We delineate which stars it cannot currently be used on. We then calibrate the age dependence using the Sun. The errors are propagated to understand their dependence on color and period. Representative age errors associated with the technique are estimated at ~15% (plus possible systematic errors) for late-F, G, K, & early-M stars. Ages derived via gyrochronology for the Mt. Wilson stars are shown to be in good agreement with chromospheric ages for all but the bluest stars, and probably superior. Gyro ages are then calculated for each of the active main sequence field stars studied by Strassmeier and collaborators where other ages are not available. These are shown to be mostly younger than 1Gyr, with a median age of 365Myr. The sample of single, late-type main sequence field stars assembled by Pizzolato and collaborators is then assessed, and shown to have gyro ages ranging from under 100Myr to several Gyr, and a median age of 1.2Gyr. Finally, we demonstrate that the individual components of the three wide binaries XiBooAB, 61CygAB, & AlphaCenAB yield substantially the same gyro ages. ", "machine_abstract": "We present an analysis of the ages derived by applying the gyrochronological method to a sample of open clusters with known ages (from literature) in order to assess its reliability as well as possible systematics associated with it. We find that the age estimates are generally consistent within their uncertainties but there is some evidence for a small bias towards younger ages when compared against the true cluster ages. This bias may be due to the fact that we have used only one rotation period per star which does not take into account any scatter or spread in periods observed among coeval stars. The results presented here suggest that this technique can provide useful constraints on stellar ages if applied carefully taking into consideration all relevant sources of uncertainty. Keywords: Age determination, Open clusters, Rotation periods, Gyrochronology. 1 Introduction Stellar ages play a crucial role in many areas of astrophysics ranging from Galactic archaeology to exoplanet science. In particular, accurate ages are needed to understand how planets form and evolve over time. However, determining precise ages for individual stars remains challenging because they span several orders of magnitude in mass and luminosity and exhibit complex evolutionary histories. For example, while main-sequence turn-off ages can be determined accurately through photometric techniques such as fitting theoretical isochrones to colour-magnitude diagrams (CMDs), these methods cannot be easily extended beyond the red giant branch where the effects of convection become important. Furthermore, even though asteroseismic observations allow us to probe the interiors of evolved stars, the interpretation of the resulting data requires detailed modelling of the structure and evolution of each star individually. As a result, other approaches must be explored to determine ages for large samples of stars spanning different stages of evolution. Gyrochronology provides another avenue for estimating ages based on the spin-down rate of magnetic activity cycles driven by dynamo processes operating at the base of the solar convective zone (Barnes 2003) . It has been shown that the Rossby number R o , defined as the ratio between the rotation period P rot and the convective overturning timescale", "paraphrased_abstract": "A definite number is given by Rossby's number, which is a ratio of the rotation period P rot to the overturning time, which is determined by the dynamo processes that operate at the base of the solar convective zone (Barnes 2003). However, the astronomy is still a difficult task for individual stars, because they are several orders of magnitude in size and luminosity and have complex evolutionary histories. To determine the precise ages of individual stars, it is necessary to examine them individually in detail, and, moreover, to extend the methods of astronomy, which can be carried out by means of photometrics, and by combining theoretical isochrones with the color-coded chromatogram (CMD), one cannot, for example, expand the red giant branch of the sun, where convection is present, to a large extent, but there is no easy way of doing this, and it is necessary to apply astronomy to large quantities of stars, for example, for the observation of the interior of evolved stars, for the analysis of the internal structure and evolution of stars, it is necessary to develop, for the first time, a precise model of the evolution of stars and of the structure and evolution of stars, in particular to the study of the ages of the stars of the stars of the stars, in the course of evolution, to determine their ages", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0301, "title": "Planck Scale Unification in a Supersymmetric Standard Model", "abstract": "  We show how gauge coupling unification near the Planck scale $M_P\\sim 10^{19}$ GeV can be achieved in the framework of supersymmetry, facilitating a full unification of all forces with gravity. Below the conventional GUT scale $M_{GUT}\\sim 10^{16}$ GeV physics is described by a Supersymmetric Standard Model whose particle content is that of three complete $\\mathbf{27}$ representations of the gauge group $E_6$. Above the conventional GUT scale the gauge group corresponds to a left-right symmetric Supersymmetric Pati-Salam model, which may be regarded as a ``surrogate SUSY GUT'' with all the nice features of SO(10) but without proton decay or doublet-triplet splitting problems. At the TeV scale the extra exotic states may be discovered at the LHC, providing an observable footprint of an underlying $E_6$ gauge group broken at the Planck scale. Assuming an additional low energy $U(1)_X$ gauge group, identified as a non-trivial combination of diagonal $E_6$ generators, the $\\mu$ problem of the MSSM can be resolved. ", "machine_abstract": "We present the results of an analysis of the supersymmetric standard model with minimal supergravity boundary conditions at the grand unification scale, including all one-loop corrections to gauge and Yukawa couplings as well as two-loop contributions to the running of the soft supersymmetry breaking parameters.  We find that this scenario is compatible with current experimental bounds on sparticle masses if tan beta is large (tan beta > 50) or small (tan beta < 10). In addition we show how the lightest Higgs boson mass can be predicted within this framework for any value of tan beta between 1 and 60. Finally, we discuss the implications of our results for future searches for supersymmetry at colliders such as LHC. The supersymmetric standard model has been studied extensively over many years [1] . It provides a natural solution to the hierarchy problem by introducing new particles which cancel quadratic divergences associated with radiative corrections to the scalar potential [2] , while also providing a candidate particle for dark matter [3] . In recent years there have been several studies [4] - [8] investigating whether it is possible to construct models where the electroweak symmetry breaking sector is described by the MSSM [9] but the underlying physics is governed by some more fundamental theory valid at higher energies. This approach is motivated by the fact that the MSSM suffers from fine-tuning problems [10] due to its sensitivity to unknown high-scale physics [11] . If these problems are solved then the MSSM may provide a good description of nature up to very high scales [12] . One possibility would be to embed the MSSM into a Grand Unified Theory [13] based upon SO(10), although other possibilities exist [14] . Another possibility is to consider theories with extra dimensions [15] - [17] .", "paraphrased_abstract": "A theoretical model of the supersymmetric standard is known, and it is an important model to be found, and the structure of the structure is so complete, so that it is very stable at the very high scale. In the past, several studies have been conducted to find out whether it is possible to construct such a model with the electroweak symmetry breaking region, and to impose the underlying physics upon the more fundamental physics. The most probable scenario is the arithmetic of the small and the large masses of tan beta, and of the large quantities of tan beta, respectively, which corresponds to the size of Higgs\u2019 boson. We show that our analysis of the supersymmetric standard model is compatible with the current theoretical estimates for tan beta, as long as the tan beta is large or small. We also show that tan beta is smaller than the tan beta, and that the lightest mass of Higgs\u2019 boson is obtained by assuming tan beta at a value of tan beta. It is proposed that the supersymmetric standard model be embedded in a grand-unified theory based on SO(10), and of which there are many possibilities. The purpose of this is that the MSSM suffers from some fine-tuning problems, and in these problems there are some more fundamental laws of physics, which are applicable to high", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0685, "title": "PSR J1453+1902 and the radio luminosities of solitary versus binary millisecond pulsars", "abstract": "  We present 3 yr of timing observations for PSR J1453+1902, a 5.79-ms pulsar discovered during a 430-MHz drift-scan survey with the Arecibo telescope. Our observations show that PSR J1453+1902 is solitary and has a proper motion of 8(2) mas/yr. At the nominal distance of 1.2 kpc estimated from the pulsar's dispersion measure, this corresponds to a transverse speed of 46(11) km/s, typical of the millisecond pulsar population. We analyse the current sample of 55 millisecond pulsars in the Galactic disk and revisit the question of whether the luminosities of isolated millisecond pulsars are different from their binary counterparts. We demonstrate that the apparent differences in the luminosity distributions seen in samples selected from 430-MHz surveys can be explained by small-number statistics and observational selection biases. An examination of the sample from 1400-MHz surveys shows no differences in the distributions. The simplest conclusion from the current data is that the spin, kinematic, spatial and luminosity distributions of isolated and binary millisecond pulsars are consistent with a single homogeneous population. ", "machine_abstract": "We report on timing observations made with the Arecibo Observatory of PSR J1453+1903, which is one of only two known isolated (non-millisecond) pulsars in globular clusters. We find that its spin-down rate is consistent with it being powered by magnetic dipole radiation; this implies an age for the cluster of about 12 Gyrs. The pulsar's dispersion measure indicates that it lies at a distance of about 4 kpc from Earth. Its inferred surface dipolar magnetic field strength is 1.1 x 10^10 Gauss, making it among the strongest ever measured for any pulsar. This value is also comparable to those found for other young pulsars in open clusters.     In addition we have observed PSR J1453 + 1903 over several years using the Green Bank Telescope. From these data we can determine the pulsar's flux density as a function of frequency. Using our measurements together with previously published results, we show that there are no significant differences between the spectral indices of solitary and binary millisecond pulsars.", "paraphrased_abstract": "Using our data and the observations of the Green Bank Telescope, we have derived the spectral index of the pulsars. We have observed PSR J1453+1903 for several years on the Green Bank Telescope, and it is possible to calculate the radiance density of the pulsar as a function of frequency. The radiance of the pulsar is found to be in the vicinity of the magnetic dipole, which implies a radiance of about twelve joules. We have obtained the position of the pulsar in the vicinity of the Green Bank. It is known that the pulsar is a non-millisecond pulsar, in globular clusters. We also discovered the pulsar's dispersion, which is about four kpc from the Earth. The dispersion is about four kpc from the Earth, and its mass is 1.1 x 1010 gauss, which is the strongest in the universe, and which is similar to that of other pulsars in open clusters. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3313, "title": "Straggler Identification in Round-Trip Data Streams via Newton's Identities and Invertible Bloom Filters", "abstract": "  We introduce the straggler identification problem, in which an algorithm must determine the identities of the remaining members of a set after it has had a large number of insertion and deletion operations performed on it, and now has relatively few remaining members. The goal is to do this in o(n) space, where n is the total number of identities. The straggler identification problem has applications, for example, in determining the set of unacknowledged packets in a high-bandwidth multicast data stream. We provide a deterministic solution to the straggler identification problem that uses only O(d log n) bits and is based on a novel application of Newton's identities for symmetric polynomials. This solution can identify any subset of d stragglers from a set of n O(log n)-bit identifiers, assuming that there are no false deletions of identities not already in the set. Indeed, we give a lower bound argument that shows that any small-space deterministic solution to the straggler identification problem cannot be guaranteed to handle false deletions. Nevertheless, we show that there is a simple randomized solution using O(d log n log(1/epsilon)) bits that can maintain a multiset and solve the straggler identification problem, tolerating false deletions, where epsilon>0 is a user-defined parameter bounding the probability of an incorrect response. This randomized solution is based on a new type of Bloom filter, which we call the invertible Bloom filter. ", "machine_abstract": "We present an algorithm for identifying stragglers in round-trip data streams using Newton's identities, which are derived by applying the chain rule to matrix exponentials. We show that our approach is more efficient than existing methods based on linear regression or singular value decomposition (SVD). Our method also has lower memory requirements compared with SVD-based approaches. Finally, we demonstrate how our technique can be used to identify stragglers in real-world datasets such as Google Analytics clickstream logs. The problem of identifying stragglers arises when processing large volumes of data in parallel systems. Stragglers are slow processors whose presence degrades overall system performance. Identifying these stragglers allows us to take corrective action so that they do not affect other processes running simultaneously. For example, if a straggler process takes too much time to complete its task, then it may be possible to offload some work onto another processor to speed up execution.", "paraphrased_abstract": "We present a new algorithm for detecting stragglers in large, parallel systems. Often, the stragglers are slow processors, which interfere with the performance of the others. Identifying stragglers allows us to fix the stragglers so that they don't interfere with other processes. For example, if the straggler is running too slowly, we may delegate some tasks to another process to speed up the execution. We show that our method is more efficient than others that use linear regression or SVD, which are based on the chain rule of the exponentials. Our method is also much more memory efficient than the SVD method. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1218, "title": "The impact of radio feedback from active galactic nuclei in cosmological simulations: Formation of disk galaxies", "abstract": "  In this paper, we present a new implementation of feedback due to active galactic nuclei (AGN) in cosmological simulations of galaxy formation. We assume that a fraction of jet energy, which is generated by an AGN, is transferred to the surrounding gas as thermal energy. Combining a theoretical model of mass accretion onto black holes with a multiphase description of star-forming gas, we self-consistently follow evolution of both galaxies and their central black holes. The novelty in our model is that we consider two distinct accretion modes: standard radiatively efficient thin accretion disks and radiatively inefficient accretion flows which we will generically refer to as RIAFs; motivated by theoretical models for jet production in accretion disks, we assume that only the RIAF is responsible for the AGN feedback. We find that, after an initial episode of bursting star formation, the accretion rate onto the central black hole drops so that the accretion disk switches to a RIAF structure. At this point, the feedback from the AGN becomes efficient and slightly suppresses star formation in the galactic disk and almost completely halts star formation in the bulge. As a result, the nucleus becomes a stochastically fuelled low-luminosity AGN (Seyfert galaxy) with recurrent short-lived episodes of activity after the star bursts. Our model predicts several properties of the low-luminosity AGN including the bolometric luminosity, jet powers, the effect on kpc-scale of the radio jet and the AGN lifetime, which are in broad agreement with observations of Seyfert galaxies and their radio activity. We also find that the mass ratios between the central black hole and the the host spheroid at z = 0 are ~10^{-3} regardless of the strength of either supernova feedback or AGN feedback. (abridged) ", "machine_abstract": "We study the effect of AGN feedback on galaxy formation using hydrodynamical zoom-in cosmological simulations with different prescriptions for AGN feedback and compare them to observations. We find that our fiducial model, which includes both thermal energy injection into gas particles surrounding black holes (BHs) as well as kinetic outflows driven by BH winds, reproduces many observed properties of massive galaxies at z = 0 including their stellar mass function, sizes, morphologies, star formation rates, metallicities, and cold gas fractions. In this model, we also find that most of the stars are formed inside dark matter halos with masses between 10 11 M\u2299 < Mhalo < 5 \u00d7 10 12 M\u2299, while lower-mass halos contribute only about 10% of all stars. The majority of these low-mass halos host dwarf galaxies or satellite galaxies around more massive central galaxies.", "paraphrased_abstract": "Using this model we have a lot of star-formation in the form of heat and mass, morphology, star-formation rates, metallicities and cold gases, and we can study the formation of galaxies by the aquiferal method. In the first place, the model, which considers the thermal effect of the aquifer in the black hole and the kinetic evaporation by aquifers, enables us to reproduce the mass function of galaxies with different mass numbers and masses, the size, the morphology, the density, the metality, the coldness of the gaseous gases, the density, the height, and the number of stars. In the second, in the third, we consider the size and the mass of stars, and the composition, the mass of stars, the formation rates, the thickness of the gases, the amount of gases, the proportion of gaseous gases. We also consider the effects of the ejection of gases from black holes, which are located in the lower half of the sky, which are mainly of the mass of 10 11 M, and 10 12 M., while in the lower half of the sky, there are hardly any stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3228, "title": "A Planetary System Around HD 155358: The Lowest Metallicity Planet Host Star", "abstract": "  We report the detection of two planetary mass companions to the solar-type star HD 155358. The two planets have orbital periods of 195.0 and 530.3 days, with eccentricities of 0.11 and 0.18. The minimum masses for these planets are 0.89 and 0.50 Jupiter masses respectively. The orbits are close enough to each other, and the planets are sufficiently massive, that the planets are gravitationally interacting with each other, with their eccentricities and arguments of periastron varying with periods of 2300--2700 years. While large uncertainties remain in the orbital eccentricities, our orbital integration calculations indicate that our derived orbits would be dynamically stable for at least 10^8 years. With a metallicity [Fe/H] of -0.68, HD 155358 is tied with the K1III giant planet host star HD 47536 for the lowest metallicity of any planet host star yet found. Thus, a star with only 21% of the heavy-element content of our Sun was still able to form a system of at least two Jovian-mass planets and have their orbits evolve to semi-major axes of 0.6-1.2 AU. ", "machine_abstract": "We report the discovery and characterization of an extrasolar planet orbiting the metal-poor star HD 155358 ([Fe/H] = -0.4). We used high-precision radial velocity measurements obtained with HARPS to detect this planet, which has a minimum mass of 1.5 M\u2295 in a 4-day circular orbit around its host star. This is one of only two planets known to date that orbits such a low-metallicity star. Using archival photometry we find no evidence for transit events caused by additional bodies in the system. Our results are consistent with formation models where giant planets form more easily at higher metallicities than terrestrial planets. In addition, our observations provide new constraints on the frequency of short-period super-Earths as well as their orbital properties.         Keywords: Extrasolar planet - Super Earth - Radial Velocity - Metal-Poor Stars", "paraphrased_abstract": "The data obtained by HARPS were a unique one, and they could be considered to be quite unique. In addition, we have been able to find out the existence of long-lasting super-Earths, and the orbital properties of these planets. The following are the results of our study: the discovery and characterization of an extrasolar planet in the metal-poor HD 155358, whose iron content is -0.4. This planet is one of the few planets known so far to orbit the metal-poor star. This is one of only two planets known to date which orbit such a low-iron star. The study reveals that this planet has a minimum mass of 1.5 M, in a four-day orbit around its host star. This planet is one of only two planets known to have a metal-poor radiance around such a low-iron star. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0283, "title": "Testing and selection of cosmological models with $(1+z)^6$ corrections", "abstract": "  In the paper we check whether the contribution of $(-)(1+z)^6$ type in the Friedmann equation can be tested. We consider some astronomical tests to constrain the density parameters in such models. We describe different interpretations of such an additional term: geometric effects of Loop Quantum Cosmology, effects of braneworld cosmological models, non-standard cosmological models in metric-affine gravity, and models with spinning fluid. Kinematical (or geometrical) tests based on null geodesics are insufficient to separate individual matter components when they behave like perfect fluid and scale in the same way. Still, it is possible to measure their overall effect. We use recent measurements of the coordinate distances from the Fanaroff-Riley type IIb (FRIIb) radio galaxy (RG) data, supernovae type Ia (SNIa) data, baryon oscillation peak and cosmic microwave background radiation (CMBR) observations to obtain stronger bounds for the contribution of the type considered. We demonstrate that, while $\\rho^2$ corrections are very small, they can be tested by astronomical observations -- at least in principle. Bayesian criteria of model selection (the Bayesian factor, AIC, and BIC) are used to check if additional parameters are detectable in the present epoch. As it turns out, the $\\Lambda$CDM model is favoured over the bouncing model driven by loop quantum effects. Or, in other words, the bounds obtained from cosmography are very weak, and from the point of view of the present data this model is indistinguishable from the $\\Lambda$CDM one. ", "machine_abstract": "We present the results of testing different cosmological models against observations on large scale structure, supernovae Ia luminosity distances and CMB anisotropies in combination with WMAP5 data. We use the most recent observational data available at the time when this work was done (June 2008). The main goal is to test whether or not there are deviations between theoretical predictions for these observables and their observed values that can be explained by including higher order terms in the expansion of the Hubble parameter $H_0 = 100h_0$ km s-1 Mpc-1 as suggested recently by some authors.  In particular we consider two classes of models:  1)  Models where the dark energy density $\\rho_{DE}$ evolves according to an equation of state $P = w \\rho$ with $w \\sim -1 + \\alpha H_0$, $\\alpha > 0$. 2)  Models where the dark matter particles have self-interactions described by a cross section $\\sigma_M \\propto \\frac{1}{v^2}$, $v \\sim H_0$.", "paraphrased_abstract": "The main purpose of this study is to test whether or not the theoretical predictions for these observations are correct and whether or not the explanation of the expansion of the Hubble parameter H-0 = 100 H-0 x M-c-1 may be in the form of the oblique parameter H-0 = 100 H-0 x M-c-1 has been brought up recently. We will present two classes of models: 1) ones in which the dark energy density (P=w-rho) is developed according to an equation of state P=w-rho with $w-sim -1 + alpha - alpha > 0; 2). In particular, we consider two classes of models: 1) ones in which the dark matter particles have self-interactions defined by cross-sections: sigma M-propto perto frac12, sim - 0h0. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.095, "title": "Displacement of the Sun from the Galactic Plane", "abstract": "  We have carried out a comparative statistical study for the displacement of the Sun from the Galactic plane (z_\\odot) following three different methods. The study has been done using a sample of 537 young open clusters (YOCs) with log(Age) < 8.5 lying within a heliocentric distance of 4 kpc and 2030 OB stars observed up to a distance of 1200 pc, all of them have distance information. We decompose the Gould Belt's member in a statistical sense before investigating the variation in the z_\\odot estimation with different upper cut-off limits in the heliocentric distance and distance perpendicular to the Galactic plane. We found z_\\odot varies in a range of ~ 13 - 20 pc from the analys is of YOCs and ~ 6 - 18 pc from the OB stars. A significant scatter in the z_\\odot obtained due to different cut-off values is noticed for the OB stars although no such deviation is seen for the YOCs. We also determined scale heights of 56.9(+3.8)(-3.4} and 61.4(+2.7)(-2.4) pc for the distribution of YOCs and OB stars respectively. ", "machine_abstract": "We present new results on the displacement of the Sun from the galactic plane based on Hipparcos data and recent determinations of the solar motion with respect to the local standard of rest (LSR). We find that the Sun is displaced by about 0.5 kpc in the direction towards the constellation Cetus, which agrees well with previous estimates obtained using different methods. The observed displacement can be explained as due to the combined effect of the gravitational potential of the Galaxy and the peculiar velocity of the Local Group with respect to it.     Keywords: Solar System dynamics, Galactic rotation curve, Local Group kinematics, Galactocentric distance     1 Introduction     In this work we study the position of the Sun within our galaxy. This problem has been addressed previously by several authors who have used different techniques ranging from statistical studies of open clusters [1] or OB associations [2] , to direct measurements of proper motions [3] . Here we use the most accurate available determination of the solar motion [4] together with the latest measurement of the circular speed at large distances [5] to determine the position of the Sun relative to the galactic plane.", "paraphrased_abstract": "This problem has been analyzed by several authors, who have used various methods, from statistical study of clusters, to OB-associations. The most recent kinematic study of the circular motion, and of the distance of the sun, will be used to determine the position of the sun relative to the galactic plane. The new results, taken from Hipparcos and recent results of the kinematic analysis of the local standard of rest (LSR), will be discussed. We find that the Sun is displaced by about 0.5 km per swarm, which agrees with previous estimates. We find that the Sun is moving by approximately 0.5 swarm per swarm per swarm per swarm, which agrees well with previous estimates. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0578, "title": "Expressing Combinatorial Optimization Problems by Systems of Polynomial Equations and the Nullstellensatz", "abstract": "  Systems of polynomial equations over the complex or real numbers can be used to model combinatorial problems. In this way, a combinatorial problem is feasible (e.g. a graph is 3-colorable, hamiltonian, etc.) if and only if a related system of polynomial equations has a solution. In the first part of this paper, we construct new polynomial encodings for the problems of finding in a graph its longest cycle, the largest planar subgraph, the edge-chromatic number, or the largest k-colorable subgraph.   For an infeasible polynomial system, the (complex) Hilbert Nullstellensatz gives a certificate that the associated combinatorial problem is infeasible. Thus, unless P = NP, there must exist an infinite sequence of infeasible instances of each hard combinatorial problem for which the minimum degree of a Hilbert Nullstellensatz certificate of the associated polynomial system grows.   We show that the minimum-degree of a Nullstellensatz certificate for the non-existence of a stable set of size greater than the stability number of the graph is the stability number of the graph. Moreover, such a certificate contains at least one term per stable set of G. In contrast, for non-3- colorability, we found only graphs with Nullstellensatz certificates of degree four. ", "machine_abstract": "The nullstellensatz is an important theorem in algebraic geometry, which states that if a system of polynomial equations has no solution over some field then it must have at least one solution over its algebraic closure.  In this work we show how to use the nullstellensatz as part of a general framework for expressing combinatorial optimization problems (COPs) as systems of polynomial equations.   We demonstrate our approach on several COPs including maximum clique, minimum vertex cover, and maximum independent set.   ... This research was supported by NSF grant CCF-0430020. The authors would like to thank Professors David Avis and Michael Shmoys for their helpful comments during the preparation of this manuscript. The nullstellensatz provides a powerful tool for solving certain classes of computational problems expressed as systems of polynomial equations. It states that if such a system does not admit any solutions over a given field K, then there exists at least one solution over the algebraic closure of K. In this paper, we present a general method for transforming combinatorial optimization problems into systems of polynomial equations whose solvability can be determined using the nullstellenstaz. Our approach relies on representing each feasible solution of the COP as a point in a vector space V equipped with a suitable inner product. Then, we express the objective function of the problem as a linear combination of polynomials in V . Finally, we formulate the problem as a system of polynomial equations by requiring that all points corresponding to feasible solutions satisfy the objective function simultaneously. As examples, we apply our technique to three well-known NP-hard combinatorial optimization problems: Maximum Clique, Minimum Vertex Cover, and Maximum Independent Set.", "paraphrased_abstract": "This work was supported by NSF grant CF-073006. In this paper we present a general approach for converting combinatorial optimization problems into polynomial equations, for which solvability is determined by the nullstellensatz. We introduce the nullstellensatz by defining the points of the possible solutions in a vector space, a vector of which has a proper inner product, and, finally, a function of all the points corresponding to the desired solution in a vector space V, in order to obtain a solution at the same time as the objective function. In this paper we introduce a general method for the transformation of combinatorial optimization problems into systems of polynomial equations which are solvable by the nullstellensatz. This method is used in the study of three well-known NP-hard combinatorial optimization problems, the maximum clique, the minimum clique, and the maximum independent set. The nullstellensatz is an important theorem in algebraic geometry, in which if a system of polynomial equations does not exist in a field, then there must be a solution over the algebraic closure. In this paper, we show how to apply this nullstellensatz to some COPs, including maximum clique, minimum vertex cover and maximum independent set. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1371, "title": "Isolated horizons in higher-dimensional Einstein-Gauss-Bonnet gravity", "abstract": "  The isolated horizon framework was introduced in order to provide a local description of black holes that are in equilibrium with their (possibly dynamic) environment. Over the past several years, the framework has been extended to include matter fields (dilaton, Yang-Mills etc) in D=4 dimensions and cosmological constant in $D\\geq3$ dimensions. In this article we present a further extension of the framework that includes black holes in higher-dimensional Einstein-Gauss-Bonnet (EGB) gravity. In particular, we construct a covariant phase space for EGB gravity in arbitrary dimensions which allows us to derive the first law. We find that the entropy of a weakly isolated and non-rotating horizon is given by $\\mathcal{S}=(1/4G_{D})\\oint_{S^{D-2}}\\bm{\\tilde{\\epsilon}}(1+2\\alpha\\mathcal{R})$. In this expression $S^{D-2}$ is the $(D-2)$-dimensional cross section of the horizon with area form $\\bm{\\tilde{\\epsilon}}$ and Ricci scalar $\\mathcal{R}$, $G_{D}$ is the $D$-dimensional Newton constant and $\\alpha$ is the Gauss-Bonnet parameter. This expression for the horizon entropy is in agreement with those predicted by the Euclidean and Noether charge methods. Thus we extend the isolated horizon framework beyond Einstein gravity. ", "machine_abstract": "We study the isolated horizon (IH) solutions to the four dimensional Einstein-Gauss-Bonnet theory with negative cosmological constant, and show that there are no IHs for generic values of coupling constants. We also find that the only possible IH solution is given by the Schwarzschild-AdS black hole. This result implies that the entropy of an IH can be expressed as one quarter of its area in this case.  I. INTRODUCTORY REMARK In recent years, it has been shown that the entropy of an isolated horizon (IH), which is defined as the boundary of future null infinity on a spacetime [1] , can be identified with one quarter of its surface area [2] . In particular, this relation holds true even when quantum corrections due to matter fields are taken into account [3] . The purpose of this work is to investigate whether or not such a relation still holds true in higher dimensions within the context of Einstein-Gauss-Bonnet (EGB) gravity [4] . The EGB action reads, where ${}^{(n)}$ denotes the nth order Lovelock polynomial [5]  and $R_{abcd} = R_{acbd} - \\frac{1}{2}R_{[a}g_{bc]d} + \\frac{1}{3}R_{[abc]}^d$ is the Riemann tensor associated with the metric $g_{ab}$. Here, $\\Lambda$ represents the cosmological constant whose sign determines the signature of the spacetime [6] : positive if $\\Lambda > 0$; negative if $\\Lambda < 0$.", "paraphrased_abstract": "1 n2R [agbd] + frac13R d is the Riemann Tensor associated with the metric gab. 1 This result is consistent with the fact that the entropy of the IH can be estimated for one-quarter of its surface area. 3 (n)  is the cosmological constant whose sign determines the signature of the spacetime: positive if Lambda > 0 and negative if Lambda > 0; negative if Lambda 0. The purpose of this study is to determine whether or not this result holds in higher dimensions in the context of the EGB gravity. In particular, it is shown that the entropy of an isolated horizon, which is defined as the boundary of future infinity on a spacetime, is one-quarter of its surface; this is especially the case when quantum corrections due to the field of matter are taken into account. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0326, "title": "A WENO Algorithm for the Growth of Ionized Regions at the Reionization Epoch", "abstract": "  We investigate the volume growth of ionized regions around UV photon sources with the WENO algorithm, which is an effective solver of photon kinetics in the phase space described by the radiative transfer equation. We show that the volume growth rate, either of isolated ionized regions or of clustered regions in merging, generally consists of three phases: fast or relativistic growth phase at the early stage, slow growth phase at the later stage, and a transition phase between the fast and slow phases. We also show that the volume growth of ionized regions around clustered sources with intensity $\\dot{E}_i$ ($i=1, 2, ...$) would have the same behavior as a single source with intensity $\\dot{E}=\\sum_i\\dot{E}_i$, if all the distances between nearest neighbor sources $i$ and $j$ are smaller than $c(t^i_c+t^j_c)$, $t^i_c$ being the time scale $t_c$ of source $i$. Therefore, a tightly clustered UV photon sources would lead to a slow growth of ionized volume. This effect would be important for studying the redshift-dependence of 21cm signals from the reionization epoch. ", "machine_abstract": "We present an algorithm to solve numerically the time-dependent equations describing the growth of ionized regions in the early universe, based on the Weighted Essentially Non-Oscillatory (WENO) scheme. The numerical solution is obtained by solving simultaneously two coupled partial differential equations that describe the evolution of the ionization fraction and temperature fields inside each cell of a grid covering the computational domain. We show how this method can be used to study the reionization process driven by UV photons emitted by galaxies. In particular we compare our results with those obtained using the Smooth Particle Hydrodynamics technique. Our tests indicate that both methods are able to reproduce qualitatively similar results but there are some quantitative differences between them which may have important consequences when studying the statistical properties of the IGM during the reionization epoch.     Keywords: Numerical Methods, Computational Science, Cosmology, Reionization", "paraphrased_abstract": "In this work we have tested a method for the calculation of the temperature and the growth of ionized regions in the early universe and have shown that it can be used to study the reionization of galaxies in a realistic manner. We compare our method with that of the spherical particle hydrodynamics. We show how it is possible to use the method for the study of the reionization of galaxies by UV light. The numerical solution is obtained by solving two partial differential equations of the same time, which are to be solved in parallel, and that correspondingly, we obtain the distribution of the concentration of the ions and the temperature in each cell of the grid that surrounds the simulation domain. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2568, "title": "Probing non-standard decoherence effects with solar and KamLAND neutrinos", "abstract": "  It has been speculated that quantum gravity might induce a \"foamy\" space-time structure at small scales, randomly perturbing the propagation phases of free-streaming particles (such as kaons, neutrons, or neutrinos). Particle interferometry might then reveal non-standard decoherence effects, in addition to standard ones (due to, e.g., finite source size and detector resolution.) In this work we discuss the phenomenology of such non-standard effects in the propagation of electron neutrinos in the Sun and in the long-baseline reactor experiment KamLAND, which jointly provide us with the best available probes of decoherence at neutrino energies E ~ few MeV. In the solar neutrino case, by means of a perturbative approach, decoherence is shown to modify the standard (adiabatic) propagation in matter through a calculable damping factor. By assuming a power-law dependence of decoherence effects in the energy domain (E^n with n = 0,+/-1,+/-2), theoretical predictions for two-family neutrino mixing are compared with the data and discussed. We find that neither solar nor KamLAND data show evidence in favor of non-standard decoherence effects, whose characteristic parameter gamma_0 can thus be significantly constrained. In the \"Lorentz-invariant\" case n=-1, we obtain the upper limit gamma_0<0.78 x 10^-26 GeV at 95% C.L. In the specific case n=-2, the constraints can also be interpreted as bounds on possible matter density fluctuations in the Sun, which we improve by a factor of ~ 2 with respect to previous analyses. ", "machine_abstract": "We study the possibility that nonstandard interactions (NSI) between neutrinos and matter can be probed by using solar and reactor neutrino data simultaneously, in particular through their combined effect on the survival probability P(\u03bde\u2192\u03bde). We find that NSI parameters are constrained to values below 0.1 for most combinations of standard oscillation parameters allowed at 3\u03c3 CL by current global fits. The strongest constraints arise when combining solar and KamLAND data sets. In this case we obtain upper bounds on |\u03b5ee|, |\u03b5\u00b5\u03c4 | < 0.06 \u2212 0.07 depending on the value of \u03b813. These results improve upon previous limits obtained from solar or reactor experiments alone.     Introduction     Neutrino oscillations have been observed in many different types of experiments [1] . However, there is still no direct evidence for the existence of new physics beyond the Standard Model (SM), such as sterile neutrinos [2] , lepton number violation [3] , extra dimensions [4] , supersymmetry [5] , etc.. Many extensions of the SM predict additional contributions to the effective four-fermion interaction Lagrangian [6] which could lead to observable deviations from the predictions of the SM [7, 8] . For example, it has recently been shown [9] that some models of quantum gravity [10] may induce an energy dependent refractive index n = 1 + \u03b5E/E0 where E0 is a characteristic scale associated with the underlying theory [11] . This would result in a modification of the vacuum mixing angle sin2\u03b812 = 1\u2212cos2\u03b812 \u2248 1+\u03b5/2+O(\u03b53) [12] leading to potentially large effects on the propagation of neutrinos [13] .   In addition to these theoretical motivations, there exist several experimental indications pointing towards possible new physics beyond the SM [14] : i) Large atmospheric [15] and solar [16] neutrino flux deficits; ii) LSND [17] and MiniBooNE [18] anomalies indicating short-baseline \u03bd\u03bc \u2192 \u03bde appearance transitions not predicted within three-flavor neutrino oscillations [19] ; iii) Anomalies in the measurement of the muon anomalous magnetic moment", "paraphrased_abstract": "Besides the theoretical considerations, the results of the experiments we have examined are many: i) the astronomical and solar neutrino flux deficits; ii) the LSND and MiniBooNE anomalies demonstrating short-baseline -e transitions not predicted by three-color neutrino oscillations; iii) the measurement of the magnetic moment anomalous to muons by muons; iv) the measurement of the muon anomalous magnetic moment in a muon, which is unknown. Many experiments have been conducted in which neutrinos are observed, but there is no direct evidence for any new physics beyond the SM. Some of these models, for example, have been developed by a certain quantum gravity. A certain energy dependent refractive index n = 1 + E0, where E0 is a characteristic parameter of the underlying theory. Moreover, many of the extensions of the SM predict additional contributions to the effective four-fermion Lagrangian, which could lead to some unexpected deviations from the predictions of the SM. We propose, for example, that the NSI parameters should be limited to 0.1 for most combinations of standard oscillation parameters, e.g. for the upper limit of e., e.,  = 0.06  0.", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0809, "title": "Wide Field Surveys and Astronomical Discovery Space", "abstract": "  I review the status of science with wide field surveys. For many decades surveys have been the backbone of astronomy, and the main engine of discovery, as we have mapped the sky at every possible wavelength. Surveys are an efficient use of resources. They are important as a fundamental resource; to map intrinsically large structures; to gain the necessary statistics to address some problems; and to find very rare objects. I summarise major recent wide field surveys - 2MASS, SDSS, 2dfGRS, and UKIDSS - and look at examples of the exciting science they have produced, covering the structure of the Milky Way, the measurement of cosmological parameters, the creation of a new field studying substellar objects, and the ionisation history of the Universe. I then look briefly at upcoming projects in the optical-IR survey arena - VISTA, PanSTARRS, WISE, and LSST. Finally I ask, now we have opened up essentially all wavelength windows, whether the exploration of survey discovery space is ended. I examine other possible axes of discovery space, and find them mostly to be too expensive to explore or otherwise unfruitful, with two exceptions : the first is the time axis, which we have only just begun to explore properly; and the second is the possibility of neutrino astrophysics. ", "machine_abstract": "The discovery space for astronomical research is vast, with many different types of surveys being conducted at all wavelengths across the electromagnetic spectrum. In this talk I will discuss how wide field optical imaging surveys have been used to discover new classes of objects in our Universe such as quasars, galaxies, clusters of galaxies, supernovae, gamma ray bursts etc., and also how these surveys are now providing data on dark energy which drives cosmic acceleration. The next generation of large area surveys (such as LSST) will provide an even greater volume of data that can be exploited by researchers worldwide. This talk will give examples of some recent results obtained using data from current and past surveys including the Sloan Digital Sky Survey (SDSS), Panoramic Survey Telescope & Rapid Response System 1 (Pan-STARRS1), Dark Energy Survey (DES), VISTA Kilo-Degree Infrared Galaxy survey (VIKING).", "paraphrased_abstract": "Then, we will present some of the most recent results from the Sloan Digital Sky Survey (SDSS), the Panoramic Telescope & Rapid Response System (Pan-STARRS), the Dark Energy Survey (DES), and the Visible Kilo-de-crystal Infrared Galaxy Survey (VIKING). This talk will also present recent results obtained by using the latest results of the Sloan Digital Sky Survey (SDSS), the Panoramic Telescope & Rapid Response System (Pan-STARRS1), the Dark Energy Survey (DES), and the Visible Kilo-decrystal Infrared Galaxy Survey (VIKING). In this work I will describe how many different optical surveys have been carried out in the world to discover new quasars, galaxies, clusters, supernovae, and gamma-ray bursts, and how these surveys now demonstrate the dark energy which drives the speed of the universe. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3465, "title": "Cosmological Simulations of the Preheating Scenario for Galaxy Cluster Formation: Comparison to Analytic Models and Observations", "abstract": "  We perform a set of non--radiative cosmological simulations of a preheated intracluster medium in which the entropy of the gas was uniformly boosted at high redshift. The results of these simulations are used first to test the current analytic techniques of preheating via entropy input in the smooth accretion limit. When the unmodified profile is taken directly from simulations, we find that this model is in excellent agreement with the results of our simulations. This suggests that preheated efficiently smoothes the accreted gas, and therefore a shift in the unmodified profile is a good approximation even with a realistic accretion history. When we examine the simulation results in detail, we do not find strong evidence for entropy amplification, at least for the high-redshift preheating model adopted here. In the second section of the paper, we compare the results of the preheating simulations to recent observations. We show -- in agreement with previous work -- that for a reasonable amount of preheating, a satisfactory match can be found to the mass-temperature and luminosity-temperature relations. However -- as noted by previous authors -- we find that the entropy profiles of the simulated groups are much too flat compared to observations. In particular, while rich clusters converge on the adiabatic self--similar scaling at large radius, no single value of the entropy input during preheating can simultaneously reproduce both the core and outer entropy levels. As a result, we confirm that the simple preheating scenario for galaxy cluster formation, in which entropy is injected universally at high redshift, is inconsistent with observations. ", "machine_abstract": "We present cosmological hydrodynamic simulations that follow the formation of galaxy clusters in the preheating scenario, where gas is heated by an early generation of stars before it collapses into dark matter haloes. We compare our results with observations of X-ray luminosity-temperature relations (L-T) at z = 0 as well as temperature profiles out to large radii. Our simulated L-T relation agrees very well with observational data over three orders of magnitude in luminosity. The slope of the observed L-T relation steepens towards lower temperatures while we find no such trend in our simulation. This discrepancy may be due to additional heating mechanisms not included in our model or due to systematic uncertainties in the observational sample. In addition, we show that the entropy profile of our simulated cluster population can reproduce the shape of observed profiles within their scatter. However, there are significant differences between individual simulated clusters and real systems which could be caused by non-gravitational processes like AGN feedback.", "paraphrased_abstract": "We show how the density of our simulated clusters can be reproduced in the scattering. We also show that our cluster population can be reproduced in the scattering, and that it is able to reproduce the pattern of the observed patterns in their scattering. We have cosmological hydrodynamics and models for the formation of galaxy clusters in the heat of the early universe, before it turns to dark matter. The observed density is lower than the observed density, and we do not find this characteristic in our model. The inclination of the inclination of the inclination of the inclination is higher, and the temperature is lower, but it is not shown in our simulation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2666, "title": "Mass spectrum of diquarks and mesons in the color--flavor locked phase of dense quark matter", "abstract": "  The spectrum of meson and diquark excitations of dense quark matter is considered in the framework of the Nambu -- Jona-Lasinio model with three types of massless quarks in the presense of a quark number chemical potential $\\mu$. We investigate the effective action of meson- and diquark fields both at sufficiently large values of $\\mu>\\mu_c\\approx 330$ MeV, where the color--flavor locked (CFL) phase is realized, and in the chirally broken phase of quark matter ($\\mu<\\mu_c$). In the last case all nine pseudoscalar mesons are Nambu -- Goldstone (NG) bosons, whereas the mass of the scalar meson nonet is twice the dynamical quark mass. In the chirally broken phase the pseudoscalar diquarks are not allowed to exist as stable particles, but the scalar diquarks might be stable only at a rather strong interaction in the diquark channel. In the case of the CFL phase, all NG bosons of the model are realized as scalar and pseudoscalar diquarks. Moreover, it turns out that massive diquark excitations are unstable for this phase. In particular, for the scalar and pseudoscalar octets of diquark resonances a mass value around 230 MeV was found numerically. In contrast, mesons are stable particles in the CFL phase. Their masses lie in the interval 400$\\div$500 MeV for not too large values of $\\mu>\\mu_c$. ", "machine_abstract": "We study the mass spectrum of diquark-antidiquark bound states, as well as that of meson-like excitations, in the CFL phase of QCD at high density using an effective field theory approach. We find that there are two types of diquark-anti-diquark bound states with masses around \\sim 10 MeV and 100 MeV respectively. The lightest state is identified to be the scalar diquark while the heaviest one corresponds to the axial vector diquark. In addition we also obtain the masses for various mesonic excitations which can be classified into three categories according to their transformation properties under flavor SU(3) group. These results provide useful information on the possible hadronic degrees of freedom in compact stars. PACS numbers: 11.10.Wx, 12.38.Mh, 14.20.Dh  I . INTRODUCTION Dense nuclear matter may exist inside neutron stars (NSs). It has been suggested by many authors [1] that such matter could undergo a transition to a new state where quarks become deconfined but still confined within some finite volume due to strong interactions between them [2] , i.e., quark matter [3] . This state was first proposed by Bodmer [4] and Witten [5] independently about thirty years ago. Since then it has attracted much attention both theoretically [6] - [8] and experimentally [9] . The most interesting feature of this state is its superfluidity [10] . If the temperature T of the system drops below the critical value Tc = 0.2 GeV [11] , quarks will condense into Cooper pairs [12] and form a condensate [13] . As a result, the ground state becomes macroscopically coherent [14] and behaves like a superfluid [15] . Such a state is called Color Flavor Locking (CFL) [16] because all quarks have the same Fermi momentum kF [17] and thus share the same gap parameter \u2206 [18] . Due to the pairing effect [19] , quarks acquire different electric charges [20] : 1/3 for up/down quarks [21] and 2/3 for strange quar", "paraphrased_abstract": "The density of the particles is insignificant and has the appearance of a superfluid. It is a state called color fusion because all the quarks have the same Fermi momentum and the same gap-factor. The fusion is characterized by two different states, the scalar diquark and the axial diquark. The masses are in a spectrum of two different classes, with masses of 110 and 100 MeV, respectively. We have studied the mass spectrum of diquarks and meson-like excitations in the CFL phase of QCD at high density. This state is called CFL because all the quarks have the same Fermi momentum, and have the same gap parameter [17]. Because of this, the quarks acquire different electric charges: half for up/down quarks, half for strange quarries. Besides, we have calculated the mass of various accelerated mesonics, which are classified into three classes: Succinct and taste-free. Dense nuclear matter may exist inside NSs. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0758, "title": "Long time existence of smooth solutions for the rapidly rotating shallow-water and Euler equations", "abstract": "  We study the stabilizing effect of rotational forcing in the nonlinear setting of two-dimensional shallow-water and more general models of compressible Euler equations. In [H. Liu and E. Tadmor, Phys. D 188 (2004), no. 3-4, 262-276] we have shown that the pressureless version of these equations admit global smooth solution for a large set of sub-critical initial configurations. In the present work we prove that when rotational force dominates the pressure, it \\emph{prolongs} the life-span of smooth solutions for t < ln(1/d); here d << 1 is the ratio of the pressure gradient measured by the inverse squared Froude number, relative to the dominant rotational forces measured by the inverse Rossby number. Our study reveals a ``nearby'' periodic-in-time approximate solution in the small d-regime, upon which hinges the long time existence of the exact smooth solution. These results are in agreement with the close-to periodic dynamics observed in the ``near inertial oscillation'' (NIO) regime which follows oceanic storms. Indeed, our results indicate the existence of smooth, ``approximate periodic'' solution for a time period of \\emph{days}, which is the relevant time period found in NIO obesrvations. ", "machine_abstract": "We prove that there exists an open set of initial data such that the corresponding solution to the rapidly rotating shallow water (RSW) or Euler equations is globally smooth in space-time, provided it stays sufficiently close to some equilibrium state. The proof relies on a combination of energy estimates with suitable weighted Sobolev inequalities. We also show how our results can be extended to more general systems of conservation laws which are not necessarily strictly hyperbolic. In particular we consider the case where one of the characteristic speeds vanishes at a point but remains bounded away from zero elsewhere.  Introduction  The purpose of this work is twofold. First, we establish global regularity properties for the rapidly rotating shallow water system and its generalization to arbitrary dimension. Second, we provide new insights into the structure of these models by showing how they fit within a larger class of nonlinear dispersive partial differential equations. Our main result reads as follows:  Main Theorem 1 Let u 0 \u2208 H s , s > n/2 + 2, then there exist constants C = C(n), K = K(n) such that if", "paraphrased_abstract": "Xu 0  H s  n/2 \u2013 2; then there are constants C = C(n) and K = K(n), so that if the nth element is larger than nth, then there are constants C = C(n) and K = K(n); and, if nth element is smaller than nth element, there is an error of some kind, a delay of some kind. Our main results are as follows: the main result is: a nth element, the u0 is a subordinate of the subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate subordinate inverse acquiescence ecstasy and acquittal of the subordinate subordinate subordinates and subordinates of the subordinate subordinates and subordinates of the subordinate subordinates, i.e., i", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0405, "title": "An invariance principle for semimartingale reflecting Brownian motions in domains with piecewise smooth boundaries", "abstract": "  Semimartingale reflecting Brownian motions (SRBMs) living in the closures of domains with piecewise smooth boundaries are of interest in applied probability because of their role as heavy traffic approximations for some stochastic networks. In this paper, assuming certain conditions on the domains and directions of reflection, a perturbation result, or invariance principle, for SRBMs is proved. This provides sufficient conditions for a process that satisfies the definition of an SRBM, except for small random perturbations in the defining conditions, to be close in distribution to an SRBM. A crucial ingredient in the proof of this result is an oscillation inequality for solutions of a perturbed Skorokhod problem. We use the invariance principle to show weak existence of SRBMs under mild conditions. We also use the invariance principle, in conjunction with known uniqueness results for SRBMs, to give some sufficient conditions for validating approximations involving (i) SRBMs in convex polyhedrons with a constant reflection vector field on each face of the polyhedron, and (ii) SRBMs in bounded domains with piecewise smooth boundaries and possibly nonconstant reflection vector fields on the boundary surfaces. ", "machine_abstract": "We consider the problem of existence and uniqueness of solutions to stochastic differential equations driven by semimartingales reflected at the boundary of a domain with piecewise smooth boundary, where the coefficients are allowed to be discontinuous across the boundary. We prove that under suitable conditions on the coefficients there exists an invariant measure which is absolutely continuous wrt Lebesgue measure. The proof relies heavily on results obtained recently by Krylov and Rozovskii (Annals of Probability, vol. 31, no. 1, pp. 1-40) concerning the ergodicity of diffusion processes in bounded domains. In particular we use their result about the convergence rate of the transition density function towards its equilibrium distribution. As applications we study two examples. First we show how our results can be used to obtain new insights into the behavior of certain financial models. Second we give sufficient conditions for the existence of stationary distributions for one-dimensional diffusions with jumps.", "paraphrased_abstract": "At first we consider the general applicability of our analysis to certain models of financial behaviour, and then we introduce some of our findings to a number of cases. First we show how our analysis can be applied to new insights into the behaviour of certain financial models. In our first example we give the necessary conditions for the existence of stationary distributions for the one-dimensional diffusions with jumps. This problem involves the existence and uniqueness of solutions to the stochastic differential equations in a domain with a smooth edge, where the coefficients of the equation are discontinuous across the boundary. We prove that in the presence of suitable conditions on the coefficients there exists an invariant measure that is completely continuous with the Lebesgue measure. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3498, "title": "Measurement of Newtonian fluid slip using a torsional ultrasonic oscillator", "abstract": "  The composite torsional ultrasonic oscillator, a versatile experimental system, can be used to investigate slip of Newtonian fluid at a smooth surface. A rigorous analysis of slip-dependent damping for the oscillator is presented. Initially, the phenomenon of finite surface slip and the slip length are considered for a half-space of Newtonian fluid in contact with a smooth, oscillating solid surface. Definitions are revisited and clarified in light of inconsistencies in the literature. We point out that, in general oscillating flows, Navier's slip length b is a complex number. An intuitive velocity discontinuity parameter of unrestricted phase is used to describe the effect of slip on measurement of viscous shear damping. The analysis is applied to the composite oscillator and preliminary experimental work for a 40 kHz oscillator is presented. The Non-Slip Boundary Condition (NSBC) has been verified for a hydrophobic surface in water to within ~60 nm of |b|=0 nm. Experiments were carried out at shear rate amplitudes between 230 and 6800 /s, corresponding to linear displacement amplitudes between 3.2 and 96 nm. ", "machine_abstract": "We present an experimental method for measuring the slip length in Newtonian fluids at low Reynolds number by means of a torsional ultrasonic oscillator (TUSO). The TUSO is immersed into the liquid and driven to vibrate with angular frequency . A torque proportional to the shear stress acting on its surface causes it to rotate, which can be detected electrically via a pickup coil wound around the shaft. We show that this rotation leads to a phase lag between driving voltage and current through the pickup coil. This phase lag depends on the slip length as well as other parameters such as viscosity and density of the surrounding medium. By fitting our theoretical model to measurements we are able to extract the slip length from the data. Our results agree very well with previous experiments performed under similar conditions. In addition, we demonstrate how the measurement principle presented here could also be used to measure the slip length of non-Newtonian fluids.", "paraphrased_abstract": "It is a mathematical equation, and the theory of the theory of the theory of the measurement can be applied to the measurement of Newtonian fluids. In this experiment we show how we can measure Newtonian fluids with low Reynolds. The TUSO is immersed in the liquid, and twirled by a tassel wound around the shaft. We show that a rotational lag is observed in the voltage and current of the tassels. We show that the voltage and current are influenced by the slip and the density of the surrounding medium. In this experiment we present an experiment in which we measure the slip and the density of Newtonian fluids with low Reynolds. A rotation of the tassel consists in a phase-tassel which varies with the temperature and the density of the material. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4162, "title": "The Central Kiloparsec of Seyfert and Inactive Host Galaxies: a Comparison of Two-Dimensional Stellar and Gaseous Kinematics", "abstract": "  We investigate the properties of the two-dimensional distribution and kinematics of ionised gas and stars in the central kiloparsecs of a matched sample of nearby active (Seyfert) and inactive galaxies, using the SAURON Integral Field Unit on the William Herschel Telescope. The ionised gas distributions show a range of low excitation regions such as star formation rings in Seyferts and inactive galaxies, and high excitation regions related to photoionisation by the AGN. The stellar kinematics of all galaxies in the sample show regular rotation patterns typical of disc-like systems, with kinematic axes which are well aligned with those derived from the outer photometry and which provide a reliable representation of the galactic line of nodes. After removal of the non-gravitational components due to e.g. AGN-driven outflows, the ionised gas kinematics in both the Seyfert and inactive galaxies are also dominated by rotation with global alignment between stars and gas in most galaxies. This result is consistent with previous findings from photometric studies that the large-scale light distribution of Seyfert hosts are similar to inactive hosts. However, fully exploiting the two-dimensional nature of our spectroscopic data, deviations from axisymmetric rotation in the gaseous velocity fields are identified that suggest the gaseous kinematics are more disturbed at small radii in the Seyfert galaxies compared with the inactive galaxies, providing a tentative link between nuclear gaseous streaming and nuclear activity. ", "machine_abstract": "We present the first two-dimensional kinematics for both stars and gas in the central kiloparsecs (kpc) of two nearby galaxies, NGC 3227 and NGC 4151. The stellar kinematics are derived from near-infrared integral field spectroscopy obtained with SINFONI on VLT/UT4. We use these data to construct dynamical models that include contributions from both stars and gas. Our results show that the mass distribution is dominated by dark matter within 1 kpc radius in both cases. However, we find evidence for an additional component of hot ionized gas at large radii in NGC 4151 which may be associated with outflows driven by nuclear activity. This work demonstrates how spatially resolved observations can provide important insights into galaxy formation processes. Keywords: Galaxy dynamics; Near-infrared spectroscopy; Integral Field Spectroscopy. Subject headings: Black holes - accretion disks; Nuclear star clusters", "paraphrased_abstract": "Then we investigated the formation of the galaxy in two galaxies, NGC 3227 and NGC 4151. In the central kpc of NGC 4151, we observed the formation of black holes. The authors were concerned with black holes in the accretion disks. In the north-eastern kpc of NGC 4151, we found that the mass of the dark matter was dominant at a radius of 5 kpc. We interpreted the results of the numerical analysis and the analytical results of the SINFONI, which were used for the computation of the kinetics of the fusion.  We propose to estimate the density of the black holes in the central kpc of the two galaxies: NGC 3227 and NGC 4151. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0927, "title": "A Symplectic Test of the L-Functions Ratios Conjecture", "abstract": "  Recently Conrey, Farmer and Zirnbauer conjectured formulas for the averages over a family of ratios of products of shifted L-functions. Their L-functions Ratios Conjecture predicts both the main and lower order terms for many problems, ranging from n-level correlations and densities to mollifiers and moments to vanishing at the central point. There are now many results showing agreement between the main terms of number theory and random matrix theory; however, there are very few families where the lower order terms are known. These terms often depend on subtle arithmetic properties of the family, and provide a way to break the universality of behavior. The L-functions Ratios Conjecture provides a powerful and tractable way to predict these terms. We test a specific case here, that of the 1-level density for the symplectic family of quadratic Dirichlet characters arising from even fundamental discriminants d \\le X. For test functions supported in (-1/3, 1/3) we calculate all the lower order terms up to size O(X^{-1/2+epsilon}) and observe perfect agreement with the conjecture (for test functions supported in (-1, 1) we show agreement up to errors of size O(X^{-epsilon}) for any epsilon). Thus for this family and suitably restricted test functions, we completely verify the Ratios Conjecture's prediction for the 1-level density. ", "machine_abstract": "We present an explicit formula for the coefficients in the expansion of certain Eisenstein series into Dirichlet characters, and use it to prove that the ratios conjecture is true at all primes p with $(p-1)/2$ odd.  We also give numerical evidence supporting our result by computing these coefficients up through degree 12. The Langlands program predicts that there should be deep connections between number theory and representation theory.   In particular, one expects that the values of many arithmetic functions can be expressed as special values of automorphic forms on algebraic groups over Q.    One such function is the Dedekind zeta-function $\\zeta_K(s) = \\sum_{n \\geq 1} \\frac{1}{n^s} \\prod_{\\mathfrak{p}|n} (1-\\frac{1}{N_\\mathfrak{p}})^{-1}$ associated to any number field $K$.   For example, if $K = \\mathbf{Q}(\\sqrt{D})$ where $D > 1$ is square-free then we have the functional equation  $$ \\zeta_K(s)^2 = \\frac{\\pi^{-s}}{2}\\sin\\left(\\frac{\\pi s}{2}\\right)\\Gamma(1 - s)\\zeta_K(1-s). $$    This implies that the value of $\\zeta_K(s + 1)/\\zeta_K(s)$ is equal to the constant term of the Laurent expansion of $\\sin(\\pi s/2)/\\sin(\\pi (1-s))$ around $s=1$.   If $L/K$ is a Galois extension of number fields then the Artin reciprocity law states that this ratio equals the complex conjugation map on the group $G = \\mathrm{Gal}(L/K)$.   More generally, let $\\mathcal{O}_L$ denote the ring of integers of $L$, and define the local factorization type of $L/K$ to be the set of prime ideals $\\mathfrak{p} \\subset \\mathcal{O}_L$ which are ramified or inert in $L/K$.   Then the global factorization type of $L / K$ is defined to be the multiset union of the local factorization types of all finite extensions of $K$ contained in $L$.", "paraphrased_abstract": "The Langlands program predicts that there will be deep connections between numbers and representations. One of these connections is the zeta-function dedekind, zeta-k2 = cn geq 1 fr1 prod mathfrakp|n1 mathfrakp1-1, which is a special form of arithmetical expressions for the algebraic group, where a certain group of p, is a Galois extension of a field, and the Artin reciprocity law of the ratio mathfrakp1 mr2(L/K): the value of zeta_K(s) is equal to the constant value of sin(pi s/2) sin(pi (1-s) zeta_K(s) -zeta-K(s) 2sin(pi s/2) Gamma_K(s) -zeta_K(s) -__[5]|2|6] -|7|-, - -- ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1115, "title": "Strange Nucleon Form Factors from $ep$ and $\\nu p$ Elastic Scattering", "abstract": "  The recent parity-violating $ep$ forward-scattering elastic asymmetry data from Jefferson Lab (HAPPEx and G0), when combined with the $\\nu p$ elastic cross section data from Brookhaven (E734), permit an extraction of the strangeness contribution to the vector and axial nucleon form factors for momentum transfers in the range $0.45 < Q^2 < 1.0$ GeV$^2$. These results, combined with the recent determination of the strange vector form factors at $Q^2 = 0.1$ GeV$^2$ (SAMPLE, HAPPEx, PVA4, G0) have been interpreted in terms of $uuds\\bar{s}$ configurations very different from the kaon-loop configurations usually associated with strangeness in the nucleon. New experiments are being proposed to improve the state of our knowledge of the $\\nu p$ elastic cross section -- these new experiments will push the range of $Q^2$ to much lower values, and greatly increase the precision of the $\\nu p$ elastic data. One outcome of this can be a measurement of the strangeness contribution to the nucleon spin, $\\Delta s$. Nuclear targets (e.g. C or Ar) are to be used in these neutrino experiments, and so a deep understanding of the nuclear physics, particularly in regard to final state effects, is needed before the potential of these precision experiments can be fully realized. ", "machine_abstract": "We present the results for strange nucleon form factors obtained in our recent analysis of elastic ep scattering data at low momentum transfer, Q^2 < 1 GeV^2.  The experimental information on the proton electric charge radius is extracted by fitting to the Rosenbluth separation method using the standard dipole parametrization for the electromagnetic form factor G_E(Q^2). We find that the value of the proton charge radius r_p = 0.84 fm agrees well with other determinations based on different methods but has smaller uncertainty than previous extractions. In addition we extract the values of the magnetic moments of the proton and neutron as well as their anomalous magnetic moments. Our results are compared with those obtained within various theoretical models. Finally, we discuss possible implications of these findings for the determination of the strangeness content of the nucleon. PACS numbers: 13.60.Le, 14.20.Dh, 11.30.Er", "paraphrased_abstract": "And we also find the magnetic moment of the proton and the neutron, and the anomalous moment of the proton and the neutron, and discuss the implications of this result. We conclude by comparing the result with the results of other experimental tests. The data obtained from our study of elastic ep scattering, and a low-energy transfer, were obtained. We obtained the values of the electric charge radius of the proton and the ep-radius, which correspond to the simplest definition of the electromagnetic form factor, the E-radius, for the electromagnetic form factor, G-E. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0848, "title": "Generation of different Bell states within the SPDC phase-matching bandwidth", "abstract": "  We study the frequency-angular lineshape for a phase-matched nonlinear process producing entangled states and show that there is a continuous variety of maximally-entangled states generated for different mismatch values within the natural bandwidth. Detailed considerations are made for two specific methods of polarization entanglement preparation, based on type-II spontaneous parametric down-conversion (SPDC) and on SPDC in two subsequent type-I crystals producing orthogonally polarized photon pairs. It turns out that different Bell states are produced at the center of the SPDC line and on its slopes, corresponding to about half-maximum intensity level. These Bell states can be filtered out by either frequency selection or angular selection, or both. Our theoretical calculations are confirmed by a series of experiments, performed for the two above-mentioned schemes of producing polarization-entangled photon pairs and with two kinds of measurements: frequency-selective and angular-selective. ", "machine_abstract": "We demonstrate that it is possible to generate all four Bell states in one single nonlinear crystal by using two pump beams with orthogonal polarizations and slightly different wavelengths, which are generated via second-harmonic generation (SHG) inside an optical parametric oscillator (OPO). The OPO consists of a periodically poled lithium niobate (PPLN) crystal as nonlinear medium and a concave mirror for cavity feedback. We show experimentally that this approach allows us to obtain high-visibility quantum interference between photons emitted at degenerate wavelength pairs across the entire PPLN acceptance bandwidth. This method can be used to simplify future experiments on continuous-variable entanglement distribution over large distances.     Quantum information processing requires the ability to create and manipulate entangled states of light. In particular, the Bell state measurement plays a key role in many applications such as teleportation or quantum repeaters [1] . However, generating these highly nonclassical states is challenging because they require indistinguishable photon pairs [2] , which cannot be produced deterministically [3] . In recent years, several approaches have been developed to overcome this problem [4] . One possibility is based on spontaneous parametric down-conversion (SPDC), where a pump beam creates correlated pairs of signal and idler photons [5] . By adjusting the relative phases of the pump fields [6] , it has become possible to produce any desired superposition of the four Bell states [7, 8] . Another option uses squeezed vacuum states [9] or displaced number states [10] instead of coherent laser pulses [11] . These methods allow for efficient generation of entangled states but usually suffer from low visibility due to imperfections [12] .", "paraphrased_abstract": "It is a matter of great importance to know the properties of the Bell, and this is especially true for the teleportation and the repeater. The Bell states, particularly when they are measured in the light of the emitted light, have the potential for entanglement and may be of considerable interest to teleporters and quantum repeaters. It is the aim of this work that we will show that the two pump-beams can be used to produce a pair of Bell states, of a different wavelength, in the direction of the concave mirror, in the direction of the cavity, so that the PPLN crystal can be used as nonlinear medium. Its medium is a bilinear lithium niobate crystal, and it is a concave mirror. This method has been studied for a long time, and the results show that we can obtain high-degree quantum interference in the quantum interference of the two photons at a degenerate wavelength, with the highest possible precision, and that the same quantum interference may be produced across the entire PPLN range. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1118, "title": "Determination of intrinsic switching field distributions in perpendicular recording media: numerical study of the $\\Delta H(M, \\Delta M)$ method", "abstract": "  We present a numerical study of the $\\Delta H(M,\\Delta M)$ method and its ability to accurately determine intrinsic switching field distributions in interacting granular magnetic materials such as perpendicular recording media. In particular, we study how this methodology fails for large ferromagnetic inter-granular interactions, at which point the associated strongly correlated magnetization reversal cannot be properly represented by the mean-field approximation, upon which the $\\Delta H(M,\\Delta M)$ method is based. In this study, we use a 2-dimensional array of symmetric hysterons that have an intrinsic switching field distribution of standard deviation $\\sigma$ and ferromagnetic nearest-neighbor interactions $J$. We find the $\\Delta H(M,\\Delta M)$ method to be very accurate for small $J/\\sigma$ values, while substantial errors develop once the effective exchange field becomes comparable with $\\sigma$, corroborating earlier results from micromagnetic simulations. We furthermore demonstrate that this failure is correlated with deviations from data set redundancy, which is a key property of the mean-field approximation. Thus, the $\\Delta H(M,\\Delta M)$ method fails in a well defined and quantifiable manner that can be easily assessed from the data sets alone. ", "machine_abstract": "We present an analysis of the recently proposed $\\Delta H(M ,\\Delta M )$ method for determining the distribution function of the intrinsic switching fields (ISF). The ISF is defined as the minimum applied magnetic field required to switch magnetization vectors between two stable states with opposite directions along the easy axis. We show that this method can be used only if the initial state of the system has been prepared by applying a sufficiently large external magnetic field and then removing it. In addition we demonstrate how one can use the same approach to determine the distribution functions of other important parameters such as anisotropy constant and exchange stiffness coefficient. Finally, we discuss possible applications of our results to experimental studies on perpendicularly recorded data storage devices. DOI: 10.1088/1742-6596/aa6b20 The determination of the distribution function of the switching fields in perpendicular recording media is crucial for understanding their physical properties and improving their performance. Recently, a new method was suggested which allows one to extract information about the switching field distribution directly from measurements of hysteresis loops [1] . This method relies on measuring the dependence of the coercive force $H_c$ on the magnitude $M_0$ of the saturation magnetization vector at different values of the angle $\\theta$ between the direction of the external magnetic field and the easy axis of the sample. It turns out that the measured dependence $H_c(M_0, \\theta)$ contains all necessary information needed to reconstruct the switching field distribution function $P_{H}(H)$ using the following formula:  $$P_H(H) = \\frac{1}{2}\\delta(H - H_c(M_0)) + \\frac{1}{2} \\int_{-\\infty}^{+\\infty} P_M(M_0) \\delta(H - H_c[M_0]) dM_0 \\tag{1} $$ where $P_M(M_0)$ denotes the probability density function of the saturation magnetization.  In Ref. 1, Eq. (1) was derived under the assumption that the initial state of the sample had already been prepared by applying a strong enough external magnetic field and then turning off the field.", "paraphrased_abstract": "In the previous article, we introduce the new method of analyzing the inverse of the hysteresis-loop \u2013 we consider the inverse of the inverse of the inverse of the inverse. We discuss here the analysis of the two experiments in which we have used this method, and we also discuss the inverse of the inverse of the inverse of the inverse. The inverse of this equation is a new method, which allows the determination of the inverse of the inverse of the inverse of the polarity, and the inverse of the polarity. This method is based on the measurement of the proportion of the coercive force on the magnitude of the saturation magnetization vector at different angles of the angle between the direction of the external magnetic field and the angle of the easy axis of the sample. This calculation has been based on the assumption that the initial state of the sample was already prepared by the application of a sufficiently strong external magnetic field, and then turned off the magnetic field. We have presented an analysis of a newly proposed method of measuring the proportion of the distribution of the magnetic field of the inflections of hysteresis-loops, whose inverse is the proportion of the force of the hysteresis-loops with respect to the size of the sample and its amplitude,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0613, "title": "Finite temperature corrections and embedded strings in noncommutative geometry and the standard model with neutrino mixing", "abstract": "  The recent extension of the standard model to include massive neutrinos in the framework of noncommutative geometry and the spectral action principle involves new scalar fields and their interactions with the usual complex scalar doublet. After ensuring that they bring no unphysical consequences, we address the question of how these fields affect the physics predicted in Weinberg-Salam theory, particularly in the context of the Electroweak phase transition. Applying the Dolan-Jackiw procedure, we calculate the finite temperature corrections, and find that the phase transition is first order. The new scalar interactions significantly improve the stability of the Electroweak Z string, through the ``bag'' phenomenon described by Watkins and Vachaspati. (Recently cosmic strings have climbed back into interest due to new evidence). Sourced by static embedded strings, an internal space analogy of Cartan's torsion is drawn, and a possible Higgs-force-like `gravitational' effect of this non-propagating torsion on the fermion masses is described. We also check that the field generating the Majorana mass for the $\\nu_R$ is non-zero in the physical vacuum. ", "machine_abstract": "We study finite temperature effects on the Standard Model (SM) by using the Seiberg-Witten map to embed it into noncommutative geometry, which is then used as an infrared regulator for the SM at high temperatures. We find that the embedding leads to new terms in the effective action involving fermions and gauge bosons, which are suppressed by powers of the noncommutativity scale $\\Lambda$. These terms can be interpreted as contributions due to virtual particles propagating along closed loops in spacetime. In particular we show how these loop contributions lead to modifications of the Higgs potential and the masses of the W and Z bosons. The results obtained here agree well with those found previously within the context of the electroweak phase transition.  The work presented here was supported by the Natural Sciences and Engineering Research Council of Canada under grant number RGPIN-2007-03516 .  Introduction: Noncommutative geometry has been shown to provide a useful framework for studying quantum field theories [1] , including the Standard Model [2] - [4] . It also provides a natural ultraviolet regularization scheme [5] . In this letter we will use the Seiberg-Witten map [6] to embed the Standard Model into noncommutative space-time [7, 8] . This allows us to calculate finite temperature corrections to the Standard Model [9] . Finite temperature corrections have recently received much attention because they may play an important role in understanding the physics behind the electroweak phase transition [10] - [12] .", "paraphrased_abstract": "I have also been able to obtain the normalization of the standard model by means of the Seiberg-Witten map. It is an infrared regulator of the standard model. In this paper we will apply the Seiberg-Witten map to the Standard Model in noncommutative space. This in turn will serve as a direct and indirect temperature correction for the Standard Model. We are interested in the noncommutative nature of the Standard Model in terms of the electron and gauge bosons, which are weakened by the noncommutative power of the Lambda-power. Then, in the higher temperature, we will calculate the entropy of the Higgs Potential and the mass of the W and Z bosons. The results presented here are in keeping with the results of the electroweak phase transition. We will study the effect of the entropy on the Standard Model, which we use the Seiberg-Witten map, to embed the Standard Model in noncommutative space, which is then used to regulate the SM at high temperatures. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1191, "title": "Improved Quantum Hard-Sphere Ground-State Equations of State", "abstract": "  The London ground-state energy formula as a function of number density for a system of identical boson hard spheres, corrected for the reduced mass of a pair of particles in a sphere-of-influence picture, and generalized to fermion hard-sphere systems with two and four intrinsic degrees of freedom, has a double-pole at the ultimate \\textit{regular} (or periodic, e.g., face-centered-cubic) close-packing density usually associated with a crystalline branch. Improved fluid branches are contructed based upon exact, field-theoretic perturbation-theory low-density expansions for many-boson and many-fermion systems, appropriately extrapolated to intermediate densities, but whose ultimate density is irregular or \\textit{random} closest close-packing as suggested in studies of a classical system of hard spheres. Results show substantially improved agreement with the best available Green-function Monte Carlo and diffusion Monte Carlo simulations for bosons, as well as with ladder, variational Fermi hypernetted chain, and so-called L-expansion data for two-component fermions. ", "machine_abstract": "We present an improved quantum hard-sphere ground-state equation-of-state (EOS) for the description of dense matter in astrophysics and nuclear physics, which is based on the exact solution to the Schr\u00f6dinger equation with a repulsive delta-function potential. The EOS has been derived by solving numerically the corresponding integral equations using the method of successive iterations. We have also obtained analytical expressions for the pressure and energy density as functions of the number density at zero temperature. Our results are compared with those calculated previously within various approximations such as the virial expansion up to second order, the Carnahan-Starling approximation, and the Percus-Yevick approximation. It turns out that our new EOS agrees well with these previous calculations over wide ranges of densities and temperatures. In particular, it reproduces very accurately the low-density limit where the ideal gas law holds exactly.     Keywords: Equation of state", "paraphrased_abstract": "Besides, it is a new method of obtaining the density of the earth as a function of its density. The EOS was computed by numerically solving the integral equations for each element, and, besides, a number of analytical expressions for the pressure and the energy density as functions of the number density at zero temperature. We show that our new method agrees with the previous ones, even for large quantities and temperatures. The new method, which is based on the virial expansion up to second order, consists of Carnahan-Starling, the Percus-Yevick approximation, and the Percus-Yevick approximation. We have, however, already obtained some very accurate results, with regard to the low-volume limit, where the ideal gas law is precisely observable. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3062, "title": "Boutroux curves with external field: equilibrium measures without a minimization problem", "abstract": "  The nonlinear steepest descent method for rank-two systems relies on the notion of g-function. The applicability of the method ranges from orthogonal polynomials (and generalizations) to Painleve transcendents, and integrable wave equations (KdV, NonLinear Schroedinger, etc.).   For the case of asymptotics of generalized orthogonal polynomials with respect to varying complex weights we can recast the requirements for the Cauchy-transform of the equilibrium measure into a problem of algebraic geometry and harmonic analysis and completely solve the existence and uniqueness issue without relying on the minimization of a functional. This addresses and solves also the issue of the ``free boundary problem'', determining implicitly the curves where the zeroes of the orthogonal polynomials accumulate in the limit of large degrees and the support of the measure. The relevance to the quasi--linear Stokes phenomenon for Painleve equations is indicated. A numerical algorithm to find these curves in some cases is also explained.   Technical note: the animations included in the file can be viewed using Acrobat Reader 7 or higher. Mac users should also install a QuickTime plugin called Flip4Mac. Linux users can extract the embedded animations and play them with an external program like VLC or MPlayer. All trademarks are owned by the respective companies. ", "machine_abstract": "We study the existence and uniqueness of equilibrium states for some classes of potentials in the presence of an external magnetic field, using results on the asymptotic behavior of solutions to Schr\u00f6dinger equations obtained by J. Bourgain and M. Goldstein.  We consider potentials which are bounded below but not necessarily convex or even continuous; we also allow the external magnetic fields to be time-dependent (but still periodic). In particular, our results apply to the case where the potential is given by the sum of a quadratic form plus a small perturbation. The main tool used here is the so-called ``Bourgain-Goldstein method'', which consists in studying the evolution of the Wigner transform of the solution to the Schr\u00f6dinger equation under the action of the unitary group generated by the corresponding Hamiltonian.  Introduction  The purpose of this article is to prove that there exists at most one equilibrium state associated to certain classes of potentials in the context of quantum mechanics when these potentials interact with an external magnetic field. This result was first proved by D. Ruelle [Ru] , who showed that if V : T d \u2192 R satisfies suitable conditions then there exists exactly one equilibrium measure \u00b5V . These assumptions include the fact that V should have superlinear growth at infinity and satisfy a condition known as ``uniform ellipticity''. However, it turns out that many interesting examples do not fall into this category. For example, let us mention the following two examples:  \u2022 If V = \u2212|x| 2 + |y| 2 , then V does not grow faster than linearly at infinity.", "paraphrased_abstract": "We use the so-called Bourgain-Goldstein method, which is based on the asymptote of solutions to Schr\u00f6dinger equations obtained by J. Bourgain and M. Goldstein. The most important thing we have to do is to give a precise picture of the state of equilibrium of certain classes of potentials in the presence of a magnetic field. The corresponding results are as follows: If V = x 2 + y 2, then V grows exponentially with the same infinity. This assumption, known as uniform ellipticity, can be verified by the following assumptions: that V must be superlinear at infinity and that a condition known as uniform ellipticity is satisfied. This conclusion was first proved by D. Ruelle, who showed that if V = T d  R, he found that V must be superlinear at infinity and satisfy the condition called uniform ellipticity. In this regard, we present here the so-called Bourgain-Goldstein method, which involves examining the evolution of the Wigner transform of the solution to the Schr\u00f6dinger equation under the influence of the unitary group of the Hamiltonian. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.022, "title": "Three Particle Correlations from STAR", "abstract": "  Two-particle correlations have shown modification to the away-side shape in central Au+Au collisions relative to $pp$, d+Au and peripheral Au+Au collisions. Different scenarios can explain this modification including: large angle gluon radiation, jets deflected by transverse flow, path length dependent energy loss, Cerenkov gluon radiation of fast moving particles, and conical flow generated by hydrodynamic Mach-cone shock-waves. Three-particle correlations have the power to distinguish the scenarios with conical emission, conical flow and Cerenkov radiation, from other scenarios. In addition, the dependence of the observed shapes on the $p_T$ of the associated particles can be used to distinguish conical emission from a sonic boom (Mach-cone) and from QCD-Cerenkov radiation. We present results from STAR on 3-particle azimuthal correlations for a high $p_T$ trigger particle with two softer particles. Results are shown for $pp$, d+Au and high statistics Au+Au collisions at $\\sqrt{s_{NN}}$=200 GeV. An important aspect of the analysis is the subtraction of combinatorial backgrounds. Systematic uncertainties due to this subtraction and the flow harmonics v2 and v4 are investigated in detail. The implications of the results for the presence or absence of conical flow from Mach-cones are discussed. ", "machine_abstract": "The STAR experiment at RHIC has measured the three-particle correlation function in Au+Au collisions at $\\sqrt{s_{NN}}$ =200 GeV and 62.4 GeV, as well as p+p interactions at 200 GeV.  The results are compared to theoretical predictions based on hydrodynamic models with different initial conditions for the fireball evolution.  It is found that the data favor an initial state which contains fluctuations in both density and flow velocity.  These fluctuations can be described by a Gaussian distribution whose width increases with decreasing beam energy.  This behavior is consistent with expectations from gluon saturation effects predicted within the Color Glass Condensate framework. PACS numbers: 25.75.-q, 11.15.Tk, 12.38.Mh  Three particle correlations have been studied extensively over the past decade using proton-proton (p+p) and nucleus-nucleus (A+B) collision experiments.   In particular, it was shown that these measurements provide important information about the space-time structure of the emitting source created during the early stages of heavy ion collisions.  For example, the shape of the two-particle azimuthal correlation function depends strongly on whether or not there exists collective flow in the system.  Similarly, the magnitude of this effect also provides insight into how much entropy is produced in the final state.  Finally, the presence of nonflow contributions such as resonance decays and jets complicates the interpretation of experimental results obtained via traditional methods.  To overcome some of these difficulties, several new techniques were developed recently to extract more detailed information...", "paraphrased_abstract": "The results are compared with theoretical predictions from the hydrodynamic models of the initial stage of the explosion. The data derived from these experiments provide important information about the underlying structure of the ion source in the early stages of collisions. It was shown that this analysis was especially useful for revealing the morphology of the ion source in the early stages of heavy ion collisions. Three particle correlations have been studied extensively during the last decade in proton-proton (p+p) and A+b collisions. The results have shown that the three-particle correlation coefficient is very strongly dependent on whether or not there is a flow of atoms in the system, and the magnitude of this effect is also important for the entropy of the final state. In the STAR experiment at the RHIC, we have measured the three-particle correlation coefficient in the collisions between Au and A, at a pressure of 100 GeV and a pressure of 200 GeV, and a pressure of pps at a pressure of 200 GeV, and the two-particle interaction between pp and p at a temperature of 200 GeV. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2819, "title": "An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF", "abstract": "  Admission control as a mechanism for providing QoS requires an accurate description of the requested flow as well as already admitted flows. Since 802.11 WLAN capacity is shared between flows belonging to all stations, admission control requires knowledge of all flows in the WLAN. Further, estimation of the load-dependent WLAN capacity through analytical model requires inputs about channel data rate, payload size and the number of stations. These factors combined point to a centralized admission control whereas for 802.11 DCF it is ideally performed in a distributed manner. The use of measurements from the channel avoids explicit inputs about the state of the channel described above. BUFFET, a model based measurement-assisted distributed admission control scheme for DCF proposed in this paper relies on measurements to derive model inputs and predict WLAN saturation, thereby maintaining average delay within acceptable limits. Being measurement based, it adapts to a combination of data rates and payload sizes, making it completely autonomous and distributed. Performance analysis using OPNET simulations suggests that BUFFET is able to ensure average delay under 7ms at a near-optimal throughput. ", "machine_abstract": "In this paper, we propose an autonomous distributed admission control scheme to improve the performance and fairness in wireless local area networks (WLANs). The proposed scheme is based on the concept that each station maintains its own queue length information by using the packet inter-arrival time at the physical layer. In addition, it uses the number of active stations as well as their transmission rates to determine whether or not new connections are admitted into the network. We show through simulation results that our scheme can achieve better throughput than existing schemes while maintaining good fairness among competing stations. Keywords: Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement. 1 Introduction With the rapid development of mobile computing devices such as laptops, PDAs, smart phones etc., there has been growing interest in providing high quality services over wireless local area networks (WLANS) [1] . However, due to limited bandwidth resources available in WLANs, efficient resource management becomes crucially important [2] . The most widely used medium access control protocol in current commercial WLAN products is the IEEE 802.11 Distributed Coordination Function (DCF), which provides both contention-based channel access mechanism called Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) [3] , and contention-free service via Point Coordinated Function (PCF) [4] . Although CSMA/CA allows multiple stations to share the same radio channel simultaneously without any centralized coordination, it suffers from poor system performance when the traffic load increases [5] . This problem is mainly caused by the hidden terminal effect [6] where two nodes may transmit packets to one another simultaneously causing collisions. To alleviate these problems, several approaches have been proposed [7 -10] . Among them, the authors in [8] introduced a simple but effective method known as Virtual Reservation Channel (VRC) to reduce the probability of collision between data frames transmitted by different stations. They also presented a modified version of VRC [9] to further enhance the performance of CSMA/CA under heavy loads. However, all these works assume that the number of active stations within the", "paraphrased_abstract": "\" In recent times, with the rapid development of mobile devices and computers, a greater interest has been shown in the development of WLANs, which in turn have been occupied by the desire to provide a high-quality service. This is especially the case in cases when there is a great deal of overlap in the network, a resulting in confusion between stations, which in turn results in a collision. In the present study, we propose an autonomous, distributed and fair access system for WLANs. The system is based on the principle that each station calculates its own interval by calculating the arrival time of the packet at the physical layer, the time of arrival of the packet at the transmitter and the time of arrival of the packet, and the number of active stations, and the rate of transmission are determined. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3267, "title": "The local stellar velocity field via vector spherical harmonics", "abstract": "  We analyze the local field of stellar tangential velocities for a sample of $42 339$ non-binary Hipparcos stars with accurate parallaxes, using a vector spherical harmonic formalism. We derive simple relations between the parameters of the classical linear model (Ogorodnikov-Milne) of the local systemic field and low-degree terms of the general vector harmonic decomposition. Taking advantage of these relationships we determine the solar velocity with respect to the local stars of $(V_X,V_Y,V_Z)=(10.5, 18.5, 7.3)\\pm 0.1$ \\kms. The Oort's parameters determined by a straightforward least-squares adjustment in vector spherical harmonics, are $A=14.0\\pm 1.4$, $B=-13.1\\pm 1.2$, $K=1.1\\pm 1.8$, and $C=-2.9\\pm 1.4$ \\kmspc. We find a few statistically significant higher degree harmonic terms, which do not correspond to any parameters in the classical linear model. One of them, a third-degree electric harmonic, is tentatively explained as the response to a negative linear gradient of rotation velocity with distance from the Galactic plane, which we estimate at $\\sim -20$ \\kmspc. The most unexpected and unexplained term within the Ogorodnikov-Milne model is the first-degree magnetic harmonic representing a rigid rotation of the stellar field about the axis $-Y$ pointing opposite to the direction of rotation. This harmonic comes out with a statistically robust coefficient $6.2 \\pm 0.9$ \\kmspc, and is also present in the velocity field of more distant stars. The ensuing upward vertical motion of stars in the general direction of the Galactic center and the downward motion in the anticenter direction are opposite to the vector field expected from the stationary Galactic warp model. ", "machine_abstract": "We present an algorithm for the decomposition of the local stellar kinematics into vector spherical harmonic functions (VSH). The method is applied to simulated data and real observations, where we recover the underlying VSH coefficients with high accuracy. We show that our approach can be used as a powerful tool in galactic dynamics studies by recovering the gravitational potential of the Milky Way's dark matter halo. In addition, it allows us to study the anisotropy of the stellar orbits on different scales.     Keywords: Vector spherical harmonics, Galactic dynamics, Stellar kinematics, Gravitational potentials     1 Introduction     Spherical Harmonic Analysis has been widely used over many decades to analyse astronomical datasets such as galaxy surveys or star counts. However, this technique cannot easily be extended to deal with non-scalar quantities like velocities or accelerations. This problem was overcome by expanding these quantities onto vector spherical harmonics (VSH) which are defined as tensor products of scalar spherical harmonics [1] . These new basis functions have already found applications in fields ranging from cosmology [2] , solar physics [3] , heliophysics [4] and geophysics [5] .   In recent years there has been growing interest in using VSHs to model the observed properties of galaxies [6] - [8] . For example, they were recently employed to decompose the line-of-sight component of the stellar kinematics [9] . Here, we extend their use to also include the tangential components of the stellar motions. As a result, we obtain a complete description of the three-dimensional distribution of the stellar kinematics within each spatial bin. Moreover, since the expansion coefficients depend only on angular coordinates, they can be determined independently at every point along the line-of-sight. Therefore, our method does not require any assumptions about the symmetry of the system under investigation.  2 Vector spherical harmonics", "paraphrased_abstract": "This technique has been used in cosmology, solar physics, heliophysics, and geophysics. It is the first time that we have used the tensors of scalar spherical harmonics for the observable properties of galaxies. Moreover, we extend the tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential tangential ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3209, "title": "In search of dying radio sources in the local universe", "abstract": "  Up till now very few dying sources were known, presumably because the dying phase is short at centimeter wavelengths. We therefore have tried to improve the statistics on sources that have ceased to be active, or are intermittently active. The latter sources would partly consist of a fossil radio plasma left over from an earlier phase of activity, plus a recently restarted core and radio jets. Improving the statistics of dying sources will give us a better handle on the evolution of radio sources, in particular the frequency and time scales of radio activity. We have used the WENSS and NVSS surveys, in order to find sources with steep spectral indices, associated with nearby elliptical galaxies. In the cross correlation we presently used only unresolved sources, with flux densities at 1.4 GHz larger than 10 mJy. The eleven candidates thus obtained were observed with the VLA in various configurations, in order to confirm the steepness of the spectra, and to check whether active structures like flat-spectrum cores and jets are present, perhaps at low levels. We estimated the duration of the active and relic phases by modelling the integrated radio spectra using the standard models of spectral evolution. We have found six dying sources and three restarted sources, while the remaining two candidates remain unresolved also with the new VLA data and may be Compact Steep Spectrum sources, with an unusually steep spectrum. The typical age of the active phase, as derived by spectral fits, is in the range 10^7 - 10^8 years. For our sample of dying sources, the age of the relic phase is on average shorter by an order of magnitude than the active phase. ", "machine_abstract": "We have carried out an extensive survey for dying radio galaxies at 1.4 GHz using the Australia Telescope Compact Array (ATCA). The sample consists of all known FRII-type radio galaxies with redshifts less than 0.1 and angular extents greater than 10 arcsec, excluding those that are too close to bright background sources or other nearby radio galaxies. We find no evidence for any significant number of such objects within our sample. This is consistent with previous studies which suggest that most radio galaxies die before they can be detected as extended structures on arcminute scales. However we do detect several examples of compact steep spectrum emission associated with previously identified dying radio galaxies. These may represent the final stages of evolution prior to complete cessation of nuclear activity. In addition there appears to be some correlation between the presence of these features and the optical properties of their host galaxies. Finally we present new observations of two well studied dying radio galaxies, 3C31 and NGC315, showing that both continue to evolve over time.", "paraphrased_abstract": "\u201cBut a further investigation showed that some of these radio galaxies were not dead until they could be identified at the arcminute scale. These are the first phases of the decay of the universe, before it becomes a nuclear death sentence. This is in keeping with previous work which indicates that most radio galaxies have died out before they are recognizable as arcminute-scale structures. There were also several examples of a narrow swarming, sharp-tailed spectral emission associated with a few radio galaxies that had been previously studied and that have a long lifespan. We present new observations of two well-studied radio galaxies, 3C31 and NGC315, which have been studied for some time. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0536, "title": "Molecular Line Emission from Massive Protostellar Disks: Predictions for ALMA and the EVLA", "abstract": "  We compute the molecular line emission of massive protostellar disks by solving the equation of radiative transfer through the cores and disks produced by the recent radiation-hydrodynamic simulations of Krumholz, Klein, & McKee. We find that in several representative lines the disks show brightness temperatures of hundreds of Kelvin over velocity channels ~10 km s^-1 wide, extending over regions hundreds of AU in size. We process the computed intensities to model the performance of next-generation radio and submillimeter telescopes. Our calculations show that observations using facilities such as the EVLA and ALMA should be able to detect massive protostellar disks and measure their rotation curves, at least in the nearest massive star-forming regions. They should also detect significant sub-structure and non-axisymmetry in the disks, and in some cases may be able to detect star-disk velocity offsets of a few km s^-1, both of which are the result of strong gravitational instability in massive disks. We use our simulations to explore the strengths and weaknesses of different observational techniques, and we also discuss how observations of massive protostellar disks may be used to distinguish between alternative models of massive star formation. ", "machine_abstract": "We present predictions for molecular line emission from massive protostellar disks, based on models that include detailed chemical networks coupled to physical disk structure calculations. We find that CO is likely to be optically thick in most lines within the frequency range accessible with current radio telescopes (e.g., ALMA). However, we predict significant differences between the observed profiles of different isotopologues due to optical depth effects. In addition, we show that high-J transitions of HCO+ are expected to have higher intensities than low-J transitions because they arise closer to the central star where temperatures are higher. Finally, we discuss how these results can be used as diagnostics to test theoretical models of planet formation.     Keywords: Disk evolution, Planet formation, Protoplanetary disk, Radio astronomy, Star formation         1 Introduction     The detection of circumstellar disks around young stars has been one of the major accomplishments of modern observational astrophysics over the past two decades. These observations provide important constraints on theories of planet formation by revealing the physical conditions under which planets form. For example, recent studies suggest that giant planets may form at large distances from their host stars through gravitational instability (Boss 1997; Durisen et al. 2007 ). This process requires relatively cool gas and dust temperatures (T<300 K) so that the Toomre Q parameter remains below unity throughout the disk (Toomre 1964) . If this condition is met, then self-gravitating clumps will grow until they reach masses comparable to those of Jupiter or Saturn before being disrupted by tidal forces (Gammie 2001; Rice et al. 2003) . Since the temperature distribution in such disks depends sensitively on the amount of heating provided by stellar radiation, it should be possible to use spatially resolved observations of infrared excesses to determine whether gravitationally unstable regions exist near the outer edge of protoplanetary disks (Kenyon & Hartmann 1987; Chiang & Goldreich 1997; D'Alessio et al. 1998; Calvet et al. 2002; Dullemond 2002a ) .", "paraphrased_abstract": "The results show that CO is probably thicker in the most visible radii of the current radio telescopes (e.g., ALMA). Moreover, there is a large difference in the size of the line between high and low isotopologues. As the temperature of the radii is sensitive to the heat of the sun, it is possible to use the spatially resolved measurements of infrared spectra to test whether there are gravitationally unstable regions near the outer edge of the disk (Kenyon & Hartmann 1987; Chiang & Goldreich 1997; D'Alessio et al. 1998; Calvet et al. 2002; Dullemond 2002). These measurements have been of great importance to the scientific understanding of the formation of planets, since they can also serve as a constraint to theories of planet formation, which can be applied to the formation of planets. The observations of the circumstellar disk around young stars have been an important achievement of the modern astrophysics in the last two decades. Recent studies suggest that giant planets are formed at great distances from their host stars by gravitational turbulence (Boss 1997; Durisen et al. 2007). In this case, the temperature of the disk should be sufficiently cool, and the Toomre Q must be at least one, so", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.188, "title": "Quantum Quenches in Extended Systems", "abstract": "  We study in general the time-evolution of correlation functions in a extended quantum system after the quench of a parameter in the hamiltonian. We show that correlation functions in d dimensions can be extracted using methods of boundary critical phenomena in d+1 dimensions. For d=1 this allows to use the powerful tools of conformal field theory in the case of critical evolution. Several results are obtained in generic dimension in the gaussian (mean-field) approximation. These predictions are checked against the real-time evolution of some solvable models that allows also to understand which features are valid beyond the critical evolution.   All our findings may be explained in terms of a picture generally valid, whereby quasiparticles, entangled over regions of the order of the correlation length in the initial state, then propagate with a finite speed through the system. Furthermore we show that the long-time results can be interpreted in terms of a generalized Gibbs ensemble. We discuss some open questions and possible future developments. ", "machine_abstract": "We study the dynamics of quantum quenches in extended systems, focusing on the case where the initial state is an eigenstate of some local operator and the final Hamiltonian has no such symmetry. We show that for generic states this leads to relaxation towards equilibrium with a characteristic time scale which grows exponentially with system size. This behavior can be understood by considering the effect of rare regions whose energy gap scales as $1/N$, leading to exponential decay of correlations at late times. In contrast, we find that if the initial state is chosen to have maximal overlap with the ground state of the final Hamiltonian then there are no relaxation effects whatsoever. Finally, we discuss how these results may be relevant to experiments studying cold atoms in optical lattices.  I. INTRODUCTORY REMARK The problem of understanding the nonequilibrium dynamics following a sudden change in parameters (the so-called quench) continues to attract considerable interest both theoretically [1] and experimentally [2] . The main focus so far has been on closed quantum many-body systems described by Hamiltonians with short-range interactions [3] , but recently attention has shifted to open quantum systems [4] . In particular, it was shown [5] that even when the initial state is highly excited, the evolution after a global quench will eventually relax into thermal equilibrium [6] . However, recent studies [7, 8] suggest that relaxation does not occur generically in open quantum systems, i.e., when the initial state is prepared by coupling the system to another reservoir or bath [9] . Instead, one expects equilibration only under certain conditions [10] : For example, if the initial state is close enough to the ground state of the new Hamiltonian [11] , or if the spectrum of the post-quench Hamiltonian contains a sufficiently large number of low-lying excitations [12] .", "paraphrased_abstract": "... Nevertheless, it has been shown that if the initial state is close enough to the ground of the new Hamiltonian, or if the spectrum of the post-quench Hamiltonian contains sufficiently many low-lying excitations. In this sense, relaxation can be expected only under certain conditions: For example, when the initial state is near the ground of the new Hamiltonian, or if the spectrum of the post-quench Hamiltonian contains enough of the low-lying excitations [13]. Then, in the last experiment, we show how the quantum relaxation of extended systems, where the first state is an eigenstate of a local operator, and the second state is not of this symmetry. This phenomenon is analyzed by the effect of rare regions, whose energy gap is $n$, which leads to exponential decay of the correlations at late times. Then, we discuss how these results might be useful for the investigation of cold atoms in optical lattices. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3073, "title": "The distance and neutral environment of the massive stellar cluster Westerlund 1", "abstract": "  The goal of this study is to determine a distance to Westerlund 1 independent of the characteristics of the stellar population and to study its neutral environment, using observations of atomic hydrogen. The HI observations are taken from the Southern Galactic Plane Survey to study HI absorption in the direction of the HII region created by the members of Westerlund 1 and to investigate its environment as observed in the HI line emission. A Galactic rotation curve was derived using the recently revised values for the Galactic centre distance of $R_\\odot = 7.6$ kpc, and the velocity of the Sun around the Galactic centre of $\\Theta_\\odot = 214$ km s$^{-1}$. The newly determined rotation model leads us to derive a distance of $3.9\\pm 0.7$ kpc to Westerlund 1, consistent with a location in the Scutum-Crux Arm. Included in this estimate is a very careful investigation of possible sources of error for the Galactic rotation curve. We also report on small expanding HI features around the cluster with a maximum dynamic age of 600,000 years and a larger bubble which has a minimum dynamic age of 2.5 million years. Additionally we re-calculated the kinematic distances to nearby HII regions and supernova remnants based on our new Galaxic rotation curve. We propose that in the early stages of the development of Wd 1 a large interstellar bubble of diameter about 50 pc was created by the cluster members. This bubble has a dynamic age similar to the age of the cluster. Small expanding bubbles, with dynamical ages $\\sim 0.6$ Myr are found around Wd 1, which we suggest consist of recombined material lost by cluster members through their winds. ", "machine_abstract": "We present new near-infrared photometry for the open cluster Westerlund 1, which is located in the Galactic bulge at a distance of 8 kpc (projected). We find that the red giant branch tip lies at K = 12.6 mag, corresponding to an age of about 10 Gyrs. The main sequence turn-off point occurs around K = 14.5 mag, indicating an age between 5 and 7 Gyrs. This places it among the oldest open clusters known. Using our data we derive a mean metallicity [Fe/H] = -1.3 \u00b1 0.1 dex. From this value and using theoretical models we estimate the mass of the most massive star in the cluster as Mmax = 120 - 140M\u2299. In addition, we have obtained deep optical images with HST/ACS/WFC2 covering the central region of the cluster. These reveal several dozen very bright stars within a radius of only 20 arcsec.", "paraphrased_abstract": "And, by means of the analysis of the three-dimensional photometric data, we have computed a mean metallicity of 1.3  0.1 dex; we derive a theoretical value for the mass of the largest star in the cluster as Mmax = 120 - 140 M. Moreover, we have acquired deep optical images, ACS and WFC, covering the central region of the cluster. We find that the wing of the red giant branch lies at K = 12.6, a radius of about ten miles, and the center of the branch is at K = 14.5, at which the age of the main star is between five and seven. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4012, "title": "Black Hole Evaporation in an Expanding Universe", "abstract": "  We calculate the quantum radiation power of black holes which are asymptotic to the Einstein-de Sitter universe at spatial and null infinities. We consider two limiting mass accretion scenarios, no accretion and significant accretion. We find that the radiation power strongly depends on not only the asymptotic condition but also the mass accretion scenario. For the no accretion case, we consider the Einstein-Straus solution, where a black hole of constant mass resides in the dust Friedmann universe. We find negative cosmological correction besides the expected redshift factor. This is given in terms of the cubic root of ratio in size of the black hole to the cosmological horizon, so that it is currently of order $10^{-5} (M/10^{6}M_{\\odot})^{1/3} (t/14 {Gyr})^{-1/3}$ but could have been significant at the formation epoch of primordial black holes. Due to the cosmological effects, this black hole has not settled down to an equilibrium state. This cosmological correction may be interpreted in an analogy with the radiation from a moving mirror in a flat spacetime. For the significant accretion case, we consider the Sultana-Dyer solution, where a black hole tends to increase its mass in proportion to the cosmological scale factor. In this model, we find that the radiation power is apparently the same as the Hawking radiation from the Schwarzschild black hole of which mass is that of the growing mass at each moment. Hence, the energy loss rate decreases and tends to vanish as time proceeds. Consequently, the energy loss due to evaporation is insignificant compared to huge mass accretion onto the black hole. Based on this model, we propose a definition of quasi-equilibrium temperature for general conformal stationary black holes. ", "machine_abstract": "We study the evaporation process of black holes (BHs) in an expanding universe by using the tunneling method and the WKB approximation. We find that, for large BH masses M \u226b Mc2 = 3 \u00d7 10^8M\u2299, where Mc is the critical mass at which the Hawking temperature vanishes, the lifetime of the BH decreases with increasing M as t ~ M-1/2. For small BH masses M < Mc2 , we show that the lifetime increases exponentially with decreasing M . The results are compared to those obtained within the framework of quantum field theory on curved space-time. It turns out that our predictions agree well with these results when one takes into account the effect of back reaction due to particle creation during the evaporation process.  PACS numbers: 04.20.-q; 98.80.Cq  I. INTRODUCTORY REMARK The discovery of Hawking radiation [1] has led to renewed interest in the problem of black hole (BH) evaporation [2] - [4] . In this work, we will use the tunneling method [5] - [8] to calculate the decay rate of massive BHs in an expanding universe [9] . II. BLACK HOLE EVAPORATION IN AN EXPANDING UNIVERSE  A. Tunneling Method In order to apply the tunneling method to the case of evaporating BHs, it is convenient to introduce new coordinates (t\u2032, r\u2032), related to the original ones (t,r) through the following transformations [10] :", "paraphrased_abstract": "A study of the evaporation rate of black holes in the expanding universe was undertaken. Then we calculated the decay rate of the large black holes (BHs) in an expanding universe by the use of the tunneling method. The tunneling method was chosen to estimate the decay rate of large black holes (BHs) in the expanding universe., i.e., by using the WKB method, we find that, for large BHs, M  Mc2 = 3  108M, where M  Mc2 is the critical mass at which the Hawking temperature vanishes, the lifetime of the BH decreases with the increasing M, as t  M-1/2. For small BHs, M  Mc2 = 3  108M, the lifetime of the BH decreases with increasing M. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0146, "title": "Mid-infrared laser light nulling experiment using single-mode conductive waveguides", "abstract": "  Aims: In the context of space interferometry missions devoted to the search of exo-Earths, this paper investigates the capabilities of new single mode conductive waveguides at providing modal filtering in an infrared and monochromatic nulling experiment; Methods: A Michelson laser interferometer with a co-axial beam combination scheme at 10.6 microns is used. After introducing a Pi phase shift using a translating mirror, dynamic and static measurements of the nulling ratio are performed in the two cases where modal filtering is implemented and suppressed. No additional active control of the wavefront errors is involved. Results: We achieve on average a statistical nulling ratio of 2.5e-4 with a 1-sigma upper limit of 6e-4, while a best null of 5.6e-5 is obtained in static mode. At the moment, the impact of external vibrations limits our ability to maintain the null to 10 to 20 seconds.; Conclusions: A positive effect of SM conductive waveguide on modal filtering has been observed in this study. Further improvement of the null should be possible with proper mechanical isolation of the setup. ", "machine_abstract": "We report on the first mid-infrared (MIR) laser light nulling experiment with an integrated optics approach based on silicon-on-insulator technology and suspended core fibers. The experimental setup consists of two fiber-coupled MIR lasers, one for each arm of a Michelson interferometer. A pair of suspended-core fibers is used to couple the output beams into a 2x2 multimode interference coupler (MMI). In order to achieve high extinction ratio in this MMI-based beam splitter we use a novel design that incorporates a tapered region at its input port. We demonstrate a maximum extinction ratio of more than 40 dB between both arms of our interferometer by adjusting the phase difference between them. This value corresponds to a minimum intensity contrast of 1e-10 which is sufficient for most applications such as space telescopes or free-space optical communication systems. Finally, we show how the performance can be further improved by optimizing the coupling efficiency through the suspended-core fibers.", "paraphrased_abstract": "Then we will show how to improve the efficiency by adjusting the distance between the two arms of the interferometer, thereby ensuring a high extinction of more than 40 dB between the two arms of the interferometer by means of a novel configuration which makes use of the curved section of the input port. We will then report the first MIR-null laser experiment using integrated optics, based on silicon-on-insulator, and on suspended-core fibers. The resulting system consists of two MIR-null lasers, one for each arm of the interferometer, and a pair of suspended-core fibers for the modulation of the output beams. We demonstrate that the maximum extinction ratio is over forty dB between the two arm of our interferometer. We show that this ratio corresponds to an absolute minimum of contrast of 1e-10, which is sufficient for most applications, such as astronomy and communication in space. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3377, "title": "Halo-model signatures from 380,000 SDSS Luminous Red Galaxies with photometric redshifts", "abstract": "  We analyze the small-scale clustering in \"MegaZ-LRG\", a large photometric-redshift catalogue of Luminous Red Galaxies extracted from the imaging dataset of the Sloan Digital Sky Survey. MegaZ-LRG, presented in a companion paper, spans the redshift range 0.4 < z < 0.7 with an r.m.s. redshift error dz ~ 0.03(1+z), covering 5,914 deg^2 to map out a total cosmic volume 2.5 h^-3 Gpc^3. In this study we use 380,000 photometric redshifts to measure significant deviations from the canonical power-law fit to the angular correlation function in a series of narrow redshift slices, in which we construct volume-limited samples. These deviations are direct signatures of the manner in which these galaxies populate the underlying network of dark matter haloes. We cleanly delineate the separate contributions of the \"1-halo\" and \"2-halo\" clustering terms and fit our measurements by parameterizing the halo occupation distribution N(M) of the galaxies. Our results are successfully fit by a \"central\" galaxy contribution with a \"soft\" transition from zero to one galaxies, combined with a power-law \"satellite\" galaxy component, the slope of which is a strong function of galaxy luminosity. The large majority of galaxies are classified as central objects of their host dark matter haloes rather than satellites in more massive systems. The effective halo mass of MegaZ-LRG galaxies lies in the range log_10 (M_eff / h^-1 M_sol) = 13.61 - 13.8 (increasing with redshift, assuming large-scale normalization sigma_8 = 0.8) for corresponding number densities in the range n_g = 5.03 - 0.56 x 10^-4 h^3 Mpc^-3. Our results confirm the usefulness of the halo model for gaining physical insight into the patterns of galaxy clustering. ", "machine_abstract": "We present the results of an analysis of the clustering properties of luminous red galaxies (LRGs) in the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use a sample of 380,000 LRGs selected to have 0.4 < zphot < 1.0 and Mr < --21.5 + 5logh. The angular correlation function is measured for this sample using the Landy & Szalay estimator on scales between 10' and 100'. To account for redshift space distortions we measure the projected cross-correlation functions wp(rp), where rp = Dproj/[H(z)/H0], H(z) is the Hubble parameter at redshift z, and H0 is its value today. These measurements are made over a range of transverse separations corresponding to physical scales ranging from 2 h-1 Mpc to 20 h-1 Mpc. In addition, we also measure the real-space two-point correlation function by applying the method developed by Eisenstein et al. (2007) . This measurement is performed only out to a maximum separation of 60 h-1 Mpc due to the limited number density of our galaxy sample.", "paraphrased_abstract": "In addition, we measure the space-time two-point correlation function, as well as the density of real space in the physical scales of 2 h-1 Mpc and 20 h-1 Mpc. In this way we consider the redshifting of the galaxies in the DR7 data. We apply the method of Eisenstein et al. (2005) to a sample of 380,000 galaxies, which have a size of 0.6  zphot  1.0 and a size of 2.5 + 0.5logh. This method is employed in a wide range of transverse distances, from 2 h-1 Mpc to 20 h-1 Mpc. We apply the method of two-point correlation to the actual space two-point function, which is based on the method developed by Eisenstein et al. (2005). The angular correlation is derived from the Landy et al. function on scales from tens of feet to a hundred feet. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1445, "title": "Deformed Wigner crystal in a one-dimensional quantum dot", "abstract": "  The spatial Fourier spectrum of the electron density distribution in a finite 1D system and the distribution function of electrons over single-particle states are studied in detail to show that there are two universal features in their behavior, which characterize the electron ordering and the deformation of Wigner crystal by boundaries. The distribution function has a $\\delta$-like singularity at the Fermi momentum $k_F$. The Fourier spectrum of the density has a step-like form at the wavevector $2k_F$, with the harmonics being absent or vanishing above this threshold. These features are found by calculations using exact diagonalization method. They are shown to be caused by Wigner ordering of electrons, affected by the boundaries. However the common Luttinger liquid model with open boundaries fails to capture these features, because it overestimates the deformation of the Wigner crystal. An improvement of the Luttinger liquid model is proposed which allows one to describe the above features correctly. It is based on the corrected form of the density operator conserving the particle number. ", "machine_abstract": "We study the ground state properties of an interacting electron gas confined to a onedimensional (1D) quantum dot with parabolic confinement potential and repulsive Coulomb interaction between electrons. We show that, for sufficiently strong confining potential, the system undergoes a phase transition into a deformed Wigner crystal at low temperatures. The results are obtained by using density functional theory within the local spin-density approximation combined with exact diagonalization method. In this regime, we find that the charge distribution is characterized by alternating peaks separated by valleys which become more pronounced as temperature decreases. This behavior can be understood in terms of formation of a periodic structure due to inter-particle correlations. Our results suggest that such structures may exist experimentally in semiconductor nanowires or carbon nanotubes. Introduction:-In recent years there has been considerable interest in studying the electronic properties of nanostructures [1] . One dimensional systems have attracted particular attention because they provide a unique opportunity to investigate fundamental physical phenomena like Luttinger liquid [2] , fractional statistics [3] , and Wigner crystallization [4] . Theoretical studies of 1D quantum dots (QDs), i.e., QDs with only one dimension much smaller than other two dimensions, were first carried out by Lieb et al [5] who showed that these systems exhibit interesting features including shell filling effects [6] . Subsequently, several authors studied various aspects of QD physics [7, 8] . For example, it was shown that the energy spectrum of a QD depends strongly on its shape [9] . It also turns out that the single particle wave functions of a QD depend sensitively on the boundary conditions [10] . Recently, some experimental progress has been made towards realizing 1D QDs [11] - [13] . However, most experiments so far have focused mainly on transport measurements [14] - [16] rather than direct imaging [17] . Therefore, theoretical investigations play an important role in understanding the underlying physics of these systems [18] - [20] . In this work, we consider a model consisting of N non-interacting fermions confined to a 1D QD with parabolic confinement potential V(x). The total energy E tot = \u2211 i=1...N", "paraphrased_abstract": "There is no need to mention that in the last few years, there has been much interest in the study of the electronic properties of nanostructures. The development of these nanostructures has attracted much attention, especially since they are a unique opportunity to study the fundamental properties of the vapors, the fractions and the Wigner crystallization... In the last few years, considerable progress has been made towards the realization of 1D quantum dots, that is, quantum dots with dimensions much smaller than the two dimensions, and that the one dimension, the more tiny, is hardly sufficiently large. In the last decade, however, much research has been done on the electronic properties of the nanostructures, namely, of one dimension, which, in general, is far from being made. This is because the fundamental physics of the nanostructures is unique. Thus, the fundamental physics of these systems has been investigated, especially in the case of Liu Xiao, whose shell-filled state is revealed, and also in the case of a shell-filled state, and a deformed crystal. This system is investigated in the present paper by the theoretical studies of a ring of one dimension, which is much smaller than two dimensions, by Lieb et al., who showed that these systems are very interesting and they are subject to the shell-filling effect. Then we develop", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1895, "title": "CIV 1549 as an Eigenvector 1 Parameter for Active Galactic Nuclei", "abstract": "  [Abridged] We have been exploring a spectroscopic unification for all known types of broad line emitting AGN. The 4D Eigenvector 1 (4DE1) parameter space shows promise as a unification capable of organizing quasar diversity on a sequence primarily governed by Eddington ratio. This paper considers the role of CIV 1549 measures with special emphasis on the CIV 1549 line shift as a principal 4DE1 diagnostic. We use HST archival spectra for 130 sources with S/N high enough to permit reliable CIV 1549 broad component (BC) measures. We find a CIV 1549 BC profile blueshift that is strongly concentrated among (largely radio-quiet: RQ) sources with FWHM(H beta BC) < 4000 km/s (which we call Population A). Narrow line Seyfert 1 (NLSy1, with FWHM H beta < 2000 km/s) sources belong to this population but do not emerge as a distinct class. The systematic blueshift, widely interpreted as arising in a disk wind/outflow, is not observed in broader lined AGN which we call Population B. We find new correlations between FWHM(CIV 1549 BC) and CIV 1549 line shift as well as the equivalent width of CIV 1549. They are seen only in Pop. A sources. CIV 1549 measures enhance the apparent dichotomy at FWHM(Hbeta BC) approx. 4000 \\kms\\ (Sulentic et al. 2000) suggesting that it has more significance in the context of Broad Line Region structure than the more commonly discussed RL vs. RQ dichotomy. Black hole masses computed from FWHM CIV 1549 BC for about 80 AGN indicate that the CIV 1549 width is a poor virial estimator. Comparison of mass estimates derived from Hbeta BC and CIV 1549 reveals that the latter show different and nonlinear offsets for population A and B sources. A significant number of sources also show narrow line CIV 1549 emission. We present a recipe for CIV 1549 narrow component extraction. ", "machine_abstract": "We present the results of our analysis on the X-ray spectrum and variability properties of CIV 1549, which is one of the brightest Seyfert galaxies in the sky at soft X-rays (0.5-2 keV). We find that its spectral shape can be well described by a power law with photon index \u0393 = 2.1 \u00b1 0.2 plus two thermal components; one component has temperature kT = 0.3 +0.4 \u22120.1 keV while another has higher temperature kT = 3.7 +1.6 \u22121.1 keV. The luminosity ratio between these two thermal components is L h /L l \u2248 5.9 +2.8 \u22122.1 . In addition to this multi-component continuum model, we also include several emission lines such as Fe K\u03b1 line and OVII triplet. Our best-fit parameters are consistent with those obtained previously using ASCA data.     Using the Chandra HETG observation taken during 2001-2002, we have investigated the short-term variability behavior of CIV 1549. We found no significant time lag between different energy bands within the observed bandpasses. However, there appears to exist some correlation between flux variations in hard energies (> 4 keV) and those in softer energies (< 4 keV), although it does not appear to be strictly linear relationship. This result suggests that the origin of the short-term variability may be due to reprocessing of harder photons into softer ones rather than intrinsic fluctuations of the primary source itself.     Finally, we examine whether or not CIV 1549 shows any evidence for rapid aperiodic variability. By applying wavelet transform techniques to the light curve extracted from the central region of the galaxy, we detect strong signals corresponding to periods ranging from 10 - 100 s. These periodicities are most likely associated with quasi-periodic oscillations (QPOs).     We conclude that CIV 1549 is probably powered by accretion onto supermassive black holes.", "paraphrased_abstract": "We present the results of our study of the X-ray spectrum and the variability properties of CIV 1549, which are the brightest in the Seyfert constellation, at a wavelength of about 0.2  m. We show that the X-ray spectrum of this galaxy is well described by a power law based on  = 2.1  0.2, a crystalline sphere with a temperature of 0.3  0.3  0.1  0.1, a second crystalline sphere with a temperature of 3.6  1.7  0.9  1.1 keV. Besides the crystalline sphere, we also add several emission lines, such as the line of Fe K and the OVII triplet. We show that the crystalline sphere is probably formed by accretion of supermassive black holes. Then we test whether or not CIV 1549 shows rapid aperiodic fluctuation. We find that CIV 1549 is probably accreted by supermassive black holes. We have investigated CIV 1549 with the Chandra HETG in 2001-2002, which was observed during 2001 and 2002. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1972, "title": "Critical edge behavior in unitary random matrix ensembles and the thirty fourth Painleve transcendent", "abstract": "  We describe a new universality class for unitary invariant random matrix ensembles. It arises in the double scaling limit of ensembles of random $n \\times n$ Hermitian matrices $Z_{n,N}^{-1} |\\det M|^{2\\alpha} e^{-N \\Tr V(M)} dM$ with $\\alpha > -1/2$, where the factor $|\\det M|^{2\\alpha}$ induces critical eigenvalue behavior near the origin. Under the assumption that the limiting mean eigenvalue density associated with $V$ is regular, and that the origin is a right endpoint of its support, we compute the limiting eigenvalue correlation kernel in the double scaling limit as $n, N \\to \\infty$ such that $n^{2/3}(n/N-1) = O(1)$. We use the Deift-Zhou steepest descent method for the Riemann-Hilbert problem for polynomials on the line orthogonal with respect to the weight $|x|^{2\\alpha} e^{-NV(x)}$. Our main attention is on the construction of a local parametrix near the origin by means of the $\\psi$-functions associated with a distinguished solution of the Painleve XXXIV equation. This solution is related to a particular solution of the Painleve II equation, which however is different from the usual Hastings-McLeod solution. ", "machine_abstract": "We study critical edge behavior for certain classes of unitary random matrices, which are related to the so-called \\(\\Gamma\\) ensemble introduced by Dyson. We show that the distribution function of the largest eigenvalue is given by an explicit formula involving the thirty-fourth Painleve transcendent. This result can be viewed as a generalization of Tracy-Widom's famous theorem on the asymptotic distribution of the largest eigenvalue of Gaussian Unitary Ensemble (GUE). The proof relies heavily on the fact that the corresponding probability density functions have compact support. As applications we obtain new results about the number variance and the spacing between consecutive eigenvalues near the edge. In particular, our analysis leads to a simple expression for the leading order term of the number variance at the edge.  Introduction  The main purpose of this work is to investigate the statistical properties of the largest eigenvalue of some classes of unitary random matrices. These matrices appear naturally when one considers the scattering theory associated with Schr\u00f6dinger operators with point interactions [21, 22] . They also arise in connection with the so-called \\(\\Phi\\) ensemble studied recently by Jimbo et al. [17] , where they were used to describe the statistics of zeros of Riemann zeta-functions. More generally, these matrices play important roles in various branches of mathematics such as integrable systems, combinatorics, number theory and mathematical physics [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41] . In recent years there has been considerable interest in studying the statistical properties of the spectrum of large dimensional random matrices [1, 2, 3, 4, 5, 6, 7, 8, 9] . One of the most celebrated results in this direction was obtained independently by Wigner [42] and Dyson [12] who showed that the joint probability density function of the eigenvalues of n x n GUE converges weakly to", "paraphrased_abstract": "I will give a brief introduction of the results. We are concerned with the eigenvalue of certain classes of unitary random matrices. These are naturally found in Schr\u00f6dinger operators with point interactions, and have been studied in a different context. This is the case with the so-called Phi matrices which Dyson has introduced. The results show that the distribution of the largest eigenvalue is given by a formula which takes into account the thirty-fourth and the thirteenth Painleve. This result imposes a strong support on the corresponding probability density functions. The study of critical edge behaviour is based on the generalized theorem of Tracy-Widom's asymptotic distribution of the largest eigenvalue of the Gaussian eigenvalue. We show that the distribution of the largest eigenvalue is given by a formula involving the thirty-fourth painleve. This formula is based on the famous theorem of Tracy-Widom on the asymptotic distribution of the largest eigenvalue of the Gaussian eigenvalue. As a consequence, we have found a very simple expression for the leading order term of the large number of eigenvalues. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0004, "title": "On constraining a transiting exoplanet's rotation rate with its transit spectrum", "abstract": "  We investigate the effect of planetary rotation on the transit spectrum of an extrasolar giant planet. During ingress and egress, absorption features arising from the planet's atmosphere are Doppler shifted by of order the planet's rotational velocity (~1-2 km/s) relative to where they would be if the planet were not rotating. We focus in particular on the case of HD209458b, which ought to be at least as good a target as any other known transiting planet. For HD209458b, this shift should give rise to a small net centroid shift of ~60 cm/s on the stellar absorption lines. Using a detailed model of the transmission spectrum due to a rotating star transited by a rotating planet with an isothermal atmosphere, we simulate the effect of the planet's rotation on the shape of the spectral lines, and in particular on the magnitude of their width and centroid shift. We then use this simulation to determine the expected signal-to-noise ratio for distinguishing a rotating from a non-rotating planet, and assess how this S/N scales with various parameters of HD209458b. We find that with a 6 m telescope, an equatorial rotational velocity of ~2 km/s could be detected with a S/N~5 by accumulating the signal over many transits over the course of several years. With a 30 m telescope, the time required to make such a detection reduces to less than 2 months. ", "machine_abstract": "We present an analytical method to constrain the rotation period and obliquity of a planet by fitting its observed transmission or emission spectrum, which is distorted due to Doppler beaming effect caused by the planet's orbital motion around its host star.  We show that this method can provide useful constraints on the rotation rates for planets whose periods are longer than their orbital periods (Pspin > Porb). For example, we find that Kepler-62f has a minimum possible spin period of about 8 days if it orbits in the same plane as its host star; otherwise, its spin period could be up to 20 days. This result suggests that Kepler-62f may have a large spin-orbit misalignment angle. The proposed method also provides additional information beyond the Rossiter-McLaughlin effect when applied to multi-transit observations. In addition, our results suggest that the atmospheric composition of Kepler-62f should not vary significantly over time scales shorter than 10 days.", "paraphrased_abstract": "In the case of Kepler-62, for example, we find that the maximum possible spin is 8 days, if it is in the same plane as its host star; otherwise it is 20 days. In addition, the proposed method offers a useful feature for the obliquity of a planet when it is longer than its orbital period (Psrbn). We present an analytical method to constrain the rotation rate and the obliquity of a planet by estimating the obliquity and the obliquity of its star by the Doppler-ray effect caused by the orbital motion of the planet. We also find that Kepler-62 has a long rotation period of about 8 days. The other, 20 days, is enough for it to be of considerable accuracy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0872, "title": "Relativistic Tennis with Photons: Demonstration of Frequency Upshifting by a Relativistic Flying Mirror through Two Colliding Laser Pulses", "abstract": "  Since the advent of chirped pulse amplification1 the peak power of lasers has grown dramatically and opened the new branch of high field science, delivering the focused irradiance, electric fields of which drive electrons into the relativistic regime. In a plasma wake wave generated by such a laser, modulations of the electron density naturally and robustly take the shape of paraboloidal dense shells, separated by evacuated regions, moving almost at the speed of light. When we inject another counter-propagating laser pulse, it is partially reflected from the shells, acting as relativistic flying (semi-transparent) mirrors, producing an extremely time-compressed frequency-multiplied pulse which may be focused tightly to the diffraction limit. This is as if the counterstreaming laser pulse bounces off a relativistically swung tennis racket, turning the ball of the laser photons into another ball of coherent X-ray photons but with a form extremely relativistically compressed to attosecond and zeptosecond levels. Here we report the first demonstration of the frequency multiplication detected from the reflection of a weak laser pulse in the region of the wake wave generated by the driver pulse in helium plasma. This leads to the possibility of very strong pulse compression and extreme coherent light intensification. This Relativistic Tennis with photon beams is demonstrated leading to the possibility toward reaching enormous electromagnetic field intensification and finally approaching the Schwinger field, toward which the vacuum nonlinearly warps and eventually breaks, producing electron-positron pairs. ", "machine_abstract": "We report on the experimental demonstration of frequency upshift in two colliding laser pulses using a relativistically flying mirror (RFM). The RFM is realized as an ultrathin foil accelerated to high velocities by intense femtosecond laser pulses focused onto it at grazing incidence angle. We show that the collision between the counter-propagating laser pulses leads to the generation of new frequencies, which are shifted towards higher values compared to those generated without the presence of the RFM. This effect can be explained within the framework of nonlinear optics and quantum electrodynamics. Our results demonstrate the possibility for generating high-energy photons via collisions of laser pulses in vacuum. These findings may have important implications for future applications such as particle acceleration or gamma-ray sources based on table-top experiments.     In recent years there has been growing interest in studying the interaction of ultra-intense lasers with matter under extreme conditions [1] . One particular area of research focuses on the investigation of novel phenomena associated with the propagation of light in vacuum [2] , where the effects of strong field QED [3] become relevant [4] . For example, the emission of energetic electrons [5] and positrons [6] into vacuum was observed experimentally [7-9] when intense laser pulses were focused onto thin foils [10] . Moreover, the production of energetic photons [11] and pairs [12] in vacuum was predicted theoretically [13-15] .   In this Letter we present our experimental study of another interesting phenomenon related to the propagation of light in vacuo -the so-called relativistic tennis [16] . It consists of two counterpropagating laser pulses interacting with each other inside a vacuum chamber [17] . When these pulses collide they generate new frequencies [18] , which are shifted towards higher energies [19] . This effect occurs due to the fact that the electric fields of both pulses add coherently [20] leading to the formation of a standing wave pattern [21] . As a result, the intensity of the standing wave increases significantly [22] causing the appearance of new frequencies [23] .     Here we report on the first experimental observation of the relativistic tennis effect [24] . To achieve this goal, we used a relativistically flying mirror [25] , which", "paraphrased_abstract": "Then, despite the fact that it was in this very chamber, we present a new experiment in the area of light in a vacuum. We will discuss in detail the experimental demonstration of the phenomenon of the relativistic tennis. The experimental observation is carried out by the relativistic tennis. The result is the detection of new frequencies, and a significant increase in the intensity of the wave. In addition, the formation of the new waves is quite strong, and it is possible to obtain higher frequencies. This effect is mainly observed in the experiment of ultrathin layers of light, in the vacuum chamber, in the form of two counterpropagating laser pulses, interacting with each other, which results in a strong wave, in which the electric field is sufficiently expanded to form a wave pattern, and this pattern is reflected by the two pulses, and the intensity of the wave is increased. This phenomenon is in particular found in the field of strong field QED, where strong fields are important. In the present work, we show how the phenomenon of relativistic tennis can be interpreted. The phenomenon of relativistic tennis is called in this context. The mirror is a thin foil, accelerated with high speeds by femtosecond laser pulses. The beams are reflected by a grazing incidence. This grazing incidence gives the effect of a higher wave frequency. This phenomenon is explained by the principle", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3614, "title": "The Mean and Scatter of the Velocity Dispersion-Optical Richness Relation for maxBCG Galaxy Clusters", "abstract": "  The distribution of galaxies in position and velocity around the centers of galaxy clusters encodes important information about cluster mass and structure. Using the maxBCG galaxy cluster catalog identified from imaging data obtained in the Sloan Digital Sky Survey, we study the BCG-galaxy velocity correlation function. By modeling its non-Gaussianity, we measure the mean and scatter in velocity dispersion at fixed richness. The mean velocity dispersion increases from 202+/-10 km/s for small groups to more than 854+/-102 km/s for large clusters. We show the scatter to be at most 40.5+/-3.5%, declining to 14.9+/-9.4% in the richest bins. We test our methods in the C4 cluster catalog, a spectroscopic cluster catalog produced from the Sloan Digital Sky Survey DR2 spectroscopic sample, and in mock galaxy catalogs constructed from N-body simulations. Our methods are robust, measuring the scatter to well within one-sigma of the true value, and the mean to within 10%, in the mock catalogs. By convolving the scatter in velocity dispersion at fixed richness with the observed richness space density function, we measure the velocity dispersion function of the maxBCG galaxy clusters. Although velocity dispersion and richness do not form a true mass-observable relation, the relationship between velocity dispersion and mass is theoretically well characterized and has low scatter. Thus our results provide a key link between theory and observations up to the velocity bias between dark matter and galaxies. ", "machine_abstract": "We present new measurements of the mean and scatter in the velocity dispersion-optical richness relation (VRR) for galaxy clusters selected by their red-sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use two different methods to measure cluster velocities, one based on spectroscopic observations of member galaxies and another that uses photometric redshift estimates of all galaxies within a fixed aperture centered on each cluster's brightest cluster galaxy (BGG). The results are consistent with previous studies at low-redshift but show significant differences when compared to recent work done at higher redshifts. The discrepancies between our results and those presented in earlier works may be due to systematic effects associated with the measurement techniques used or could indicate evolution in the VRR over time. In either case, these results highlight the need for further investigation into this relationship as well as other scaling relations involving galaxy clusters.     Keywords: galaxy cluster, optical richness", "paraphrased_abstract": ", in a sphere, and in a circle. The data are based on spectroscopic observations of the members of the cluster, while the data are taken from a redshift estimate of the brightest cluster galaxy (GGG) within a fixed aperture, fixed to each cluster's brightest cluster galaxy. The result is in agreement with earlier studies at low redshifts, but is in significant contrast to earlier studies at higher redshifts. In our work, we present new estimates of the mean and scatter of the velocity and optical richness of the cluster, as well as the other scaling characteristics of the cluster, namely, the velocity, the optical richness, the redshift, and the ratio of the redshift of the'sequence,' which are chosen by the redshift of the cluster, and are compared with those of the previous work at higher redshifts. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0823, "title": "Dynamics of Localization Phenomena for Hardcore Bosons in Optical Lattices", "abstract": "  We investigate the behavior of ultracold bosons in optical lattices with a disorder potential generated via a secondary species frozen in random configurations. The statistics of disorder is associated with the physical state in which the secondary species is prepared. The resulting random potential, albeit displaying algebraic correlations, is found to lead to localization of all single-particle states. We then investigate the real-time dynamics of localization for a hardcore gas of mobile bosons which are brought into sudden interaction with the random potential. Regardless of their initial state and for any disorder strength, the mobile particles are found to reach a steady state characterized by exponentially decaying off-diagonal correlations and by the absence of quasi-condensation; when the mobile particles are initially confined in a tight trap and then released in the disorder potential, their expansion is stopped and the steady state is exponentially localized in real space, clearly revealing Anderson localization. ", "machine_abstract": "We study the dynamics of hard-core bosons on an optical lattice with random disorder and nearest-neighbor hopping, using exact diagonalization techniques. We find that there is a crossover between two different regimes as we increase the strength of disorder. In one regime (weak disorder), the system shows Anderson localization behavior; while in another regime (strong disorder) it exhibits Bose glass behavior. The transition point depends strongly on the filling fraction of particles per site. For low fillings, this transition occurs at relatively small values of disorder strengths. However, for higher fillings, the transition to the Bose glass phase takes place only when the disorder becomes very strong. This suggests that the presence of interactions can significantly affect the nature of the ground state of the system even if they are weak compared to other energy scales such as the bandwidth or the disorder strength.     Introduction     Disorder plays an important role in determining many properties of condensed matter systems. It has been shown recently that disorder can lead to interesting phenomena like quantum Hall effect [1] , metal-insulator transitions [2] , and superconductivity [3] . One of the most studied models which incorporates both disorder and interaction effects is the so-called Anderson model [4] . In its simplest form, this model describes non-interacting electrons moving through a disordered medium. Although the original formulation was restricted to electronic degrees of freedom, it has also been extended to describe various physical situations involving interacting particles [5] - [8] .   In recent years, ultracold atoms have emerged as promising candidates for simulating complex quantum mechanical problems [9] - [11] . These experiments provide us with unprecedented control over all relevant parameters of the problem under consideration [12] - [14] . Moreover, these systems allow us to explore new physics beyond what is possible in conventional solid-state materials [15] - [17] . Ultracold atomic gases trapped in optical lattices offer unique opportunities to investigate the interplay between disorder and interactions [18] - [20] . Recently, several experimental groups [21] - [23] have observed signatures of Anderson localization [24] in cold atom systems by studying the transport properties of the gas across the lattice.", "paraphrased_abstract": "But the effects of these phenomena have recently been shown to be very interesting. One of the most studied of these is the Anderson model, which in its simplest form combines non-interacting electrons moving through a disjointed medium, and this model is also extended to include the physical properties of interacting particles. In particular, it is possible to study the effect of interactions on the formation of the ground, even if they are small at the scale of the frequency and the number of sites. In this way, it is possible to obtain an unprecedented control over all the necessary parameters of the problem. For example, we study the dynamics of bosons on an optical lattice with a random disorder and a neighbouring hopping. In one of these, we find the Anderson localization; whereas in the other, the Bose-glass gradation is very strong. In the recent years, the cold atom has become a promising model for the simulation of the dynamics of complex systems. This model, although limited to electronic degrees, has been extended to describe a variety of physical conditions, such as Hall Effect, metal-insulator transitions, superconductivity, and superconductivity. In this study, we investigate the dynamics of hard-core bosons in an optical sphere, with random disorder and neighboring, using exact diagonalisation. The transition between these two classes is a result of the strong disorder", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3276, "title": "Feynman-Weinberg Quantum Gravity and the Extended Standard Model as a Theory of Everything", "abstract": "  I argue that the (extended) Standard Model (SM) of particle physics and the renormalizable Feynman-Weinberg theory of quantum gravity comprise a theory of everything. I show that imposing the appropriate cosmological boundary conditions make the theory finite. The infinities that are normally renormalized away and the series divergence infinities are both eliminated by the same mechanism. Furthermore, this theory can resolve the horizon, flatness, and isotropy problems of cosmology. Joint mathematical consistency naturally yields a scale-free, Gaussian, adiabatic perturbation spectrum, and more matter than antimatter. I show that mathematical consistency of the theory requires the universe to begin at an initial singularity with a pure $SU(2)_L$ gauge field. I show that quantum mechanics requires this field to have a Planckian spectrum whatever its temperature. If this field has managed to survive thermalization to the present day, then it would be the CMBR. If so, then we would have a natural explanation for the dark matter and the dark energy. I show that isotropic ultrahigh energy (UHE) cosmic rays are explained if the CMBR is a pure $SU(2)_L$ gauge field. The $SU(2)_L$ nature of the CMBR may have been seen in the Sunyaev-Zel'dovich effect. I propose several simple experiments to test the hypothesis. ", "machine_abstract": "We present an extension to the standard model that includes gravity, based on Feynman's ideas about quantum gravity. The extended standard model is formulated in terms of gauge fields for all known interactions (including gravity) and fermions with spin 1/2 or 1. We show how this theory can be derived from first principles using Feynman diagrams. In addition we discuss some phenomenological consequences such as neutrino masses and dark matter candidates. Finally we comment briefly on possible experimental tests of our proposal. This work was supported by NSF grant PHY-0456747. A theory of everything should include gravity along with other fundamental forces. Here we propose one such theory which extends the standard model including gravitational effects. Our approach follows closely Feynman's original idea of formulating quantum gravity in terms of gauge fields coupled to fermions. Using Feynman diagrams we derive the extended standard model from first principles. Some phenomenological consequences are discussed, e.g., neutrino mass generation via seesaw mechanisms and dark matter candidates. Possible experiments testing our proposal are also mentioned.", "paraphrased_abstract": "And we propose a theory that includes gravitation. Here, we propose a theory which includes gravity and the other fundamental forces. This is a theory of everything, which must include gravitation and the other fundamental forces. We propose an extension of the standard model, which includes gravity, by the example of Feynman's original ideas. We propose the extended model by the use of gauge fields and fermions. Then we present a phenomenological account of the results: neutrino masses, dark matter. Then, if the experimental results are shown, we propose a preliminary experiment to prove our approach. This research was supported by NSF grant PHY-0456747. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2086, "title": "Third-order cosmological perturbations of zero-pressure multi-component fluids: Pure general relativistic nonlinear effects", "abstract": "  Present expansion stage of the universe is believed to be mainly governed by the cosmological constant, collisionless dark matter and baryonic matter. The latter two components are often modeled as zero-pressure fluids. In our previous work we have shown that to the second-order cosmological perturbations, the relativistic equations of the zero-pressure, irrotational, multi-component fluids in a spatially near flat background effectively coincide with the Newtonian equations. As the Newtonian equations only have quadratic order nonlinearity, it is practically interesting to derive the potential third-order perturbation terms in general relativistic treatment which correspond to pure general relativistic corrections. Here, we present pure general relativistic correction terms appearing in the third-order perturbations of the multi-component zero-pressure fluids. We show that, as in a single component situation, the third-order correction terms are quite small (~ 5 x10^{-5} smaller compared with the relativistic/Newtonian second-order terms) due to the weak level anisotropy of the cosmic microwave background radiation. Still, there do exist pure general relativistic correction terms in third-order perturbations which could potentially become important in future development of precision cosmology. We include the cosmological constant in all our analyses. ", "machine_abstract": "We study the third-order cosmological perturbation theory for a zero pressure multicomponent fluid in an expanding universe with arbitrary spatial curvature and anisotropic stress. We derive the exact expressions for all relevant quantities up to third order, including the gauge-invariant density contrasts, velocity divergence, shear tensor, vorticity vector, energy fluxes, and anisotropic stresses. The results are presented in terms of the background evolution equations as well as the linear growth rate functions. In particular, we show that there is no pure GR contribution at second order but only at third order. This implies that any detection of such effects would be a clear signature of modified gravity theories beyond GR. Finally, we discuss some applications of our formalism to specific models of dark matter and dark energy. Cosmology has been one of the most successful areas of modern physics. It provides us not only with important information about the early history of the Universe, but also serves as a powerful tool to test fundamental physical laws on large scales. However, it is now widely believed that standard cosmology based on Einstein's General Relativity (GR) cannot explain many observed phenomena, especially those related to late-time cosmic acceleration [1] . Therefore, alternative theoretical frameworks have been proposed over the past few decades [2] , among which scalar-tensor theories [3] and f(R) gravity [4] are two popular examples. These new theories generally predict different gravitational dynamics than what is predicted by GR, so they can lead to observable deviations from GR predictions [5] . In this work, we will focus on studying the third-order cosmological perturbations of a zero-pressure multi-component system in an expanding universe with spatially curved geometry [6] . Our goal is to provide a complete set of exact solutions for various perturbed quantities up to third order using the covariant approach developed recently [7, 8] . As shown below, these solutions contain both first-and second-order contributions due to the presence of non-vanishing anisotropic stress and spatial curvature [9] . They may therefore serve as useful tools to investigate possible signatures of modified gravity theories beyond", "paraphrased_abstract": "The study of the third order cosmological perturbations in an expanding universe with an arbitrary spatial curvature and an anisotropic stress is proposed. It has been studied in a number of scientific and popular works over the last few decades. These theories have mainly based on the general relativity of Einstein, which is the basis of many observed phenomena, especially those related to the late-stage cosmic acceleration. It is a fact that the new theories are generally based on different gravity dynamics, which are not found in the GR, and hence the results are observable. The third-order covariance of these solutions has been recently developed. As shown in the figure below, these solutions have both first and second-order contributions, due to the existence of the non-vanishing anisotropic stress and the corresponding spatial curvature. The solutions can be used to investigate the signatures of a modified gravity theory, beyond GR. We will study the third-order cosmological perturbations of a zero-pressure fluid, in an expanding universe with arbitrary radiance and asymmetrical dimensions. We will investigate the results of our experiment in the form of linear evolution equations and the linear growth rate functions. We have obtained the exact solutions of the two-dimensional equations for the two-dimensional quantities, with the help of the covariant covariant technique. We have obtained the exact", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3969, "title": "Diversity of MIMO Multihop Relay Channels - Part I: Amplify-and-Forward", "abstract": "  In this two-part paper, we consider the multiantenna multihop relay channels in which the source signal arrives at the destination through N independent relaying hops in series. The main concern of this work is to design relaying strategies that utilize efficiently the relays in such a way that the diversity is maximized. In part I, we focus on the amplify-and-forward (AF) strategy with which the relays simply scale the received signal and retransmit it. More specifically, we characterize the diversity-multiplexing tradeoff (DMT) of the AF scheme in a general multihop channel with arbitrary number of antennas and arbitrary number of hops. The DMT is in closed-form expression as a function of the number of antennas at each node. First, we provide some basic results on the DMT of the general Rayleigh product channels. It turns out that these results have very simple and intuitive interpretation. Then, the results are applied to the AF multihop channels which is shown to be equivalent to the Rayleigh product channel, in the DMT sense. Finally, the project-and-forward (PF) scheme, a variant of the AF scheme, is proposed. We show that the PF scheme has the same DMT as the AF scheme, while the PF can have significant power gain over the AF scheme in some cases. In part II, we will derive the upper bound on the diversity of the multihop channels and show that it can be achieved by partitioning the multihop channel into AF subchannels. ", "machine_abstract": "In this work, we study the diversity-multiplexing tradeoff (DMT) for amplify-andforward multihop relay channels with multiple-input and multiple-output (MIMO). We first derive an upper bound on DMT by using the cut-set theorem. Then, based on the derived upper bound, we propose two schemes to achieve the optimal DMT performance in different scenarios. In particular, when all relays have full-duplex capability, our proposed scheme achieves the optimal DMT at high multiplexing gain region; while if only half-duplex relays are available, then our proposed scheme can still achieve the optimal DMT except that it is achieved at low multiplexing gain region instead. Finally, simulation results show that both proposed schemes outperform existing ones significantly. Index Terms-Diversity-Multiplexing Tradeoff, Multiple-Input MultipleOutput, Amplify-and-Forward", "paraphrased_abstract": "In this study, we consider the diversity multiplier tradeoff (DMT) of amplitude and the forward motion of multihops in a multihop system, based on a combination of multiple inputs and multiple outputs. We first obtain a theoretical upper bound on the DMT, and then we propose two ways to obtain the optimal DMT for different scenarios. For example, if all relays have full-duplex capability, our proposed scheme achieves the optimal DMT at a high multiplexing range. But when all relays have half-duplex capability, our proposed scheme achieves the optimal DMT at a low multiplexing range. In short, the proposed scheme achieves the optimal DMT in the highest multiplexing range. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.05, "title": "A deeper search for the progenitor of the Type Ic Supernova 2002ap", "abstract": "  (Abridged) We present a search for the progenitor star of the Type Ic Supernova 2002ap in deep, high quality pre-explosion observations taken with the Canada-France-Hawaii Telescope (CFHT). Aligning high-resolution Hubble Space Telescope (HST) observations of the supernova itself with the archival CFHT images allowed us to pinpoint the location of the progenitor site on the ground based observations. We find that a source visible in the B and R band pre-explosion images close to the position of the SN is (1) not coincident with the SN position within the uncertainties of our relative astrometry, and (2) is still visible ~ 4.7 yrs post-explosion in late-time observations taken with the William Herschel Telescope. We therefore conclude that it is not the progenitor of SN 2002ap. Comparing our luminosity limits with stellar models of single stars at appropriate metallicity (Z=0.008) we conclude that any single star progenitor must have experienced at least twice the standard mass loss rates during pre-Wolf-Rayet evolution, been initially > 30-40M(Sun) and exploded as a Wolf-Rayet star of final mass 10-12M(Sun). Alternatively an initially less massive progenitor may have evolved in an interacting binary system. We constrain any possible binary companion to a main sequence star of < 20M(Sun), a neutron star or a black hole. By combining the pre-explosion limits with the ejecta mass estimates and constraints from X-ray and radio observations we conclude that any binary interaction most likely occurred as Case B mass transfer, either with or without a subsequent common envelope evolution phase. ", "machine_abstract": "We present new optical and near-infrared photometry obtained with the Hubble Space Telescope (HST) in order to study the late-time evolution of the supernova remnant N132D, which is associated with the type Ic supernova SN2002ap. We find that the light curve of this supernova can be well fitted by a model consisting of two components: an initial power-law decline followed by a slower exponential decay. The best-fit parameters are consistent with those found previously using ground-based data. However, we also find evidence for additional flux at wavelengths longer than 1 micron after day 1000. This excess emission may arise from dust formed during the explosion or subsequent interaction between the ejecta and circumstellar material. In addition, our HST images reveal several bright knots along the southern edge of the remnant. These knots appear to have been ejected recently as they show no signs of fading over time scales ranging from months to years.", "paraphrased_abstract": "A new, near-infrared image was obtained from the Hubble Space Telescope, to investigate the late evolution of the supernova remnant N132D, which is associated with the type Ic supernova SN2002ap. We found the light curve of this supernova to be well fitted by the model, consisting of an initial decay of the power law, followed by a gradual re-death. We found that the supernova\u2019s light curve was well predicted by a model consisting of two components: the initial decomposition, followed by a slow, exponential decay. The most appropriate parameter was obtained from the ground. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1697, "title": "Effect of the Spatial Dispersion on the Shape of a Light Pulse in a Quantum Well", "abstract": "  Reflectance, transmittance and absorbance of a symmetric light pulse, the carrying frequency of which is close to the frequency of interband transitions in a quantum well, are calculated. Energy levels of the quantum well are assumed discrete, and two closely located excited levels are taken into account. A wide quantum well (the width of which is comparable to the length of the light wave, corresponding to the pulse carrying frequency) is considered, and the dependance of the interband matrix element of the momentum operator on the light wave vector is taken into account. Refractive indices of barriers and quantum well are assumed equal each other. The problem is solved for an arbitrary ratio of radiative and nonradiative lifetimes of electronic excitations. It is shown that the spatial dispersion essentially affects the shapes of reflected and transmitted pulses. The largest changes occur when the radiative broadening is close to the difference of frequencies of interband transitions taken into account. ", "machine_abstract": "We study theoretically and numerically the effect of spatial dispersion (SD) on the shape of a light pulse propagating through an InGaAs/GaAs quantum well (QW). We show that SD leads to significant changes in the temporal profile of the transmitted pulse, which can be used for its characterization. The results are obtained by solving Maxwell's equations using the finite-difference time-domain method with periodic boundary conditions. It is shown that the presence of SD causes the appearance of additional peaks at both sides of the main peak of the transmitted pulse. These peaks become more pronounced as the QW width increases.     Keywords: Light propagation, Finite difference time domain method, Quantum wells, Spatial dispersion. 1 Introduction     A number of recent studies have been devoted to investigating the effects of spatial dispersion (SD), also known as nonlocality or transverse momentum conservation [1] , on various physical phenomena such as nonlinear wave dynamics [2] - [4] , spontaneous emission [5] , and transport [6] . This interest has been motivated mainly by the fact that many semiconductor devices operate under conditions where SD plays an important role [7, 8] .   In this work we consider the problem of light transmission through a single-mode quantum well (QW) structure [9] . Our aim is to investigate how SD affects the shape of the transmitted pulse. To do so, we solve Maxwell's equations using the finitedifference time-domain (FDTD) method [10] with periodic boundary conditions [11] . As it will be demonstrated below, our numerical simulations reveal that SD gives rise to new features in the temporal profile of a transmitted pulse.", "paraphrased_abstract": "It is well known that the transverse momentum conservation is the key to all the phenomena of nonlinear wave motions, evaporation, and transport. In this work, we examine the effect of the transverse momentum conservation on the shape of a light pulse, which is propagated by an InGaAs/GaAs quantum well. We study the effect of the transverse momentum conservation on the shape of a transient pulse. We investigate how the transient pulse is shaped by the transient pulse by the transient pulse. We study the corresponding numerical results, which show that the transient pulse is shaped by the transient pulse. This result is confirmed by numerical simulations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1633, "title": "Large Attractive Depletion Interactions in Soft Repulsive-Sphere Binary Mixtures", "abstract": "  We consider binary mixtures of soft repulsive spherical particles and calculate the depletion interaction between two big spheres mediated by the fluid of small spheres, using different theoretical and simulation methods. The validity of the theoretical approach, a virial expansion in terms of the density of the small spheres, is checked against simulation results. Attention is given to the approach toward the hard-sphere limit, and to the effect of density and temperature on the strength of the depletion potential. Our results indicate, surprisingly, that even a modest degree of softness in the pair potential governing the direct interactions between the particles may lead to a significantly more attractive total effective potential for the big spheres than in the hard-sphere case. This might lead to significant differences in phase behavior, structure and dynamics of a binary mixture of soft repulsive spheres. In particular, a perturbative scheme is applied to predict the phase diagram of an effective system of big spheres interacting via depletion forces for a size ratio of small and big spheres of 0.2; this diagram includes the usual fluid-solid transition but, in the soft-sphere case, the metastable fluid-fluid transition, which is probably absent in hard-sphere mixtures, is close to being stable with respect to direct fluid-solid coexistence. From these results the interesting possibility arises that, for sufficiently soft repulsive particles, this phase transition could become stable. Possible implications for the phase behavior of real colloidal dispersions are discussed. ", "machine_abstract": "We study the phase behavior and structure of binary mixtures composed of soft repulsive spheres with attractive depletants, which are modeled as hard-spheres that interact only via excluded volume interactions. We find that these systems exhibit rich phase diagrams including gas-liquid coexistence at low temperatures for all compositions studied here (0.25 < f < 0.75), where f is the fraction of particles made up by the smaller species. The liquid-gas binodal lines shift to higher pressures upon increasing the size ratio between the two components. For large size ratios we observe an additional fluid-fluid transition line along which both fluids have similar densities but different structures. This new fluid state has been observed experimentally in colloidal suspensions containing nonadsorbing polymer chains. Our results show good agreement with experimental data on colloid-polymer mixtures over wide ranges of temperature, pressure, and composition.  I. INTRODUCTIO N The presence of small particles can dramatically affect the properties of larger ones through depletion forces [1] . These effects play important roles in many physical phenomena such as protein crystallization [2] , gelation [3] , and sedimentation [4] . Depending on their sizes relative to each other, the mixture may be either miscible or immiscible [5] . In addition, there exist regions of metastability [6] and even multiple phases [7, 8] . A number of theoretical studies [9] - [11] have investigated the effect of depletion attractions on the phase diagram of simple model systems. However, most of them focused on idealized models neglecting hydrodynamic interactions [12] , finite-size effects [13] , polydispersity [14] , and particle shape [15] . Only recently did some authors [16] take into account more realistic features like Brownian motion [17] , electrostatic repulsion [18] , and van der Waals attraction [19] . Despite this progress, it remains difficult to predict the exact location of the critical point [20] due to strong correlations [21] among the particles [22] . Moreover, the influence of depletion forces on the structural [23] and dynamical [24] properties of complex fluids still needs further investigation [25] . In recent years, experiments [26]", "paraphrased_abstract": "This is particularly interesting in fluids such as colloidal suspensions, where the polymer chains have no absorbing properties. They are enriched by many studies, especially of the formation of soft and unpleasant particles with attractive depleting properties, and of solids with absorbed volume. This is because a mixture composed of these small particles can significantly affect the properties of larger ones, especially if it is subjected to a depleting effect, which in turn leads to a very large number of phases, which, when the volume is increased, is of a more pronounced shape. In addition, in these particles there are more metastability, in some cases even multiple phases. In fact, the structure of the system is still not understood, as it is still difficult to determine the exact location of the critical point of the solution, due to the strong correlations of the particles. In the last years, there have been many studies in which small particles influence the structure of large particles, and they, too, are influenced by the forces of depletion. This has been a subject of great interest for many years, and it is still not easy to identify the exact location of the critical point, as there are strong and strong correlations. In this sense, the result is good, as compared with experiments on colloidal suspensions containing no-adsorbing polymer chains. In addition, there are several metas", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0322, "title": "Emergence of spatiotemporal chaos driven by far-field breakup of spiral waves in the plankton ecological systems", "abstract": "  Alexander B. Medvinsky \\emph{et al} [A. B. Medvinsky, I. A. Tikhonova, R. R. Aliev, B.-L. Li, Z.-S. Lin, and H. Malchow, Phys. Rev. E \\textbf{64}, 021915 (2001)] and Marcus R. Garvie \\emph{et al} [M. R. Garvie and C. Trenchea, SIAM J. Control. Optim. \\textbf{46}, 775-791 (2007)] shown that the minimal spatially extended reaction-diffusion model of phytoplankton-zooplankton can exhibit both regular, chaotic behavior, and spatiotemporal patterns in a patchy environment. Based on that, the spatial plankton model is furtherly investigated by means of computer simulations and theoretical analysis in the present paper when its parameters would be expected in the case of mixed Turing-Hopf bifurcation region. Our results show that the spiral waves exist in that region and the spatiotemporal chaos emerge, which arise from the far-field breakup of the spiral waves over large ranges of diffusion coefficients of phytoplankton and zooplankton. Moreover, the spatiotemporal chaos arising from the far-field breakup of spiral waves does not gradually involve the whole space within that region. Our results are confirmed by means of computation spectra and nonlinear bifurcation of wave trains. Finally, we give some explanations about the spatially structured patterns from the community level. ", "machine_abstract": "Spiral wave is an important pattern observed in many natural systems, such as chemical reactions and biological populations. In this study we investigate how spiral waves evolve into spatiotemporal chaotic patterns through their interactions with each other using a simple model for plankton population dynamics. We find that when two or more spiral waves collide they can either annihilate themselves or form new spirals depending on initial conditions. The newly formed spirals may also interact with existing ones to produce complex spatiotemporal structures including labyrinthine patterns. Our results suggest that spiral waves are not necessarily stable but could be unstable under certain circumstances. Spiral waves have been found in various physical, chemical and biological systems [1] . They play crucial roles in determining the dynamical behaviors of these systems [2] , e.g., in cardiac tissue [3] , BZ reaction [4] , semiconductor lasers [5] , and plankton ecosystems [6] . In recent years there has been growing interest in studying the formation and evolution of spiral waves [7, 8] . It was shown that spiral waves can undergo different types of instabilities [9] which lead to complicated spatiotemporal patterns [10] . For example, it was reported that spiral waves can become unstable due to collisions between them [11] . This instability leads to the birth of new spiral waves [12] . These newborn spirals then interact with one another resulting in the formation of complex spatiotempual structures [13] . However, most previous studies focused only on local interactions among spiral waves [14, 15] while ignoring possible effects caused by distant interactions [16] .", "paraphrased_abstract": "Spiral waves have been observed in a number of physical, chemical, biological systems, including cardiac tissue, the BZ reaction, the semiconductor laser, and the ecosystems of plankton. Several studies have been made in the last few years on the evolution and formation of spiral waves. It was found that spiral waves can become unstable due to collisions between them. Then the newborn spirals come together and form a complex system. This study is conducted to investigate the evolution and formation of spiral waves by the interaction of two or more spiral waves, using a simple model of population dynamics, which will help to describe the complex nature of spiral waves. In this study we find that spiral waves can be prone to instabilities in their morphology, resulting in complicated spatial patterns. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.123, "title": "Direct calculation of interfacial tensions from computer simulation: Results for freely jointed tangent hard sphere chains", "abstract": "  We develop a methodology for the calculation of surface free energies based on the probability distribution of a wandering interface. Using a simple extension of the NpT sampling, we allow the interface area to randomly probe the available space and evaluate the surface free energy from histogram analysis and the corresponding average. The method is suitable for studying systems with either continuous or discontinuous potentials, as it does not require explicit evaluation of the virial. The proposed algorithm is compared with known results for the surface tension of Lennard--Jones and Square Well fluid, as well as for the interface tension of a bead--spring polymer model and good agreement is found. We also calculate interfacial tensions of freely jointed tangent hard sphere chains on athermal walls for a wide range of chain lengths and densities. The results are compared with three different theoretical approaches, Scaled Particle Theory, the Yu and Wu density functional theory and an analytical approximation based on the latter approach. Whereas SPT only yields qualitative results, the last two approaches are found to yield very good agreement with simulations. ", "machine_abstract": "We present results on the direct measurement of interfacial tension between two coexisting phases in a model system consisting of freely-jointed tangent hard-sphere chains (FJTHSC). The FJTHSC are modeled by an off-lattice Monte Carlo algorithm that allows us to study systems with up to N = 1000 particles at temperatures ranging from T = 0.5 to 1.0 and densities ranging from \u03c1 = 0.6 to 0.8. We find that the interfacial tension is strongly dependent upon temperature, density, chain length, and bond angle distribution. In particular we show how the interfacial tension can be used as a probe into the local structure near interfaces.     Keywords: Interfacial tension, Computer simulations, Hard spheres, Chain molecules, Phase separation, Structure factor, Bond angle distribution, Local order parameter, Order-disorder transition, Freely-jointed", "paraphrased_abstract": "In our model we study a set of hard spheres, a set of hard-shelled chains, the separation of the phases, the structure factor, the angle distribution, the order-disorder transition, the free ring-binding. We present a system of tangent hard spheres whose chains are tangent-separated. We find that the tangent ring-binding is very sensitive to temperature, density, chain length, and the bond angle. We show how this interfacial tension can be used to probe the local structure of the interface. We show that the interfacial tension is strongly dependent on temperature, density, chain length and the bond angle. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0237, "title": "Spin-polarized transport in II-VI magnetic resonant tunneling devices", "abstract": "  We investigate electronic transport through II-VI semiconductor resonant tunneling structures containing diluted magnetic impurities. Due to the exchange interaction between the conduction electrons and the impurities, there arises a giant Zeeman splitting in the presence of a moderately low magnetic field. As a consequence, when the quantum well is magnetically doped the current-voltage characteristics shows two peaks corresponding to transport for each spin channel. This behavior is experimentally observed and can be reproduced with a simple tunneling model. The model thus allows to analyze other configurations. First, we further increase the magnetic field, which leads to a spin polarization of the electronic current injected from the leads, thus giving rise to a relative change in the current amplitude. We demonstrate that the spin polarization in the emitter can be determined from such a change. Furthermore, in the case of a magnetically doped injector our model shows a large increase in peak amplitude and a shift of the resonance to higher voltages as the external field increases. We find that this effect arises from a combination of giant Zeeman splitting, 3-D incident distribution and broad resonance linewidth. ", "machine_abstract": "We report on the spin polarization and magnetoresistance properties of GaAs/(Cd,Mn)Te/GaAs quantum well structures grown by molecular beam epitaxy (MBE). The samples were designed to have an asymmetric barrier structure with different Mn concentrations at each side of the barrier. We find that the degree of spin polarization is strongly dependent on both the applied bias voltage and temperature. At low temperatures we observe large negative magnetoresistances which are attributed to spin-dependent scattering processes within the barriers. These results demonstrate that these structures can be used as efficient spin injectors into semiconductors such as Si or Ge. In recent years there has been considerable interest in developing semiconductor-based spintronic devices for applications ranging from high-density data storage systems to novel electronic components [1] . One promising approach involves using ferromagnetic materials as source/drain contacts in field-effect transistors [2] , where the current through the device depends upon the relative alignment between the electron spins in the channel region and those in the contact regions [3] . In this work we present experimental studies of spin polarized transport in GaAs/Cd 0.9 Mn 0.1 Te/GaAs double-barrier heterostructures grown by molecular-beam epitaxy [4] . Our aim was to investigate how the degree of spin polarization varies under different conditions of applied bias voltage and temperature, and also to determine whether it is possible to use these structures as efficient spin injectors for other semiconductors such as silicon [5] .", "paraphrased_abstract": "In recent years, there has been an increasing interest in the development of semiconductor-based spintronic devices for a wide range of purposes, from high-density data storage to electronic devices. One promising approach is to use ferroelectrics as source and drain for a device whose current depends on the relationship between the electrons in the channel and the voids in the contact regions. In this work, we have investigated the effect of spin polarization on the polarity of electrons in GaAs and Cd, Mn and Te in MBE. At low temperatures, we observe large negative polarities, which can be attributed to the spin-dependent scattering of the barrier., resulting in a very efficient spin polarization. Moreover, it is found that the spin polarization is highly dependent on the voltage and temperature. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2999, "title": "Accurate QCD predictions for heavy-quark jets at the Tevatron and LHC", "abstract": "  Heavy-quark jets are important in many of today's collider studies and searches, yet predictions for them are subject to much larger uncertainties than for light jets. This is because of strong enhancements in higher orders from large logarithms, ln(p_t/m_Q). We propose a new definition of heavy-quark jets, which is free of final-state logarithms to all orders and such that all initial-state collinear logarithms can be resummed into the heavy-quark parton distributions. Heavy-jet spectra can then be calculated in the massless approximation, which is simpler than a massive calculation and reduces the theoretical uncertainties by a factor of three. This provides the first ever accurate predictions for inclusive b- and c-jets, and the latter have significant discriminatory power for the intrinsic charm content of the proton. The techniques introduced here could be used to obtain heavy-flavour jet results from existing massless next-to-leading order calculations for a wide range of processes. We also discuss the experimental applicability of our flavoured jet definition. ", "machine_abstract": "We present accurate theoretical calculations of jet production in hadronic collisions, including all relevant radiative corrections to next-to-leading order (NLO) accuracy. The results are obtained using an automated procedure based on numerical techniques that allow us to perform fully exclusive NLO computations with arbitrary cuts on final-state particles. We apply our method to compute the cross sections for inclusive jet production as well as dijet events containing one or two b quarks produced in pp collisions at the Fermilab Tevatron and CERN Large Hadron Collider. Our results show good agreement with available experimental data over a wide range of kinematic variables. In addition we provide detailed comparisons between different parton distribution functions (PDFs). These results can be used by both theorists and experimentalists to improve their understanding of perturbative Quantum Chromodynamics (QCD), which is the theory underlying strong interactions among quarks and gluons.     Jet production in hadronic colliders has been studied extensively since its discovery more than 40 years ago. It provides important information about the fundamental properties of matter such as quark masses and couplings, as well as tests of quantum chromodynamics (QCD)\u2014the theory describing the interaction of quarks and gluon fields. At leading-order (LO) in perturbation theory, the process involves only three colored partons\u2014a quark-antiquark pair and a gluon\u2014and it is described by tree-level Feynman diagrams. However, higher orders in perturbation theory introduce additional contributions involving virtual photons and/or gluons, as well as real emissions of extra partons into the final state. This leads to large logarithms of ratios of scales appearing in the calculation, which must be resummed to obtain reliable theoretical predictions. For example, the LO contribution to the total cross section for producing a single jet in proton-proton collisions at center-of-mass energy $s$ is given by     $$\\sigma_{pp}(1jet) = \\frac{1}{\\sqrt{s}} \\Big[\\frac{2}{3} + \\mathcal{O}(\\alpha_s)\\Big] \\sigma_TOT,", "paraphrased_abstract": "We use the resulting numerical method to calculate the cross-section of the jet produced by proton-proton collisions with the energy of s, which is shown by a matrix, and the corresponding parts of the numbers: a fractional fraction, the fractional fraction of the resonant fractions. We use the corresponding numerical method to compute the cross-sections of the jet produced by proton-proton collisions with a centre of mass of s, and the fractional fraction of the fractions of the final state is given by a fractional fraction of sigma -  pp1 jet,  a fractional fraction of sigma  -2 - - -mathcalO (alpha s) - - -  -  -    -  -  - - -  -  - - -  - - -  - -   - - - - - -  - - - - - -  -  -   - -  - - -", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1865, "title": "Young massive star clusters in the era of the Hubble Space Telescope", "abstract": "  The Hubble Space Telescope (HST) has been instrumental in the discovery of large numbers of extragalactic young massive star clusters (YMCs), often assumed to be proto-globular clusters (GCs). As a consequence, the field of YMC formation and evolution is thriving, generating major breakthroughs as well as controversies on annual (or shorter) time-scales. Here, I review the long-term survival chances of YMCs, hallmarks of intense starburst episodes often associated with violent galaxy interactions. In the absence of significant external perturbations, the key factor determining a cluster's long-term survival chances is the shape of its stellar initial mass function (IMF). It is, however, not straightforward to assess the IMF shape in unresolved extragalactic YMCs. I also discuss the latest progress in worldwide efforts to better understand the evolution of entire cluster populations, predominantly based on HST observations, and conclude that there is an increasing body of evidence that GC formation appears to be continuing until today; their long-term evolution crucially depends on their environmental conditions, however. ", "machine_abstract": "The formation and evolution of young massive star clusters (YMSCs) is an important topic for understanding galaxy formation, especially at high redshifts.  In this review we summarize recent progress on YMSC studies using data obtained with the Hubble Space Telescope (HST). We focus primarily on results based on HST observations of distant galaxies that are undergoing intense episodes of star-formation activity. The main goal of these investigations has been to study how individual stars form within dense stellar environments such as those found in YMSCs. This information can then be used to understand how entire galaxies evolve over time. A number of different techniques have been developed to investigate the properties of YMSCs including photometry, spectroscopy, and direct imaging. These methods provide complementary views into the physical processes involved in cluster formation and evolution. For example, photometric surveys allow us to determine the ages and masses of large samples of YMSCs while spectroscopic studies yield detailed measurements of chemical abundances and kinematics. Direct imaging provides spatially resolved information about the internal structure of YMSCs which allows us to probe their dynamical state.", "paraphrased_abstract": "It is known as a kind of star-forming trough, and the main goal of the study is to study how individual stars form in dense stellar atmospheres, such as those found in YMSCs. Then, we can study the evolution of whole galaxies. The photometric technique can determine the age and mass of large samples, spectroscopic methods can calculate the chemical abundance and kinematics of the stars, and direct imaging allows us to measure the yaw rate and density of the clusters. This research aims to study how a cluster of YMSCs is formed and evolution is being conducted. We present a review of recent progress in YMSC research, using the Hubble Space Telescope, in which we have collected data from distant galaxies in which star formation is intense. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2925, "title": "Failure mechanisms and surface roughness statistics of fractured Fontainebleau sandstone", "abstract": "  In an effort to investigate the link between failure mechanisms and the geometry of fractures of compacted grains materials, a detailed statistical analysis of the surfaces of fractured Fontainebleau sandstones has been achieved. The roughness of samples of different widths W is shown to be self affine with an exponent zeta=0.46 +- 0.05 over a range of length scales ranging from the grain size d up to an upper cut-off length \\xi = 0.15 W. This low zeta value is in agreement with measurements on other sandstones and on sintered materials. The probability distributions P(delta z,delta h) of the variations of height over different distances delta z > d can be collapsed onto a single Gaussian distribution with a suitable normalisation and do not display multifractal features. The roughness amplitude, as characterized by the height-height correlation over fixed distances delta z, does not depend on the sample width, implying that no anomalous scaling of the type reported for other materials is present. It is suggested, in agreement with recent theoretical work, to explain these results by the occurence of brittle fracture (instead of damage failure in materials displaying a higher value of zeta = 0.8). ", "machine_abstract": "We study the failure mechanism in uniaxial compression tests on samples of Fontainebleau sandstones with different initial porosities (from 0 to 20%). We show that, for all tested porosities, the stress-strain curves exhibit three distinct regimes corresponding to: elastic deformation; micro-cracking; and macro-fracturing. The onset of each regime is characterized by an abrupt change in the slope of the curve. In addition, we find that the macroscopic strength decreases as the porosity increases. Finally, we investigate how the fracture surfaces are affected by the porosity. Our results indicate that the average height fluctuations increase with increasing porosity. This suggests that the pore space plays an important role during the crack propagation process.     Keywords: Fracture mechanics, Surface roughness, Porosity, Sandstone, Compression test, Failure mechanism         1 Introduction     Rocks are heterogeneous materials made up of grains bonded together by cementing material such as clay or quartz [1] . Due to their complex microstructure, rocks can be considered as disordered media [2] , which makes them particularly interesting systems to study statistical properties at various scales [3] . For example, it has been shown recently [4] that the distribution of local stresses inside a rock sample depends strongly on its internal structure. Moreover, the mechanical behavior of rocks under external loading conditions also exhibits strong statistical features [5] .   In this work, we focus our attention on the effect of porosity on the mechanical response of rocks subjected to compressive loads [6] . Indeed, porosity is one of the most important parameters controlling the physical properties of rocks [7, 8] . It affects both the elastic moduli [9] and the brittleness [10] of rocks. However, despite numerous studies devoted to the influence of porosity on the physical properties of rocks [11\u201313] , very little information exists about the relationship between porosity and the mechanical behavior of rocks [14] .     2 Materials and methods  2.1 Samples preparation and characterization The experiments were performed using cylindrical specimens of Fontainebleau limestone [15] . These samples have diameters ranging", "paraphrased_abstract": "Besides, the rock is subject to great demands, and the physical properties of the rocks are particularly strong. There is no lack of study about the effects of porosity on the physical properties of rocks. Indeed, the elasticity of the rock is one of the most important parameters for the physical properties of rocks; besides, the properties of the elasticity are influenced by porosity. It is also known that the porosity is one of the most important factors for the mechanical properties of rocks; it is a factor that influences both the elastic moduli and the brittleness of the rock. We study the effect of porosity on the mechanical properties of the rocks subjected to compression. We find that for all porosity, the stress-strain curves exhibit three distinct regimes: elastic deformation, micro-cracking, and macro-fracturing. In each of these regimes, the strength is reduced and the macroscopic strength decreases. In addition, we show that the height increases with the increasing porosity of the rock. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3665, "title": "An array of low-background $^3$He proportional counters for the Sudbury Neutrino Observatory", "abstract": "  An array of Neutral-Current Detectors (NCDs) has been built in order to make a unique measurement of the total active flux of solar neutrinos in the Sudbury Neutrino Observatory (SNO). Data in the third phase of the SNO experiment were collected between November 2004 and November 2006, after the NCD array was added to improve the neutral-current sensitivity of the SNO detector. This array consisted of 36 strings of proportional counters filled with a mixture of $^3$He and CF$_4$ gas capable of detecting the neutrons liberated by the neutrino-deuteron neutral current reaction in the D$_2$O, and four strings filled with a mixture of $^4$He and CF$_4$ gas for background measurements. The proportional counter diameter is 5 cm. The total deployed array length was 398 m. The SNO NCD array is the lowest-radioactivity large array of proportional counters ever produced. This article describes the design, construction, deployment, and characterization of the NCD array, discusses the electronics and data acquisition system, and considers event signatures and backgrounds. ", "machine_abstract": "The Sudbury Neutrino Experiment (SNO) is designed to detect solar neutrinos via neutral current interactions in heavy water and charged-current interactions with deuterium nuclei, as well as by detecting anti-neutrinos produced by nuclear reactors on Earth.  The SNO experiment will use an array of 3 He proportional counters immersed in liquid scintillator to measure the energy deposited by recoil protons resulting from these reactions.  This report describes the design considerations that led to our choice of detector technology and discusses some of the technical challenges associated with this approach. We also present results obtained using a prototype counter constructed at TRIUMF which demonstrate its suitability for use in the SNO experiment. Finally we discuss plans for constructing a large number of such detectors at the SNOLAB underground laboratory near Sudbury Ontario Canada. The Sudbury Neutrino experiment (SNO) is being built to study neutrino properties through detection of solar neutrinos and antineutrinos from terrestrial nuclear power plants. In order to achieve high sensitivity it must be able to accurately determine both the direction and energy of incident particles. To accomplish this task the SNO collaboration has chosen to use a novel technique based upon the measurement of the total kinetic energy of recoiling proton(s). These measurements are made possible by placing a thin layer of pure heavy water inside each of several thousand small cylindrical proportional counters filled with gaseous helium-3. The light output of the scintillation fluid provides information about the position of interaction while the charge collected by the anode wire gives the total energy deposit. A detailed description of the physics motivation behind this approach can be found elsewhere [1] .", "paraphrased_abstract": "In this report, we present some preliminary results of the SNO experiment, a prototype of which has been developed at TriUMF, and which proves its suitability for use in the SNO experiment. The Sudbury Neutrino experiment is a laboratory dedicated to the investigation of the neutrino properties of the earth and its nuclear reactors. The Sudbury Neutrino experiment is to detect solar neutrinos, and antineutrinos produced by nuclear reactors on Earth. The SNO experiment consists of a system for the measurement of the total kinetic energy of protons, whose light is transmitted to the detector by the light of the scintillator, which gives information about the angle of the collision, while the charged current of the anode gives the total energy of the reaction. The Sudbury Neutrino experiment is intended to investigate the nucleus of solar neutrinos and the antineutrinos produced by nuclear reactors on earth. The SNO experiment will use a thin layer of pure water inside each of a few thousand helium-containing counters filled with gaseous helium, which is disposed in a helium-filled chamber. Then, in a vacuum chamber, a layer of pure water is inserted into each of a few thousand little cylindrical counters, filled with gaseous helium. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2069, "title": "Building Portable Thread Schedulers for Hierarchical Multiprocessors: the BubbleSched Framework", "abstract": "  Exploiting full computational power of current more and more hierarchical multiprocessor machines requires a very careful distribution of threads and data among the underlying non-uniform architecture. Unfortunately, most operating systems only provide a poor scheduling API that does not allow applications to transmit valuable scheduling hints to the system. In a previous paper, we showed that using a bubble-based thread scheduler can significantly improve applications' performance in a portable way. However, since multithreaded applications have various scheduling requirements, there is no universal scheduler that could meet all these needs. In this paper, we present a framework that allows scheduling experts to implement and experiment with customized thread schedulers. It provides a powerful API for dynamically distributing bubbles among the machine in a high-level, portable, and efficient way. Several examples show how experts can then develop, debug and tune their own portable bubble schedulers. ", "machine_abstract": "We present an open-source framework, called Bubblesched, that allows to build portable thread schedulers for hierarchical multiprocessors (HMPs). The main idea is to use bubbles as scheduling units and schedule them on different levels in HMP hierarchy using a set of rules. We have implemented two schedulers: one based on work stealing and another one based on load balancing. Both schedulers are able to run efficiently on top of Bubblesched without any modifications. Our experimental results show that both schedulers outperform state-of-the-art solutions by up to 3\u00c2 times when running parallel applications with fine-grained tasks. In addition, we demonstrate how our scheduler can be used to implement efficient task-parallel algorithms such as graph coloring or matrix multiplication. This research was supported by Russian Science Foundation grant 14-50-00040. We present an opensource framework, called Bubblesched: it allows to build portable threadschedulers for hierarchicalmultiprocessors(HMPs)andrunefficientlyonthemwithoutanymodifications.Theframeworkusesbubblesastheschedu-lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules.Wehaveimplementedtwo-schedulers:onebasedonstealingworkandanotheronesupportedbyloadbalancing.BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified.Ourexperimentalresultsshowthatbothschedulersoutperformstate-oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks.Inaddition,weillustratethattheframeworkcanbeusedtomakeefficienttask-parallelandalgorithmssuchastask-coloringanoregularmatrixmultiplication.", "paraphrased_abstract": "The resulting results show that these determinants outperform the state-of-the-art solutions by three times when used for coarse tasks. We also show how this determinant can be used to perform efficient tasks, such as graph coloring or matrix multiplication. In addition, we show that it can be used to develop efficient tasks-parallel, such as graph coloring or matrix multiplication. We present an open-source system called Bubblesched, which is used for the scheduling of threads for Hierarchical Multiprocessors (HMPs), and is executed with no modification. The system is composed of bubbles, which are configured into calendars and which can be placed at different levels of the hierarchy, according to a set of rules. Two tasks, one based on stealing, and the other based on balancing, are successful on the basis of Bubblesched. Our experiment shows that these two tasks outperform the best solutions of the world, up to three times, when running parallel applications with coarse tasks. In addition, we show that our system is capable of implementing efficient parallel algorithms, such as graph coloring and matrix multiplication. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.278, "title": "Evidence for Pre-SN Mass Loss in the Galactic SNR 3C 58", "abstract": "  We discuss the findings of a comprehensive imaging and spectroscopic survey of the optical emission associated with the supernova remnant 3C 58 (Fesen et al. 2007) as they relate to the topic of pre-SN mass loss. Spectroscopically measured radial velocities of ~450 emission knots within the remnant show two distinct kinematic populations of optical knots: a high-velocity group with radial velocities in the range of 700 - 1100 km/s and a lower velocity group exhibiting radial expansion velocities below ~250 km/s. We interpret the high-velocity knots as ejecta from the SN explosion and the low-velocity knots as shocked circumstellar material likely resulting from pre-SN mass loss. The chemical signatures of the two populations also show marked differences. The high velocity group includes a substantial number of knots with notably higher [N II]/H-alpha ratios not seen in the lower velocity population, suggesting greater nitrogen enrichment in the SN ejecta than in the CSM. These results are compared with evidence for pre-SN mass loss in the Crab Nebula, perhaps the SNR most similar to 3C 58. These SNRs may comprise two case studies of pre-SN mass loss in relatively low mass (~8 - 10 solar masses) core-collapse SN progenitors. ", "machine_abstract": "We present new near-infrared (NIR) observations of the supernova remnant (SNR) 3C58, which show that its central star is surrounded by an extended shell-like structure with a radius of ~0.5 pc and mass loss rate of ~10-5 Msun/yr. The NIR emission shows no evidence for dust formation or shock heating at this location; instead it appears to be dominated by line emission from ionized gas. We argue that these results are consistent with the presence of a pre-supernova wind driven by the progenitor star prior to its explosion as a Type II-P supernova about 3000 years ago. This interpretation implies that the progenitor was a red supergiant with initial mass between 8-15 solar masses. If so, then our results provide direct observational support for theoretical models of massive stellar evolution. In addition, we find that the observed properties of the circumstellar material surrounding the progenitor can account for most of the unusual characteristics of the radio spectrum of 3C58.", "paraphrased_abstract": "We also find that the polar regions around the globular core of the globular core can account for most of the unusual properties of the globular spectral band. This suggests that the globular core is a red supergiant, of about 8 or 9 solar masses. In this interpretation, we assume that the globular core was a red supergiant, with an initial mass of tens of thousands of solar masses. We have made observations of the supernova remnant, 3C58, which showed a shell around its central star, with a radius of 0.5 pc and a mass of 10-5 Msun/yr. We are convinced that this remnant was the globular core of a type II-P supernova, which occurred around 3000 years ago. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.16, "title": "Measurement of the production of charged pions by protons on a tantalum target", "abstract": "  A measurement of the double-differential cross-section for the production of charged pions in proton--tantalum collisions emitted at large angles from the incoming beam direction is presented. The data were taken in 2002 with the HARP detector in the T9 beam line of the CERN PS. The pions were produced by proton beams in a momentum range from 3 \\GeVc to 12 \\GeVc hitting a tantalum target with a thickness of 5% of a nuclear interaction length. The angular and momentum range covered by the experiment ($100 \\MeVc \\le p < 800 \\MeVc$ and $0.35 \\rad \\le \\theta <2.15 \\rad$) is of particular importance for the design of a neutrino factory. The produced particles were detected using a small-radius cylindrical time projection chamber (TPC) placed in a solenoidal magnet. Track recognition, momentum determination and particle identification were all performed based on the measurements made with the TPC. An elaborate system of detectors in the beam line ensured the identification of the incident particles. Results are shown for the double-differential cross-sections ${{\\mathrm{d}^2 \\sigma}} / {{\\mathrm{d}p\\mathrm{d}\\theta}}$ at four incident proton beam momenta (3 \\GeVc, 5 \\GeVc, 8 \\GeVc and 12 \\GeVc). In addition, the pion yields within the acceptance of typical neutrino factory designs are shown as a function of beam momentum. The measurement of these yields within a single experiment eliminates most systematic errors in the comparison between rates at different beam momenta and between positive and negative pion production. ", "machine_abstract": "The measurement was performed at the Cyclotrons and Accelerators Laboratory (CYCLONE) in JINR, Dubna using the proton beam with energy E = 1 GeV. The experiment was carried out to study the pion production in nuclear reactions induced by relativistic protons on nuclei Ta(p, \u03c0+). The experimental setup included two scintillation counters S1 and S2 for registration of particles emitted into forward hemisphere, three plastic scintillator detectors S3-S5 for measuring the angular distribution of secondary particles produced in the reaction under investigation. The results obtained are compared with calculations based on the model developed earlier [1] .  Introduction  Pion production is one of the most important processes in hadronic interactions which play an essential role in many fields such as astrophysics [2] , cosmic ray physics [3] , accelerator technology [4] etc.. In this work we present new data on the pion production in nuclear collisions induced by relativistic protons interacting with nuclei Ta(p,\u03c0 + ). These measurements were performed at CYCLONE laboratory in JINR-Dubna [5] . Experimental Setup The experimental setup used in our experiments consisted of:  -two scintillation counters S1 and S2; -three plastic scintillator detectors; -a set of collimators; -the target made of natural tantalum foil 0.1 mm thick placed between the first pair of scintillation counters; -the trigger system consisting of four scintillation counters T1-T4.  The layout of the experimental setup is shown schematically in Fig. 1 . The main parameters of the detector system are listed in Table I . The signals from all detectors were recorded by means of CAMAC modules [6] .", "paraphrased_abstract": "As it were, we obtained new data on the production of pions in nuclear collisions induced by relativistic protons on the nucleus Ta (p, +) [12][13] by relativistic protons on the nucleus Ta (p, +)[13]. We performed this experiment at the CYCLONE Laboratory in JINR, Dubna. The experiment consists of: two scintillators S1 and S2; three plastic scintillators S3 and S5 for detecting the distribution of secondary particles produced by the reaction, comparing the results with the model obtained earlier. The apparatus used for this experiment is shown in Figure 1; the main details of the instrument are listed in Table I. The detectors are composed of: a pair of scintillators S1 and S2; a pair of collimators S3 and S5; a target made of natural tantalum foil 0.1 mm thick, positioned between the first pair of scintillators; a trigger system composed of four scintillators T1 and T4. The measurements were conducted at the Cyclotron and Accelerator Laboratory (CYCLONE), in JINR, Dubna. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2507, "title": "STBCs from Representation of Extended Clifford Algebras", "abstract": "  A set of sufficient conditions to construct $\\lambda$-real symbol Maximum Likelihood (ML) decodable STBCs have recently been provided by Karmakar et al. STBCs satisfying these sufficient conditions were named as Clifford Unitary Weight (CUW) codes. In this paper, the maximal rate (as measured in complex symbols per channel use) of CUW codes for $\\lambda=2^a,a\\in\\mathbb{N}$ is obtained using tools from representation theory. Two algebraic constructions of codes achieving this maximal rate are also provided. One of the constructions is obtained using linear representation of finite groups whereas the other construction is based on the concept of right module algebra over non-commutative rings. To the knowledge of the authors, this is the first paper in which matrices over non-commutative rings is used to construct STBCs. An algebraic explanation is provided for the 'ABBA' construction first proposed by Tirkkonen et al and the tensor product construction proposed by Karmakar et al. Furthermore, it is established that the 4 transmit antenna STBC originally proposed by Tirkkonen et al based on the ABBA construction is actually a single complex symbol ML decodable code if the design variables are permuted and signal sets of appropriate dimensions are chosen. ", "machine_abstract": "In this work, we present the design and analysis of space-time block codes (STBC) based on extended clifford algebras. The proposed STBC is constructed by using an orthogonal basis for the underlying algebra. We show that our code has full diversity order with respect to quasi-static Rayleigh fading channels. Furthermore, it achieves maximum coding gain over all other known algebraic STBCs in terms of minimum determinant criterion. Finally, we provide simulation results which demonstrate the performance gains achieved by the proposed scheme compared to existing schemes. Index Terms -Space time block codes, Algebraic coding theory, Quasi-static Rayleigh faded channel, Minimum determinant criterion. I. INTRODUCTIO N Space Time Block Codes (STBC), introduced by Alamouti [1] , are widely used in wireless communications due to their simple structure and high data rate [2] . In recent years there have been many efforts devoted towards designing new classes of STBCs [3] - [8] . The main goal behind these designs was to achieve higher spectral efficiency while maintaining good error performances under different transmission conditions such as multipath propagation or frequency selective fading [9] . However, most of the existing works focus only on achieving better error performances without considering the effect of hardware limitations [10] . This motivates us to develop a class of STBCs which can be implemented efficiently at low complexity [11] . Recently, several authors [12] - [14] have shown that some well-known families of finite fields like Galois field GF(q) [15] , Finite Ring [16] , Quaternion [17] etc., can also be represented by certain types of non-commutative rings called Clifford algebras [18] . These representations allow one to construct various signal constellations [19] , modulation techniques [20] , and communication systems [21] . Motivated by these facts, in [22] , we presented a novel construction of STBCs based on representation of Clifford algebras. It was shown that the proposed STBC provides significant improvement in bit error rates (BER) when compared to conventional STBCs [23] .", "paraphrased_abstract": "\u201cThe present work describes the construction of an algebraic representation of the clifford algebra. The proposed representation of the clifford algebra is in accordance with the basic rules of the algebra. The algebraic representation of the clifford algebra is in agreement with the corresponding logical encoding. Its main purpose is to provide higher spectral efficiency and good error performances in different modes of transmission, such as multipath propagation or frequency selective fading. Recent studies have shown that certain families of the finite domains, such as Galois GF (q), Finite Ring GF(q), Quaternion, Quaternion, etc., can also be represented by certain non-commutative ring algebras, which are called Clifford algebras, and they are used in various signal-modulation techniques, modulation techniques and communication systems. We are now discussing an innovative technique for constructing the space-time block code based on extended clifford algebras. In the course of the work we have established the mathematical construction of the clifford algebra. In this method we prove that the proposed coding system is fully diversity-dependent, and the minimum criterion of coding is high. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1174, "title": "Deaconstructing Functions on Quadratic Surfaces into Multipoles", "abstract": "  Any homogeneous polynomial $P(x, y, z)$ of degree $d$, being restricted to a unit sphere $S^2$, admits essentially a unique representation of the form $\\lambda + \\sum_{k = 1}^d [\\prod_{j = 1}^k L_{kj}]$, where $L_{kj}$'s are linear forms in $x, y$ and $z$ and $\\lambda$ is a real number. The coefficients of these linear forms, viewed as 3D vectors, are called \\emph{multipole} vectors of $P$. In this paper we consider similar multipole representations of polynomial and analytic functions on other quadratic surfaces $Q(x, y, z) = c$, real and complex. Over the complex numbers, the above representation is not unique, although the ambiguity is essentially finite. We investigate the combinatorics that depicts this ambiguity. We link these results with some classical theorems of harmonic analysis, theorems that describe decompositions of functions into sums of spherical harmonics. We extend these classical theorems (which rely on our understanding of the Laplace operator $\\Delta_{S^2}$) to more general differential operators $\\Delta_Q$ that are constructed with the help of the quadratic form $Q(x, y, z)$. Then we introduce modular spaces of multipoles. We study their intricate geometry and topology using methods of algebraic geometry and singularity theory. The multipole spaces are ramified over vector or projective spaces, and the compliments to the ramification sets give rise to a rich family of $K(\\pi, 1)$-spaces, where $\\pi$ runs over a variety of modified braid groups. ", "machine_abstract": "We present an algorithm for decomposing functions defined over quadratic surfaces in three dimensions, such as the surface of a sphere or ellipsoid, into multipole expansions. The method is based on representing the function using spherical harmonics and then expanding each term in this representation into a sum of products of Legendre polynomials with coefficients that are determined by solving a linear system of equations. We demonstrate our approach through several examples including computing the electrostatic potential due to point charges located at various positions around a dielectric sphere immersed in water. Our results show that we can accurately compute the electrostatic potential even when there are many sources distributed throughout space. This work was supported by NSF grant DMS-0852653 (CAREER). Spherical harmonic decomposition has been used extensively in computational physics applications ranging from quantum chemistry [1] , molecular dynamics [2] , and plasma simulations [3] . In these applications, one often needs to represent a given function f(r) defined over some domain \u2126 in terms of its expansion coefficients C lm :", "paraphrased_abstract": "Several examples are presented. One example is to calculate the potential of a sphere immersed in water, which is filled with point charges. The solution is based on the spherical harmonic representation, which is multiplied by the number of the coefficients of the function, which are a function of a square root of a quadratic surface, such as a sphere or a ellipsoid, into an exponential expansion. This approach has been studied in a number of different scientific disciplines, ranging from chemistry, molecular dynamics, plasma simulations, and so on. The application of the method is to turn functions defined in quadratic surfaces in three dimensions, such as a sphere or an ellipsoid, into multipole expansions. This technique is based on the principle of spherical harmonics and expands every term in this representation in the form of a tessellation, with coefficients that are determined by solving a linear system of equations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.143, "title": "Arc-like distribution of high CO(J=3-2)/CO(J=1-0) ratio gas surrounding the central star cluster of the supergiant HII region NGC 604", "abstract": "  We report the discovery of a high CO(J=3-2)/CO(J=1-0) ratio gas with an arc-like distribution (``high-ratio gas arc'') surrounding the central star cluster of the supergiant HII region NGC 604 in the nearby spiral galaxy M 33, based on multi-J CO observations of a 5' $\\times$ 5' region of NGC 604 conducted using the ASTE 10-m and NRO 45-m telescopes. The discovered ``high-ratio gas arc'' extends to the south-east to north-west direction with a size of $\\sim$ 200 pc. The western part of the high-ratio gas arc closely coincides well with the shells of the HII regions traced by H$\\alpha$ and radio continuum peaks. The CO(J=3-2)/CO(J=1-0) ratio, R_{3-2/1-0}, ranges between 0.3 and 1.2 in the observed region, and the R_{3-2/1-0} values of the high-ratio gas arc are around or higher than unity, indicating very warm (T_kin > 60 K) and dense (n(H_2) > 10^{3-4} cm^{-3}) conditions of the high-ratio gas arc. We suggest that the dense gas formation and second-generation star formation occur in the surrounding gas compressed by the stellar wind and/or supernova of the first-generation stars of NGC 604, i.e., the central star cluster of NGC 604. ", "machine_abstract": "We present new observations of the molecular line emission in the vicinity of the massive young open cluster Westerlund 1 (Wd-1). The data were obtained with the Atacama Submillimeter Telescope Experiment (ASTE), which is an 8 m telescope located at Pampa la Bola, Chile. We detected strong 12CO and 13CO J = 3-2 lines toward Wd-1 using the ASTE 10 GHz band receiver system. In addition to these lines we also observed several other transitions including C18O 1-0, 2-1, and 3-2 as well as CS 7-6 and SO 2 25-26. These observations reveal that there are two distinct components of molecular gas associated with this object; one component has relatively low excitation temperature while another shows much higher temperatures. This latter component appears to be distributed along an arc-like structure around the central star cluster. Our results suggest that the hotter component may have been heated by shocks produced during the expansion of the stellar winds and/or supernovae explosions within the open cluster.", "paraphrased_abstract": "We have discovered new lines in the vicinity of the young, young open cluster Westerlund-1. The observations made by the Atacama Telescope, which is located at Pampa la Bola, Chile. The results of this study indicate that the central gas is composed of two distinct gaseous components: one is relatively cold, and the other is very hot. We suspect that the hotter gaseous component has been heated by the wind and explosions in the central star cluster. The observations indicate that the uppermost component is heated by the evanescence of the central star cluster. We report the new observations of the emission of the gaseous line at Wd-1. These observations show that the gaseous component is composed of two parts: one is low in excitation, and the other has a higher excitation temperature. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2842, "title": "The design of a very high-Q superconductor electromechanical clock", "abstract": "  We discuss theoretically the properties of an electromechanical oscillator whose operation is based upon the cyclic, quasi-conservative conversion between gravitational potential, kinetic, and magnetic energies. The system consists of a strong-pinning type-II superconductor square loop subjected to a constant external force and to magnetic fields. The loop oscillates in the upright position at a frequency that can be tuned in the range 10-1000 Hz, and has induced in it a rectified electrical current. The emphasis of this paper is on the evaluation of the major remaining source of losses in the oscillations. We argue that such losses should be associated with the viscous vibration of pinned flux lines in the superconductor Nb-Ti wire, provided the oscillator is kept close to zero Kelvin, under high-vacuum, and the magnetic field is sufficiently uniform. We discuss how other different sources of loss would become negligible for such operational conditions, so that a very high quality factor Q exceeding 10^(10) might in principle be reached by the oscillator. The prospective utilization of such oscillator as a low-frequency high-Q clock is analyzed.Since publication the ideas in this paper have been explored both by the author and elsewhere, in applications covering Metrology, quantum systems, and gravimetry. ", "machine_abstract": "We present the design and performance of an extremely low-noise, high-accuracy cryogenic mechanical oscillator based on a suspended silicon nitride membrane with integrated Josephson junctions. The device is designed to operate at temperatures below 1 K in a dilution refrigerator environment. We demonstrate that this system can be used as both a highly stable reference frequency source for microwave electronics or as a sensitive probe of quantum mechanics by measuring the vacuum fluctuations of its own motion.     A key requirement for many applications of quantum information science is the ability to generate and detect single photons. In order to achieve these goals it will be necessary to develop new technologies capable of generating and detecting individual quanta of light. One promising approach involves coupling semiconductor nanocrystals (quantum dots) to optical cavities such as Fabry-Perot resonators1-5. These devices are expected to have important applications ranging from quantum optics6-8 to solid-state quantum computing9-11. However, one major challenge facing their development has been achieving sufficiently large Purcell factors12-14 so that spontaneous emission rates into the cavity mode become comparable to those observed in atomic systems15-17. This problem may be overcome using photonic crystal cavities18-20 which allow for strong confinement of electromagnetic fields within small volumes21-23.", "paraphrased_abstract": "The most important requirement for the development of quantum information is the ability to obtain single atoms of light. One of the problems in the development of these devices is that they cannot reach the level of Purcell values which are comparable to those found in atomic systems. This is a problem that may be overcome by using the polarized silicon nitride, which has a Josephson junction. We show how such a device can be used as a stable reference for microwave electronics and as a sensitive probe for quantum mechanics by measuring the fluctuations of the motion of its own motion., it is used in the design and operation of a cryogenic mechanical oscillator, based on a suspended silicon nitride membrane, which has junctions of Josephson. We show how it can be used for the high-frequency sensitivity of microwave electronics or as a sensitive probe for quantum mechanics by measuring the vacuum fluctuations of its own motion. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0839, "title": "Dahlberg's bilinear estimate for solutions of divergence form complex elliptic equations", "abstract": "  We consider divergence form elliptic operators $L=-\\dv A(x)\\nabla$, defined in $\\mathbb{R}^{n+1}=\\{(x,t)\\in\\mathbb{R}^{n}\\times\\mathbb{R}\\}, n \\geq 2$, where the $L^{\\infty}$ coefficient matrix $A$ is $(n+1)\\times(n+1)$, uniformly elliptic, complex and $t$-independent. Using recently obtained results concerning the boundedness and invertibility of layer potentials associated to such operators, we show that if $Lu=0$ in $\\mathbb{R}^{n+1}_+$, then for any vector-valued ${\\bf v} \\in W^{1,2}_{loc},$ we have the bilinear estimate $$|\\iint_{\\mathbb{R}^{n+1}_+} \\nabla u \\cdot \\bar{{\\bf v}} dx dt |\\leq C\\sup_{t>0} \\|u(\\cdot,t)\\|_{L^2(\\mathbb{R}^n)}(\\||t \\nabla {\\bf v}\\|| + \\|N_*{\\bf v}\\|_{L^2(\\mathbb{R}^n)}),$$ where $\\||F\\|| \\equiv (\\iint_{\\mathbb{R}^{n+1}_+} |F(x,t)|^2 t^{-1} dx dt)^{1/2},$ and where $N_*$ is the usual non-tangential maximal operator. The result is new even in the case of real symmetric coefficients, and generalizes the analogous result of Dahlberg for harmonic functions on Lipschitz graph domains. ", "machine_abstract": "In this article, we prove the Dahlberg's bilinear estimate in the case where the coefficients are allowed to be complex-valued functions and the domain is not necessarily smooth. We also give an application of our result by proving that if $(u_n)$ is a sequence of solutions of the following problem    $$-\\Delta u_n = f_n \\tag{1}$$   on $\\Omega$ with $f_n$ bounded in $L^2(\\Omega)$ then there exists $u\\in H^1_0(\\Omega)$ such that    $$\\|u-u_n\\|_{H^1(\\Omega)} \\to 0 \\quad \\text{as } n\\to \\infty.$$  Let us consider the following problem:    \\begin{cases}     -\\Delta u = f & \\Omega \\\\     u=0 & \\partial \\Omega.   \\end{cases} We assume that $f$ belongs to $H^{-1}(\\Omega)$ and satisfies the compatibility condition    $$\\int_{\\partial \\Omega}\\bar{f}v = 0 \\quad \\forall v \\in H^1_0(\\Omega).$$    Let $(u_n)_{n\\geq 1}$ be a sequence of weak solutions of (1), i.e.,    $$\\int_\\Omega \\nabla u_n \\cdot \\nabla \\phi + \\int_\\Omega u_n\\phi = \\int_\\Omega f\\phi \\quad \\forall \\phi \\in H^1_0(\u03a9).$$    Then it follows from the Lax-Milgram theorem that $(u_n)_n$ converges weakly to some function $u\\in H^1_0\\cap H^2(\\Omega)$.    In order to show that $(u_n-u)_n$ converges strongly to zero in $H^1_0$, one needs to establish the so-called ``Dahlberg's bilinear estimates'' which were first proved by Dahlberg in the case when the coefficients are real-valued functions and the boundary is sufficiently regular.", "paraphrased_abstract": "In order to prove that (u-n)(n)(n)(n)n(n)[2] H-0 (-omega) (): we first have to prove the so-called Dahlberg\u2019s bilinear estimates, which were first proved by Dahlberg when the coefficients were real values and the boundary was sufficiently regular. We also demonstrate the generality of this result by the following example: Let us assume that 1[2]] (][3] = [3], [6], [7],,  u-u-n = [8] [10][12] in H1[13] - with [10]1[14] = [15] [16], and we assume that [16] in H1[14], [16] [16], [17], [17], [17][17],[17] [17],[17], [17] ,[17],,, [17]|\" [17] [17],[18", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0791, "title": "Spectral analysis of Swift long GRBs with known redshift", "abstract": "  We study the spectral and energetics properties of 47 long-duration gamma-ray bursts (GRBs) with known redshift, all of them detected by the Swift satellite. Due to the narrow energy range (15-150 keV) of the Swift-BAT detector, the spectral fitting is reliable only for fitting models with 2 or 3 parameters. As high uncertainty and correlation among the errors is expected, a careful analysis of the errors is necessary. We fit both the power law (PL, 2 parameters) and cut--off power law (CPL, 3 parameters) models to the time-integrated spectra of the 47 bursts, and present the corresponding parameters, their uncertainties, and the correlations among the uncertainties. The CPL model is reliable only for 29 bursts for which we estimate the nuf_nu peak energy Epk. For these GRBs, we calculate the energy fluence and the rest- frame isotropic-equivalent radiated energy, Eiso, as well as the propagated uncertainties and correlations among them. We explore the distribution of our homogeneous sample of GRBs on the rest-frame diagram E'pk vs Eiso. We confirm a significant correlation between these two quantities (the \"Amati\" relation) and we verify that, within the uncertainty limits, no outliers are present. We also fit the spectra to a Band model with the high energy power law index frozen to -2.3, obtaining a rather good agreement with the \"Amati\" relation of non-Swift GRBs. ", "machine_abstract": "We present the results of spectral analysis for all Swift bursts with measured redshifts and durations longer than 2 s, using data obtained by the Burst Alert Telescope (BAT) on board Swift satellite. We find that most of these bursts are best described as blackbody emission in combination with an additional power-law component at higher energies. The temperature of this blackbody component is found to be correlated with the peak energy of the spectrum E p . This correlation can be explained if we assume that the observed blackbody emission comes from photospheric radius expansion during the prompt phase of the burst. In addition, there seems to exist another correlation between the blackbody temperature T bb , the luminosity L iso and the duration t 90 . The existence of such correlations suggests that the physical mechanism responsible for producing the blackbody emission may also play some role in determining other properties of the bursts.    Introduction  Gamma-ray bursts (GRB), discovered more than 40 years ago [1] , have been studied extensively since their discovery [2] . However, many questions about them remain unanswered [3] . One important question concerns the origin of the gamma-rays produced in GRBs [4] . It has been suggested that they could come from internal shocks [5] or magnetic reconnection [6] within relativistic jets launched by collapsing massive stars [7, 8] . Alternatively, it was proposed that they might result from external shocks driven into surrounding medium [9] . Another open issue is whether GRBs are standard candles [10] . If so, then one would expect that different bursts should show similar temporal and spectral behaviors [11] . On the contrary, observations suggest that GRBs exhibit large diversity [12] . Finally, the nature of the progenitors of GRBs remains unknown [13] .", "paraphrased_abstract": "As you can see, we observe a great diversity of colors and shapes, but in general there are no single patterns, as one would expect. GRBs are also known as gamma-rays, the source of which is still unclear. It has been suggested that they might come from internal shocks or magnetic reconnections in relativistic jets of the collapsing stars, or from external shocks in the surrounding medium. One important question is, how come GRBs are produced? There are many possibilities for the origin of GRBs; some believe they are created by internal shocks or by magnetic reconnection in the relativistic jets that collapse large stars; some argue that they could be produced by external shocks that came from within the sphere of the surrounding medium. This is to say, some of these GRBs are caused by the emission of the blackbody in combination with a higher-energy component. This criterion of the blackbody temperature, the luminosity, the duration of t 90 is also found to be correlated with the temperature of the blackbody. The temperature of this blackbody component is correlated with the peak energy of the spectrum E p, so that it can be explained by the observation that the blackbody is caused by the growth of the ethereal radius during the immediate phase of the explosion. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2392, "title": "New interpretation for energy gap $\\Delta$ of the cut-off approximation in the BCS theory of superconductivity", "abstract": "  This paper concerns the solution of the self-consistency equation for energy gap parameter $\\Delta_{\\bf k}$ in the BCS theory of superconductivity. We show that there exists a well-defined relation between the solution for energy gap parameter amplitude $|\\Delta_{\\bf k}|$ for a general interaction $V_{{\\bf k},{\\bf k}'}$ and energy gap $\\Delta$ obtained by using the cut-off approximation. The relation between $|\\Delta_{\\bf k}|$ and $\\Delta$ indicates that $\\Delta$ is a weighted average over $|\\Delta_{\\bf k}|$ of electronic states within cut-off energy $\\xi_c$ around the Fermi surface. In this interpretation for $\\Delta$, $\\xi_c$ is not a property of $V_{{\\bf k},{\\bf k}'}$, but a parameter specifying the energy range within which the weighted average over $|\\Delta_{\\bf k}|$ is taken. We show that the proper choice for the value of $\\xi_c$ is only a few $k_BT_c$ (i.e., $\\xi_c/k_BT_c$ is about 3 or 4). We also show that the cut-off approximation, even with $\\xi_c/k_BT_c=\\infty$, is a good approximation when it is used to calculate quantities such as the condensation energy and the specific heat, but it leads to significant overestimation for the Josephson critical current density of a Josephson junction if $\\xi_c/k_BT_c \\gg 1$ is assumed. ", "machine_abstract": "We present an alternative explanation to the origin of the energy gap $\\Delta$ in the cut-off approximation used by Bardeen, Cooper and Schrieffer (BCS) in their famous 1957 work on superconductivity.  We show that this gap is not due to the presence of phonons but rather it arises as a consequence of the fact that electrons are fermions with spin 1/2. In particular we find that the value of the gap depends only on the number of particles involved in the process and does not depend explicitly on temperature or any other physical parameter. This result agrees well with experimental data obtained recently at high temperatures.  The new interpretation presented here also sheds light into some puzzling results found in recent experiments performed on cuprate compounds which have been widely studied because they exhibit very high critical temperatures. The most important feature of these materials is the existence of two different gaps in the excitation spectrum corresponding to electron-like quasiparticles and hole-like excitations respectively.", "paraphrased_abstract": "This new interpretation of this new work adds to some of the curious results that have recently been published on cuprates, which have been studied with great interest, because they have a very high critical temperature. In addition, this new study of cuprates reveals a most curious feature of these materials, the existence of two distinct, quasiparticles, which resemble the electrons, and the holes. We show that this gap does not arise from phonons, but simply from fermions with spin1/2. The second difference in energy is not caused by phonons, but by fermions, which are fermions with spin 1/2. Moreover, the difference in energy between the two particles depends on the number of particles, and on the temperature and the physical state of the particles. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3385, "title": "Constrained simulations of the local universe: I. Mass and motion in the Local Volume", "abstract": "  It has been recently claimed that there is no correlation between the distribution of galaxies and their peculiar velocities within the Local Volume (LV), namely a sphere of R=7/h Mpc around the Local Group (LG). It has been then stated that this implies that either locally dark matter is not distributed in the same way as luminous matter, or peculiar velocities are not due to fluctuations in mass. To test that statement a set of constrained N-body cosmological simulations, designed to reproduce the main observed large scale structure, have been analyzed. The simulations were performed within the flat-Lambda, open and flat matter only CDM cosmogonies. Two unconstrained simulations of the flat-Lambda and open CDM models were performed for comparison. LG-like objects have been selected so as to mimic the real LG environment. The local gravitational field due to all halos found within each LV is compared with the exact gravitational field induced by all matter in the simulation. We conclude that there is no correlation between the exact and the local gravitational field obtained by pairwise newtonian forces between halos. Moreover, the local gravitational field is uncorrelated with the peculiar velocities of halos. The exact gravitational field has a linear correlation with peculiar velocities but the proportionality constant relating the velocity with gravitational field falls below the prediction of the linear theory. Upon considering all matter inside the LVs, the exact and local gravitational accelerations show a much better correlation, but with a considerable scatter independent on the cosmological models. The main conclusion is that the lack of correlation between the local gravitation and the peculiar velocity fields around LG-like objects is naturally expected in the CDM cosmologies. ", "machine_abstract": "We present constrained realizations of the mass distribution in the nearby universe, based on the Millennium Run N-body simulation (Springel et al., 2005) combined with galaxy redshift surveys. We use two different methods to construct mock catalogues for comparison with observations: The first method is based on the conditional luminosity function formalism developed by Yang et al. (2003) , which we apply to galaxies selected from the Sloan Digital Sky Survey Data Release 7 (Abazajian et al., 2009 ). In this approach, each galaxy has an assigned halo mass drawn from its luminosity-dependent probability density function. The second method uses the Halo Occupation Distribution model introduced by Zheng et al. (2005) . Here, the number of central and satellite galaxies within haloes are determined using their respective HODs as functions of host halo mass. For both approaches, we compare our results against observational data sets including the 2MASS Redshift Survey (Huchra et al., 2012; Bilicki & Chodorowski, 2013), 6dF Galaxy Survey (Jones et al., 2004 ) and the WiggleZ Dark Energy Survey (Drinkwater et al., 2010).", "paraphrased_abstract": "The first method, using the Yang et al. (2002) method, is based on the conditional luminosity function, which Yang et al. (2003) applied to the galaxies from the Sloan Digital Sky Survey (Abazajian et al., 2009) and on the luminosity of the host galaxy. The second method, using the HOD-independent density function, applies to the HOD-independent density function, and uses the method of halo occupation in the first method to evaluate the distribution of the nearby universe, by combining the Millennium Run simulation with the redshift surveys. The halo occupation model is used in the second method to compute the mass distribution of the host galaxy, based on the luminosity-dependent density function of the host. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3792, "title": "Gaussian-Basis Monte Carlo Method for Numerical Study on Ground States of Itinerant and Strongly Correlated Electron Systems", "abstract": "  We examine Gaussian-basis Monte Carlo method (GBMC) introduced by Corney and Drummond. This method is based on an expansion of the density-matrix operator rho by means of the coherent Gaussian-type operator basis Lambda and does not suffer from the minus sign problem. The original method, however, often fails in reproducing the true ground state and causes systematic errors of calculated physical quantities because the samples are often trapped in some metastable or symmetry broken states. To overcome this difficulty, we combine the quantum-number projection scheme proposed by Assaad, Werner, Corboz, Gull and Troyer in conjunction with the importance sampling of the original GBMC method. This improvement allows us to carry out the importance sampling in the quantum-number-projected phase-space. Some comparisons with the previous quantum-number projection scheme indicate that, in our method, the convergence with the ground state is accelerated, which makes it possible to extend the applicability and widen the range of tractable parameters in the GBMC method. The present scheme offers an efficient practical way of computation for strongly correlated electron systems beyond the range of system sizes, interaction strengths and lattice structures tractable by other computational methods such as the quantum Monte Carlo method. ", "machine_abstract": "We propose the Gaussian-basis Monte Carlo method to study ground states of itinerant electron systems with strong correlations, which is based on the variational principle in terms of the trial wave function expressed by the product of Gutzwiller-Jastrow factors. The proposed method can be applied to any kind of electronic structure calculation methods such as density functional theory (DFT) or quantum Monte Carlo (QMC). We demonstrate that our method works well even when we use DFT results as input data instead of QMC ones. In addition, it enables us to calculate physical quantities directly related to the correlation effects without using complicated numerical procedures. As an example application, we show how this method can be used to investigate magnetic properties of strongly correlated materials. Our method will provide useful information about the nature of magnetism in these materials. This work was supported by JSPS KAKENHI Grant Number JP16K05481", "paraphrased_abstract": "Hence, we show that our method is quite useful in analyzing the magnetic properties of strongly correlated materials. It can be applied to all kinds of electronic structures, from density functional theory to quantum Monte Carlo. In our study, we are showing that this method is suitable for all types of electronic structures, from density functional theory to quantum Monte Carlo. The method we use is Gaussian, and can be used for any electronic structure, including density functional theory or quantum Monte Carlo. We show that the method is suitable for both DFT and QMC. We show that this method is well suited to the case even when we use DFT and QMC instead of QMC. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2056, "title": "The Zeta-Function of a p-Adic Manifold, Dwork Theory for Physicists", "abstract": "  In this article we review the observation, due originally to Dwork, that the zeta-function of an arithmetic variety, defined originally over the field with p elements, is a superdeterminant. We review this observation in the context of a one parameter family of quintic threefolds, and study the zeta-function as a function of the parameter \\phi. Owing to cancellations, the superdeterminant of an infinite matrix reduces to the (ordinary) determinant of a finite matrix, U(\\phi), corresponding to the action of the Frobenius map on certain cohomology groups. The parameter-dependence of U(\\phi) is given by a relation U(\\phi)=E^{-1}(\\phi^p)U(0)E(\\phi) with E(\\phi) a Wronskian matrix formed from the periods of the manifold. The periods are defined by series that converge for $|\\phi|_p < 1$. The values of \\phi that are of interest are those for which \\phi^p = \\phi so, for nonzero \\phi, we have |\\vph|_p=1. We explain how the process of p-adic analytic continuation applies to this case. The matrix U(\\phi) breaks up into submatrices of rank 4 and rank 2 and we are able from this perspective to explain some of the observations that have been made previously by numerical calculation. ", "machine_abstract": "The zeta-function is the main object in number theory and algebraic geometry.  In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non-trivial topology.   The basic idea is that one considers the Feynman path integral over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm.    This leads naturally to the concept of a \"quantum torus\" whose zeta function encodes information about the spectrum of the corresponding QFT.   We will also discuss some recent results concerning the relation between the zeta functions of certain families of quantum tori and their associated modular forms. Quantum Field Theory (QFT) has been developed into a powerful tool for studying physical phenomena at the most fundamental level. However, many interesting questions remain open due to the difficulty of performing calculations beyond perturbation theory. One way out of these difficulties is to use ideas from mathematics such as those coming from number theory or algebraic geometry. In particular, we are interested in using the so-called \"zeta-functions\" of algebraic varieties to obtain new insights into QFTs. These objects encode important information about the underlying physics but they are notoriously difficult to compute explicitly. In my talk I'll give examples where explicit computations have been performed successfully and show how these techniques could lead to further progress in our understanding of QFTs.", "paraphrased_abstract": "The idea is that the Feynman-path integral over all the fields of spacetime is an infinitely long functional integration, which is then regularized by a substitution of the space of fields by a vector space of a suitable size, and thus gives the form of a \"quantum torus\" whose zeta function contains information about the spectrum of the corresponding QFT. I will show several examples of explicit computations and will discuss how these computations can lead to new insights into the QFT. The concept of the zeta function is an important subject in mathematics, but it is very difficult to calculate explicitly. Moreover, there are many interesting questions that remain unsolved, and one method of dealing with them is mathematics, particularly number theory and algebraic geometry. It is in this way that the concept of the zeta function is introduced, and he is able to use it to study the QFT on a curved space-time. The main idea is that Feynman's integral over all the fields of space-time is an infinite dimensional functional integration, and the equations can be regularized by replacing the space of the fields with a vector space, which is a fixed coordinate. This is the basic idea of a 'quantum torus' whose zeta function encodes information about the spectrum of the QFT.", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0516, "title": "The Kinematics of the Ultra-Faint Milky Way Satellites: Solving the Missing Satellite Problem", "abstract": "  We present Keck/DEIMOS spectroscopy of stars in 8 of the newly discovered ultra-faint dwarf galaxies around the Milky Way. We measure the velocity dispersions of Canes Venatici I and II, Ursa Major I and II, Coma Berenices, Hercules, Leo IV and Leo T from the velocities of 18 - 214 stars in each galaxy and find dispersions ranging from 3.3 to 7.6 km/s. The 6 galaxies with absolute magnitudes M_V < -4 are highly dark matter-dominated, with mass-to-light ratios approaching 1000. The measured velocity dispersions are inversely correlated with their luminosities, indicating that a minimum mass for luminous galactic systems may not yet have been reached. We also measure the metallicities of the observed stars and find that the 6 brightest of the ultra-faint dwarfs extend the luminosity-metallicity relationship followed by brighter dwarfs by 2 orders of magnitude in luminosity; several of these objects have mean metallicities as low as [Fe/H] = -2.3 and therefore represent some of the most metal-poor known stellar systems. We detect metallicity spreads of up to 0.5 dex in several objects, suggesting multiple star formation epochs. Having established the masses of the ultra-faint dwarfs, we re-examine the missing satellite problem. After correcting for the sky coverage of the SDSS, we find that the ultra-faint dwarfs substantially alleviate the discrepancy between the predicted and observed numbers of satellites around the Milky Way, but there are still a factor of ~4 too few dwarf galaxies over a significant range of masses. We show that if galaxy formation in low-mass dark matter halos is strongly suppressed after reionization, the simulated circular velocity function of CDM subhalos can be brought into approximate agreement with the observed circular velocity function of Milky Way satellite galaxies. [slightly abridged] ", "machine_abstract": "We present an analysis of the kinematic properties of the ultra-faint satellites (UFS) of the Milky Way, using data from the Sloan Digital Sky Survey and the Dark Energy Survey. We find that all UFS are consistent with being on circular orbits around their host galaxy, which is in agreement with previous studies. However, we also show that this result can be explained by assuming that these galaxies have been tidally disrupted over time. In particular, we demonstrate how tidal disruption could explain both the observed number density profile as well as the velocity dispersion profiles for each satellite system. Finally, we discuss our results within the context of other recent work studying the missing satellite problem. The discovery of more than 100 new dwarf galaxies orbiting the Milky Way has led to renewed interest in understanding the formation history of the Local Group. While many of these newly discovered systems appear to follow similar scaling relations to those found among brighter dwarfs such as the classical dwarf spheroidal galaxies, there remain several puzzling differences between them. For example, while most bright dwarfs exhibit significant rotation velocities, only one of the recently discovered faintest satellites shows any evidence of rotation . Furthermore, while the majority of bright dwarfs lie close to the virial radius of the Milky Way , nearly half of the fainter satellites reside at distances greater than 300 kpc . In addition to these observational challenges, theoretical models predict that dark matter halos should contain far fewer subhalos than are currently known to exist . This discrepancy -known as the \"missing satellite problem\"-has motivated numerous investigations into possible solutions ranging from modifications to standard cold dark matter theory to alternative theories of gravity .", "paraphrased_abstract": "I will talk about the kinematics of the ultra-faint satellites of the Milky Way. We are able to show that all UFS are angularly aligned around their host star, which is in keeping with the previous results. However, we also show that the angularity of these satellites, and also the velocity dispersion of the satellites, is not always the same as it is in the past. In addition to these difficulties, theoretical models predict that the halos should contain much fewer subhalos than the current theory suggests. This discrepancy is called the \"missing satellite\" problem, and has been investigated with various solutions, from modifying the cold dark matter theory to an alternative theory of gravity. Moreover, the kinematic properties of the ultra-faint satellites (UFS) in the Milky Way are also in accordance with the previous study. All of these UFS are found to be in a circular orbit around their host galaxy, which is in agreement with previous observations. Moreover, although the most bright UFS are located in the victorian radius of the Milky Way, only one of the faint UFS is observed to have rotation, whereas only one of the new UFS is observable. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1457, "title": "Chaos and Symmetry in String Cosmology", "abstract": "  We review the recently discovered interplay between chaos and symmetry in the general inhomogeneous solution of many string-related Einstein-matter systems in the vicinity of a cosmological singularity. The Belinsky-Khalatnikov-Lifshitz-type chaotic behaviour is found, for many Einstein-matter models (notably those related to the low-energy limit of superstring theory and M-theory), to be connected with certain (infinite-dimensional) hyperbolic Kac-Moody algebras. In particular, the billiard chambers describing the asymptotic cosmological behaviour of pure Einstein gravity in spacetime dimension d+1, or the metric-three-form system of 11-dimensional supergravity, are found to be identical to the Weyl chambers of the Lorentzian Kac-Moody algebras AE_d, or E_{10}, respectively. This suggests that these Kac-Moody algebras are hidden symmetries of the corresponding models. There even exists some evidence of a hidden equivalence between the general solution of the Einstein-three-form system and a null geodesic in the infinite dimensional coset space E_{10} / K(E_{10}), where K(E_{10}) is the maximal compact subgroup of E_{10}. ", "machine_abstract": "We study the dynamics of string cosmologies with nontrivial dilaton potentials, focusing on their chaotic behavior. We show that for certain classes of potentials there are regions where trajectories can be trapped by unstable fixed points or periodic orbits. In these cases we find that the system is not ergodic but has an infinite number of attractors which correspond to different values of the Hubble parameter H(t). The existence of such attractor solutions may have important consequences for the evolution of our universe. For example, it could explain why the present value of H(t) differs so much from its initial value at t = 0. It also provides a possible explanation for the observed flatness problem since the volume V (t) grows exponentially fast during inflation while the energy density decreases as 1/V (t). The results presented here were obtained using numerical methods based on the fourth-order Runge-Kutta algorithm combined with Newton's method for finding roots.", "paraphrased_abstract": "In this paper we present numerical results based on the fourth-order Runge-Kutta-Kutta equation, combined with Newton\u2019s method of finding roots. We also present a possible explanation for the observed flatness in the presence of inflation, as the volume V increases exponentially as the value of V decreases, as the density of atoms decreases. Our numerical results are shown in the form of four-order Runge-Kutta and Newton\u2019s method of finding roots. The dynamics of string cosmologies with non-trivial dilaton potentials is investigated, presenting an example of its chaotic behavior. For some of the potentials there are also points where trajectories can be trampled by unstable points or by periodic orbits. These points are called 'tall trajectories', and these points have a constant number of attractors that correspond to the various values of the Hubble parameter H(t). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0506, "title": "Space-time percolation", "abstract": "  The contact model for the spread of disease may be viewed as a directed percolation model on $\\ZZ \\times \\RR$ in which the continuum axis is oriented in the direction of increasing time. Techniques from percolation have enabled a fairly complete analysis of the contact model at and near its critical point. The corresponding process when the time-axis is unoriented is an undirected percolation model to which now standard techniques may be applied. One may construct in similar vein a random-cluster model on $\\ZZ \\times \\RR$, with associated continuum Ising and Potts models. These models are of independent interest, in addition to providing a path-integral representation of the quantum Ising model with transverse field. This representation may be used to obtain a bound on the entanglement of a finite set of spins in the quantum Ising model on $\\ZZ$, where this entanglement is measured via the entropy of the reduced density matrix. The mean-field version of the quantum Ising model gives rise to a random-cluster model on $K_n \\times \\RR$, thereby extending the Erdos-Renyi random graph on the complete graph $K_n$. ", "machine_abstract": "We study the space-time evolution of an ensemble of particles in one dimension, which are initially distributed randomly and uniformly on a line segment [0, L] with periodic boundary conditions.  We show that for any fixed time t > 0 there exists a critical density $\\rho_c(t)$ such that if $ \\rho_0 \\geq \\rho_c(t)$ then all particles will eventually collide at some point $t_0$ between $0 < t_0 \\leq t$; otherwise they never collide.  The main result is to prove that this critical density satisfies the following scaling laws:  $$\\rho_c(t) \\sim \\frac{1}{\\sqrt{4\\pi t}} \\quad \\text{as } t \\to \\infty $$ and  $$\\rho_c(L) \\sim \\frac{1}{L} \\quad \\text{as } L \\to \\infty .$$ This means that as we increase either the length or the duration of our experiment, the probability of collision decreases exponentially fast.", "paraphrased_abstract": "The main thing is to show that the critical density, in any t 0 > 0, corresponds to the following scaling laws: rho c(t) sim frac1sqrt4pi t quad textas t to infty , and rho c(L) sim frac1l quad textas t to infty >, and rho c(L) quad textas  L  to infty  .. The main result is that at any fixed t0 0 0 rho c(t)   aho c(t), all particles will collide at some time t0 leq t, otherwise they will never collide. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0754, "title": "Metal Cooling in Simulations of Cosmic Structure Formation", "abstract": "  The addition of metals to any gas can significantly alter its evolution by increasing the rate of radiative cooling. In star-forming environments, enhanced cooling can potentially lead to fragmentation and the formation of low-mass stars, where metal-free gas-clouds have been shown not to fragment. Adding metal cooling to numerical simulations has traditionally required a choice between speed and accuracy. We introduce a method that uses the sophisticated chemical network of the photoionization software, Cloudy, to include radiative cooling from a complete set of metals up to atomic number 30 (Zn) that can be used with large-scale three-dimensional hydrodynamic simulations. Our method is valid over an extremely large temperature range (10 K < T < 10^8 K), up to hydrogen number densities of 10^12 cm^-3. At this density, a sphere of 1 Msun has a radius of roughly 40 AU. We implement our method in the adaptive mesh refinement (AMR) hydrodynamic/N-body code, Enzo. Using cooling rates generated with this method, we study the physical conditions that led to the transition from Population III to Population II star formation. While C, O, Fe, and Si have been previously shown to make the strongest contribution to the cooling in low-metallicity gas, we find that up to 40% of the metal cooling comes from fine-structure emission by S, when solar abundance patterns are present. At metallicities, Z > 10^-4 Zsun, regions of density and temperature exist where gas is both thermally unstable and has a cooling time less than its dynamical time. We identify these doubly unstable regions as the most inducive to fragmentation. At high redshifts, the CMB inhibits efficient cooling at low temperatures and, thus, reduces the size of the doubly unstable regions, making fragmentation more difficult. ", "machine_abstract": "We present results on the metal cooling function for cosmological simulations, including both primordial and secondary coolants (H$\\alpha$, CII, OI, SiIII, Fe). We find that the inclusion of metals has little effect on the temperature distribution at z>5 but significantly affects it below this redshift. The effects are more pronounced when we include photo-ionization heating by UV background radiation. In particular, we show that the gas density profiles become shallower with decreasing redshifts due to stronger radiative losses near galactic centers. This leads to an increase in the central galaxy masses as well as the number densities of low-mass galaxies. These changes can be understood within the framework of our analytic model which is based on the assumption of hydrostatic equilibrium between gravity and pressure gradients. Our results suggest that the observed properties of high-z Lyman break galaxies may not be reproduced without taking into account the presence of heavy elements.     Keywords: Cosmology", "paraphrased_abstract": "Our results show that the radiance of the gaseous gases, which are deeper at the lower redshifts, becomes shallower in the central globular masses, and in the number of low-mass galaxies. This effect is even more dramatic when the light of metals is added. The metal cooling function is investigated for cosmological simulations, including primordial and secondary coolants (H alpha, CII, OI, SiIII, Fe). In particular, we show that the density of gaseous gases becomes shallower with decreasing redshifts, owing to the more intense losses near the central globular centers. This causes an increase of central galaxy masses, and the number of low-mass galaxies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1576, "title": "A Catalog of Star Cluster Candidates in M33", "abstract": "  We present a new catalog of star cluster candidates in the nearby spiral galaxy M33. It is based on eight existing catalogs wherein we have cross-referenced identifications and endeavored to resolve inconsistencies between them. Our catalog contains 451 candidates of which 255 are confirmed clusters based on HST and high resolution ground-based imaging. The catalog contains precise cluster positions (RA and Dec), magnitudes and colors in the UBVRIJHKs filters, metallicities, radial velocities, masses and ages, where available, and galactocentric distances for each cluster. The color distribution of the M33 clusters appears to be similar to those in the Large Magellanic Cloud with major peaks at (B-V)o~0.15, and (B-V)o~0.65. The intrinsic colors are correlated with cluster ages, which range from 10^{7.5} to 10^{10.3} years. The age distribution of the star clusters supports the notion of rapid cluster disruption with a slope of alpha=-1.09 +/- 0.07 in the dN_{cluster}/dt ~ t^{alpha} relation. In addition, comparison to theoretical single stellar population models suggests the presence of an age-metallicity relation among these clusters with younger clusters being more metal-rich. Analysis of the radial distribution of the clusters yields some evidence that younger clusters (age <~ 1 Gyr) may be more concentrated toward the center of M33 than older ones. A similar comparison with the radial profile of the M33 field stars shows the clusters to be more centrally concentrated at the greater than 99.9% confidence level. Possible reasons for this are presented and discussed; however, the overwhelming conclusion seems to be that a more complete and thorough cluster search is needed covering at least 4 square degrees centered on M33. ", "machine_abstract": "We present the results of an automated search for star cluster candidates (SCCs) using deep HST/ACS images covering most of the disk and halo of M33, obtained as part of program GO-10229. The SCCs were identified by applying the DAOPHOT photometry package to detect sources with high surface brightnesses relative to their local backgrounds. We then applied several selection criteria based on color-magnitude diagrams (CMDs), luminosity functions (LFs), and radial profiles to identify bona fide SCCs among these bright objects. A total of 1,082 candidate clusters are found within our survey area; we estimate that about half of them may be real open clusters or associations. These newly discovered SCCs will provide important targets for future spectroscopic studies aimed at understanding how star formation proceeds in low-metallicity environments such as those found in dwarf galaxies like M33.", "paraphrased_abstract": "I have calculated the number of SCCs, and the figure of one atom of the sky is estimated to be a hundred and thirty-seven. I will present an example of the search for open clusters, a cluster of a hundred and forty. The search was carried out by the astronomy system of the Adarnac Observatory, GO-10229. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4044, "title": "Nature of phase transition in magnetic thin films", "abstract": "  We study the critical behavior of magnetic thin films as a function of the film thickness. We use the ferromagnetic Ising model with the high-resolution multiple histogram Monte Carlo (MC) simulation. We show that though the 2D behavior remains dominant at small thicknesses, there is a systematic continuous deviation of the critical exponents from their 2D values. We observe that in the same range of varying thickness the deviation of the exponent $\\nu$ is very small from its 2D value, while exponent $\\beta$ suffers a larger deviation. Moreover, as long as the film thickness is fixed, i. e. no finite size scaling is done in the $z$ direction perpendicular to the film, the 3D values of the critical exponents cannot be attained even with very large (but fixed) thickness. The crossover to 3D universality class cannot therefore take place without finite size scaling applied in the $z$ direction, in the limit of numerically accessible thicknesses. From values of exponent $\\alpha$ obtained by MC, we estimate the effective dimension of the system. We conclude that with regard to the critical behavior, thin films behave as systems with effective dimension between 2 and 3. ", "machine_abstract": "We have studied the nature of phase transitions in FePt and CoPt alloys by measuring their magnetization as functions of temperature, applied field strength, and film thicknesses. The results show that both systems undergo first-order phase transitions at temperatures below 400 K with hysteresis loops. We also found that the coercivity increases rapidly when approaching to the Curie point (Tc) for all samples investigated here. This behavior is attributed to the increase of anisotropy energy density near Tc.     In addition, we observed an interesting phenomenon where the coercive fields are strongly dependent on sample thicknesses. For example, the coercivities of FePt(5 nm), FePt(10 nm), and FePt(20 nm) films were measured to be 0.6 T, 1.1 T, and 2.2 T respectively. These values are much higher than those reported previously for bulk materials.", "paraphrased_abstract": "We have also studied the nature of the phase transitions in FePt and CoPt by measuring their magnetic properties as a function of temperature, the applied field strength, and the thickness of the film. The results are that FePt and CoPt are first-order phases, and their phase transitions are hysterically rearranged. The coercivity of the films of FePt, nm, and nm is 0.6 T, 1.1 T, and 2.2 T, which are much higher than the one we have measured for the bulk materials. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4479, "title": "Nonequilibrium quantum-impurities: from entropy production to information theory", "abstract": "  Nonequilibrium steady-state currents, unlike their equilibrium counterparts, continuously dissipate energy into their physical surroundings leading to entropy production and time-reversal symmetry breaking. This letter discusses these issues in the context of quantum impurity models driven out of equilibrium by attaching the impurity to leads at different chemical potentials and temperatures. We start by pointing out that entropy production is often hidden in traditional treatments of quantum-impurity models. We then use simple thermodynamic arguments to define the rate of entropy production. Using the scattering framework recently developed by the authors we show that the rate of entropy production has a simple information theoretic interpretation in terms of the Shannon entropy and Kullback-Leibler divergence of nonequilibrium distribution function. This allows us to show that the entropy production is strictly positive for any nonequilibrium steady-state. We conclude by applying these ideas to the Resonance Level Model and the Kondo model. ", "machine_abstract": "We present an overview on the nonequilibrium dynamics of open quantum systems, with emphasis in the role played by entropy and information-theoretic concepts such as mutual information or entanglement entropies. We discuss how these quantities can be used to characterize different phases of matter emerging out of non-equilibrium steady states (NESS). In particular we focus our attention on two paradigmatic examples of NESS: i) thermalization processes induced by coupling a system to its environment; ii) driven-dissipative many-body systems where dissipation is engineered through external driving fields. Finally, we briefly comment on some recent results concerning the characterization of out-of-equilibrium phase transitions using information-theoretical tools. Nonequilibrium phenomena are ubiquitous in nature and play a crucial role for understanding the physics behind several important problems ranging from energy conversion devices [1] , transport properties [2] , biological motors [3] , to quantum computing [4] . The study of nonequilibrium statistical mechanics has been pioneered by Onsager [5] who introduced the concept of linear response theory [6] which allows one to describe macroscopic fluctuations around equilibrium [7, 8] . In this context it was shown that fluctuation relations [9] provide fundamental constraints on the statistics of work performed during a process [10] . These relations have been experimentally tested [11] and recently generalized beyond their original formulation [12] . Moreover, they were also extended to include higher-order cumulants [13] and time-reversal breaking [14] . Another key ingredient in the description of nonequilibrium phenomena is represented by the notion of entropy production [15] . This quantity measures the irreversibility associated with a given process [16] and plays a central role in the second law of thermodynamics [17] . It turns out that entropy production provides useful insights into the structure of nonequilibrium stationary states [18] . Indeed, it has been shown [19, 20] that if the entropy production vanishes then the state is necessarily described by Gibbs measure [21] . However, when the entropy production does not vanish there exist infinitely many possible nonequilibrium stationary states [22] .", "paraphrased_abstract": "When it has not vanished, it has the corresponding Gibbs measure, but when it has not vanished, it can still be described by the Gibbs measure. In this respect, the entropy yields good insights into the structure of equilibrium. This is an indispensable feature of the mathematical theory of non-equilibrium, and is particularly useful in the study of phenomena arising from the transition between different states of matter, which are not equilibrium, or from the movement of energy, or from the synthesis of materials or biological systems. This phenomenon is everywhere in nature, and is crucial to the physics of a number of important problems, such as energy-converting devices, transport properties, biological motors, and quantum computing. Onsager, the pioneer of equilibrium, introduced the theory of linear response, which allows to describe macroscopic fluctuations of the equilibrium, and to describe the consequences of the process, which are fundamentally imposed on the statistics of work performed by the subject. This idea has been experimentally tested and extended to more complex, higher-order, or inter-related, entropy, and in particular, we have considered two paradigmatic examples of entropy: i) thermal changes of a system entangling with the environment, ii) a system driven dissipation by the external driving field, and finally", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3488, "title": "Optimal Occulter Design for Finding Extrasolar Planets", "abstract": "  One proposed method for finding terrestrial planets around nearby stars is to use two spacecraft--a telescope and a specially shaped occulter that is specifically designed to prevent all but a tiny fraction of the starlight from diffracting into the telescope. As the cost and observing cadence for such a mission will be driven largely by the separation between the two spacecraft, it is critically important to design an occulter that can meet the observing goals while flying as close to the telescope as possible. In this paper, we explore this tradeoff between separation and occulter diameter. More specifically, we present a method for designing the shape of the outer edge of an occulter that is as small as possible and gives a shadow that is deep enough and large enough for a 4m telescope to survey the habitable zones of many stars for Earth-like planets. In particular, we show that in order for a 4m telescope to detect in broadband visible light a planet 0.06 arcseconds from a star shining $10^{10}$ times brighter than the planet requires a specially-shaped occulter 50m in diameter positioned about $72,000$ km in front of the telescope. ", "machine_abstract": "We present an optimal design method to find the best occulting mask that can be used in direct imaging searches for extrasolar planets.  The proposed method is based on the concept of entropy maximization, which has been widely applied in many fields such as information theory and statistical mechanics.   We show how this concept can be extended into the field of optics by introducing a new quantity called optical entropy (OE).   By using OE we are able to quantify the amount of information contained within each point spread function produced by different masks.   This allows us to determine the most efficient mask shape with respect to its ability to detect faint companions around bright stars.   ... [more]  In order to demonstrate our approach, we have performed numerical simulations to compare the performance of several candidate masks against one another.   Our results indicate that the proposed method provides significant improvement over existing methods when it comes to finding the optimal mask shapes for detecting faint companions around bright host stars.", "paraphrased_abstract": "The proposed method is based on the entropy maximization principle, which has been widely applied in many fields, such as information theory and statistical mechanics. This principle is applied to optics, where we introduce a new, additional quantity called ocular entropy (OE), which we can quantify by estimating the information contained in each point of the spread function of different masks. We demonstrate that our method is superior to other methods for the determination of faint companions around bright stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0765, "title": "Measuring the Decoherence of a Quantronium Qubit with the Cavity Bifurcation Amplifier", "abstract": "  Dispersive readouts for superconducting qubits have the advantage of speed and minimal invasiveness. We have developed such an amplifier, the Cavity Bifurcation Amplifier (CBA) [10], and applied it to the readout of the quantronium qubit [2]. It consists of a Josephson junction embedded in a microwave on-chip resonator. In contrast with the Josephson bifurcation amplifier [17], which has an on-chip capacitor shunting a junction, the resonator is based on a simple coplanar waveguide imposing a pre-determined frequency and whose other RF characteristics like the quality factor are easily controlled and optimized. Under proper microwave irradiation conditions, the CBA has two metastable states. Which state is adopted by the CBA depends on the state of a quantronium qubit coupled to the CBA's junction. Due to the MHz repetition rate and large signal to noise ratio we can show directly that the coherence is limited by 1/f gate charge noise when biased at the sweet spot - a point insensitive to first order gate charge fluctuations. This architecture lends itself to scalable quantum computing using a multi-resonator chip with multiplexed readouts. ", "machine_abstract": "We report on an experiment to measure decoherence in a quantronium qubit using the cavity bifurcation amplifier (CBA). The CBA is used as a dispersive readout device for the quantronium, and we show that it can be operated at high bandwidths without significant back-action noise or added phase noise. We demonstrate measurement times up to 1 ms per data point while maintaining quantum-limited sensitivity. This allows us to perform measurements over many relaxation cycles of the system, which are necessary to extract information about its dynamics. In addition, we use this technique to study the effect of different environmental conditions on the coherence time of our system. Our results indicate that the CBA has great potential for future experiments studying open quantum systems. Quantum mechanics predicts that macroscopic objects such as atoms will exhibit wave-like behavior when they interact with light fields. However, these predictions have not been experimentally verified because measuring the properties of individual particles requires extremely sensitive detectors. Here, we present a new method for detecting single photons based on a nonlinear optical process known as parametric down-conversion. Using this approach, we detect single photons emitted by a trapped atom interacting with a laser field inside a high-finesse optical cavity. By coupling the output of the cavity into a second cavity containing a nonlinear crystal, we generate pairs of entangled photons whose frequencies differ by twice the frequency of the pump beam. These photon pairs are then detected simultaneously by two silicon avalanche photodiodes operating in Geiger mode.", "paraphrased_abstract": "The quantum mechanics has predicted that particles interacting with light can behave like wavelets. This prediction has not been proved experimentally, because a measurement of the properties of a single particle requires a very sensitive detector. In this work, we report a test to measure the coherence of a quantum atom using a cavity bifurcation amplifier (CBA), which is used to recombine the optical output of the atom into a second cavity filled with a nonlinear crystal, which produces entangled photons whose frequencies differ by twice the frequency of the pump. We incorporated this method into a silicon avalanche photodiode, operated in Geiger mode. We show that it is possible to measure decoherence in a quantum atom with the help of a CBA, and that we can operate at high bandwidths without any special back-action or added phase noise. In this way we can study the properties of atoms in a system open to all, and we can also investigate the effect of a few external conditions on the coherence of the system. The CBA can be used for dispersion of the quantum atom, which has high bandwidth and low sensitivity. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2285, "title": "The 3D soft X-ray cluster-AGN cross-correlation function in the ROSAT NEP survey", "abstract": "  X-ray surveys facilitate investigations of the environment of AGNs. Deep Chandra observations revealed that the AGNs source surface density rises near clusters of galaxies. The natural extension of these works is the measurement of spatial clustering of AGNs around clusters and the investigation of relative biasing between active galactic nuclei and galaxies near clusters.The major aims of this work are to obtain a measurement of the correlation length of AGNs around clusters and a measure of the averaged clustering properties of a complete sample of AGNs in dense environments. We present the first measurement of the soft X-ray cluster-AGN cross-correlation function in redshift space using the data of the ROSAT-NEP survey. The survey covers 9x9 deg^2 around the North Ecliptic Pole where 442 X-ray sources were detected and almost completely spectroscopically identified. We detected a >3sigma significant clustering signal on scales s<50 h70^-1 Mpc. We performed a classical maximum-likelihood power-law fit to the data and obtained a correlation length s_0=8.7+1.2-0.3 h_70-1 Mpc and a slope gamma=1.7$^+0.2_-0.7 (1sigma errors). This is a strong evidence that AGNs are good tracers of the large scale structure of the Universe. Our data were compared to the results obtained by cross-correlating X-ray clusters and galaxies. We observe, with a large uncertainty, that the bias factor of AGN is similar to that of galaxies. ", "machine_abstract": "We present an analysis of the spatial correlation between galaxy clusters and active galactic nuclei (AGNs) detected by the ROSAT North Ecliptic Pole Survey (NEP). We use a sample of AGNs selected with the hardness ratio method, which is insensitive to obscuration effects due to dusty torii around AGNs. The clustering signal for this sample shows no significant difference compared to that obtained using optically-selected samples at similar redshifts. This suggests that there are not many heavily obscured AGNs among our sample. Using the best-fit model parameters derived from the auto-correlations of both galaxy clusters and AGNs, we find that the amplitude of the cross-correlation function agrees well with theoretical predictions based on linear biasing models. However, the observed shape of the cross-correlation functions differs significantly from those predicted by these simple models. In particular, the observed cross-correlation function has a steeper slope than expected near zero separation distance.", "paraphrased_abstract": "The amplitude of the cross-correlations is remarkably similar to that predicted by the linear model of the equations. Nevertheless, the shape of the cross-correlations is markedly different from that expected by these simple models. In this paper, we investigate the correlations of galaxy clusters and active galactic nuclei identified by the ROSAT survey of the North Pole. The clustering pattern of our sample is a rough one, and does not exhibit much difference in size from the one obtained from the optically selected ones at similar redshifts. We apply the same approach to a sample of active galactic nuclei detected by the NEP. This sample is selected with the hardness of the torii, and thus has no obscuration at all compared to that of optically selected ones. The signal in our sample shows little difference from the one obtained from optically selected ones at similar redshifts. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0036, "title": "A twisted FZZ-like dual for the two-dimensional black hole", "abstract": "  We review and study the duality between string theory formulated on a curved exact background (the two dimensional black hole) and string theory in flat space with a tachyon-like potential. We generalize previous results in this subject by discussing a twisted version of the Fateev-Zamolodchikov-Zamolodchikov conjecture. This duality is shown to hold at the level of N-point correlation functions on the sphere topology, and connects tree-level string amplitudes in the Euclidean version of the 2D black hole (x time) to correlation functions in a non-linear sigma-model in flat space but in presence of a tachyon wall potential and a linear dilaton. The dual CFT corresponds to the perturbed 2D quantum gravity coupled to c<1 matter (x time), where the operator that describes the tachyon-like potential can be seen as a n=2 momentum mode perturbation, while the usual sine-Liouville operator would correspond to the vortex sector n =1. We show how the sine-Liouville interaction term arises through a twisting of the marginal deformation introduced here, and discuss such 'twisting' as a non-trivial realization of the symmetries of the theory. After briefly reviewing the computation of correlation functions in sine-Liouville CFT, we give a precise prescription for computing correlation functions in the twisted model. To show the new version of the correspondence we make use of a formula recently proven by S. Ribault and J. Teschner, which connects the correlation functions in the Wess-Zumino-Witten theory to correlation functions in the Liouville theory. Conversely, the duality discussed here can be thought of as a free field realization of such remarkable formula. ", "machine_abstract": "We present an exact solution to the classical equations of motion in two dimensions, which is interpreted as describing a rotating black hole with angular momentum J = M . The metric has the form ds2 = \u2212dt2+(1+cosh2r)d\u03b82\u2212r2dr2, where r and \u03b8 are polar coordinates on the plane. This solution can be obtained by performing a duality transformation on the usual BTZ black hole (with no rotation). We show that this new solution satisfies all the required physical conditions at infinity. In particular we find that it describes a regular black hole horizon located atr+ = \u221a3M , where M is the mass parameter appearing in the original BTZ solution. Finally, we discuss some possible generalizations of our results. Introduction:-In recent years there have been many attempts to construct solutions to Einstein's field equations corresponding to rotating black holes [1] - [4] . One particularly interesting class of such solutions was found by Ba\u00f1ados, Teitelboim and Zanelli (BTZ), who showed how one could obtain a static black hole solution in three dimensional anti-de Sitter space-time [5] . The most important feature of these solutions is their asymptotic behaviour; they describe black holes whose event horizons are completely determined by global quantities like total energy or charge [6] . However, despite being very useful tools for studying quantum gravity phenomena [7, 8] , these solutions do not provide any information about local properties of the spacetime near the horizon [9] . It would therefore seem desirable to try to extend them into more complicated geometries containing additional parameters characterizing the internal structure of the black hole [10] . One way of doing so is to consider higher-dimensional extensions of the BTZ solution [11] . Another possibility is to perform a duality transformation on known solutions [12] . For example, if we start with the Schwarzschild solution written in terms of spherical coordinates, then after applying a suitable coordinate transformation we will get another solution expressed in terms of oblate spheroidal coordinates [13] .", "paraphrased_abstract": "In recent years many attempts have been made to formulate solutions to the equations of Einstein\u2019s gravity that are suitable for the study of black holes, particularly of the angular velocity J = M. The solution of the equation is of the form ds2 = dt2+(1+cosh2r)ds2r2dr2, where d and  are polar coordinates. The resulting solution is of the form ds2 = ds2=(1+cosh2r2) d2r2dr2, in which the polar coordinates of the sphere are displaced by the polar coordinates. In this way it is possible to achieve the necessary physical conditions at infinite lengths. One of the most interesting things in the recent years has been the realization of solutions to Einstein\u2019s field equations corresponding to a rotating black hole, with angular momentum J = M. This metric, ds2=dt2+ d2r2dr22r2dr22, has the form ds2 = ds2=ds2+(1+cosh2r2), where d is the mass parameter, and d is the mass parameter of the original BTZ. The next step is to", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1094, "title": "Flame Evolution During Type Ia Supernovae and the Deflagration Phase in the Gravitationally Confined Detonation Scenario", "abstract": "  We develop an improved method for tracking the nuclear flame during the deflagration phase of a Type Ia supernova, and apply it to study the variation in outcomes expected from the gravitationally confined detonation (GCD) paradigm. A simplified 3-stage burning model and a non-static ash state are integrated with an artificially thickened advection-diffusion-reaction (ADR) flame front in order to provide an accurate but highly efficient representation of the energy release and electron capture in and after the unresolvable flame. We demonstrate that both our ADR and energy release methods do not generate significant acoustic noise, as has been a problem with previous ADR-based schemes. We proceed to model aspects of the deflagration, particularly the role of buoyancy of the hot ash, and find that our methods are reasonably well-behaved with respect to numerical resolution. We show that if a detonation occurs in material swept up by the material ejected by the first rising bubble but gravitationally confined to the white dwarf (WD) surface (the GCD paradigm), the density structure of the WD at detonation is systematically correlated with the distance of the deflagration ignition point from the center of the star. Coupled to a suitably stochastic ignition process, this correlation may provide a plausible explanation for the variety of nickel masses seen in Type Ia Supernovae. ", "machine_abstract": "We present results on flame evolution during type Ia supernova (SN) explosions, based on two-dimensional hydrodynamic simulations with detailed nuclear reaction networks for both deflagrations and detonations. We find that the transition to detonation is triggered by shock-induced turbulent mixing at densities around 10$^{9}$ g/cm$^3$. The resulting detonation wave propagates through the entire white dwarf star within about 1 s after ignition. In this scenario, the observed light curve can be reproduced if we assume an initial central density of 2 x $10^8$ g/cm$^3$ or higher. For lower values of the central density, the explosion fails to produce enough 56Ni to explain observations. This work was supported by NASA grant NNX10AD03G. Keywords: Flame propagation, Hydrodynamics, Nuclear burning, Supernovae, White dwarfs. Subject headings: Nucleosynthesis", "paraphrased_abstract": "This work was supported by NASA grant NNX-10AD03G. Species examined: flame propagation, hydrodynamics, nuclear burning, white dwarfs. We present results on the evolution of a type Ia supernova explosion based on two-dimensional hydrodynamic models with detailed nuclear reactions for the deflagration and detonation. We find that the transition from detonation to detonation takes place by the shock of a sudden collision with the central layer, which is about a tenth of the central layer. For a lower central layer, the explosion does not produce enough 56Ni to explain the light curve. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3551, "title": "Modelling diffusional transport in the interphase cell nucleus", "abstract": "  In this paper a lattice model for diffusional transport of particles in the interphase cell nucleus is proposed. Dense networks of chromatin fibers are created by three different methods: randomly distributed, non-interconnected obstacles, a random walk chain model, and a self avoiding random walk chain model with persistence length. By comparing a discrete and a continuous version of the random walk chain model, we demonstrate that lattice discretization does not alter particle diffusion. The influence of the 3D geometry of the fiber network on the particle diffusion is investigated in detail, while varying occupation volume, chain length, persistence length and walker size. It is shown that adjacency of the monomers, the excluded volume effect incorporated in the self avoiding random walk model, and, to a lesser extent, the persistence length, affect particle diffusion. It is demonstrated how the introduction of the effective chain occupancy, which is a convolution of the geometric chain volume with the walker size, eliminates the conformational effects of the network on the diffusion, i.e., when plotting the diffusion coefficient as a function of the effective chain volume, the data fall onto a master curve. ", "machine_abstract": "The aim of this study is to develop an improved model for simulating diffusion processes within biological cells, especially those occurring at the nuclear membrane and its associated structures.  The proposed approach involves coupling two existing models; one that describes the movement of molecules through the cytoplasm (the fluid part of the cell) using Brownian dynamics simulations, with another which represents the nucleus as a porous medium containing immobile obstacles.  This latter component has been developed by considering the geometry of the nuclear pore complex network, which consists of cylindrical pores connected via narrower channels.   In order to validate our new hybrid model we have performed a series of numerical experiments on synthetic data generated from both individual particle tracking and Monte Carlo methods.  We find good agreement between these results and those obtained from our own computational scheme, thereby demonstrating the accuracy of our method. Finally, we apply our new modelling framework to investigate how changes in the structure of the nuclear pore complexes can affect the rate of molecular exchange across the nuclear envelope.", "paraphrased_abstract": "In order to validate this new model, we incorporated the results of the numerical analysis of individual particles and Monte Carlo simulations into our new unified model. To make our model suitable, we incorporated two existing models, one which describes the diffusion of molecules in the cytoplasm (the fluid part of the cell), and another which models the nucleus as a porous medium which is immobile. In this new unified model, we converged two different models, one which modeled the flow of molecules through the cytoplasm (the fluid part of the cell), with the second model that represented the permeability of the nucleus as a porous medium with immobile obstacles. We found the adherence of our model to the Monte Carlo method, demonstrating the precision of our model. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2938, "title": "Improving Stellar and Planetary Parameters of Transiting Planet Systems: The Case of TrES-2", "abstract": "  We report on a spectroscopic determination of the atmospheric parameters and chemical abundance of the parent star of the recently discovered transiting planet {TrES-2}. A detailed LTE analysis of a set of \\ion{Fe}{1} and \\ion{Fe}{2} lines from our Keck spectra yields $T_\\mathrm{eff} = 5850\\pm 50$ K, $\\log g = 4.4\\pm 0.1$, and [Fe/H] $= -0.15\\pm 0.10$. Several independent checks (e.g., additional spectroscopy, line-depth ratios) confirm the reliability of our spectroscopic $T_\\mathrm{eff}$ estimate. The mass and radius of the star, needed to determine the properties of the planet, are traditionally inferred by comparison with stellar evolution models using $T_\\mathrm{eff}$ and some measure of the stellar luminosity, such as the spectroscopic surface gravity (when a trigonometric parallax is unavailable, as in this case). We apply here a new method in which we use instead of $\\log g$ the normalized separation $a/R_\\star$ (related to the stellar density), which can be determined directly from the light curves of transiting planets with much greater precision. With the $a/R_\\star$ value from the light curve analysis of Holman et al. \\citeyearpar{holman07b} and our $T_\\mathrm{eff}$ estimate we obtain $M_\\star = 0.980\\pm0.062 M_\\odot$ and $R_\\star = 1.000_{-0.033}^{+0.036} R_\\odot$, and an evolutionary age of $5.1^{+2.7}_{-2.3}$ Gyr, in good agreement with other constraints based on the strength of the emission in the \\ion{Ca}{2} H & K line cores, the Lithium abundance, and rotation. The new stellar parameters yield improved values for the planetary mass and radius of $M_p = 1.198 \\pm 0.053 M_\\mathrm{Jup}$ and $R_p = 1.220^{+0.045}_{-0.042} R_\\mathrm{Jup}$, confirming that {TrES-2} is the most massive among the currently known nearby ($d\\lesssim 300$ pc) transiting hot Jupiters. [Abridged] ", "machine_abstract": "We present the results of an analysis aimed at improving the stellar parameters for the host star of planet TrES-2, as well as its planetary system properties. We use high-precision photometry obtained with the MOST satellite to derive new values for the orbital period (P = 3.819 days), transit epoch (T0 = 2454000 MJD) and radius ratio (Rp/Rs = 0.11). These are combined with existing radial velocity data in order to refine the mass estimates for both components of this double-lined spectroscopic binary. Our best-fit model yields masses of 1.06 \u00b1 0.04M\u2299 and 0.84 \u00b1 0.03M\u2299 for the primary and secondary stars respectively, along with radii of 1.16 \u00b1 0.02R\u2299 and 0.91 \u00b1 0.01R\u2299 . This leads us to revise upward our previous estimate of the age of the system by about 50%, placing it firmly within the range expected for planets formed via core accretion theory.", "paraphrased_abstract": "It is a new approach, using the MOST-Earth, to calculate the orbital period (P = 3.819 days) and the transit time (T0 = 2454000 MJD), and the radius (Rp = 0.11) of the two elements. This, however, does not negate our previous estimate of the age of the system, which is in the range expected for planets formed by the accretion of cores. We also rely on the radial velocity data from the MOST-Earth to find the radial mass for the first and second stars, and the radii of 1.16  0.02 and 1.91  0.01 for the secondary stars, respectively, and the radius of 1.16  0.02 and 0.91  0.01. We have the results of an investigation of the host star of the constellation TrES-2, and the planetary properties. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1878, "title": "Orbital-Free Density Functional Theory: Kinetic Potentials and Ab-Initio Local Pseudopotentials", "abstract": "  In the density functional (DF) theory of Kohn and Sham, the kinetic energy of the ground state of a system of noninteracting electrons in a general external field is calculated using a set of orbitals. Orbital free methods attempt to calculate this directly from the electron density by approximating the universal but unknown kinetic energy density functional. However simple local approximations are inaccurate and it has proved very difficult to devise generally accurate nonlocal approximations. We focus instead on the kinetic potential, the functional derivative of the kinetic energy DF, which appears in the Euler equation for the electron density. We argue that the kinetic potential is more local and more amenable to simple physically motivated approximations in many relevant cases, and describe two pathways by which the value of the kinetic energy can be efficiently calculated. We propose two nonlocal orbital free kinetic potentials that reduce to known exact forms for both slowly varying and rapidly varying perturbations and also reproduce exact results for the linear response of the density of the homogeneous system to small perturbations. A simple and systematic approach for generating accurate and weak ab-initio local pseudopotentials which produce a smooth slowly varying valence component of the electron density is proposed for use in orbital free DF calculations of molecules and solids. The use of these local pseudopotentials further minimizes the possible errors from the kinetic potentials. Our theory yields results for the total energies and ionization energies of atoms, and for the shell structure in the atomic radial density profiles that are in very good agreement with calculations using the full Kohn-Sham theory. ", "machine_abstract": "We present an orbital-free density functional theory (OF-DFT) that is free of any explicit reference to the electronic wavefunctions, i.e., it does not require solving Schr\u00f6dinger's equation for each electron in order to obtain the ground-state energy or other properties. The key idea behind our approach is to replace the kinetic potentials appearing in the usual OF-DFT expressions by their ab initio counterparts obtained directly from the local pseudopotentials used in standard quantum chemistry calculations. We show how this can be done systematically using only information about the occupied states provided by such calculations. As a result we are able to calculate the total energies as well as various other quantities within OF-DFT with essentially no additional computational cost compared to conventional molecular dynamics simulations based on classical force fields. This opens up new possibilities for studying complex systems where both electronic structure effects and thermal fluctuations play important roles simultaneously.     Orbital-free density functional theory (DFT), which was originally developed more than 50 years ago [1] , has become one of the most widely used methods in theoretical physics [2] . In its original formulation [3] , the basic quantity of interest is the so-called exchange-correlation potential $\\psi^{xc}$, whose knowledge allows us to compute all relevant physical observables at zero temperature. However, since the exact form of $\\psi^{xc}$ is unknown, practical applications rely on approximate functionals [4] .   In recent decades there have been significant efforts devoted to developing accurate approximations for $\\psi^{xc}$ [5] . These include semi-local [6] and hybrid [7] functionals, as well as many-body perturbation theories [8] . While these approaches provide very good results for many classes of materials [9] , they often fail when applied to strongly correlated systems [10] . Moreover, even if the chosen approximation yields reasonable results for some property, it may perform poorly for others [11] . For example, while hybrid functionals yield excellent band gaps [12] , they tend to overestimate lattice constants [13] .     An alternative strategy consists in replacing the unknown $\\psi^{xc}$ by its first-principles counterpart calculated directly from the underlying atomic interactions [14] . Although this approach", "paraphrased_abstract": "We propose an approach to the decomposition of kinetic potentials in the traditional form of the decomposition of atoms, and an alternative strategy for obtaining the corresponding value of the atoms of the atoms in the atoms. This strategy is particularly useful for the case of highly constrained systems, where electronic structure and thermal fluctuations are important simultaneously. An orbital-free density functional theory (ODF) was first developed more than fifty years ago, and has become a popular theory in theoretical physics. Its basic principle is the exchange-correlation potential Psixc, which is able to calculate all relevant physical observables at zero temperature. However, as the form of Psixc is unknown, the practical applications depend on the estimation of kinetic potentials in the occupied state of the underlying structure. We propose a new method of study for a given form of DFT, which does not require any further calculations, and only uses the information provided by the occupied state of the occupied states. In particular, in the case of the aeon, we find the corresponding function, which is the most precise of all. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0748, "title": "Ratios of star cluster core and half-mass radii: a cautionary note on intermediate-mass black holes in star clusters", "abstract": "  There is currently much interest in the possible presence of intermediate-mass black holes in the cores of globular clusters. Based on theoretical arguments and simulation results it has previously been suggested that a large core radius -- or particularly a large ratio of the core radius to half-mass radius -- is a promising indicator for finding such a black hole in a star cluster. In this study N-body models of 100000 stars with and without primordial binaries are used to investigate the long-term structural evolution of star clusters. Importantly, the simulation data is analysed using the same processes by which structural parameters are extracted from observed star clusters. This gives a ratio of the core and half-mass (or half-light) radii that is directly comparable to the Galactic globular cluster sample. As a result, it is shown that the ratios observed for the bulk of this sample can be explained without the need for an intermediate-mass black hole. Furthermore, it is possible that clusters with large core to half-light radius ratios harbour a black-hole binary (comprised of stellar mass black holes) rather than a single massive black hole. This work does not rule out the existence of intermediate-mass black holes in the cores of at least some star clusters. ", "machine_abstract": "We present new results for the ratio between the core radius rc and the half-light radius r h of open star clusters, based on high-resolution HST imaging data obtained with the Advanced Camera for Surveys (ACS). We find that this ratio is systematically larger than previously reported values by about 0.2 dex. This discrepancy can be explained if we assume that some of these clusters contain an intermediate mass black hole at their center. The presence of such objects would lead to smaller observed values of rc/r h . Our findings are consistent with previous studies which have suggested that many open clusters may harbor IMBHs. In addition, our results provide further evidence against the hypothesis that all open clusters host central massive stars.     Keywords: Open Star Cluster; Core Radius; Half-Mass Radius; Black Hole; Intermediate Mass Black Hole; ACS/HRC FOV; Galaxy", "paraphrased_abstract": "Moreover, the results are also convincing as to the hypothesis that all open clusters contain large massive stars. - rc and rh - Open cluster, core, half-light, black hole, intermediate-mass black hole, ACS/HRC; galaxy, open cluster, rc and rh - We study the ratio between the radius of the core and the radius of the half-light, and show that the ratio is almost twice the number of previous measurements. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1672, "title": "Principal Component Analysis and Automatic Relevance Determination in Damage Identification", "abstract": "  This paper compares two neural network input selection schemes, the Principal Component Analysis (PCA) and the Automatic Relevance Determination (ARD) based on Mac-Kay's evidence framework. The PCA takes all the input data and projects it onto a lower dimension space, thereby reduc-ing the dimension of the input space. This input reduction method often results with parameters that have significant influence on the dynamics of the data being diluted by those that do not influence the dynamics of the data. The ARD selects the most relevant input parameters and discards those that do not contribute significantly to the dynamics of the data being modelled. The ARD sometimes results with important input parameters being discarded thereby compromising the dynamics of the data. The PCA and ARD methods are implemented together with a Multi-Layer-Perceptron (MLP) network for fault identification in structures and the performance of the two methods is as-sessed. It is observed that ARD and PCA give similar accu-racy levels when used as input-selection schemes. There-fore, the choice of input-selection scheme is dependent on the nature of the data being processed. ", "machine_abstract": "The damage identification problem is formulated as an inverse problem, where the objective is to identify the location and severity of damages by minimizing the difference between simulated responses using finite element analysis (FEA) and measured data. The number of unknowns can be very large due to the presence of multiple sensors or measurement points. In this study, we propose two methods for reducing the dimensionality of the problem: principal component analysis (PCA), which reduces the dimension of the response space; and automatic relevance determination (ARD), which reduces the dimensions of both the input parameter space and the output response space simultaneously. Both PCA and ARD are implemented within the Bayesian framework so that uncertainties associated with these reduction techniques can also be accounted for during the optimization process. A numerical example involving a cantilever beam subjected to static loading is presented to demonstrate the effectiveness of the proposed approaches. Principal component analysis (PCA) and automatic relevance determination (AR", "paraphrased_abstract": "The numerical example encloses a cantilever beam subjected to static load is shown. The damage identification problem is an inverse problem, where the goal is to identify the location and the severity of the damages by minimizing the difference between simulated and measured data. The PCA and the AR are implemented in a Bayesian framework, so that the uncertainties associated with these two reductions can also be accounted for in the optimization process. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0189, "title": "Monoid generalizations of the Richard Thompson groups", "abstract": "  The groups G_{k,1} of Richard Thompson and Graham Higman can be generalized in a natural way to monoids, that we call M_{k,1}, and to inverse monoids, called Inv_{k,1}; this is done by simply generalizing bijections to partial functions or partial injective functions. The monoids M_{k,1} have connections with circuit complexity (studied in another paper). Here we prove that M_{k,1} and Inv_{k,1} are congruence-simple for all k. Their Green relations J and D are characterized: M_{k,1} and Inv_{k,1} are J-0-simple, and they have k-1 non-zero D-classes. They are submonoids of the multiplicative part of the Cuntz algebra O_k. They are finitely generated, and their word problem over any finite generating set is in P. Their word problem is coNP-complete over certain infinite generating sets.   Changes in this version: Section 4 has been thoroughly revised, and errors have been corrected; however, the main results of Section 4 do not change. Sections 1, 2, and 3 are unchanged, except for the proof of Theorem 2.3, which was incomplete; a complete proof was published in the Appendix of reference [6], and is also given here. ", "machine_abstract": "We introduce monoids that are generalizations of the classical groups of Richard Thompson, and we show how to construct these monoids using finite automata.  We also give an explicit description for all elements in terms of their normal forms with respect to the monoid generators.   Finally, we prove that our monoids have decidable word problems by showing that they can be embedded into free inverse monoids. The results presented here were obtained during my PhD thesis at ETH Zurich under the supervision of Prof. Dr. Peter Schupp (ETH Zurich) and Prof. Dr. Michael Steinert (University of Hamburg).    Keywords: Group theory, Monoids, Automata, Decidability, Word problem, Free inverse monoids, Finite automata, Computational complexity theory, Groups generated by finite automata, Generalized Thompson groups, Elementary amenable groups, Group actions on trees, Group actions on graphs", "paraphrased_abstract": "There are some examples: group theory, monoids, automata, determinability, word problem, free inverse monoids, computational complexity theory, generalized Thompson groups, elementary amenable groups, group actions on trees, group actions on graphs. We present here a collection of works on the subject, which I carried out in the course of my doctoral studies at ETH Zurich, under the supervision of Prof. Dr. Peter Schupp (ETH Zurich) and Prof. Michael Steinert (University of Hamburg). The results of my dissertation are presented here in the form of a research thesis under the supervision of Prof. Dr. Peter Schupp and Prof. Michael Steinert (University of Hamburg). We introduce a new class of monoids, which is generalized by Richard Thompson, and we present a method for forming such monoids by means of finite automata. We introduce the notion of decidability of our monoids, and give an explicit description of the normal form of the elements in the generators of our monoids. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0562, "title": "Interplay between thermal percolation and jamming upon dimer adsorption on binary alloys", "abstract": "  Using Monte Carlo simulations we study jamming and percolation processes upon the random sequential adsorption of dimers on binary alloys with different degrees of structural order. We obtain the equimolar mixtures used as substrates by applying the isomorphism between an alloy and the Ising model (conserved order parameter). The annealing temperature $T$ of the mixture then is a continuous parameter that characterizes the different sets of substrates, shaping the deposition process. As the alloy undergoes an order-disorder phase transition at the Onsager critical temperature ($T_{c}$), the jamming and percolating properties of the dimers deposited over the substrate are subjected to non-trivial changes. These are reflected in a density-temperature phase diagram with three well-defined regions. We find that for $T < T^* = 1.22 T_{c}$ the occurrence of jamming prevents the onset of percolating clusters, while percolation is possible for $T > T^{*}$. Particular attention is focused close to $T^{*}$, where the interplay between jamming and percolation restricts fluctuations, forcing exponents seemingly different from the standard percolation universality class. By analogy with a thermal transition, we study the onset of percolation by using the {\\it temperature} (in this case, the substrate annealing temperature) as a control parameter. By proposing thermal scaling Ansatzes we analyze the behavior of the percolation threshold and its thermally induced fluctuations. Also, the fractal dimension of the percolating cluster is determined. Based on these measurements and the excellent data collapsing, we conclude that the universality class of standard percolation is preserved for all temperatures. ", "machine_abstract": "We study the interplay between thermal percolating states and jammed states in disordered systems by using Monte Carlo simulations for dimers adsorbing onto binary alloy surfaces with different compositions. We find that there is an optimal composition at which both types of states coexist, leading to a maximum entropy production rate. The coexistence state has been observed experimentally as well. Our results provide new insights into how energy can be transferred most efficiently through complex networks. Energy transfer efficiency plays a crucial role in many physical processes such as heat conduction [1] , chemical reactions [2] , and biological transport [3] . In particular, it determines whether or not a system will reach equilibrium [4] . In this work we focus on one specific type of non-equilibrium process -thermal percolation [5] . Thermal percolation occurs when particles are injected randomly into a network [6] . Particles then diffuse along the network until they encounter each other [7, 8] . When two particles meet, their energies combine irreversibly [9] . This leads to a cascade-like spreading of particle density [10] . As more particles are added, the number of clusters increases [11] . Eventually these clusters merge together [12] forming a single cluster spanning across the entire network [13] . At this point all particles have combined into a giant cluster [14] . It was shown recently [15] that the transition from isolated clusters to a single connected cluster corresponds to a phase transition [16] . For example, in the case of random resistor networks [17] , the transition temperature T c depends only on the average resistance R av [18] : , where k B is Boltzmann's constant [19] . However, if the distribution of resistances P (R) is broad enough [20] , the transition becomes first-order [21] .", "paraphrased_abstract": "And if we are able to give a broad distribution of resistances, we will achieve first-order order. We have been able to show in our work that the energy transfer efficiency of complex networks is important, and that this is in particular a consideration for the stability of a system. We have studied the interplay between the two thermal percolation states and the encircling states in a disordered system by means of Monte Carlo simulations of dimers absorbing onto binary alloys with different compositions. We have found that the ideal composition is to have both types of percolating states at equilibrium, and the maximum energy evaporation is achieved. In addition, we have found that, when two particles meet, the eigenvalues of their energies merge irreversibly and the density of their particles increases. In this way all particles are united into a gigantic cluster. The transition from one cluster to another occurs at the temperature of c; for instance, the temperature of c is based only on the average resistance R av, where k B is the constant of Boltzmann. Our research shows how the thermal percolation of atoms and the jamming of atoms are related by Monte Carlo simulations and with the results that the best combination of these two states is reached. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0088, "title": "Link concordance, homology cobordism, and Hirzebruch-type defects from iterated p-covers", "abstract": "  We obtain new invariants of topological link concordance and homology cobordism of 3-manifolds from Hirzebruch-type intersection form defects of towers of iterated p-covers. Our invariants can extract geometric information from an arbitrary depth of the derived series of the fundamental group, and can detect torsion which is invisible via signature invariants. Applications illustrating these features include the following: (1) There are infinitely many homology equivalent rational 3-spheres which are indistinguishable via multisignatures, eta-invariants, and L2-signatures but have distinct homology cobordism types. (2) There is an infinite family of 2-torsion (amphichiral) knots, including the figure eight knot, with non-slice iterated Bing doubles; as a special case, we give the first proof of the conjecture that the Bing double of the figure eight knot is not slice. (3) There exist infinitely many torsion elements at any depth of the Cochran-Orr-Teichner filtration of link concordance. ", "machine_abstract": "We study the link concordance group $\\mathcal C_n$ for an integer $n\\geq 3$ by using iterated $p$-coverings.  We show that if $L\\in \\mathcal C_n$ is obtained as the result of applying an iterated $p$-covering to some knot $K$ in $S^3$ with $p<n$ then there exists a finite sequence of knots $(K_0,\\ldots,K_r)$ such that:  $K_0$ is the unknot; each $K_i$ has at most one component which is not slice (and this occurs only when $i=1$), and $L=K_r$. This gives rise to a filtration on $\\mathcal C_n$ whose associated graded object is isomorphic to the direct sum of the groups $H^1(M;\\mathbb Z/p)$ where $M$ runs over all smooth manifolds of dimension $n$ equipped with a spin structure. In particular we obtain new proofs of:  (1) The order of the torsion subgroup of $\\mathcal C_n$ divides $2^{n-2}$. (2) If $L\\in \\mathcal C_n\\setminus \\mathcal C_{n+1}$ is prime then it is slice. (3) There are infinitely many primes in $\\mathcal C_n$.", "paraphrased_abstract": "It's possible to measure the order of the torsion subgroup of mathcal n with a number of mathcal nsetminus mathcal n+1, so that if mathcal nsetminus mathcal n+1 is prime, then mathcal n+1 is a prime. Then a subgroup of mathcal mathcal nsetminus mathcal n+1 is prime, so that mathcal n+1 is a prime. mathcal n3 is a composite mathcal n3 in S33, where there is an infinite number of knots  (K_0, ldots, r) in a linear, mathcal mathcal n, 132) in a mathcal n3, as mathcal n12 ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1524, "title": "Studies of Cosmic Rays with GeV Gamma Rays", "abstract": "  We describe the role of GeV gamma-ray observations with GLAST-LAT (Gamma-ray Large Area Space Telescope - Large Area Telescope) in identifying interaction sites of cosmic-ray proton (or hadrons) with interstellar medium (ISM). We expect to detect gamma rays from neutral pion decays in high-density ISM regions in the Galaxy, Large Magellanic Cloud, and other satellite galaxies. These gamma-ray sources have been detected already with EGRET (Energetic Gamma Ray Experiment Telescope) as extended sources (eg. LMC and Orion clouds) and GLAST-LAT will detect many more with a higher spatial resolution and in a wider spectral range. We have developed a novel image restoration technique based on the Richardson-Lucy algorithm optimized for GLAST-LAT observation of extended sources. Our algorithm calculates PSF (point spread function) for each event. This step is very important for GLAST-LAT and EGRET image analysis since PSF varies more than one order of magnitude from one gamma ray to another depending on its energy as well as its impact point and angle in the instrument. The GLAST-LAT and EGRET image analysis has to cope with Poisson fluctuation due to low number of detected photons for most sources. Our technique incorporates wavelet filtering to minimize effects due to the fluctuation. Preliminary studies on some EGRET sources are presented, which shows potential of this novel image restoration technique for the identification and characterisation of extended gamma-ray sources. ", "machine_abstract": "The Fermi Large Area Telescope (LAT) is the primary instrument on NASA's Fermi satellite, launched in June 2008 to study high-energy gamma rays and cosmic rays. The LAT has been taking data since August 4, 2008, and it continues to operate today. This talk will present highlights of recent results obtained by the LAT collaboration using this unique dataset. These include new measurements of the diffuse extragalactic background light at high energies, constraints on dark matter annihilation models, and studies of blazars as potential sources for ultra-high energy cosmic rays. I will also discuss some of the challenges associated with analyzing such large datasets. Finally, I will briefly describe plans for future observations that could be made with the LAT. Keywords: High-Energy Astrophysics, Dark Matter, Blazar, Ultra-High Energy Cosmic Ray, Diffuse Extragalactic Background Light. Title: Studies of Cosmic Rays With GeV Gamma Rays", "paraphrased_abstract": "I shall also discuss the future plans for the use of the LAT. The Fermi Large Area Telescope (LAT) is the principal instrument on NASA\u2019s satellite, launched in June 2008, and is used for studies of gamma rays and cosmic rays. It was launched in June 2008 to study the gamma rays and the cosmic rays. This LAT has been operating since August 4, 2008 and continues to operate to this day. The LAT, launched in June 2008, is the primary instrument on the satellite, launched by NASA to study cosmic rays. It is the first of its kind on the planet, and it has been taking data since its inception in June 2008. The LAT is the first of its kind on the Earth, and it is the primary instrument on the satellite. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2323, "title": "Bandstructure and optical properties of $\\alpha-LiIO_{3}$ crystal", "abstract": "  The bandstructure was calculated by the full-potential linearized augmented plane wave method. The result reveals two important insights to the novel second harmonic generation (SHG) of alpha-phase lithium iodate ($\\alpha-LiIO_{3}$) crystal: the existence of finite intra-band momentum matrix elements due to the non-inversion symmetry of the crystal illuminating the potential of the intra-band transition, and the strong covalent bonding between the $I$-atoms and the ligand $O$-atoms resulting the condition of the double-resonance. An inter-band transition scenario in SHG as $\\alpha-LiIO_{3}$ in nano-structure is proposed. The optical properties were calculated within the theoretical framework of the time-dependent perturbation of the independent-particle model. The dielectric tensors and the refractive index were evaluated. Comparisons between the predictions and the results were made: the x-ray near edge absorption spectra; the refractive index at the static limit, and at finite frequencies. Possible factors attributing the calculation errors is discussed. ", "machine_abstract": "The electronic structure, the density of states (DOS) and the bandgap energy are calculated by using the full-potential linearized augmented plane wave method within the local-density approximation (LDA). The results show that the DOS is dominated by O-2p orbitals in valence bands while I-5s orbital dominates conduction bands. The direct gap at \u0393 point is found to be 1.8 eV which agrees well with experimental value. The imaginary part of dielectric function shows two peaks around 2.5 eV and 4.0 eV corresponding to transitions between O-2p and I-5s orbitals respectively. The real part of dielectric function exhibits one peak near 3.6 eV due to interband transition. The refractive index decreases rapidly as photon energy increases beyond 5 eV. The reflectivity spectrum has three peaks located at about 0.9 eV, 2.2 eV and 3.7 eV.", "paraphrased_abstract": "The optical properties are represented by three peaks, one near 0.9 eV, two near 2.2 eV, and one near 3.7 eV. The electronic structure, the density of states, and the bandgap energy are obtained by the full-potential linearized method of the LDA. The density of states in the valence band, and the density of the conduction band in the conduction band, is dominated by O-2p. The optical density is dominated by O-2p, and I-5 is dominated by conduction bands. The optical density is dominated by O-2p, whereas I-5 is dominant in conduction bands. The imaginary part of the dielectric spectrum has two peaks at 2.5 eV and 4.0 eV, the peaks of the O-2p and I-5 orbitals. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2102, "title": "The lower mass function of the young open cluster Blanco 1: from 30 Mjup to 3 Mo", "abstract": "  We performed a deep wide field optical survey of the young (~100-150 Myr) open cluster Blanco1 to study its low mass population well down into the brown dwarf regime and estimate its mass function over the whole cluster mass range.The survey covers 2.3 square degrees in the I and z-bands down to I ~ z ~ 24 with the CFH12K camera. Considering two different cluster ages (100 and 150 Myr), we selected cluster member candidates on the basis of their location in the (I,I-z) CMD relative to the isochrones, and estimated the contamination by foreground late-type field dwarfs using statistical arguments, infrared photometry and low-resolution optical spectroscopy. We find that our survey should contain about 57% of the cluster members in the 0.03-0.6 Mo mass range, including 30-40 brown dwarfs. The candidate's radial distribution presents evidence that mass segregation has already occured in the cluster. We took it into account to estimate the cluster mass function across the stellar/substellar boundary. We find that, between 0.03Mo and 0.6Mo, the cluster mass distribution does not depend much on its exact age, and is well represented by a single power-law, with an index alpha=0.69 +/- 0.15. Over the whole mass domain, from 0.03Mo to 3Mo, the mass function is better fitted by a log-normal function with m0=0.36 +/- 0.07Mo and sigma=0.58 +/- 0.06. Comparison between the Blanco1 mass function, other young open clusters' MF, and the galactic disc MF suggests that the IMF, from the substellar domain to the higher mass part, does not depend much on initial conditions. We discuss the implications of this result on theories developed to date to explain the origin of the mass distribution. ", "machine_abstract": "We present new photometry in the optical and near-infrared bands for the open cluster Blanco 1 (M35). The data were obtained with the 2 m telescope at the Observatorio Astron\u00f3mico Nacional de San Pedro M\u00e1rtir, Baja California Sur, Mexico; the 0.9 m telescope at the Cerro Tololo Inter-American Observatory, Chile; and the Nordic Optical Telescope, La Palma, Canary Islands, Spain. We have used these observations together with archival data from the Hubble Space Telescope archive to derive an improved age estimate for this open cluster. Our results show that M35 is about 300 Myr old, which makes it one of the youngest open clusters known within 100 pc of the Sun. Using our derived age we have determined the initial mass function (IMF) of M35 down to masses as low as 3 MJUP . This IMF shows no evidence for any significant deviation from the Salpeter power law index over more than three orders of magnitude in mass.", "paraphrased_abstract": "In order to get the mass of the group, we have calculated the first mass in terms of the first mass in the first order of the three orders of the scale. The results of this calculation and the archives of the Hubble Space Telescope confirm that the age of the group is about 300 years old. It is one of the youngest clusters known within a hundred miles of the sun. We use these results to estimate the age of M35. Its distance is only a couple of kilometers, and its size is about 300 metres in diameter. We also present the results of the two meter telescope of the Observatorio Astronommico Nacional de San Pedro M\u00e1rtir, Baja California Sur, Mexico; the 0.9 meter telescope of the Cerro Tololo InterAmerican Observatory, Chile; the Nordic Optical Telescope, La Palma, Canary Islands, Spain. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.343, "title": "Prospects for precision measurements of atomic helium using direct frequency comb spectroscopy", "abstract": "  We analyze several possibilities for precisely measuring electronic transitions in atomic helium by the direct use of phase-stabilized femtosecond frequency combs. Because the comb is self-calibrating and can be shifted into the ultraviolet spectral region via harmonic generation, it offers the prospect of greatly improved accuracy for UV and far-UV transitions. To take advantage of this accuracy an ultracold helium sample is needed. For measurements of the triplet spectrum a magneto-optical trap (MOT) can be used to cool and trap metastable 2^3S state atoms. We analyze schemes for measuring the two-photon $2^3S \\to 4^3S$ interval, and for resonant two-photon excitation to high Rydberg states, $2^3S \\to 3^3P \\to n^3S,D$. We also analyze experiments on the singlet-state spectrum. To accomplish this we propose schemes for producing and trapping ultracold helium in the 1^1S or 2^1S state via intercombination transitions. A particularly intriguing scenario is the possibility of measuring the $1^1S \\to 2^1S$ transition with extremely high accuracy by use of two-photon excitation in a magic wavelength trap that operates identically for both states. We predict a ``triple magic wavelength'' at 412 nm that could facilitate numerous experiments on trapped helium atoms, because here the polarizabilities of the 1^1S, 2^1S and 2^3S states are all similar, small, and positive. ", "machine_abstract": "We present the prospects for high-precision measurement of the 1s2p 3P-1s2s 3S transition in atomic helium with an optical frequency comb (OFC). The OFC is stabilized to a high-finesse cavity and locked to a narrow linewidth laser at 1083 nm, which serves as a local oscillator. We show that this system can be used to measure the absolute frequencies of two transitions in helium with uncertainties below 100 kHz. This will allow us to determine the fine-structure constant \u03b1 with relative uncertainty better than 2\u00d710\u221210 by measuring the ratio between these two frequencies. In addition we demonstrate how the same setup could be used to perform tests of fundamental physics beyond the Standard Model such as searches for time variation of fundamental constants or violations of Lorentz invariance. Optical frequency combs are powerful tools for precise metrology [1\u20133] . They have been successfully applied to many different fields including ultra-stable lasers [4] , gravitational wave detection [5] , and quantum optics [6] . In particular they provide unprecedented possibilities for high-precision measurement [7\u20139] . Here we propose to use them to improve our knowledge on the value of the fine structure constant [10] . To achieve this goal it is necessary to measure the absolute frequencies f(1s2p 3P1) = 929 072 631 770 Hz [11] and f(1s2s 3S1) = 929 073 761 828 Hz [12] of two transitions in helium. These values were determined previously with uncertainties of about 300 kHz [13] but recent theoretical calculations suggest that their accuracy may be improved significantly [14\u201318] .", "paraphrased_abstract": "A tessellation is necessary in order to determine the absolute frequencies of two transitions in helium. This measurement was obtained in the previous two hundred kHz. But now new theories suggest that the accuracy may be better than 300 kHz. We present here the possibilities of a high-precision measurement of the transitions in helium, and show how this measurement can be used for the investigation of fundamental physics beyond the Standard Model., for example, to probe the time variation of the fundamental constants, to deviate from the Lorentz invariance. This method is well suited to the measurement of the transitions in helium, and in particular can be used to measure the relative frequencies of two transitions in helium. We show that the system can be used to measure the absolute frequencies of the transitions in helium, with uncertainties of no more than 100 kHz. We demonstrate that the ratio between the two frequencies is less than 10 1010 if we measure the ratio between the two frequencies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2184, "title": "A remarkable moduli space of rank 6 vector bundles related to cubic surfaces", "abstract": "  We study the moduli space $\\fM^s(6;3,6,4)$ of simple rank 6 vector bundles $\\E$ on $\\PP^3$ with Chern polynomial $1+3t+6t^2+4t^3$ and properties of these bundles, especially we prove some partial results concerning their stability. We first recall how these bundles are related to the construction of sextic nodal surfaces in $\\PP^3$ having an even set of 56 nodes (cf. \\cite{CaTo}). We prove that there is an open set, corresponding to the simple bundles with minimal cohomology, which is irreducible of dimension 19 and bimeromorphic to an open set $\\fA^0$ of the G.I.T. quotient space of the projective space $\\fB:=\\{B\\in \\PP(U^\\vee\\otimes W\\otimes V^\\vee)\\}$ of triple tensors of type $(3,3,4)$ by the natural action of $SL(W)\\times SL(U)$. We give several constructions for these bundles, which relate them to cubic surfaces in 3-space $\\PP^3$ and to cubic surfaces in the dual space $(\\PP^3)^{\\vee}$. One of these constructions, suggested by Igor Dolgachev, generalizes to other types of tensors. Moreover, we relate the socalled {\\em cross-product involution} for $(3,3,4)$-tensors, introduced in \\cite{CaTo}, with the Schur quadric associated to a cubic surface in $\\PP^3$ and study further properties of this involution. ", "machine_abstract": "We study the moduli spaces of stable rank six vector bundles on smooth cubic surfaces, and their relation with the geometry of these surfaces. We show that there is an open subset in this moduli space which parametrizes families of indecomposable vector bundles whose general member has no global sections but admits a filtration by line bundles such that each successive quotient is globally generated. This gives rise to a natural map from the moduli space into the Picard variety of the surface. The image of this map turns out to be a divisor in the Picard variety, called the Brill-Noether locus. In particular we prove that for any given integer n > 0, if the Brill-Noether number $\\nu(n)$ equals one then the corresponding Brill-Noether divisor contains infinitely many rational curves. Finally, using results about the cohomology ring of the moduli space, we give examples of indecomposable rank six vector bundles over smooth cubic surfaces having exactly two linearly independent global sections.", "paraphrased_abstract": "Using these results we present two linearly independent, inhomogeneous moduli for smooth cubic surfaces. Among these moduli we present examples of indecomposable moduli of five-pointed six-pointed vectors, with just two radii of cohomology. For each integer n1 n is a corresponding radii of Brill-Noether, and the result is that for every radii of Brill-Noether, one has infinitely many rational curves. Thus we introduce the moduli of five-pointed six-pointed vectors in smooth cubic surfaces and investigate their relation to the geometry of those surfaces. The moduli of this moduli space is divided into subsets of a sort which comprises of indecomposable vectors whose general members have no global section but which are entangled in a filtered enfolding of line bundles in order to create a global quotient. We have found that in this subset there is a void in the moduli space which parametrizes families of indecomposable vectors whose general parts have no global sections, but which allow for a filtration by line bundles to create an infinitely large number of linear curves. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1436, "title": "Hybrid inflation followed by modular inflation", "abstract": "  Inflationary models with a superheavy scale F-term hybrid inflation followed by an intermediate scale modular inflation are considered. The restrictions on the power spectrum P_R of curvature perturbation and the spectral index n_s from the recent data within the power-law cosmological model with cold dark matter and a cosmological constant can be met provided that the number of e-foldings N_HI* suffered by the pivot scale k_*=0.002/Mpc during hybrid inflation is suitably restricted. The additional e-foldings needed for solving the horizon and flatness problems are generated by modular inflation with a string axion as inflaton. For central values of P_R and n_s, the grand unification scale comes out, in the case of standard hybrid inflation, close to its supersymmetric value M_GUT=2.86 x 10^16 GeV, the relevant coupling constant is relatively large (0.005-0.14), and N_HI* is between 10 and 21.7. In the shifted [smooth] hybrid inflation case, the grand unification scale can be identified with M_GUT for N_HI*=21 [N_HI*=18]. ", "machine_abstract": "We consider the possibility that our universe underwent two stages of accelerated expansion, first hybrid inflation and then modular inflation.  We show how this scenario can be realized in string theory with an explicit example based on type IIB orientifolds compactified to four dimensions on Calabi-Yau threefolds. In particular we find that there are many possible realizations of such models which lead to realistic values for the cosmological parameters. The model is consistent with all current experimental constraints including those coming from measurements of the cosmic microwave background anisotropies as well as from direct searches at colliders. Finally we discuss some phenomenological aspects of these scenarios. Introduction: Inflationary theories provide one of the most compelling explanations for several puzzles associated with the standard hot big bang cosmology [1] . They predict that primordial quantum fluctuations generated during inflation should have left their imprint on the temperature anisotropies observed today in the Cosmic Microwave Background (CMB) [2] . In recent years it has been shown that supersymmetric grand unified theories (GUTs), like SO(10) , naturally give rise to inflationary potentials [3] , while also providing a successful unification scheme [4] . However, GUT scale inflation suffers from the so-called \u03b7-problem [5] : the predicted value of the tensor-to-scalar ratio r = 16\u01eb H /\u03b7 2 [6] leads to too large CMB quadrupole anisotropies [7, 8] unless \u01eb H \u226a 1 [9] or \u03b7 \u226b 10 \u22129 [10] . This problem may be alleviated if the inflaton potential contains flat directions [11] . These arise quite generically in supergravity [12] and string theory [13] due to non-perturbative effects [14] . A particularly interesting class of flat directions arises when the gauge group is broken down to its maximal subgroup [15] . Such flat directions were studied extensively in [16] where they were called \"moduli\" fields since they parametrize the size and shape of extra dimensions [17] . Moduli fields play an important role in string theory [18] because they determine the vacuum expectation values of various moduli fields appearing in the low energy effective action [19]", "paraphrased_abstract": "It was shown in the late 1970s that the cosmological supersymmetry of the unified physics (SO) [11], a general theory of supersymmetricity, naturally gave rise to the inflation potential. However, the GUT scale of inflation, with its so-called problem, suffers from the so-called -problem, and the corresponding ratio of tensor to scalar is r = 16H  22  2, it leads to too large quadrupoles  1,  10  10  11, it is also a problem of the non-perturbative effects. In addition, the general model of the sphere of the universe is presented in detail, a threefold model of the Calabi-Yau equation, a particular example of which is based on the type IIB sphere of the tropics, a very accelerated inflation and a modular inflation, and the synthesis of the model is discussed. There are many possibilities for the sphere of the universe, but the one that is most interesting is that it contains a flat line (especially when the gauge group is broken down to the maximal level). These lines are generally studied in supergravity and string theory, where they are called \"moduli\" since they are the idealization of the extradimensional dimensions. In addition", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0338, "title": "Synergistic Effects of MoDTC and ZDTP on Frictional Behaviour of Tribofilms at the Nanometer Scale", "abstract": "  The layered structure and the rheological properties of anti-wear films, generated in a rolling/sliding contact from lubricants containing zinc dialkyldithiophosphate (ZDTP) and/or molybdenum dialkyldithiocarbamate (MoDTC) additives, have been studied by dynamic nanoindentation experiments coupled with a simple modelling of the stiffness measurements. Local nano-friction experiments were conducted with the same device in order to determine the evolution of the friction coefficient as a function of the applied pressure for the different lubricant formulations. For the MoDTC film, the applied pressure in the friction test remains low (<0.5 GPa) and the apparent friction coefficient is high ($\\mu$ > 0.4). For the tribofilms containing MoDTC together with ZDTP, which permits the applied pressure to increase up to a few GPa through some accommodation process, a very low friction domain appears (0.01 < $\\mu$ < 0.05), located a few nanometers below the surface of the tribofilm. This low friction coefficient is attributed to the presence of MoS2 planes sliding over each other in a favourable configuration obtained when the pressure is sufficiently high, which is made possible by the presence of ZDTP. ", "machine_abstract": "Tribological properties are important for many applications, such as mechanical systems, transportation devices, energy conversion equipment, etc., where friction is an unwanted effect that causes wear and tear to materials. In this study, we investigated the effects of molybdenum disulfide (MoS2) nanoparticles with different sizes in combination with zinc dialkyldithiophosphate (ZDDP), which has been widely used as a lubricant additive, on tribofilm formation under boundary lubrication conditions using atomic force microscopy (AFM). The results show that the addition of MoS2 nanoparticles can significantly improve the frictional behaviour of the tribofilms formed by ZDDP additives through synergistic interactions between the two components. This improvement was found to be dependent upon both the size and concentration of the added MoS2 particles.     Keywords: nanomaterials, friction, tribochemistry, AFM", "paraphrased_abstract": "In this work, we investigated the effect of Molybdenum disulfide in different sizes in combination with zinc dithiophosphate, a widely used lubricant, and tribose-compliant lubricant. This effect was not due to the small size of the tribose, but was to be determined by the chemistry of the particles. We found that, by adding the tribose-compliant lubricants, they reacted with the lubricants in the boundary lubrication. In this study, we examined the effects of molybdenum disulfide, a lubricant, and zinc dithiophosphate, which is widely used for lubricants, on the formation of tribofilms. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0519, "title": "Cosmic Evolution of Black Holes and Spheroids. II: Scaling Relations at z=0.36", "abstract": "  We combine Hubble Space Telescope images of a sample of 20 Seyfert galaxies at z=0.36 with spectroscopic information from the Keck Telescope to determine the black hole mass - spheroid luminosity relation (M-L), the Fundamental Plane (FP) of the host galaxies and the M-sigma relation. Assuming pure luminosity evolution, we find that the host spheroids had smaller luminosity and stellar velocity dispersion than today for a fixed M. The offsets correspond to Delta log L_B,0=0.40+-0.11+-0.15 (Delta log M = 0.51+-0.14+-0.19) and Delta log sigma = 0.13+-0.03+-0.05 (Delta log M = 0.54+-0.12+-0.21), respectively for the M-L and M-sigma relation. A detailed analysis of known systematic errors and selection effects shows that they cannot account for the observed offset. The data are inconsistent with pure luminosity evolution and the existence of universal and tight scaling relations. To obey the three local scaling relations by z=0 the distant spheroids have to grow their stellar mass by approximately 60% (\\Delta log M_sph=0.20+-0.14) in the next 4 billion years. The measured evolution can be expressed as M/ M_sph ~ (1+z)^{1.5+-1.0}. Based on the disturbed morphologies of a fraction of the sample (6/20) we suggest collisional mergers with disk-dominated systems as evolutionary mechanism. ", "machine_abstract": "We present the results of an optical spectroscopic survey of galaxies in the redshift range 0.3 <z<0.7, carried out with FORS2 on VLT-UT1 (Antu). The sample consists of about 1000 objects selected by their photometric properties to be early-type galaxies or QSOs. We have measured redshifts for 97% of them using multi-object spectroscopy. In addition we obtained high-resolution imaging data in B,V,R,I bands for all targets observed spectroscopically. This dataset is used to study scaling relations between galaxy luminosity, size, velocity dispersion and black hole mass as well as correlations among these quantities themselves. Our main findings are summarized below:  -The M BH \u2212 \u03c3 relation evolves very little over this redshift interval.  -There is no evidence that the slope of the M BH \u2212 L bulge relation changes significantly with redshift; however there may be some evolution in its zero point.  -The scatter around both the M BH \u2212\u03c3 and M BH \u2212L bulge relations increases towards higher redshifts.", "paraphrased_abstract": "We describe in detail the results of an optical survey of galaxies of 0.3 z0.7, conducted by FORS2 on the ANU1 (Antu) and gathered from them about 1000 objects, which we identified as early-type galaxies or QSOs, and which we have mapped to redshifts of 90% of them. This survey has been carried out with fors2 on VLT-UT1 (Antu). The results of the survey are summarized below: - The slope of the M BH   relation is very little, whereas the elongation of the M BH  relation is somewhat resolute; the scatter around the M BH  and M BH  relations increases as redshift increases. The scatter around the M BH  and M BH  relations increases with increasing redshifts. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0016, "title": "A Covariant Holographic Entanglement Entropy Proposal", "abstract": "  With an aim towards understanding the time-dependence of entanglement entropy in generic quantum field theories, we propose a covariant generalization of the holographic entanglement entropy proposal of hep-th/0603001. Apart from providing several examples of possible covariant generalizations, we study a particular construction based on light-sheets, motivated in similar spirit to the covariant entropy bound underlying the holographic principle. In particular, we argue that the entanglement entropy associated with a specified region on the boundary in the context of the AdS/CFT correspondence is given by the area of a co-dimension two bulk surface with vanishing expansions of null geodesics. We demonstrate our construction with several examples to illustrate its reduction to the holographic entanglement entropy proposal in static spacetimes. We further show how this proposal may be used to understand the time evolution of entanglement entropy in a time varying QFT state dual to a collapsing black hole background. Finally, we use our proposal to argue that the Euclidean wormhole geometries with multiple boundaries should be regarded as states in a non-interacting but entangled set of QFTs, one associated to each boundary. ", "machine_abstract": "We propose an entanglement entropy proposal for the covariant holography in AdS/CFT correspondence, which is based on the idea that the entanglement entropy can be obtained by integrating out all degrees of freedom except those inside some region A. We show how this proposal works with two examples and compare it to other proposals. In particular we find that our proposal agrees exactly with the result given by Ryu-Takayanagi formula when applied to pure states. Our results also suggest that there should exist a connection between the entanglement entropy and the modular flow in quantum field theory. PACS: 04.50.+h, 11.10.Wx, 12.38.-t, 98.80.Cq Keywords: Entanglement entropy, holography, quantum gravity, quantum field theory. 1 Introduction The entanglement entropy has been widely used as one important tool to study strongly coupled systems such as condensed matter physics [1] , black holes [2] - [4] . It was first introduced into the context of quantum information theory [5] where it measures the amount of correlations among different parts of a system [6] . In recent years, the entanglement entropy has attracted much attention due to its close relation to the gravitational dual description [7, 8] . For example, the entanglement entropy of a CFT state corresponds to the area of minimal surface in the bulk geometry [9] . This remarkable discovery led to many interesting applications including the calculation of the central charge [10] , the proof of c-theorem [11] , the investigation of phase transitions [12] , etc.. However, most of these studies are restricted to static backgrounds or time-independent situations. Recently, several attempts have been made to generalize the concept of entanglement entropy to dynamical cases [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50", "paraphrased_abstract": "But this is only a case of staticity, and does not involve time. Recent attempts have been made to generalize the entanglement entropy to dynamic cases. These include the determination of the central charge, the proof of the c-theorem, the investigation of phase transitions, etc. The entanglement entropy has been gaining in importance, for it is a function of the gravitational dual description of the system, and in the recent years, the entanglement entropy has received much attention. The entanglement entropy of a state corresponding to the area of minimal surface of the bulk geometry. We use this method to calculate the entanglement entropy of a state in a manner that is in accordance with the formula for the entanglement entropy of the A-C-F. entropy. In our model, we are based on the principle that the entanglement entropy can be obtained by integrating all degrees of freedom except those in A. This formula, in contrast to others, corresponds to the formula given by Ryu Takayanagi, for the pure state. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3676, "title": "Virtual Photon Emission from Quark-Gluon Plasma", "abstract": "  We recently proposed an empirical approach for the Landau-Pomeranchuk-Migdal (LPM) effects in photon emission from the quark gluon plasma as a function of photon mass. This approach was based on Generalized Emission Functions (GEF) for photon emission, derived at a fixed temperature and strong coupling constant. In the present work, we have extended the LPM calculations for several temperatures and strong coupling strengths. The integral equations for (${\\bf \\tilde{f}(\\tilde{p}_\\perp)}$) and ($\\tilde{g}({\\bf \\tilde{p}_\\perp})$) are solved by the iterations method for the variable set \\{$p_0,q_0,Q^2,T,\\alpha_s$\\}, considering bremsstrahlung and $\\bf aws$ processes. We generalize the dynamical scaling variables, $x_T$, $x_L$, for bremsstrahlung and {\\bf aws} processes which are now functions of variables $p_0,q_0,Q^2,T,\\alpha_s$. The GEF introduced earlier, $g^b_T$, $g^a_T$, $g^b_L$, $g^a_L$, are also generalized for any temperatures and coupling strengths. From this, the imaginary part of the photon polarization tensor as a function of photon mass and energy can be calculated as a one dimensional integral over these GEF and parton distribution functions in the plasma. However, for phenomenological studies of experimental data, one needs a simple empirical formula without involving parton momentum integrations. Therefore, we present a phenomenological formula for imaginary photon polarization tensor as a function of \\{$q_0,Q^2,T,\\alpha_s$\\} that includes bremsstrahlung and $\\bf aws$ mechanisms along with LPM effects. ", "machine_abstract": "We propose that the virtual photon emission in quark-gluon plasma is described by the following formulae:  where  and are the energy-momentum tensors for quarks and gluons, respectively;  is the number density of quarks or gluons;  is the four-velocity vector of the fluid elements;  is the temperature of the fluid elements.  is the chemical potential of quarks (or antiquarks);  is the Debye screening mass;  is the coupling constant between quarks and gluon;  is the color factor which takes into account the fact that there exist eight different types of quarks and anti-quarks with three colors each.  The first term on right-hand side of Eq. (1)  describes the contribution to the total emissivity due to the thermal motion of quarks and gluon gas. The second term represents the effect of collective flow of quarks and glueballs.", "paraphrased_abstract": "The first part of the equation, Eq. (1), describes the contribution of enthalpy to the total emissivity of the gases. The second part is the contribution of the flow of quarks and gluons. The chemical potential of quarks or antiquarks is the polarity of Debye; the coupling constant between quarks and gluons is the color factor, taking into account that there are eight types of quarks and antiquarks with three colors. The virtual photon emission of the gaseous quark-gluon plasma is described by the following formulae: a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, and a, a, a, a, a, a, ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1047, "title": "Chromohydrodynamical instabilities induced by relativistic jets", "abstract": "  We study the properties of the chromohydrodynamical instabilities induced by a relativistic jet that crosses the quark-gluon plasma. Assuming that the jet of particles and the plasma can be described using a hydrodynamical approach, we derive and discuss the dispersion laws for the unstable collective modes. In our analysis the chromohydrodynamical equations for the collective modes are tackled in the linear response approximation. Such an approximation, valid for short time scales, allows to study in a straightforward way the dependence of the dispersion laws of the collective modes on the velocity of the jet, on the magnitude of the momentum of the collective mode and on the angle between these two quantities. We find that unstable modes arise for velocity of the jet larger than the speed of the sound of the plasma and only modes with momenta smaller than a certain values are unstable. Moreover, for ultrarelativistic velocities of the jet the longitudinal mode becomes stable and the most unstable modes correspond to relative angles between the velocity of the jet and momentum of the collective mode larger than $\\sim \\pi/8$. Our results suggest an alternative mechanism for the description of the jet quenching phenomenon, where the jet crossing the plasma loses energy exciting colored unstable modes. ", "machine_abstract": "We study the evolution of magnetized, rotating flows in the presence of strong magnetic fields and rotation using 3D numerical simulations with ideal MHD equations. We find that when the initial flow is dominated by toroidal field lines (Btor/Bp = 0.5), it becomes unstable to non-axisymmetric perturbations at t ~ 1.2P0/c where P0 is the initial pressure scale height. The instability leads to the formation of helical structures which are similar to those observed in many astrophysical systems such as protostellar disks or AGN accretion disks. In addition we also observe another type of instability for initially poloidal-dominated flows (Btor/Bp < 0.1) which develops into an axisymmetric spiral structure. This instability can be understood as a Rossby wave instability driven by differential rotation between the disk and the corona. Finally, we show that these two types of instabilities lead to different observational signatures.", "paraphrased_abstract": "We now show that a second type of instability can be observed in the initial poloidal flow (Btor/Bp  0.1) and develops into an axisymmetric spiral, like those observed in many astrophysical systems, such as protostellar disks and AGN accretion disks. In addition, we observe another type of instability in the early poloidal flows (Btor/Bp  0.1) which evolves into an axisymmetric spiral, as in Rossby waves that arise from the differential rotation of the disk and the corona. In this case, we study the evolution of magnetized and rotating flows in the presence of strong magnetic fields and rotations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1923, "title": "A Novel phase in the phase structure of the $(g\\phi^4 + h\\phi^6)_{1+1}$ field theoretic model", "abstract": "  In view of the newly discovered and physically acceptable $PT$ symmetric and non-Hermitian models, we reinvestigated the phase structure of the ($g\\phi^{4}+h\\phi^{6}$)$_{1+1}$ Hermitian model. The reinvestigation concerns the possibility of a phase transition from the original Hermitian and $PT$ symmetric phase to a non-Hermitian and $PT$ symmetric one. This kind of phase transition, if verified experimentally, will lead to the first proof that non-Hermitian and $PT$ symmetric models are not just a mathematical research framework but are a nature desire. To do the investigation, we calculated the effective potential up to second order in the couplings and found a Hermitian to Non-Hermitian phase transition. This leads us to introduce, for the first time, hermiticity as a symmetry which can be broken due to quantum corrections, \\textit{i.e.}, when starting with a model which is Hermitian in the classical level, quantum corrections can break hermiticity while the theory stays physically acceptable. In fact, ignoring this phase will lead to violation of universality when comparing this model predictions with other models in the same class of universality. For instance, in a previous work we obtained a second order phase transition for the $PT$ symmetric and non-Hermitian $(-g\\phi^{4})$ and according to universality, this phase should exist in the phase structure of the ($g\\phi^{4}+h\\phi^{6}$) model for negative $g$. Finally, among the novelties in this letter, in our calculation for the effective potential, we introduced a new renormalization group equation which describes the invariance of the bare vacuum energy under the change of the scale. We showed that without this invariance, the original theory and the effective one are inequivalent. ", "machine_abstract": "We study the phase structure of the following 1+1 dimensional quantum field theory with two coupling constants, $g,h$, $$ \\mathcal{L} = \\frac{1}{2} (\\partial \\phi_1)^2 - \\frac{1}{\\lambda_1^2} \\phi_1^2 + g \\phi_1^4 + h \\phi_1^6 $$  where $\\lambda_1$ is an energy scale parameter and $\\phi_i$ are real scalar fields.  We find that there exists a novel phase where both the massless and massive excitations exist simultaneously for some range of values of $g$ and $h$. This new phase can be regarded as a generalization of the Higgs phase to higher order interactions.  The existence of this novel phase has been confirmed by numerical simulations using the lattice regularization method. The results obtained here may have important implications on our understanding of strongly interacting systems such as QCD at finite temperature or density. In particular, we expect that the present work will shed light on the nature of chiral symmetry breaking in QCD.", "paraphrased_abstract": "But we are looking forward to the results of our study in the nature of QCD. We hope that our results will provide an important new insight into the structure of QCD. We have now studied the phase structure of the following 1+1 dimensional quantum field theory. There are two constants, g, h, where g, h, are energy parameters, and phii is a real scalar field. Hence, we conclude that the new phase is a generalization of the Higgs phase to higher orders of magnitude. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4476, "title": "Network tomography based on 1-D projections", "abstract": "  Network tomography has been regarded as one of the most promising methodologies for performance evaluation and diagnosis of the massive and decentralized Internet. This paper proposes a new estimation approach for solving a class of inverse problems in network tomography, based on marginal distributions of a sequence of one-dimensional linear projections of the observed data. We give a general identifiability result for the proposed method and study the design issue of these one dimensional projections in terms of statistical efficiency. We show that for a simple Gaussian tomography model, there is an optimal set of one-dimensional projections such that the estimator obtained from these projections is asymptotically as efficient as the maximum likelihood estimator based on the joint distribution of the observed data. For practical applications, we carry out simulation studies of the proposed method for two instances of network tomography. The first is for traffic demand tomography using a Gaussian Origin-Destination traffic model with a power relation between its mean and variance, and the second is for network delay tomography where the link delays are to be estimated from the end-to-end path delays. We compare estimators obtained from our method and that obtained from using the joint distribution and other lower dimensional projections, and show that in both cases, the proposed method yields satisfactory results. ", "machine_abstract": "We propose an algorithm for network tomography that is able to reconstruct the internal structure of a network by using only one-dimensional (1-D) measurements, i.e., link counts between pairs of nodes in the network. The proposed method can be applied to any type of networks and does not require any prior knowledge about their topology or traffic patterns. We show how our approach can be used to estimate the number of active flows at each node as well as the amount of data transmitted over each flow. Our results are validated through extensive simulations performed with real Internet traces. Network tomography has been widely studied during recent years due to its potential applications in many areas such as computer security, quality-of-service provisioning, and traffic engineering [1] . In this context, it consists of estimating some properties of the network's internal state (such as the number of active flows per node or the amount of data transferred along each flow) by observing only external information (i.e., link-level statistics). This problem becomes particularly challenging when dealing with large-scale networks since the number of possible states grows exponentially with the size of the network [2] . In order to overcome these limitations, several approaches have been recently proposed which exploit specific characteristics of the underlying network [3] , e.g., sparsity [4] - [6] , symmetry [7] , or regularity [8] . However, most existing methods assume either complete knowledge of the network topology [9]- [11] or accurate estimates of the traffic matrix [12] - [14] . Unfortunately, both assumptions may not hold in practice [15] , especially if we consider large and/or dynamic networks [16] . For example, in IP-based networks, the exact location of routers cannot always be determined [17] while the traffic matrix is usually unknown [18] . Moreover, even if the network topology were known, collecting all necessary information would still be impractical because of scalability issues [19] . Finally, obtaining accurate estimates of the traffic...", "paraphrased_abstract": "Moreover, if there is no knowledge of the network topology, and even if it is known, it is difficult to gather all necessary information, because of the high scalability of the network. In the recent years, a great deal has been said about the application of network tomography, for it has the possibility of enabling various applications such as computer security, the provision of service, and traffic engineering. In such cases, one of the main difficulties in this regard is the fact that the network is always at the same location, and the traffic network is always at the same place, and that it is always at the same time unknown, so that one can not compute any information in advance, and that in a large network, the number of possible states increases exponentially with the size of the network. However, many methods have recently been proposed, which exploit certain characteristics of the network, such as sparsity, symmetry, and regularity. For example, in IP networks the exact location of routers is not always known, while the traffic matrix is often unknown. Thus, even if the network topology is known, it is still difficult to collect all the necessary information from all sources, which is still difficult, because of the scale. This problem is particularly difficult for large networks, because the number of possible states grows exponentially with the size of the network. We propose a method for network tomography that is effective in", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2969, "title": "Spatial separation of small and large grains in the transitional disk around the young star IRS 48", "abstract": "  We present spatially resolved mid-infrared images of the disk surrounding the young star IRS 48 in the Ophiuchus cloud complex. The disk exhibits a ring-like structure at 18.7 micron, and is dominated by very strong emission from polycyclic aromatic hydrocarbons at shorter wavelengths. This allows a detailed study of the relative distributions of small and large dust grains.   Images of IRS 48 in 5 mid-infrared bands from 8.6 to 18.7 micron as well as a low resolution N-band spectrum are obtained with VLT-VISIR. Optical spectroscopy is used to determine the spectral type of the central star and to measure the strength of the Halpha line.   The 18.7 micron ring peaks at a diameter of 110 AU, with a gap of ~ 60 AU. The shape of the ring is consistent with an inclination of i = 48 +- 8 degrees. In contrast, the 7.5-13 micron PAH emission bands are centered on the source and appear to fill the gap within the ring. The measured PAH line strengths are 10-100x stronger than those typically measured for young M0 stars and can only be explained with a high PAH abundance and/or strong excess optical/UV emission. The morphology of the images, combined with the absence of a silicate emission feature, imply that the inner disk has been cleared of micron-sized dust but with a significant population of PAHs remaining. We argue that the gap can be due to grain growth and settling or to clearing by an unseen planetary or low-mass companion. IRS 48 may represent a short-lived transitional phase from a classical to a weak-line T Tauri star. ", "machine_abstract": "We present new near-infrared (NIR) polarimetric observations of the Herbig Ae star HD 142527, which reveal that its circumstellar dust is composed of two distinct populations with different grain sizes. The polarization degree decreases rapidly towards longer wavelengths at all positions along our slit except for one position where it increases again between 2.2 and 3.8 microns. We interpret this as evidence for an inner hole in the distribution of larger grains. This interpretation is supported by SED modeling using radiative transfer calculations including scattering off spherical particles. Our results suggest that the outer edge of the gap lies within 0.1 AU of the central star. In addition to the NIR data presented here we also obtained mid-infrared (MIR) spectro-polarimetry covering the wavelength range 5-20 micron. These data show no significant change in the polarization degree across the MIR bands indicating that there are no strong changes in the optical properties of the dust grains on these scales.", "paraphrased_abstract": "At the same time, we present a new and affluent (NIR) polarization of the star HD 142527, showing that its planetary dust consists of two separate collections of grain of different sizes. This observation is supported by radiative calculations, which reveal a definite gap in the distribution of larger grains. The polarization reaches its shortest wavelengths in all places on the slit, except for one position at which it increases again to 2.2 to 3.8 nm. This indicates that there is an inner hole in the distribution of the larger grains. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1948, "title": "Electronic spin transport and spin precession in single graphene layers at room temperature", "abstract": "  The specific band structure of graphene, with its unique valley structure and Dirac neutrality point separating hole states from electron states has led to the observation of new electronic transport phenomena such as anomalously quantized Hall effects, absence of weak localization and the existence of a minimum conductivity. In addition to dissipative transport also supercurrent transport has already been observed. It has also been suggested that graphene might be a promising material for spintronics and related applications, such as the realization of spin qubits, due to the low intrinsic spin orbit interaction, as well as the low hyperfine interaction of the electron spins with the carbon nuclei. As a first step in the direction of graphene spintronics and spin qubits we report the observation of spin transport, as well as Larmor spin precession over micrometer long distances using single graphene layer based field effect transistors. The non-local spin valve geometry was used, employing four terminal contact geometries with ferromagnetic cobalt electrodes, which make contact to the graphene sheet through a thin oxide layer. We observe clear bipolar (changing from positive to negative sign) spin signals which reflect the magnetization direction of all 4 electrodes, indicating that spin coherence extends underneath all 4 contacts. No significant changes in the spin signals occur between 4.2K, 77K and room temperature. From Hanle type spin precession measurements we extract a spin relaxation length between 1.5 and 2 micron at room temperature, only weakly dependent on charge density, which is varied from n~0 at the Dirac neutrality point to n = 3.6 10^16/m^2. The spin polarization of the ferromagnetic contacts is calculated from the measurements to be around 10%. ", "machine_abstract": "We report on the observation of electronic spin transport across macroscopic distances (several millimeters) in suspended single-layer graphene devices at room temperature.  The experiments are performed using nonlocal spin-valve measurements with ferromagnetic contacts made by sputtering Co/Ni multilayers onto exfoliated flakes of graphite, followed by annealing to form cobalt silicide contacts. We observe that the resistance-area product RA is strongly dependent upon the contact geometry; for example, we find that it decreases significantly when the length of one of the contacts increases beyond 1 micron. This behavior can be explained by considering the effect of disorder scattering near the interface between the metal and the carbon atoms. In addition, we show that the magnitude of the spin signal depends sensitively on the relative orientation of the magnetization directions of the two ferromagnets. These results demonstrate that the observed spin signals arise primarily due to spin injection into the graphene layer rather than being dominated by proximity effects or other mechanisms associated with the magnetic contacts themselves.", "paraphrased_abstract": "They demonstrate that the observed magnetic signals are caused mainly by spin injected into the graphene layer, rather than by the influence of proximity or other mechanisms which are responsible for the magnetic contacts themselves. We present experimental evidence for the electronic transport of electromagnetic waves in the range of several millimetres in single-layer graphene devices, suspended at room temperature. The experiments are carried out with nonlocal electrostatic calculations using ferromagnetic contacts made by sputtering Co-Ni layers onto exfoliated flakes of graphite, followed by annealing to form cobalt silicide contacts. This property is mainly due to the eddying of the metal and the carbon atoms. We find that the intensity of the electrical eddying is particularly sensitive to the direction of the contact, as it varies from one ferromagnet to the other. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3017, "title": "Vortex in axion condensate as a dark matter halo", "abstract": "  We study the possibility of the vortices formation in axion condensate on the galactic scale. Such vortices can occur as a result of global rotation of the early universe. We study analytical models of vortices and calculate exemplary galaxy rotation curves. Depending on the setup it is possible to obtain a variety of shapes which give a good qualitative agreement with observational results. However, as we show, the extremely low velocity dispersions of the axions velocity are required to form the single vortex on the galactic scales. We find that the required velocity dispersion is of the order of $\\sigma \\approx 10^{-12} \\frac{m}{s}$. This is much smaller that predicted within the present understanding of the axion physics. The vortices in axion condensate can be however formed on the much smaller scales and give seeds to the galaxy formation and to their angular momenta. On the other hand, the vortices can be formed on the galactic scales, but only if the mass of the axion-like particles is of the order of $10^{-30}$eV. In this case, the particle de Broglie wavelength is comparable with the galactic diameter. This condition must be fulfilled in order to keep the coherence of the quantum condensate on the galactic scales. ", "machine_abstract": "We propose that the vortex structure is formed by an axion-like particle (ALP) condensation and can be considered as a candidate for dark matter halo.  We show that the ALP field forms a vortex configuration with a core radius of order $10^{-22}$ m, which corresponds to the size of galactic dark matter halo. The energy density inside the vortex is about $ 10^{16}$ GeV/cm3 , which agrees well with the observed value of dark matter density. In addition we find that the vortex has a very small mass density $\\sim 10^{-26} g/cm^3$ . This implies that the vortex does not affect gravitational lensing effect significantly. Finally we discuss possible detection methods of such vortices using astronomical observations. Introduction: Axions are hypothetical particles predicted by Peccei-Quinn mechanism [1]  to solve strong CP problem [2] . They have been studied extensively both theoretically [3] - [6] and experimentally [7] - [9] . In this letter, we consider the possibility that the axion field forms a vortex configuration [10] - [12] . Such a vortex may play important roles on cosmological scales because it could explain the origin of dark matter [13] - [16] .  First, we will show that the vortex has a core radius of order 10-22m corresponding to the size of galatic dark matter halo [17] . Second, we calculate the energy density inside the vortex and find that its magnitude is consistent with the observed value of the dark matter density [18] . Thirdly, we estimate the mass density of the vortex and find that it is extremely light compared with ordinary matters. Therefore, the vortex would not affect gravitational lensing effects significantly [19] . Finally, we discuss how one might detect such a vortex observationally [20] - [22] .  Model: The Lagrangian density describing the interaction between photons and axions is given by [23] :", "paraphrased_abstract": "I am a resonant particle, predicted by the Peccei-Quinn mechanism to solve the strong CP problem. We have studied a great deal in this field. We have determined the energy density in the field and found that it is in accordance with the dark matter density, and a very small one, 10213g/cm3, that is, it is a very small, very light particle, and therefore it does not rely on gravitational lensing. Then we will give the means of detecting the structure of the axion field by observation. We will first show that the axion field has a very small radius, and it corresponds to the size of the galaxy. Then we calculate the energy density and find that it corresponds to the magnitude of the dark matter density. In addition, we find that the axion field has a very small mass, the size of which is only about 1016g /cm3, which agrees very well with the value of the dark matter density. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2118, "title": "TYZ expansion for the Kepler manifold", "abstract": "  The main goal of the paper is to address the issue of the existence of Kempf's distortion function and the Tian-Yau-Zelditch (TYZ) asymptotic expansion for the Kepler manifold - an important example of non compact manfold. Motivated by the recent results for compact manifolds we construct Kempf's distortion function and derive a precise TYZ asymptotic expansion for the Kepler manifold. We get an exact formula: finite asymptotic expansion of $n-1$ terms and exponentially small error terms uniformly with respect to the discrete quantization parameter $m\\to \\infty $ and $\\rho \\to \\infty$, $\\rho$ being the polar radius in $\\C^n$.   Moreover, the coefficents are calculated explicitly and they turned out to be homogeneous functions with respect to the polar radius in the Kepler manifold. We also prove and derive an asymptotic expansion of the obtstruction term with the coefficients being defined by geometrical quantities. We show that our estimates are sharp by analyzing the nonharmonic behaviour of $T_m$ and the error term of the approximation of the Fubini--Study metric by $m\\omega$ for $m\\to +\\infty$. The arguments of the proofs combine geometrical methods, quantization tools and functional analytic techniques for investigating asymptotic expansions in the framework of analytic-Gevrey spaces. ", "machine_abstract": "We present an explicit formula for the TYZ expansion of the Kepler manifold, which is a complete intersection in $\\mathbb{C}^3$ with defining equations $xy - zw = 0$ and $x^2 + y^2 + z^2 = 1$.  The TYZ expansion is a formal power series that encodes information about the topology of the underlying space.  We show how to compute this expansion using Gr\u00f6bner bases techniques implemented by Singular.  This example illustrates some of the difficulties involved when computing such expansions explicitly. In particular we have to deal with singularities arising from the fact that the ideal generated by the polynomials defining our variety has infinitely many minimal primes. Finally, we use the TYZ expansion to prove that the fundamental group of the Kepler manifold is infinite cyclic. Let $K$ be the complex algebraic variety defined as follows:  $$K = \\{(x,y,z) \\in \\mathbb{C}^3: xy-zw=0\\ \\wedge\\ x^2+y^2+z^2=1\\}.$$  It follows immediately from the definition that $K$ is smooth (i.e., it does not contain any singular points).  Moreover, since $K$ is compact, its Euler characteristic equals one; hence, its Betti numbers are given by:  $$\\beta_0 = 1, \\beta_1 = 0, \\beta_2 = 1.$$  As a consequence, the first few terms of the Taylor\u2013Yau\u2013Zhang (TYZ) expansion of $K$ are:", "paraphrased_abstract": "\u201cBut what about the TYZ expansion of the Kepler manifold? This is an instance of the difficulties of explicit tying together the underlying space and generating its corresponding series of terms. The TYZ expansion is a purely formal power series that encodes information about the topology of the space underlying it. The TYZ expansion is a series of numbers which gives information about the topology of the underlying space. This example demonstrates the difficulty of constructing a TYZ expansion explicitly; in particular, it involves the fact that the ideal of the polynomials that make up our variety is infinitely many of them. As a result, the first few terms of the TYZ expansion of K are:  (x, y, z),  ,  ,          . ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0274, "title": "Needlet algorithms for estimation in inverse problems", "abstract": "  We provide a new algorithm for the treatment of inverse problems which combines the traditional SVD inversion with an appropriate thresholding technique in a well chosen new basis. Our goal is to devise an inversion procedure which has the advantages of localization and multiscale analysis of wavelet representations without losing the stability and computability of the SVD decompositions. To this end we utilize the construction of localized frames (termed \"needlets\") built upon the SVD bases. We consider two different situations: the \"wavelet\" scenario, where the needlets are assumed to behave similarly to true wavelets, and the \"Jacobi-type\" scenario, where we assume that the properties of the frame truly depend on the SVD basis at hand (hence on the operator). To illustrate each situation, we apply the estimation algorithm respectively to the deconvolution problem and to the Wicksell problem. In the latter case, where the SVD basis is a Jacobi polynomial basis, we show that our scheme is capable of achieving rates of convergence which are optimal in the $L_2$ case, we obtain interesting rates of convergence for other $L_p$ norms which are new (to the best of our knowledge) in the literature, and we also give a simulation study showing that the NEED-D estimator outperforms other standard algorithms in almost all situations. ", "machine_abstract": "We consider the problem of estimating an unknown function f from noisy measurements y = Af + w, where A is a linear operator and w is white noise with known covariance matrix Cw.  We assume that the operator A has been discretized on some grid (e.g., by using finite differences or spectral methods) so that it can be represented as a large matrix.  The goal is to find an estimatef such that Ef \u2212 f  2 is minimized subject to certain constraints on the smoothness off .  In this work we propose new numerical techniques based on needlets which are able to efficiently solve these constrained optimization problems.  In particular, our approach allows us to obtain accurate estimates even when the number N of available observations is much smaller than the dimension M of the space spanned by the columns of A.  The proposed method relies on two main ingredients:  First, we use a sparse representation of functions in terms of needlets.  Second, we develop efficient iterative algorithms for solving large-scale convex optimization problems involving sparsity-promoting regularizers.  These algorithms combine ideas from compressed sensing theory and recent results about the convergence rate of alternating direction method of multipliers (ADMM).", "paraphrased_abstract": "I have introduced two new numerical methods for solving convex optimization problems involving sparse regularization. The first is that we introduce a sparse representation of the functions of needlets, and the second is that we introduce efficient iterative algorithms for solving large-scale convex optimization problems by means of sparse regularizations. This approach is based on compressed sensing theory, based on recent results on the convergence of the alternating direction method of multipliers (ADMM). We have considered the problem of estimating an unknown function F from noisy measurements y = Af + w, where A is a linear operator and w is a white noise with known covariance matrix Cw. We assume that the operator A has been discretized on some grid (e.g., by using finite differences or by spectral methods) so that it can be represented in a large matrix. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.163, "title": "Exciting the Magnetosphere of the Magnetar CXOU J164710.2-455216 in Westerlund 1", "abstract": "  We describe XMM-Newton observations taken 4.3 days prior to and 1.5 days subsequent to two remarkable events that were detected with Swift on 2006 September 21 from the candidate magnetar CXOU J164710.2-455216: (1) a 20 ms burst with an energy of 1e37 erg (15-150 keV), and (2) a rapid spin-down (glitch) with a fractionap period change of 1e-4. We find that the luminosity of the pulsar increased by a factor of 100 in the interval between observations, from 1e33 to 1e35 erg/s (0.5-8.0 keV), and that its spectrum hardened. The pulsed count rate increased by a factor of 10 (0.5-8.0 keV), but the fractional rms amplitude of the pulses decreased from 65 to 11 per cent, and their profile changed from being single-peaked to exhibiting three peaks. Similar changes have been observed from other magnetars in response to outbursts, such as that of 1E 2259+586 in 2002 June. We suggest that a plastic deformation of the neutron star's crust induced a very slight twist in the external magnetic field, which in turn generated currents in the magnetosphere that were the direct cause of the X-ray outburst. ", "machine_abstract": "We report on observations made with Chandra and XMM-Newton that reveal an X-ray flare from the magnetar CXOU J16 47 10 . 2-45 52 16 (hereafter, J1647) located within the open cluster Westerlund 1. The flare was detected by both observatories during their respective slews to point at another target; it lasted for about one hour before fading below detectability. We find no evidence for any significant change in the spin-down rate or period derivative of this source following its outburst.     This is the first time such a large event has been observed from a magnetar; we estimate that the total energy released in the flare was ~3 x 10^44 erg. Our analysis shows that the flare occurred when the star's magnetic field lines were nearly perpendicular to our line-of-sight. In addition, we detect pulsations from J1647 during the flare which are consistent with those seen prior to the flare. These results suggest that the flaring activity may be due to reconnection events occurring along the closed loops of the stellar magnetic field.", "paraphrased_abstract": "We conclude that this flare was produced by a magnetar with the magnetic field line nearly parallel to the star\u2019s surface, and that this flare was followed by a pulsation, in accordance with previous observations. We show that this pulsation occurred when the star\u2019s magnetic field line was parallel to our own line of sight. We also demonstrate the existence of pulsations of J1644, a magnetar with a magnetic field that is not parallel to our line of sight. We report that the flare was created by the reconnection of the magnetic field of the star with a strong magnetic field of approximately three kJ/sec. The energy released in this pulsation was as follows: a resemblance of three kJ/sec. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2985, "title": "Elusive AGN in the XMM-Newton bright serendipitous survey", "abstract": "  AIMS: We investigate here the nature of all the sources (35 in total) in the XBS survey (which is 86% optically identified) showing an optical spectrum dominated by the light from the host galaxy with no evidence (or little evidence) for the presence of an AGN. METHODS: We use the X-ray spectral analysis to assess the presence of an AGN in these sources and to characterize its properties. RESULTS: We detect AGN activity in 33 out of 35 sources. The remaining 2 sources are the ones with the lowest X-ray luminosity in the sample (L[2-10keV]<10^41 erg s^-1) and their X-ray emission could be produced within the host galaxy. We find that the ``recognition problem'' for AGN is very critical in the low-luminosity regime (at least 60% of the AGN with L[2-10keV]<10^43 erg s^-1 are elusive) becoming negligible for high X-ray luminosities (~1.5% of elusive AGN with L[2-10keV]>10^44 erg s^-1). This problem affects mostly absorbed AGN (~40% of type~2 AGN in the survey are elusive) but also a significant fraction of unabsorbed AGN (8%). CONCLUSIONS: We find that the simplest explanations of why these 33 (or most of them) AGNs are elusive are two: at low X-ray luminosities (<10^43 erg s^-1) the most important reason is the intrinsically low AGN/galaxy contrast (optical dilution) while at high luminosities (>10^44 erg s^-1) it is due to the optical absorption (in the Compton-thin regime, i.e. NH<10^24 cm^-2). Alternative hypotheses, like the presence of Compton-thick sources, BL Lac objects or ``non-standard'' AGN (e.g. with alpha_OX<1 or with weak/absorbed Narrow Line Region) are not supported by the data although we cannot exclude the presence in the sample of a few sources of these types. ", "machine_abstract": "We present results on the X-ray properties and optical counterparts for a sample of 16 sources selected as candidate active galactic nuclei (AGNs) based on their high fluxes in the 0.5-2 keV band, but which are not detected by Chandra or XMM-Newton at higher energies. The majority have been observed with Swift/XRT; all show soft spectra consistent with either obscured AGNs or low-mass X-ray binaries. We find that most of these objects lie close to galaxies brighter than R = 20 mag, suggesting they may be associated with galaxy clusters rather than individual galaxies. However, we also identify two cases where the source is apparently offset from its nearest neighbour by more than 1 arcmin, making it unlikely that this association can explain all our candidates. In addition, one object lies within an extended region of diffuse emission, while another has no obvious host galaxy despite lying only 3 arcsec away from a very faint galaxy.", "paraphrased_abstract": ", they are close to galaxies of higher than R = 20 mag, and therefore may be a swarm of galaxy clusters rather than individual galaxies. Most of these objects lie in the vicinity of galaxies of R = 20 mag, and, despite their distance of only three arcsecs, the source does not have a clear host. There are two sources, and the distance is about one-a-sixth of an arcmin, and it is unlikely that these arcmins are of any significance to all the candidates. The results of the optical properties and the optical counterparts are presented for a selected group of 18 stars, selected for the study of active galactic nuclei (AGNs), because their X-ray fluxes are high, about 0.5-2 keV, and which are not seen by Chandra and XMM-Newton, but have not been detected at higher energies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2136, "title": "The colour selection of distant galaxies in the UKIDSS Ultra-Deep Survey Early Data Release", "abstract": "  We investigate colour selection techniques for high redshift galaxies in the UKIDSS Ultra Deep Survey Early Data Release (UDS EDR). Combined with very deep Subaru optical photometry, the depth (K_AB = 22.5) and area (0.62 deg^2) of the UDS EDR allows us to investigate optical/near-IR selection using a large sample of over 30,000 objects. By using the B-z, z-K colour-colour diagram (the BzK technique) we identify over 7500 candidate galaxies at z > 1.4, which can be further separated into passive and starforming systems (pBzK and sBzK respectively). Our unique sample allows us to identify a new feature not previously seen in BzK diagrams, consistent with the passively evolving track of early type galaxies at z < 1.4. We also compare the BzK technique with the R-K colour selection of Extremely Red Objects (EROs) and the J-K selection of Distant Red Galaxies (DRGs), and quantify the overlap between these populations. We find that the majority of DRGs, at these relatively bright magnitudes are also EROs. Since previous studies have found that DRGs at these magnitudes have redshifts of z ~ 1 we determine that these DRG/ERO galaxies have SEDs consistent with being dusty star-forming galaxies or AGN at z < 2. Finally we observe a flattening in the number counts of pBzK galaxies, similar to other studies, which may indicate that we are sampling the luminosity function of passive z > 1 galaxies over a narrow redshift range. ", "machine_abstract": "We present an analysis of the colour-selection criteria for identifying high-redshift (z > 6) galaxies using data from the first public release of the UKIRT Infrared Deep Sky Survey (UKIDSS). We use photometric redshifts to select candidate z ~ 7 and 8 galaxies, and then examine their near-infrared colours as measured by UKIDSS. The majority of these candidates are found to be at lower redshift than expected; we find that this is due primarily to contamination by low-redshift interlopers with similar optical-to-near-infrared colours. However, we also identify several robust candidates which have been missed by previous surveys. These include two objects with spectroscopic confirmation of Lyman-break features at z = 7.071 and z = 7.085 respectively. We discuss possible reasons why our sample may differ from those previously published, including differences between the survey areas used and different methods of selecting targets for spectroscopy.", "paraphrased_abstract": "It was also analyzed whether there were any difference in our sample, which was compared to previous one, by comparing different survey areas, by different spectroscopic methods, and by using different spectroscopic techniques. We present the results of our study of the redshift of a number of high-reddish, z-shape-of-shape, z-shape-of-shape galaxies obtained from the first spectroscopic survey of the deep sky by UKIRT (UKIDS) for the first time. We select the candidate galaxies z-shape-of-shape-of-shape z-shape-of-shape, and then compare their color, as determined by UKIDS. The majority of these objects were found to be lower than expected, and were therefore the result of the swarming of low-redshift interlopers with similar optical-to-near-infrared colours. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.221, "title": "A Mathematica script for harmonic oscillator nuclear matrix elements arising in semileptonic electroweak interactions", "abstract": "  Semi-leptonic electroweak interactions in nuclei - such as \\beta decay, \\mu capture, charged- and neutral-current neutrino reactions, and electron scattering - are described by a set of multipole operators carrying definite parity and angular momentum, obtained by projection from the underlying nuclear charge and three-current operators. If these nuclear operators are approximated by their one-body forms and expanded in the nucleon velocity through order |\\vec{p}|/M, where \\vec{p} and M are the nucleon momentum and mass, a set of seven multipole operators is obtained. Nuclear structure calculations are often performed in a basis of Slater determinants formed from harmonic oscillator orbitals, a choice that allows translational invariance to be preserved. Harmonic-oscillator single-particle matrix elements of the multipole operators can be evaluated analytically and expressed in terms of finite polynomials in q^2, where q is the magnitude of the three-momentum transfer. While results for such matrix elements are available in tabular form, with certain restriction on quantum numbers, the task of determining the analytic form of a response function can still be quite tedious, requiring the folding of the tabulated matrix elements with the nuclear density matrix, and subsequent algebra to evaluate products of operators. Here we provide a Mathematica script for generating these matrix elements, which will allow users to carry out all such calculations by symbolic manipulation. This will eliminate the errors that may accompany hand calculations and speed the calculation of electroweak nuclear cross sections and rates. We illustrate the use of the new script by calculating the cross sections for charged- and neutral-current neutrino scattering in ^{12}C. ", "machine_abstract": "We present an open-source, free software package that computes the one-loop Feynman diagrams contributing to the hadronic weak decay amplitudes of heavy quarks into light leptons and pions.  The code is written entirely within the Wolfram Language using only built-in functions.   It can be downloaded at https://github.com/mharrison/harmonic-oscillator-matrix-elements/tree/master/Mathematica_code. We have tested our results against those obtained with FeynArts 3.9 [1] , FormCalc 8 [2] , LoopTools 2 [3] , and QCDLoop [4] .  The program has been used to compute the leading-order (LO) contributions to the helicity amplitudes for B meson decays into muons or electrons plus pions [5] .  These are required inputs for the calculation of the branching fractions [6] and CP asymmetries [7, 8] of these processes.  In addition, we provide the LO contribution to the pion form factor [9] . Our approach uses the method developed by M. Neubert [10]  which exploits the fact that the relevant loop integrals can all be expressed as linear combinations of products of two-point scalar functions.  This allows us to reduce the number of independent integrations needed to evaluate each diagram to three.  Our implementation also includes several improvements over previous calculations [11, 12] :   1)  We use the exact expression for the propagators of massive fermions instead...", "paraphrased_abstract": ", the equations for the branches and scalar factors can be expressed as two-point scalar functions, and this reduces the number of independent integrations to three. Besides, the program also contains a corresponding pion factor, which is a necessary condition for the enlargement of the branches. We have implemented this method by M. Neubert, which makes use of the fact that the loop integrals can all be expressed as linear combinations of the products of two-point scalar functions, and therefore reduce the number of independent integrations required to evaluate each diagram to three. We have tested our method against FeynArts 3.9, FormCalc 8[11], LoopTools, 2], QCDLoop,[12] [17] and have obtained good results in the computation of the helicity of heavy quarks into light leptons and pions. The code is written entirely in Wolfram, using only functions. We have presented an open source program for calculating the leading-order contributions of the helicity for B mesons to ions, electrons and pions. The helicity is calculated by means of the leading-order of the leading-order (LO) contributions to the helicity amplitude of the B meson decays to muons or electrons plus pions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0543, "title": "A detailed study of the enigmatic cluster M82F", "abstract": "  We present a detailed study of the stellar cluster M82F, using multi-band high resolution HST imaging and deep ground based optical slit and integral field spectroscopy. Using the imaging we create colour maps of the cluster and surrounding region in order to search for substructure. We find a large amount of substructure, which we interpret as the result of differential extinction across the projected face of the cluster. With this interpretation, we are able to construct a spatially resolved extinction map across the cluster which is used to derive the intrinsic flux distribution. Fitting cluster profiles (King and EFF) to the intrinsic images we find that the cluster is 15-30% larger than previous estimates, and that no strong evidence of mass segregation in this cluster exists. Using the optical spectra, we find that the age of M82F is 60-80 Myr and from its velocity conclude that the cluster is not physically associated with a large HII region that it is projected upon, both in agreement with previous studies. The reconstructed integral field maps show that that majority of the line emission comes from a nearby HII region. The spatial dependence of the line widths (implying the presence of multiple components)measured corresponds to the extinction map derived from photometry, indicating that the gas/dust clouds responsible for the extinction are also partially ionised. Even with the wealth of observations presented here, we do not find a conclusive solution to the problem of the high light-to-mass ratio previously found for this cluster and its possible top-heavy stellar IMF. ", "machine_abstract": "We present new near-infrared (NIR) spectroscopy and photometry for the brightest star in the young open cluster M82F, which is located at a distance of 3.6 kpc to the south-east of the nearby galaxy NGC 3077. The NIR spectrum shows that this object has an effective temperature T eff = 13000 K and surface gravity log g = 4.0 dex. We find no evidence for any infrared excess emission indicative of circumstellar dust around the star. Using our spectroscopic results we derive a mass of about 20 M\u2299 for the central star. This value agrees well with previous estimates based on optical spectra. From the observed luminosity and assuming solar metallicity we estimate a radius R\u22c6 \u2248 5R\u2299 for the central object. Our analysis suggests that the age of the cluster is between 1 Myr and 10 Myr. In addition, we have performed a detailed comparison of the properties of M82F with those of other open clusters containing massive stars.", "paraphrased_abstract": "In addition, we have performed a detailed analysis of the physical properties of the cluster with the help of the other giant stars. The first, M82F, was opened to the world, and was located 3.6 kpc south of the nearby galaxy NGC 3077. In the NIR spectrum, the object is characterized by a temperature of 3300 K, a gravity of 4.0 dex, and a radiance of about 4.0 dps. The central object has a radius of about 20 M, and this is in agreement with the previous calculation of optical properties. We find that the star is a bright star, the brightest star in the enlargement of the young open cluster M82F, which is a little less than a half-year old, and is located in the south of NGC 3077. We find that the star is of the age of 1 Myr to 10 Myr. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2037, "title": "The shapes, orientation, and alignment of Galactic dark matter subhalos", "abstract": "  We present a study of the shapes, orientations, and alignments of Galactic dark matter subhalos in the ``Via Lactea'' simulation of a Milky Way-size LCDM host halo. Whereas isolated dark matter halos tend to be prolate, subhalos are predominantly triaxial. Overall subhalos are more spherical than the host halo, with minor to major and intermediate to major axis ratios of 0.68 and 0.83, respectively. Like isolated halos, subhalos tend to be less spherical in their central regions. The principal axis ratios are independent of subhalo mass, when the shapes are measured within a physical scale like r_Vmax, the radius of the peak of the circular velocity curve. Subhalos tend to be slightly more spherical closer to the host halo center. The spatial distribution of the subhalos traces the prolate shape of the host halo when they are selected by the largest V_max they ever had, i.e. before they experienced strong tidal mass loss. The subhalos' orientation is not random: the major axis tends to align with the direction towards the host halo center. This alignment disappears for halos beyond 3 r_200 and is more pronounced when the shapes are measured in the outer regions of the subhalos. The radial alignment is preserved during a subhalo's orbit and they become elongated during pericenter passage, indicating that the alignment is likely caused by the host halo's tidal forces. These tidal interactions with the host halo act to make subhalos rounder over time. ", "machine_abstract": "We present the results of an analysis of the shapes, orientations, and alignments of dark matter subhalos in cosmological N-body simulations with different levels of baryonic physics included.  We find that the inclusion of baryons has little effect on the shape distribution but does affect the spin parameter distributions significantly; halos are more spherical when baryons are included than they would be if only gravity were acting upon them. The halo spins tend to be aligned perpendicularly to their major axes for all models considered here (including pure dark matter). This is consistent with previous studies which have found similar trends using other methods. However we also find evidence that this trend may not hold at very small scales where there appears to be some correlation between the direction of the angular momentum vector and the minor axis of the halo. Finally, we show that the presence or absence of baryons affects the degree of alignment between neighboring halos; halos are less strongly clustered around each other when baryons are included.", "paraphrased_abstract": "We found that the presence of baryons does not significantly affect the shape, but significantly affects the spins; the halos are more spherical when baryons are included, than if gravity were to act on them. However, we find that this trend is not sustained at the most granular scale, where there seems to be a correlation between the direction of the angular momentum vector and the minor axis of the halo. We also find that, in general, the halo spins more and more perpendicular to the major axis of the halo, as well as in other cosmological models. We present the results of an analysis of the halo spins in the N-body of dark matter, in the form of cosmological simulations with different levels of baryon physics. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1079, "title": "Instanton Induced Neutrino Majorana Masses in CFT Orientifolds with MSSM-like spectra", "abstract": "  Recently it has been shown that string instanton effects may give rise to neutrino Majorana masses in certain classes of semi-realistic string compactifications. In this paper we make a systematic search for supersymmetric MSSM-like Type II Gepner orientifold constructions admitting boundary states associated with instantons giving rise to neutrino Majorana masses and other L- and/or B-violating operators. We analyze the zero mode structure of D-brane instantons on general type II orientifold compactifications, and show that only instantons with O(1) symmetry can have just the two zero modes required to contribute to the 4d superpotential. We however discuss how the addition of fluxes and/or possible non-perturbative extensions of the orientifold compactifications would allow also instantons with $Sp(2)$ and U(1) symmetries to generate such superpotentials. In the context of Gepner orientifolds with MSSM-like spectra, we find no models with O(1) instantons with just the required zero modes to generate a neutrino mass superpotential. On the other hand we find a number of models in one particular orientifold of the Gepner model $(2,4,22,22)$ with $Sp(2)$ instantons with a few extra uncharged non-chiral zero modes which could be easily lifted by the mentioned effects. A few more orientifold examples are also found under less stringent constraints on the zero modes. This class of $Sp(2)$ instantons have the interesting property that R-parity conservation is automatic and the flavour structure of the neutrino Majorana mass matrices has a simple factorized form. ", "machine_abstract": "We study the possibility that instantons induce neutrino masses and mixings, which are compatible with current experimental data on neutrinos. We consider type IIB orientifold compactifications to four dimensions with intersecting D-branes at singularities. The Standard Model gauge group is realized by stacks of branes wrapping 3-cycles inside Calabi-Yau threefolds. In addition we include stacks of branes wrapped around 2-cycles corresponding to hidden sectors. These models can be engineered such that they have an MSSM-like spectrum. Instanton effects lead to corrections to the superpotential involving fermions localized on different stacks of branes. This leads to Majorana mass terms for right-handed neutrinos. We show how these results can be used to construct realistic string inspired models of leptogenesis. We also discuss possible phenomenological consequences of our scenario. Introduction: String theory provides many new avenues towards understanding physics beyond the Standard Model (SM). One interesting class of scenarios involves extra spatial dimensions where SM fields live on a 3-brane while gravity propagates into the bulk [1] . A particularly appealing feature of this setup is that it allows for TeV scale quantum gravity without conflicting with precision tests of general relativity [2] . In recent years there has been much interest in studying supersymmetric extensions of the SM within the context of string theory [3] - [8] . Supersymmetry stabilizes the electroweak hierarchy problem [9] , predicts unification of all coupling constants [10] and offers solutions to other open problems like dark matter [11] or baryogenesis [12] . However, despite its successes as a theoretical framework, no direct evidence for SUSY exists so far [13] . It would therefore be very exciting if some of the predictions made by SUSY could be tested experimentally [14] . One important question concerns the origin of neutrino masses [15] . While the seesaw mechanism [16] explains naturally small neutrino masses [17] , it requires additional particles not present in the minimal version of the SM [18] . An alternative approach consists in considering non-renormalizable operators induced by instantons [19] . Such contributions may arise when", "paraphrased_abstract": "One of the most interesting developments of String theory is the possibility of supersymmetrical extensions of the SM; it allows the existence of two levels of the SM, while it also allows the fusion of various elements, like dark matter or baryogenesis. There is a good deal of interest in the physics of supersymmetrical extensions of the SM, in particular involving extra spatial dimensions, where the SM is residing on a 3-brane, while gravity is flowing into the mash. This synthesis provides a new method of encoding the electromagnetic hierarchy, predicting the unification of all the coupling constants, and addressing other open problems, such as dark matter and baryogenesis. The emanation of the SM, which is also an experimental method, is an experimental method. Moreover, it is also possible to formulate a model which corresponds to the MSM, which is the corresponding MSM, and also to the D-branes. In particular, it is a method that can be used to model the synthesis of neutrino masses. We propose an investigation of the possibility that Ming's atoms induced the formation of masses and mixings, which are compatible with the existing experimental data. Moreover, we discuss possible phenomenological consequences of our experiment. The Sm-Alt-Deuce, which", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1302, "title": "The high energy emission of GRO J1655-40 as revealed with INTEGRAL spectroscopy of the 2005 outburst", "abstract": "  We present broadband (3-500 keV) INTEGRAL X-ray spectra and X-ray/optical light curves of the luminous black hole X-ray transient and relativistic jet source GRO J1655-40. Our analysis covers four Target of Opportunity observations of the outburst that started in February 2005. We find that the high energy emission of GRO J1655-40 can be modelled well with an unbroken power-law (with photon indices of 1.72+-0.03,2.21+-0.04 for the first and the second observations, respectively). These correspond to hard and thermal dominant states, respectively. In contrast to many other black hole spectra, high energy complexity in the form of a break or cut-off is not required for the hard state, contrary to previous expectations for this state. We show for the first time that Comptonization by non-thermal electrons is the dominant process for the high energy emission in the hard state. We discuss our results in terms of models for broad-band emission and accretion flows in stellar-mass black holes. ", "machine_abstract": "We report on results obtained by INTEGRAL observations during the 2005 outburst of the black hole candidate GRO J1655\u00e2\u20ac\u201c40 (Nova Muscae 1991). The source was observed in the 20-100 keV range for about 100 days, starting at MJD 53000 and ending at MJD 53300. We have analyzed these data using both ISGRI and SPI instruments aboard INTEGRAL satellite. In addition to the main spectral component which is well described by a power law model modified by an exponential cutoff, we find that there are two additional components present in the spectrum. One of them has been previously reported by other authors but its origin remains unclear. Another one appears only when fitting the whole dataset simultaneously with all three models considered here -power law plus exponential cut-off, broken power law or Comptonization model-. This new feature can be interpreted either as a reflection hump produced by cold material surrounding the central X-ray source or as a broad iron line around 6.4 keV.", "paraphrased_abstract": "The first one was characterized by a power-law derived from an exponential cut-off and the second by a comptonization. The second, which was not accounted for in the previous analysis, was not clear; but it was only discovered by averaging all the three models and comparing them. The third is the result of the spectral analysis carried out by INTEGRAL during the recent exploding of the black hole candidate GRO J1655 in Nautical Muscae, which was detected in the region from 1020 to 1000 300 s. The result of this astronomy is that it is made up of a refractory hump caused by the cold material surrounding the X-ray source and a broad iron line, which is curved about 6.4 s. The new feature, based on the inverse symmetry of the X-ray source, is characterized by a peculiarity that is well defined by a power-law modified by a cut-off of an exponential. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1738, "title": "Equation of state of isospin-asymmetric nuclear matter in relativistic mean-field models with chiral limits", "abstract": "  Using in-medium hadron properties according to the Brown-Rho scaling due to the chiral symmetry restoration at high densities and considering naturalness of the coupling constants, we have newly constructed several relativistic mean-field Lagrangians with chiral limits. The model parameters are adjusted such that the symmetric part of the resulting equation of state at supra-normal densities is consistent with that required by the collective flow data from high energy heavy-ion reactions, while the resulting density dependence of the symmetry energy at sub-saturation densities agrees with that extracted from the recent isospin diffusion data from intermediate energy heavy-ion reactions. The resulting equations of state have the special feature of being soft at intermediate densities but stiff at high densities naturally. With these constrained equations of state, it is found that the radius of a 1.4$M_\\odot$ canonical neutron star is in the range of 11.9 km$\\leq$R$\\leq$13.1 km, and the maximum neutron star mass is around 2.0$M_\\odot$ close to the recent observations. ", "machine_abstract": "We present the equation of state (EoS) for isospinasymmetric nuclear matter within the framework of relativistic mean field theory, including both nucleons and hyperons as well as their interactions via meson exchange. We consider two different parameterizations of the EoS: NL3* and TM1. The former one includes non-linear terms in the scalar self-interaction potential while the latter has been fitted to reproduce properties of finite nuclei. In addition we also study the effect on the EoS when using the so-called \"chiral limit\" instead of the standard values for the coupling constants between baryons and mesons. This work is motivated by recent experimental results obtained at GSI Darmstadt which suggest that the symmetry energy may be much softer than previously thought. It will allow us to make predictions about the composition of neutron stars and its dependence on the density profile inside these objects. Finally, we compare our results with those obtained recently by other authors.", "paraphrased_abstract": "In addition, we will present the effect of the so-called \u201cchiral limit\u201d on the corresponding kinematics of the baryons and mesons. We also present the EoS of isospinasymmetric nuclear matter in the context of the relativistic mean field, including both the nucleons and the hyperons, as well as the interactions of the mesons with the atoms. We have been working with recent results from the GSI Darmstadt, which show that the symmetry energy of the atoms is much lower than it was previously thought. We then present the equation of state for isospinasymmetric nuclear matter in the context of the relativistic mean field. This equation is calculated according to the relativistic mean field, and contains the nucleons and the hyperons, as well as the interaction of mesons. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4664, "title": "A high-frequency radio continuum study of massive young stellar objects", "abstract": "  We present high-resolution observations made with the Very Large Array (VLA) in its A configuration at frequencies between 5 and 43 GHz of a sample of five massive young stellar objects (YSOs): LkHa101, NGC2024-IRS2, S106-IR, W75N and S140-IRS1. The resolution varied from 0.04 arcsec (at 43 GHz) to 0.5 arcsec (at 5 GHz), corresponding to a linear resolution as high as 17 AU for our nearest source. A MERLIN observation of S106-IR at 23 GHz with 0.03-arcsec resolution is also presented. S106-IR and S140-IRS1 are elongated at 43 GHz perpendicular to their large scale bipolar outflows. This confirms the equatorial wind picture for these sources seen previously in MERLIN 5 GHz observations. The other sources are marginally resolved at 43 GHz. The spectral indices we derive for the sources in our sample range from +0.2 to +0.8, generally consistent with ionized stellar winds. We have modelled our sources as uniform, isothermal spherical winds, with LkHa101 and NGC2024-IRS2 yielding the best fits. However, in all cases our fits give wind temperatures of only 2000 to 5000 K, much less than the effective temperatures of main-sequence stars of the same luminosity, a result which is likely due to the clumpy nature of the winds. ", "machine_abstract": "We present the results of an unbiased survey for compact HII regions in the southern Galactic plane using data obtained with the Australia Telescope Compact Array (ATCA). The sample consists of all known OB stars within |b| < 1 degree and distances less than 5 kpc, which are associated with IRAS point sources that have been classified as having infrared excesses indicative of circumstellar disks or envelopes. We detect over 100 new compact HII regions at frequencies between 2.1 GHz and 6.0 GHz. These objects range in size from 0.01 pc to 0.5 pc and their luminosities vary by more than four orders of magnitude. Most of these newly detected compact HII regions appear to be excited by single O-type stars; however we also find several examples where two or three bright radio components are separated by only a few arcseconds. In addition, we identify a number of previously uncatalogued ultracompact HII regions whose sizes are smaller than 0.01 pc.", "paraphrased_abstract": ", there are several other examples of a smaller size, of less than a pc, which has a 1025% luminous surface and is separated by a few arcseconds. The search is based on a survey of the southern Galactic plane, using the ATCA data, with a focus on the OB stars within a 1 degree of the horizon, in the shape of a sphere, or on a sphere of the envelope, which are in the form of disks and envelopes. Besides, the survey is based on the Array of Compact Stars, the Atta Compact Star, which was launched in November 2011 and was borne in the southern Galactic plane by the ATCA data. The sample consisted of all known OB stars within a degree of b, and within a radius of less than five kpc. These objects are associated with IRAS stars, whose infrared excesses have been measured as signs of disks or envelopes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4632, "title": "Unplugging the Universe: the neglected electromagnetic consequence of decoupling", "abstract": "  This letter concentrates on the non-equilibrium evolution of magnetic field structures at the onset of recombination, when the charged particle current densities decay as neutrals are formed.   We consider the effect that a decaying magnetic flux has on the acceleration of particles via the transient induced electric field. Since the residual charged-particle number density is small as a result of decoupling, we shall consider the magnetic and electric fields essentially to be imposed, neglecting the feedback from any minority accelerated population.   We find that the electromagnetic treatment of this phase transition can produce energetic electrons scattered throughout the Universe. Such particles could have a significant effect on cosmic evolution in several ways: (i) their presence could delay the effective end of the recombination era; (ii) they could give rise to plasma concentrations that could enhance early gravitational collapse of matter by opposing cosmic expansion to a greater degree than neutral matter could; (iii) they could continue to be accelerated, and become the seed for reionisation at the later epoch $z \\approx 10$. ", "machine_abstract": "We show that, in addition to gravitational waves and neutrinos, there is an additional source of energy loss during the final stages of stellar evolution which has been largely ignored by previous authors. This arises because the universe becomes transparent to photons at redshifts z ~ 1100 (the time when matter-radiation equality occurs), allowing them to stream freely outwards into space. The resulting decrease in pressure causes the universe to expand faster than it would otherwise do, thereby accelerating its expansion rate. We estimate this effect for different types of stars and find that it can be significant - up to 10% of the total luminosity output of massive stars may be lost due to this process. In particular we predict that Type Ia supernovae should exhibit systematically lower peak luminosities compared with their observed values if they are not corrected for this effect. Finally, we discuss how our results could be tested observationally using current data on distant supernovae.", "paraphrased_abstract": "Then we consider the effect on the maximum luminosity of the distant supernovae. Moreover, we explain that our findings could be applied to the latest observations on the distant supernovae. Besides gravitational waves and neutrinos, there is also a third source of energy loss in the final stages of the stellar evolution, which has been largely ignored by previous authors. We show that, in addition to the gravitational waves and neutrinos, there is also a third source of energy loss during the final stages of the development of stars, which has been largely overlooked by previous authors. The resulting decrease in pressure causes the universe to expand faster than before, and thus accelerates its expansion. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.132, "title": "Supersymmetry versus Gauge Symmetry on the Heterotic Landscape", "abstract": "  One of the goals of the landscape program in string theory is to extract information about the space of string vacua in the form of statistical correlations between phenomenological features that are otherwise uncorrelated in field theory. Such correlations would thus represent predictions of string theory that hold independently of a vacuum-selection principle. In this paper, we study statistical correlations between two features which are likely to be central to any potential description of nature at high energy scales: gauge symmetries and spacetime supersymmetry. We analyze correlations between these two kinds of symmetry within the context of perturbative heterotic string vacua, and find a number of striking features. We find, for example, that the degree of spacetime supersymmetry is strongly correlated with the probabilities of realizing certain gauge groups, with unbroken supersymmetry at the string scale tending to favor gauge-group factors with larger rank. We also find that nearly half of the heterotic landscape is non-supersymmetric and yet tachyon-free at tree level; indeed, less than a quarter of the tree-level heterotic landscape exhibits any supersymmetry at all at the string scale. ", "machine_abstract": "We study heterotic string theory compactified to four dimensions with N=2 supersymmetry and gauge symmetry E8xE8, SO(32) or USp (16) . We consider all possible consistent models in this class that are free of tachyons at tree level and have vanishing one-loop cosmological constant. The number of such models is estimated by counting the number of inequivalent ways of embedding the gauge group into the Narain lattice. For each model we compute its massless spectrum using the freely available software package SARAH. In particular, we determine whether there exist chiral exotics charged under both Standard Model groups SU(3)c xSU(2)L xU(1)Y as well as non-chiral exotics charged only under U(1)Y . Finally, for each model we calculate the value of the lightest Higgs boson mass Mhh , which can be used to compare our results against future experimental data.     Introduction     String theory has been proposed as a candidate theory beyond the standard model of particle physics. One of the main goals of theoretical research within string phenomenology is to find realistic vacua where the Standard Model particles live together with their superpartners. This requires finding solutions to the so-called moduli stabilisation problem, i.e., stabilising the extra spatial dimensions so that they do not grow large compared to the size of the observable universe. A promising approach towards solving this problem is to use fluxes threading internal cycles of Calabi-Yau manifolds [1] .   In recent years it became clear that many different types of string theories lead to similar low-energy effective field theories [2] . These include type IIA strings [3] , IIB strings [4] , F-theory [5] , orientifolded type IIB [6] , and heterotic strings [7, 8] . It was shown that these theories are connected via dualities [9] . Duality transformations relate different descriptions of the same physical system. They allow us to translate information between seemingly very different pictures of nature. An important example is mirror symmetry [10] , which relates type IIA", "paraphrased_abstract": "This is due to the fact that the particle is at the same time a supernodal. In the recent years it has become clear that many different types of string theories have arisen, in the form of low-energy effective field theories. These theories are called low-energy effective fields, and are grouped into different types of string theories. String theory is a special case. It is developed in a form of fusion whose symmetry is doubled; if the particle is in the two categories, it will be different from the one that the world sees. In the study of the reductive spectrum, the number of inequalities is calculated by the method of SARAH, which is very popular. For each of these, we compute the massless spectrum of each of the modelers, which are free of tachyons at tree level, and which have vanishing one-loop cosmological constant. Moreover, for each of these models, we calculate the value of the lightest Higgs boson mass Mhh, which can be used to compare our results with other experimental results. Besides, we investigate the polar symmetry, the polar symmetry of the N-supersymmetry, the polar symmetry of the sphere E8xE8, and the polar symmetry of the sphere U8xE8; we calculate the massless spectrum", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2132, "title": "A quantitative study on the growth variability of tumour cell clones in vitro", "abstract": "  Objectives: In this study, we quantify the growth variability of tumour cell clones from a human leukemia cell line. Materials and methods: We have used microplate spectrophotometry to measure the growth kinetics of hundreds of individual cell clones from the Molt3 cell line. The growth rate of each clonal population has been estimated by fitting experimental data with the logistic equation. Results: The growth rates were observed to vary among different clones. Up to six clones with a growth rate above or below the mean growth rate of the parent population were further cloned and the growth rates of their offsprings were measured. The distribution of the growth rates of the subclones did not significantly differ from that of the parent population thus suggesting that growth variability has an epigenetic origin. To explain the observed distributions of clonal growth rates we have developed a probabilistic model assuming that the fluctuations in the number of mitochondria through successive cell cycles are the leading cause of growth variability. For fitting purposes, we have estimated experimentally by flow cytometry the maximum average number of mitochondria in Molt3 cells. The model fits nicely the observed distributions of growth rates, however, cells in which the mitochondria were rendered non functional (rho-0 cells) showed only a 30% reduction in the clonal growth variability with respect to normal cells. Conclusions: A tumor cell population is a dynamic ensemble of clones with highly variable growth rate. At least part of this variability is due to fluctuations in the number of mitochondria. ", "machine_abstract": "The authors have studied the growth rate and its fluctuations for different types of human cancer cells grown under controlled conditions, using an automated image analysis system to measure their size over time.  The results show that there is considerable variation between individual clones within each type of cell line as well as between different types of cell lines. This suggests that it may be possible to use this information to classify tumours into subtypes with distinct biological properties. In addition, they find evidence that the growth rates are correlated across generations of daughter cells, which could provide insight into how these correlations arise during tumorigenesis. Finally, by comparing the growth characteristics of normal and transformed cells, they conclude that transformation leads to increased heterogeneity among daughter cells. Cancer is characterized by uncontrolled proliferation of abnormal cells. Understanding the mechanisms underlying this process can help us develop new treatments against cancer. However, studying the dynamics of cancerous cell populations has been challenging because of difficulties associated with tracking large numbers of single cells simultaneously. Here we report our recent work on characterizing the growth behavior of thousands of individual cancer cells growing in culture dishes [1] . We used an automated imaging system to track the sizes of hundreds of thousands of cells belonging to several different types of human cancer cell lines ( Figure 1 ). Our results reveal significant differences in both average growth rates and growth fluctuations between different types of cell lines: some grow faster than others while also exhibiting larger fluctuations around their mean values [2] . We found that the growth rates were highly variable even when measured at the level of individual clones derived from a common parent population [3] , suggesting that the observed phenotypic diversity might reflect genetic or epigenetic variations present in the original parental population [4] .  These findings suggest that it should be possible to use such measurements to classify tumors into subtypes based on their growth characteristics [5] .", "paraphrased_abstract": "\u201cThe cancer process is the process of the involuntary proliferation of abnormal cells. The understanding of the mechanisms underlying the phenomenon is crucial for the development of new treatments. We report herein our recent work analyzing the growth behavior of hundreds of thousands of human cancer cells, grown in culture dishes. We measured the average growth rate and the variability of the growth rate of various types of cancer cells. We found that the average growth rate and the variability of the growth rate were highly variable among different types of cancer cells, indicating that the differences in phenotypic diversity were a result of the diversity of the original parent population. Thus, the growth rate was very variable, even when measured at the level of individual cells from the same family, which, in turn, may reflect the variation in the biological diversity of the original parent population. This resulted in the hypothesis that the development of the cancer cells might be divided into sub-types according to the biological characteristics. Here, we present the growth rate and its variations for a few hundred thousand human cancer cells grown in culture dishes (Figure 1). The data of growth and its fluctuations, which were collected from different clones of the same cell line, showed that the average growth and the irregularity of the average were of different types, and that the elongated growth rate of the normal cells was of increased proportions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2227, "title": "The Norma Cluster (ACO 3627): I. A Dynamical Analysis of the Most Massive Cluster in the Great Attractor", "abstract": "  A detailed dynamical analysis of the nearby rich Norma cluster (ACO 3627) is presented. From radial velocities of 296 cluster members, we find a mean velocity of 4871 +/- 54 km/s and a velocity dispersion of 925 km/s. The mean velocity of the E/S0 population (4979 +/- 85 km/s) is offset with respect to that of the S/Irr population (4812 +/- 70 km/s) by `Delta' v = 164 km/s in the cluster rest frame. This offset increases towards the core of the cluster. The E/S0 population is free of any detectable substructure and appears relaxed. Its shape is clearly elongated with a position angle that is aligned along the dominant large-scale structures in this region, the so-called Norma wall. The central cD galaxy has a very large peculiar velocity of 561 km/s which is most probably related to an ongoing merger at the core of the cluster. The spiral/irregular galaxies reveal a large amount of substructure; two dynamically distinct subgroups within the overall spiral-population have been identified, located along the Norma wall elongation. The dynamical mass of the Norma cluster within its Abell radius is 1 - 1.1 x 10^15 h^-1_73 M_Sun. One of the cluster members, the spiral galaxy WKK 6176 which recently was observed to have a 70 kpc X-ray tail, reveals numerous striking low-brightness filaments pointing away from the cluster centre suggesting strong interaction with the intracluster medium. ", "machine_abstract": "We present an analysis of the most massive cluster known to be part of the Great Attractor, based on new spectroscopic data obtained with VLT/VIMOS and archival Chandra X-ray Observatory observations. The main results are as follows:  We find that the galaxy distribution is well described by a single NFW profile out to at least 1 Mpc/h.  Using this mass model we derive a virial mass for the cluster of Mvir = 2.1 x 1015M\u2299/h70 within R200 = 0.9 h-1 70 Mpc/h. This value agrees very well with previous estimates derived using different techniques. We also show that the velocity dispersion profile can be fitted with a simple \u03b2-model up to r ~ 0.7R200. Beyond this radius there seems to be evidence for a steepening of the profile which could indicate the presence of substructures or infalling groups along the line-of-sight.", "paraphrased_abstract": "Moreover, we show that the velocity of the star can be calculated with the simple -model from 0.1 to 0.2R200. Beyond this radius, we notice a slight steepening, which could be interpreted as a sign of substructures or infallible groups. We show that the distribution of the galaxy is well characterized by a single NFW, at least 1 Mpc/h. We also show that the dispersion profile can be calculated from the simple -model up to r = 0.7 R200. The corresponding results are as follows: ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3736, "title": "Alignment and signed-intensity anomalies in WMAP data", "abstract": "  Significant alignment and signed-intensity anomalies of local features of the cosmic microwave background (CMB) are detected on the three-year WMAP data, through a decomposition of the signal with steerable wavelets on the sphere. Firstly, an alignment analysis identifies two mean preferred planes in the sky, both with normal axes close to the CMB dipole axis. The first plane is defined by the directions toward which local CMB features are anomalously aligned. A mean preferred axis is also identified in this plane, located very close to the ecliptic poles axis. The second plane is defined by the directions anomalously avoided by local CMB features. This alignment anomaly provides further insight on recent results (Wiaux et al. 2006). Secondly, a signed-intensity analysis identifies three mean preferred directions in the southern galactic hemisphere with anomalously high or low temperature of local CMB features: a cold spot essentially identified with a known cold spot (Vielva et al. 2004), a second cold spot lying very close to the southern end of the CMB dipole axis, and a hot spot lying close to the southern end of the ecliptic poles axis. In both analyses, the anomalies are observed at wavelet scales corresponding to angular sizes around 10 degress on the celestial sphere, with global significance levels around 1%. Further investigation reveals that the alignment and signed-intensity anomalies are only very partially related. Instrumental noise, foreground emissions, as well as some form of other systematics, are strongly rejected as possible origins of the detections. An explanation might still be envisaged in terms of a global violation of the isotropy of the Universe, inducing an intrinsic statistical anisotropy of the CMB. ", "machine_abstract": "We present evidence for alignment between the quadrupole, octopole and hexadecapole moments of the cosmic microwave background (CMB) temperature anisotropy on large angular scales as measured by the Wilkinson Microwave Anisotropy Probe (WMAP). We find that this alignment is statistically significant at greater than 5 sigma confidence level when compared to simulated random realizations of CMB maps with identical power spectrum but different phases.     The observed alignment can be explained if there are large-scale magnetic fields aligned along the line-of-sight which induce circular polarization in the primordial plasma prior to recombination. This effect would lead to an additional contribution to the Stokes Q parameter of the CMB radiation. In addition we show that the observed anomalous behaviour of the quadrupole moment may also be caused by such effects. Finally, we discuss possible implications of our results for inflationary models. We argue that these observations do not rule out slow-roll single-field inflation scenarios, however they provide new constraints on their parameters.", "paraphrased_abstract": "The result is that the observed anomaly of the quadrupole moment is caused by such influences. In addition, we show that the observed anomaly of the quadrupole moment can be explained by such effects. In addition, we show that the observed anomaly of the quadrupole moment could be caused by the influence of such influences. In our work, we have been studying the temperature and temperature anisotropy of the cosmic microwave background on the large scale of the Wilkinson Microwave Anisotropy Probe. Our study is significant, and has a statistical significance of five sigma. Compared with the random realization of CMB maps, which have the same power spectrum, but differing phases, the observed observation is statistically significant. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4233, "title": "Effect of magnetic field on spontaneous Fermi surface symmetry breaking", "abstract": "  We study magnetic field effects on spontaneous Fermi surface symmetry breaking with d-wave symmetry, the so-called d-wave \"Pomeranchuk instability''. We use a mean-field model of electrons with a pure forward scattering interaction on a square lattice. When either the majority or the minority spin band is tuned close to the van Hove filling by a magnetic field, the Fermi surface symmetry breaking occurs in both bands, but with a different magnitude of the order parameter. The transition is typically of second order at high temperature and changes to first order at low temperature; the end points of the second order line are tricritical points. This qualitative picture does not change even in the limit of a large magnetic field, although the magnetic field substantially suppresses the transition temperature at the van Hove filling. The field produces neither a quantum critical point nor a quantum critical end point in our model. In the weak coupling limit, typical quantities characterizing the phase diagram have a field-independent single energy scale while its dimensionless coefficient varies with the field. The field-induced Fermi surface symmetry breaking is a promising scenario for the bilayer ruthenate Sr3Ru2O7, and future issues are discussed to establish such a scenario. ", "machine_abstract": "We study the effect of an external magnetic field on the ground state properties in the Hubbard model with next-nearest neighbor hopping and spin-orbit coupling, which is relevant to iron-based superconductors. We show that the magnetic field induces a spontaneous breaking of the time-reversal symmetry at half-filling for any finite value of the spin-orbit coupling strength. The broken symmetry phase has two-fold degenerate energy bands and shows non-Fermi liquid behavior. In addition, we find that there exists another spontaneously-broken-symmetry phase without gapless excitations when the chemical potential lies between the upper and lower band edges. This phase also exhibits non-Fermi liquid behaviors. Finally, we discuss possible experimental consequences of our results. Introduction:-The discovery of high-Tc FeAs-based superconductors [1] has attracted much attention because they are believed to be unconventional [2] . It was found experimentally [3] that these materials have strong spin orbit (SO) interaction [4] , which leads to several interesting phenomena such as nematic order [5] , orbital ordering [6] , and anisotropic magnetoresistance [7] . In this Letter, we consider the following extended Hubbard model:  where c\u2020i\u03c3(ci\u03c3) creates (annihilates) an electron with spin \u03c3 =\u2191 or \u2193 at site i, n\u03b1\u03b2ij= c \u2020 \u03b1ji c \u03b2ji denotes the density matrix element between sites j and i, t represents nearestneighbor hopping amplitude, t' stands for nextnearest-neighbor hopping amplitude, U is the local Coulomb repulsion, \u03bb is the SO coupling constant, \u00b5 is the chemical potential, and B is the applied magnetic field along z-direction. Hereafter, we set t=1, t'=0.3t, U=4t, and \u03bb=0.1t unless otherwise stated [8] .", "paraphrased_abstract": "\u201cIn this paper we consider the following extended version of the Hubbard equation: where ci (ci) creates (annihilates) an electron with spin  =  or  at site i, t=the nearest neighbor hopping amplitude, t=the nearest neighbor hopping amplitude, U=the local Coulomb dispulsion, =the local Coulomb dissipation, =the local chemical potential, =the chemical potential, and B=the applied magnetic field along z direction. Moreover, we have found a second reversal of the symmetry of the time-reversal symmetry, not involving any gapless excitations, and this reversal is not accompanied by any measurable amount of the spin-orbit strength. Lastly, we discuss the consequences of our experiment. We have recently discovered a high-tc (Cr) high-temperature conductive material with a strong SO(1, which has a nematic order, a nematic order, a nematic order, an anisotropic magnetoresistance. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.362, "title": "The Intrinsic Origin of Spin Echoes in Dipolar Solids Generated by Strong Pi Pulses", "abstract": "  In spectroscopy, it is conventional to treat pulses much stronger than the linewidth as delta-functions. In NMR, this assumption leads to the prediction that pi pulses do not refocus the dipolar coupling. However, NMR spin echo measurements in dipolar solids defy these conventional expectations when more than one pi pulse is used. Observed effects include a long tail in the CPMG echo train for short delays between pi pulses, an even-odd asymmetry in the echo amplitudes for long delays, an unusual fingerprint pattern for intermediate delays, and a strong sensitivity to pi-pulse phase. Experiments that set limits on possible extrinsic causes for the phenomena are reported. We find that the action of the system's internal Hamiltonian during any real pulse is sufficient to cause the effects. Exact numerical calculations, combined with average Hamiltonian theory, identify novel terms that are sensitive to parameters such as pulse phase, dipolar coupling, and system size. Visualization of the entire density matrix shows a unique flow of quantum coherence from non-observable to observable channels when applying repeated pi pulses. ", "machine_abstract": "Spin echoes are observed when the spin system is subjected to two successive radio-frequency (RF) pulses separated by an interval, known as the pulse separation time Tsep. The first RF pulse creates a macroscopic magnetization vector M0 that precesses around the external magnetic field Bext at Larmor frequency fL = \u03b3Bext where \u03b3 is gyromagnetic ratio for nuclear spins. After the second RF pulse with flip angle \u03b82 and phase shift \u03c62 relative to the first one, the transverse component of the magnetization vector M2(t) decays exponentially due to dephasing caused by local magnetic fields created by neighboring nuclei. In this work we show how strong \u03c0-pulses can be used to generate spin echoes even if there is no net initial magnetization present before applying these pulses. We demonstrate theoretically and experimentally that such spin echoes originate intrinsically from the dipolar interactions between nuclear spins.", "paraphrased_abstract": "And the spin system is shown by two successive RF pulses, separated by an interval known as the pulse time Tsep. The first pulse consists of a macroscopic magnetization vector M0, which revolves around the external magnetic field Bext, Larmor frequency fL = Bext, where  is the gyromagnetic ratio of nuclear spins. The second pulse, which has a slightly different angle of the axis and a slightly different phase, decompositions the transverse component of M2(t), which is decaying exponentially due to the dephasing of local magnetic fields created by neighboring nuclei. In this work we show how strong  pulses can be used to create echoes, even if there is no underlying magnetism before the pulses. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0232, "title": "Cosmological perturbations in Palatini modified gravity", "abstract": "  Two approaches to the study of cosmological density perturbations in modified theories of Palatini gravity have recently been discussed. These utilise, respectively, a generalisation of Birkhoff's theorem and a direct linearization of the gravitational field equations. In this paper these approaches are compared and contrasted. The general form of the gravitational lagrangian for which the two frameworks yield identical results in the long-wavelength limit is derived. This class of models includes the case where the lagrangian is a power-law of the Ricci curvature scalar. The evolution of density perturbations in theories of the type $f(R)=R-c /R^ b$ is investigated numerically. It is found that the results obtained by the two methods are in good agreement on sufficiently large scales when the values of the parameters (b,c) are consistent with current observational constraints. However, this agreement becomes progressively poorer for models that differ significantly from the standard concordance model and as smaller scales are considered. ", "machine_abstract": "We study cosmological perturbations on spatially flat Friedmann-Lema\u00eetre-Robertson-Walker (FLRW) backgrounds with the action for f(R) theories of gravity, where R is the Ricci scalar and R its derivative wrt to an auxiliary field $\\xi$. We show that this theory can be recast as Einstein's general relativity plus a minimally coupled scalar field $\\phi$ whose potential depends only on the scale factor $a(t)$. The evolution equations are derived by perturbing the FLRW metric around the background solution. In particular we find that the perturbation equation for the gravitational potential has exactly the same form as in standard GR but with different coefficients which depend explicitly on time through the scale factor. This implies that the growth rate of matter density fluctuations will differ from that predicted by standard GR at late times when dark energy starts dominating over matter. Finally, we discuss how these results could be used to constrain the parameters of the model using current data sets such as WMAP7 or Planck.", "paraphrased_abstract": ".... We present here a new form of gravity which is based on a general theory of gravity, but with some differences in its coefficients, which depend only on the time scale factor. We are examining the effect of this on the cosmological perturbation on the surface of the Friedmann-Lema\u00eetre-Robertson-Walker background by perturbing the metric of the background of the sphere. We show that this theory is recast to Einstein\u2019s general relativity, with a scalar field, xi, which is derived from an auxiliary field xi. This is to say, that the growth rate of the matter density will be different from that predicted by standard GR at the late time when dark energy has taken its place. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1019, "title": "Probabilistic Anonymity and Admissible Schedulers", "abstract": "  When studying safety properties of (formal) protocol models, it is customary to view the scheduler as an adversary: an entity trying to falsify the safety property. We show that in the context of security protocols, and in particular of anonymizing protocols, this gives the adversary too much power; for instance, the contents of encrypted messages and internal computations by the parties should be considered invisible to the adversary.   We restrict the class of schedulers to a class of admissible schedulers which better model adversarial behaviour. These admissible schedulers base their decision solely on the past behaviour of the system that is visible to the adversary.   Using this, we propose a definition of anonymity: for all admissible schedulers the identity of the users and the observations of the adversary are independent stochastic variables. We also develop a proof technique for typical cases that can be used to proof anonymity: a system is anonymous if it is possible to `exchange' the behaviour of two users without the adversary `noticing'. ", "machine_abstract": "We consider the problem of scheduling jobs on parallel machines with probabilistic anonymity constraints, where each job has to be scheduled in such a way that its execution time is hidden among other jobs. We show how this problem can be solved by using admissible schedulers for the underlying deterministic problem. In particular we present two algorithms based on list-scheduling and randomization respectively. The first algorithm achieves a competitive ratio of 2 while the second one obtains a better performance guarantee of 1 + . Both results are tight up to constant factors. Finally, we provide some experimental evidence showing that our randomized algorithm performs well also in practice. Probabilistic anonymity arises naturally when considering privacy issues related to data mining or computational social choice problems. For example, suppose that there exists a set of n users who have to submit their preferences over m alternatives (e.g., candidates) before a certain deadline T. Each user i submits her preference profile Pi = {(ai1 , bi1 ), . . . , (aim , bim )}, where aij denotes the utility she derives from alternative j and bij indicates whether she likes it or not. Then, if all profiles are submitted simultaneously at time t, they should be processed so as to hide which user prefers what alternative. This means that no information about individual preferences should be revealed until after the deadline T.", "paraphrased_abstract": "Then if all the preferences are submitted simultaneously at time t, they must be sorted by means of algorithms to hide the preference of each of the users. To avoid this, we have to hide the preferences of each individual. So if we consider privacy issues arising from data mining or from social choice problems, then suppose that there are n users who have to submit their preferences to m alternatives (e.g., candidates) before a certain time T. Each user will submit her preference profile Pi = (ai, bi) where ai denotes the usefulness of the alternative, and ai denotes whether she likes it or not. We present a method of parallel scheduling with probabilistic anonymity, whereby all the tasks must be executed in such a way that the time of the execution is hidden among the other tasks. In the first method, we obtain a competitive ratio of two, and the second one produces a better result, and is therefore close to the constant. Finally, we provide an experiment in which our randomized algorithm does well in practice. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2859, "title": "Theory of Two-Photon Interactions with Broadband Down-Converted Light and Entangled Photons", "abstract": "  When two-photon interactions are induced by down-converted light with a bandwidth that exceeds the pump bandwidth, they can obtain a behavior that is pulse-like temporally, yet spectrally narrow. At low photon fluxes this behavior reflects the time and energy entanglement between the down-converted photons. However, two-photon interactions such as two-photon absorption (TPA) and sum-frequency generation (SFG) can exhibit such a behavior even at high power levels, as long as the final state (i.e. the atomic level in TPA, or the generated light in SFG) is narrowband enough. This behavior does not depend on the squeezing properties of the light, is insensitive to linear losses, and has potential applications. In this paper we describe analytically this behavior for travelling-wave down-conversion with continuous or pulsed pumping, both for high- and low-power regimes. For this we derive a quantum-mechanical expression for the down-converted amplitude generated by an arbitrary pump, and formulate operators that represent various two-photon interactions induced by broadband light. This model is in excellent agreement with experimental results of TPA and SFG with high power down-converted light and with entangled photons [Dayan et al., Phys. Rev. Lett. 93, 023005 (2004), Dayan et al., Phys. Rev. Lett. 94, 043602, (2005), Pe'er et al., Phys. Rev. Lett. 94, 073601 (2005)]. ", "machine_abstract": "We present the theory for two-photon interactions in broadband down-converted light, including entanglement between photons generated by spontaneous parametric down conversion (SPDC). We show that this leads to new effects such as photon bunching at zero time delay and antibunching at nonzero delays. These results are compared against experimental data obtained using SPDC sources based on periodically poled lithium niobate waveguides. The theoretical model is also used to predict the effect of varying pump bandwidths and crystal lengths on the degree of second-order coherence g(2)(0) measured experimentally. This work was supported by EPSRC grant EP/G037656/1.     In recent years there has been growing interest in quantum optics experiments involving broadband down-conversion [1\u20133]. Such experiments have led to demonstrations of novel phenomena such as single-photon switching [4], sub-Poissonian statistics [5], squeezing [6], and nonclassical correlations [7, 8]. However, many aspects of these experiments remain poorly understood due to difficulties associated with modelling the complicated nonlinear processes involved [9, 10]. Here we develop an analytical description of two-photon interactions in broad-band down-converted light which includes both temporal and spatial degrees of freedom [11, 12]. Our approach allows us to calculate the joint spectral intensity distribution of the down-converted field [13], which can then be used to determine the probability density function describing the arrival times of pairs of photons produced via spontaneous parametric downconversion [14\u201318]. As well as providing insight into the physics underlying broadband down-conversion experiments, our analysis enables quantitative predictions about the behaviour of such systems to be made.", "paraphrased_abstract": "As a result, we present an analytical interpretation of two-photon interactions in the broad-band downconversion of light, and we calculate the joint spectral intensity distribution of the downconversion field. We calculate the probability density function of the arrival times of pairs of photons, which have been generated through the entanglement of spontaneous parametric downconversion. This method is applied to the experiment of a ring of light with a broad-band downconversion. We present the theoretical theory of two-photon interactions in a downconversion of light, and of a decomposition of the light by a spontaneous parametric downconversion. This theory is compared with experimental data obtained from lithium niobate swells. We show that this theory is able to predict the second-order coherence, g(2). This work was supported by the EPSRC grant EP/G037656. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2332, "title": "Predictions from non trivial Quark-Lepton complementarity", "abstract": "  The complementarity between the quark and lepton mixing matrices is shown to provide robust predictions. We obtain these predictions by first showing that the matrix V_M, product of the quark (CKM) and lepton (PMNS) mixing matrices, may have a zero (1,3) entry which is favored by experimental data.   We obtain that any theoretical model with a vanishing (1,3) entry of V_M that is in agreement with quark data, solar, and atmospheric mixing angle leads to $\\theta_{13}^{PMNS}=(9{^{+1}_{-2}})^\\circ$. This value is consistent with the present 90% CL experimental upper limit. We also investigate the prediction on the lepton phases. We show that the actual evidence, under the only assumption that the correlation matrix V_M product of CKM and PMNS has a zero in the entry (1,3), gives us a prediction for the three CP-violating invariants J, S_1, and S_2. A better determination of the lepton mixing angles will give stronger prediction for the CP-violating invariants in the lepton sector. These will be tested in the next generation experiments. Finally we compute the effect of non diagonal neutrino mass in \"l_i -> l_j gamma\" in SUSY theories with non trivial Quark-Lepton complementarity and a flavor symmetry. The Quark-Lepton complementarity and the flavor symmetry strongly constrain the theory and we obtain a clear prediction for the contribution to \"mu -> e gamma\" and the \"tau\" decays \"tau -> e gamma\" and \"tau -> mu gamma\". If the Dirac neutrino Yukawa couplings are degenerate but the low energy neutrino masses are not degenerate, then the lepton decays are related among them by the V_M entries. On the other hand, if the Dirac neutrino Yukawa couplings are hierarchical or the low energy neutrino masses are degenerate, then the prediction for the lepton decays comes from the CKM hierarchy. ", "machine_abstract": "We present here the predictions for the decay rates and CP asymmetries in B decays into two vector mesons, based on the assumption that there is no direct coupling between quarks and leptons at low energies.  We show how this hypothesis leads to relations among different observables which are not predicted by the Standard Model (SM). These relations can be tested experimentally with high precision using data collected at LHCb or Belle II experiments.  The results presented here have been obtained within an effective field theory framework where we assume that all new physics effects appear only through higher dimensional operators suppressed by inverse powers of some large scale M . The leading order contributions to these operators come from integrating out heavy degrees of freedom such as W , Z bosons and top quark. In our analysis we consider both tree-level and loop-induced processes. Our main focus has been put on the study of rare B decays involving one photon and one lepton pair in the final state.", "paraphrased_abstract": "A major part of our study is the study of rare B decays that contain only one photon and one lepton in the final state. We study the decay of rare B decays involving one photon and one lepton in the final state. In our analysis we use tree-level and loop-level calculations. We have applied the results of our study to rare B decays involving a photon and a lepton in the final state. We present the predictions for decay and CP asymmetries of B decays into two vector mesons, based on the assumption that there is no direct connection between quarks and leptons at low energies. We illustrate how this assumption leads to relations between different observables that cannot be predicted by the Standard Model. We show that this hypothesis leads to relations that are not possible under the standard model. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3687, "title": "SIM PlanetQuest Key Project Precursor Observations to Detect Gas Giant Planets Around Young Stars", "abstract": "  We present a review of precursor observing programs for the SIM PlanetQuest Key project devoted to detecting Jupiter mass planets around young stars. In order to ensure that the stars in the sample are free of various sources of astrometric noise that might impede the detection of planets, we have initiated programs to collect photometry, high contrast images, interferometric data and radial velocities for stars in both the Northern and Southern hemispheres. We have completed a high contrast imaging survey of target stars in Taurus and the Pleiades and found no definitive common proper motion companions within one arcsecond (140 AU) of the SIM targets. Our radial velocity surveys have shown that many of the target stars in Sco-Cen are fast rotators and a few stars in Taurus and the Pleiades may have sub-stellar companions. Interferometric data of a few stars in Taurus show no signs of stellar or sub-stellar companions with separations of <5 mas. The photometric survey suggests that approximately half of the stars initially selected for this program are variable to a degree (1 sigma>0.1 mag) that would degrade the astrometric accuracy achievable for that star. While the precursor programs are still a work in progress, we provide a comprehensive list of all targets ranked according to their viability as a result of the observations taken to date. By far, the observable that moves the most targets from the SIM-YSO program is photometric variability. ", "machine_abstract": "The SIM PlanetQuest mission will detect and characterize extrasolar planets by measuring their astrometric wobble, or reflex motion in the plane of the sky.  The SIM PlanetQuest Key Project is currently planning for this exciting new space-based observational capability with an extensive ground-based program that includes observations at multiple wavelengths (optical/infrared) using several telescopes around the world.   This work presents results on two young stars observed as part of our ongoing effort to develop techniques needed to detect gas giant planets orbiting nearby solar-type stars. We present high-precision radial velocity measurements made over a period of more than three years for HD 163296 and over four years for HD 179949. These data are used to search for periodic signals indicative of planetary companions. No significant periodicities were found in either star's RV time series; however, we find evidence for long-term trends in both systems which may be due to magnetic activity cycles and/or stellar evolution effects.", "paraphrased_abstract": "The SIM PlanetQuest Project is preparing for this exciting new space observatory, with a long-term, extensive program of ground-based observation. We present a set of results based on two young stars, which were observed by our team as part of our ongoing research on the discovery of gas giant planets orbiting the nearby stars of the solar system. We report on high-precision radial velocity measurements from HD 163296 and HD 179949, to search for periodic signals from the planetary world. No significant trends were observed in either star's RV time, however, and we found that they may be due to magnetic activity, to the evolutionary effect of the star, and astronomy. The SIM PlanetQuest mission will be able to distinguish and characterize extrasolar planets by measuring their radial movement or recurrence in the sky. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1168, "title": "Suzaku Observations of Active Galactic Nuclei Detected in the Swift/BAT Survey: Discovery of \"New Type\" of Buried Supermassive Black Holes", "abstract": "  We present the Suzaku broad band observations of two AGNs detected by the Swift/BAT hard X-ray (>15 keV) survey that did not have previous X-ray data, Swift J0601.9-8636 and Swift J0138.6-4001. The Suzaku spectra reveals in both objects a heavily absorbed power law component with a column density of NH =~ 10^{23.5-24} cm^{-2} that dominates above 10 keV, and an intense reflection component with a solid angle >~ $2\\pi$ from a cold, optically thick medium. We find that these AGNs have an extremely small fraction of scattered light from the nucleus, <~ 0.5% with respect to the intrinsic power law component. This indicates that they are buried in a very geometrically-thick torus with a small opening angle and/or have unusually small amount of gas responsible for scattering. In the former case, the geometry of Swift J0601.9-8636 should be nearly face-on as inferred from the small absorption for the reflection component. The discovery of two such objects in this small sample implies that there must be a significant number of yet unrecognized, very Compton thick AGNs viewed at larger inclination angles in the local universe, which are difficult to detect even in the currently most sensitive optical or hard X-ray surveys. ", "machine_abstract": "We report on Suzaku observations for four active galactic nuclei (AGNs) detected by Swift/BAT survey, which are classified as \"obscured AGNs\" with column density larger than 10 24 cm-2 . We found that all these sources show strong Fe K emission lines and their line widths are broader than those expected from thermal broadening at kT = 100 keV. The observed line profiles can be reproduced well by relativistic disk reflection models including Compton scattering effects. These results suggest that there is an additional component to the X-ray continuum other than the standard thin accretion disks around supermassive black holes. In addition, we find that the iron abundance relative to solar value is higher than 1.5 times in three out of four objects. This suggests that the central engines of these obscured AGNs may have been buried under heavy dusty torii. Finally, we discuss possible origins of this new type of buried supermassive black holes based on our observational results.", "paraphrased_abstract": "The main engines of these dark-looking AGNs were found to be in dusty toriis. Then, we analyzed the data obtained by observing Suzaku in the vicinity of the AGNs detected by Swift\u2019s survey of the X-ray universe. All four AGNs had strong, radiating K-rays and their radiuses larger than the temperature at kT = 100 keV. This is more than can be predicted by relativistic reflection-based models, such as Compton scattering. We also describe the possible origins of this new type of buried supermassive black holes by presenting the results of our observations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2192, "title": "Analysis and packaging of radiochemical solar neutrino data. 1. Bayesian approach", "abstract": "  According to current practice, the results of each run of a radiochemical solar neutrino experiment comprise an estimate of the flux and upper and lower error estimates. These estimates are derived by a maximum-likelihood procedure from the times of decay events in the analysis chamber. This procedure has the following shortcomings: (a) Published results sometimes include negative flux estimates. (b) Even if the flux estimate is non-negative, the probability distribution function implied by the flux and error estimates will extend into negative territory; and (c) The overall flux estimate derived from the results of a sequence of runs may differ substantially from an estimate made by a global analysis of all of the timing data taken together. These defects indicate that the usual packaging of data in radiochemical solar neutrino experiments provides an inadequate summary of the data, which implies a loss of information. This article reviews this problem from a Bayesian perspective, and suggests an alternative scheme for the packaging of radiochemical solar neutrino data, which is we believe free from the above objections. ", "machine_abstract": "We present an analysis method for the detection of solar neutrinos using radioactive sources in combination with Ge detectors, which is based on a Bayesian statistical approach. The method allows to take into account all relevant experimental uncertainties including those related to background subtraction as well as systematic effects such as energy calibration or detector efficiency variations over time. We apply this new method to two different experiments at Gran Sasso National Laboratory (Italy): Borexino and GNO. In both cases we find that our results are compatible within their respective error bars with previous analyses but show smaller errors due to improved treatment of systematics. This work was supported by the German Science Foundation DFG under grant number BR 1628/2-1. AMS-02 collaboration has recently reported [1] the most precise measurement so far of the positron fraction up to energies of about 300 GeV. It shows a clear excess above the expected astrophysical background [2] . While there have been several attempts to explain these observations [3] , it remains unclear whether they can be attributed to dark matter annihilation [4] . In order to test possible explanations of the observed excess, one needs to know how many positrons are produced per annihilation event. For example, if dark matter particles annihilate predominantly into leptons, then the total number of electrons plus positrons produced per annihilation should equal four times the number of photons produced [5] . If instead dark matter annihilates mostly into quarks, then the ratio between electron-positron pairs and gamma rays will depend on the mass spectrum of the final state hadrons [6] .", "paraphrased_abstract": "This research was funded by the German Science Foundation (DFG), grant number BR 1628/2-1. AMS-02 is the first to have obtained such a measurement of the positron fraction, and it is well known that this excess is due to dark matter\u2019s annihilation. It has been well established, however, that it cannot be attributed to annihilation of dark matter. To test the possible explanation, one must know how many positrons are produced per annihilation. If dark matter is mainly composed of leptons, then the total number of electrons and positrons produced per annihilation will be four times the number of photons produced. If dark matter is in the case of quarks, then the ratio of electrons and positrons to gamma rays and gamma rays will depend on the mass of the final state hadrons. We present a new method for the detection of solar neutrinos by radioactive sources and Ge detectors, which is based on a Bayesian statistical approach. This method has been adapted to be applied to two different experiments in Gran Sasso, in Italy, in the laboratory of GNO, in which we investigate the positron positron positrons with a Ge detector. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1307, "title": "Finite-size effects in roughness distribution scaling", "abstract": "  We study numerically finite-size corrections in scaling relations for roughness distributions of various interface growth models. The most common relation, which considers the average roughness $<w_2>$ as scaling factor, is not obeyed in the steady states of a group of ballistic-like models in 2+1 dimensions, even when very large system sizes are considered. On the other hand, good collapse of the same data is obtained with a scaling relation that involves the root mean square fluctuation of the roughness, which can be explained by finite-size effects on second moments of the scaling functions. We also obtain data collapse with an alternative scaling relation that accounts for the effect of the intrinsic width, which is a constant correction term previously proposed for the scaling of $<w_2>$. This illustrates how finite-size corrections can be obtained from roughness distributions scaling. However, we discard the usual interpretation that the intrinsic width is a consequence of high surface steps by analyzing data of restricted solid-on-solid models with various maximal height differences between neighboring columns. We also observe that large finite-size corrections in the roughness distributions are usually accompanied by huge corrections in height distributions and average local slopes, as well as in estimates of scaling exponents. The molecular-beam epitaxy model of Das Sarma and Tamborenea in 1+1 dimensions is a case example in which none of the proposed scaling relations works properly, while the other measured quantities do not converge to the expected asymptotic values. Thus, although roughness distributions are clearly better than other quantities to determine the universality class of a growing system, it is not the final solution for this task. ", "machine_abstract": "We study the statistical properties of surface fluctuations for different values of the lateral size L and height H of the system, using numerical simulations on square lattices with periodic boundary conditions. We find that the probability density function (PDF) of the local slope angle \u03b8 is well described by an exponential decay at large angles, while it displays power-law tails at small slopes. The crossover between these two regimes occurs around \u03b8 = 0.5\u03c0. In addition to this behavior, we observe that the PDFs are strongly dependent upon both L and H. This dependence can be explained as finite-size effects: when L or H becomes smaller than some characteristic length scale, the PDFs become broader due to the presence of more rare events. Finally, we show how our results compare favorably with experimental data obtained from scanning tunneling microscopy measurements performed on Si(111). PACS numbers: 68.35.Bs", "paraphrased_abstract": "But, in the last place, we show that our result is highly in agreement with the results obtained by the X-ray tunneling experiments on Si (111). We aquire the statistical properties of the slope of the system from the angle of. We find that the local slope angle is linearly decayed at large angles, while the slope at small angles is dominated by the law of the tails. The divergence between these two regimes occurs approximately at  = 0.5. The geometrical properties of the surface are described by the following. At the lateral side of the system the slope angle is estimated by an exponential decay, while at the lower end it is characterized by the inverse of the powerlaw. The ratio of the slope to the distance of the two slopes is inversed, and the PDF is widened by the presence of more rare events. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3394, "title": "Modelling the energy dependencies of high-frequency QPO in black hole X-ray binaries", "abstract": "  We model energy dependencies of the quasi periodic oscillations (QPO) in the model of disc epicyclic motions, with X-ray modulation caused by varying relativistic effects. The model was proposed to explain the high frequency QPO observed in X-ray binaries. We consider two specific scenarios for the geometry of accretion flow and spectral formation. Firstly, a standard cold accretion disc with an active X-ray emitting corona is assumed to oscillate. Secondly, only a hot X-ray emitting accretion flow oscillates, while the cold disc is absent at the QPO radius. We find that the QPO spectra are generally similar to the spectrum of radiation emitted at the QPO radius, and they are broadened by the relativistic effects. In particular, the QPO spectrum contains the disc component in the oscillating disc with a corona scenario. We also review the available data on energy dependencies of high frequency QPO, and we point out that they appear to lack the disc component in their energy spectra. This would suggest the hot flow geometry in the spectral states when high frequency QPO are observed. ", "machine_abstract": "We present an analysis of the dependence on energy and luminosity of the frequencies of two types of quasi-periodic oscillations (QPOs) observed in the power density spectra of accreting black holes, namely low frequency QPOs (LFQPOs; 0.1-10 Hz), which are thought to be related to global properties of the flow close to the central object, and high frequency QPOs (HFQPOs; 10-100 Hz). We use data obtained with RXTE/PCA for four sources: GRO J1655-40, XTE J1550-564, 4U 1543-47 and H 1743-322.  The LFQPO is modelled as arising due to non-linear coupling between radial epicyclic motion at different radii within the disc. This model predicts that the centroid frequency should scale inversely proportional to the square root of the photon energy. In contrast, we find that this scaling relation does not hold when considering HFQPOs. Instead, our results suggest that these features arise due to non-linear coupling between vertical epicyclic motions at different heights above the equatorial plane.", "paraphrased_abstract": "In the present work we describe two quasiperiodic oscillations (QPOs) which have been observed in the density spectra of black holes: those of low frequency (0.1 Hz), which are thought to be related to the global properties of the flow near the central object, and those of high frequency (100 Hz) which are also thought to be related to the global properties of the flow close to the central object, and those of high frequency (100 Hz) of the same period. The LFQPO is modeled as the result of a non-linear interaction between radial epicyclic movements of the disc in the equatorial plane, which corresponds to the rotational scale of the centroid, and a vertical one. The LFQPO is considered to be the result of a non-linear interaction between the radial epicyclic motions in the equatorial plane, and the HFQPO is the result of a non-linear interaction between two horizontally-moving helioids. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3196, "title": "Photon wave mechanics and position eigenvectors", "abstract": "  One and two photon wave functions are derived by projecting the quantum state vector onto simultaneous eigenvectors of the number operator and a recently constructed photon position operator [Phys. Rev A 59, 954 (1999)] that couples spin and orbital angular momentum. While only the Landau-Peierls wave function defines a positive definite photon density, a similarity transformation to a biorthogonal field-potential pair of positive frequency solutions of Maxwell's equations preserves eigenvalues and expectation values. We show that this real space description of photons is compatible with all of the usual rules of quantum mechanics and provides a framework for understanding the relationships amongst different forms of the photon wave function in the literature. It also gives a quantum picture of the optical angular momentum of beams that applies to both one photon and coherent states. According to the rules of qunatum mechanics, this wave function gives the probability to count a photon at any position in space. ", "machine_abstract": "The present work is devoted to the study of photon wave mechanics in terms of position eigenvectors, which are introduced as solutions of the Schr\u00f6dinger equation for photons with an arbitrary energy spectrum. The concept of position eigenvector allows one to describe the state of a single photon by its position probability density distribution function (PDF). It also enables us to introduce the notion of quantum trajectory describing the evolution of this PDF over time. In particular, we show that the quantum trajectories corresponding to different initial states can be obtained from each other by means of unitary transformations. We demonstrate how these results may be used to analyze various phenomena related to the propagation of light through dispersive media. Finally, we discuss possible applications of our approach to the description of nonclassical effects associated with the emission of entangled pairs of photons. DOI: 10.1088/1742-6596/aa5e20 I. INTRODUCTORY REMARkS  In recent years there has been considerable interest in developing new approaches to studying the properties of light fields based on the concepts of quantum optics [1\u20133] . One of such approaches involves introducing the so-called position eigenvectors [4] , which play an important role in the description of the state of a single-photon field [5\u20137] . It should be noted that the use of position eigenvectors makes it possible not only to obtain information about the spatial structure of the electromagnetic field but also to investigate the temporal dynamics of the system under consideration [8, 9] . This fact opens up wide possibilities for applying the proposed method to analyzing various physical processes occurring during the propagation of light waves through dispersive media [10, 11] .     In addition, the introduction of position eigenvectors into the theory of light fields leads to the possibility of using them to describe certain nonclassical effects associated", "paraphrased_abstract": "Moreover, the use of eigenvectors in the theory of light fields can be interpreted in terms of the spatial structure of the electromagnetic field and the temporal dynamics of the system. The present study is dedicated to the study of the waves of light in terms of position eigenvectors, which are called so-called position eigenvectors, which are based on the Schr\u00f6dinger equation for photons with an arbitrary energy. Moreover, by introducing the eigenvectors into the theory of light fields, they can be used to analyze the physical properties of the system. In addition, the introduction of position eigenvectors into the theory of light fields allows one to obtain information on the spatial structure of an electromagnetic field, and at the same time to investigate the temporal dynamics of the system. This fact opens up many possibilities for studying the physical phenomena of the propagation of light in dispersed media. We present here the theoretical results of a spectral study of the physical properties of light waves in the form of position eigenvectors, which are, in fact, inherently related to the physical state of a single photon. We are in the process of implementing the eigenvectors into the spectral model of the electromagnetic field, which is characterized by the position probability density, which is calculated from the PDF of ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1364, "title": "CCD BV survey of 42 open clusters", "abstract": "  We present results of a photometric survey whose aim was to derive structural and astrophysical parameters for 42 open clusters. While our sample is definitively not representative of the total open cluster sample in the Galaxy, it does cover a wide range of cluster parameters and is uniform enough to allow for simple statistical considerations. BV wide-field CCD photometry was obtained for open clusters for which photometric, structural, and dynamical evolution parameters were determined. The limiting and core radii were determined by analyzing radial density profiles. The ages, reddenings, and distances were obtained from the solar metallicity isochrone fitting. The mass function was used to study the dynamical state of the systems, mass segregation effect and to estimate the total mass and number of cluster members. This study reports on the first determination of basic parameters for 11 out of 42 observed open clusters. The angular sizes for the majority of the observed clusters appear to be several times larger than the catalogue data indicate. The core and limiting cluster radii are correlated and the latter parameter is 3.2 times larger on average. The limiting radius increases with the cluster's mass, and both the limiting and core radii decrease in the course of dynamical evolution. For dynamically not advanced clusters, the mass function slope is similar to the universal IMF slope. For more evolved systems, the effect of evaporation of low-mass members is clearly visible. The initial mass segregation is present in all the observed young clusters, whereas the dynamical mass segregation appears in clusters older than about log(age)=8. Low-mass stars are deficient in the cores of clusters older than log(age)=8.5 and not younger than one relaxation time. ", "machine_abstract": "We present the results of our CCD photometric study of 42 open clusters in the southern hemisphere, carried out at the 1-meter telescope of the South African Astronomical Observatory (SAAO). The observations were made with an SBIG STL-1001E camera equipped with a Kodak KAF-0400 chip and Johnson V filter during three observing runs between September 1998 and February 1999. We have used DAOPHOT II to perform aperture photometry on all stars detected within each cluster field-of-view. A total number of about 15000 stars was measured for each cluster. In addition we obtained UBVRI photometry for some of these clusters using the same instrumentation as described above.  From this data set we derived the following parameters: reddening E(B-V), distance modulus DM, age t, metallicity [Fe/H] , mass function slope x, core radius rc, central surface brightness \u00b50, concentration index c, and integrated absolute magnitude M.", "paraphrased_abstract": "A total of about fifteen thousand stars were analyzed. In addition, we obtained the UBVRI parameter from some of these clusters. We have calculated the following parameters: the reddening E(B-V) and the distance modulus DM, the age t, the metallicity x, the mass function slope x, the central brightness 0, the concentration index c, the absolute magnitude M. Moreover, we have obtained the UBVRI parameter from other clusters, using the same instrumentation as described above. This study was performed at the 1-meter telescope of the SAAO. It was performed with a SBIG STL-1001E camera, equipped with a Kodak KAF-0400 chip and a Johnson V filter, during three observations, in September 1998 and February 1999. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2135, "title": "Ordering phenomena in quasi one-dimensional organic conductors", "abstract": "  Low-dimensional organic conductors could establish themselves as model systems for the investigation of the physics in reduced dimensions. In the metallic state of a one-dimensional solid, Fermi-liquid theory breaks down and spin and charge degrees of freedom become separated. But the metallic phase is not stable in one dimension: as the temperature is reduced, the electronic charge and spin tend to arrange themselves in an ordered fashion due to strong correlations. The competition of the different interactions is responsible for which broken-symmetry ground state is eventually realized in a specific compound and which drives the system towards an insulating state.   Here we review the various ordering phenomena and how they can be identified by optic and magnetic measurements. While the final results might look very similar in the case of a charge density wave and a charge-ordered metal, for instance, the physical cause is completely different. When density waves form, a gap opens in the density of states at the Fermi energy due to nesting of the one-dimension Fermi surface sheets. When a one-dimensional metal becomes a charge-ordered Mott insulator, on the other hand, the short-range Coulomb repulsion localizes the charge on the lattice sites and even causes certain charge patterns.   We try to point out the similarities and conceptional differences of these phenomena and give an example for each of them. Particular emphasis will be put on collective phenomena which are inherently present as soon as ordering breaks the symmetry of the system. ", "machine_abstract": "The ordering phenomenon is studied for the Hubbard model with nearest-neighbor hopping and on-site Coulomb repulsion at half-filling by means of the density matrix renormalization group (DMRG) method. The ground state phase diagram as functions of U/t and t/J are obtained, where J denotes the antiferromagnetic exchange coupling constant between neighboring sites. It is found that there exist three phases; paramagnetic metal, spin-density wave insulator, and charge-density wave insulator. In particular, we find that the transition point between the spin-density wave insulators and charge-density wave insulating states depends strongly on the ratio of t/J. We also study the effect of electron-phonon interaction to this system using the DMRG combined with the self-consistent phonon theory. The results show that the electron-phonon interaction enhances the stability of the charge-density wave insulatory state against the magnetic field.     Keywords: Ordering phenomenon, Hubbard model, density matrix renormalization", "paraphrased_abstract": "The study of the order of the Hubbard model, the nearest neighbouring hopping and Coulomb repulsion at half-filling, is done by means of the density matrix renormalization (DMRG) of the ground state. Then we calculate the ratio of U/t to J, where J is the ratio of the antiferromagnetic exchanges of the neighbouring sites. We also investigate the effect of electrons on this system, combining the DMRG of the self-consistent phonon theory with the self-consistent phonon theory. We study the ordering of the ground state, the Hubbard model, and the density matrix renormalization of the local sites by means of density matrix renormalization. We also study the effect of electrons on the insulating properties of the earth, using the density matrix renormalization group. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0362, "title": "(Non) Gauge Invariance of Wilsonian Effective Actions in (Supersymmetric) Gauge Theories : A Critical Discussion", "abstract": "  We give a detailed critical discussion of the properties of Wilsonian effective actions, defined by integrating out all modes above a given scale $\\mu$. In particular, we provide a precise and relatively convenient prescription how to implement the infrared cutoff $\\mu$ in any loop integral that is manifestly Lorentz invariant and also preserves global linear symmetries such as e.g. supersymmetry. We discuss the issue of gauge invariance of effective actions in general and in particular when using background field gauge. Our prescription for the IR cutoff (as any such prescription) breaks the gauge symmetry. Using our prescription, we have explicitly computed, at one loop, many terms of the Wilsonian effective action for general gauge theories, involving bosonic and fermionic matter fields of arbitrary masses and in arbitrary representations, exhibiting the non-gauge invariant (as well as the gauge invariant) terms. However, for supersymmetric gauge theories all non-gauge invariant terms cancel within each supermultiplet. This is strong evidence that in supersymmetric gauge theories this indeed defines a Lorentz, susy and gauge invariant Wilsonian effective action. As a byproduct, we obtain the explicit one-loop Wilsonian couplings for all higher-derivative terms $F D^{2n}F$ in the effective action of arbitrary supersymmetric gauge theories. ", "machine_abstract": "We discuss the gauge invariance properties of effective actions obtained by integrating out heavy degrees of freedom, such as quarks and gluons, within supersymmetric Yang-Mills theories. We show that these actions are not generally invariant under local gauge transformations unless certain conditions on their form are satisfied. These results have important consequences for the construction of gauge-invariant observables in supersymmetric gauge theories. They also provide an explanation why it is possible to construct nontrivial superpotentials even though supersymmetry does not allow any explicit breaking terms at tree level. Finally we argue that our findings can be used to resolve some puzzling features observed recently in lattice simulations of N = 1 supersymmetric QCD with four flavors. Supersymmetric Yang-Mills theories play an important role both in particle physics and string theory. Their low-energy dynamics is described by an effective action which contains all quantum corrections due to the integration over heavy fields like quarks or gluons. This effective action has been studied extensively during recent years but many questions remain open concerning its precise structure. One particular issue concerns the question whether this action is gauge invariant. It was shown already more than twenty years ago [1] that if one integrates out only massive fermions then the resulting effective action is indeed gauge invariant. However, when including also massive bosonic degrees of freedom there exist counterexamples where the effective action fails to be gauge invariant [2] . Recently, this problem attracted renewed interest because of its relevance for the understanding of non-perturbative phenomena in supersymmetric gauge theories [3, 4] . In this work we study the gauge invariance properties systematically using functional methods. Our main result is that the effective action is always gauge invariant up to total derivatives provided two conditions are met. First, the effective action must contain no higher-order time-derivatives acting on the gauge field. Second, the coefficients appearing in front of the various operators in the effective action should satisfy certain relations. For example, they cannot depend explicitly on the gauge coupling constant g. If either condition is violated then the effective action will fail to be gauge", "paraphrased_abstract": "He is the main force of a Yang-Mills-like force which, when acted on a mass of atoms, is said to be gauge-invariant. This action is also known as a gauge-invariant action in the case of a single atom. During the past twenty years, it has been known that if you incorporate only the most atoms and only the largest atoms, then the resulting effect is hardly gauge-invariant. However, when one adds enormous fermions and enormous atoms, there are some examples of the atoms which are not gauge-invariant. Our research is in the area of a supersymmetric Yang-Mills equation and explains the behavior of the effective action in the presence of atoms and atoms of the low energy, i.e., that it is gauge-invariant up to a certain degree, and therefore it is unable to be gauge-invariant in any sense. Several studies have been done in this area in recent years, but many questions remain to be answered, especially concerning its nature and structure. For example, the effective action has to be void of high-order time-derivatives, and the coefficients in front of the operators must satisfy certain relations, for example, not to depend on the gauge-invariant constant ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1561, "title": "Graviton Propagator in a Covariant Massive Gravity Theory", "abstract": "  We study the massive gravity theory proposed by Arkani-Hamed, Georgi and Schwartz. In this theory, the graviton becomes massive when general covariance is spontaneously broken through the introduction of a field that links two metrics, one of the which will eventually decouple. The excitation of this \"link\" field acts like a Goldstone boson in giving mass to the graviton. We work out the graviton and Goldstone boson propagators explicitly by means of gauge fixing terms similar to the renormalizability gauges used in gauge theories. With these propagators, we calculate the lowest order tree-level interaction between two external energy momentum tensors. The result is independent of the gauge parameter, but different from the prediction of massless gravity theory, i.e., general relativity. This difference remains even if the mass of the graviton goes to zero, in which case it gives the van Dam-Veltman-Zakharov (vDVZ) discontinuity between the propagators of a massive and massless linearized graviton. Moreover, we show that the Fierz-Pauli graviton mass term can be considered as the ``unitary gauge'' of a more general theory with an extra scalar field. We explicitly construct such a theory in which the vDVZ discontinuity arises with a graviton mass term that is different from the Fierz-Pauli mass term. This theory has a local Weyl symmetry under conformal transformations of the metric. In the case when the mass goes to zero, the Weyl summetry becomes a global symmetry. It is possible that the local Weyl symmetry will give a hint as to the form of the corresponding fully nonlinear theory having a nonzero graviton mass. ", "machine_abstract": "We study the graviton propagator in covariant massive gravity theory with an arbitrary number of gravitons and show that it is given by the sum over all Feynman diagrams which are obtained by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We also present explicit expressions for the first few terms in this expansion, including the leading order term corresponding to the usual Einstein-Hilbert action. The results presented here can be used as input into calculations involving higher-order corrections to gravitational processes such as black hole evaporation. In particular, we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times.  I. INTRODUCTORY REMARkS The purpose of this work is twofold. First, we will derive the exact expression for the graviton propagator (or Green's function) in covariant massive gravity theories with an arbitrary number of external gravitons. Second, we will use our result to calculate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of freedom associated with the massive spin-2 field. Our analysis follows closely the approach developed in Ref. [1] , where the authors studied the effect of adding massless scalar fields to the standard model of particle physics on the emission rate of Hawking radiation [2] .", "paraphrased_abstract": "The results we give will be used to formulate a theory of the gravitational convexity or Green's function in the covariance of the massless scalar field to the standard model of particle physics. We study the gravitational convexity in a covariance of the massless scalar field and show that it is a sum over all Feynman diagrams, and the sum of all Feynman diagrams arranged at the level of the tree of gravitation. The result will be shown, with explicit expressions, to the first few terms of this expansion, and the first, the first, the leading term corresponding to the usual Einstein-Hilbert action. The study of the gravitational convexity in the reversal of a Schwarzschild black hole and the addition of extra degrees of freedom of the scalar field to the standard model of particle physics, yielded a more rapid increase in the temperature of the Hawking rays. The results will be used to calculate the temperature of the Hawking rays at late times. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2496, "title": "Galactic dark matter as a bulk effect on the brane", "abstract": "  The behavior of the angular velocity of a test particle moving in a stable circular orbit in the vacuum on the brane is considered. In the brane world scenario, the four dimensional effective Einstein equation acquire extra terms, called dark radiation and dark pressure, respectively, which arise from the embedding of the 3-brane in the bulk. A large number of independent observations have shown that the rotational velocities of test particles gravitating around galaxies tend, as a function of the distance from the galactic center, toward constant values. By assuming a constant tangential velocity, the general solution of the vacuum gravitational field equations on the brane can be obtained in an exact analytic form. This allows us to obtain the explicit form of the projections of the bulk Weyl tensor on the brane, and the equation of state of the dark pressure as a function of the dark radiation. The physical and geometrical quantities are expressed in terms of observable/measurable parameters, like the tangential velocity, the baryonic mass and the radius of the galaxy. We also analyze the dynamics of test particles by using methods from the qualitative analysis of dynamical systems, by assuming a simple linear equation of state for the dark pressure. The obtained results provide a theoretical framework for the observational testing at the extra-galactic scale of the predictions of the brane world models. ", "machine_abstract": "We propose that galactic dark matter is an effective four-dimensional manifestation of extra dimensions, and we show how this can be realized in a simple model with one extra dimension compactified on S 1 /Z 2 . The fifth dimension has two 3-branes at its endpoints which are connected by a bulk scalar field. We find that the scalar field develops a kink profile along the fifth dimension due to the presence of a potential barrier between the two branes. This leads to a localized mass term for fermions living on the visible (3-)brane, which gives rise to a phenomenologically viable dark matter candidate. In addition, there exists another class of particles called Kaluza-Klein modes whose masses depend on the size of the extra dimension. These KK states have no tree-level interactions with Standard Model fields but they may contribute significantly to loop processes such as neutrino oscillations or proton decay. Finally, we discuss possible experimental signatures of our scenario.", "paraphrased_abstract": "In addition, there are other particles, such as the Kaluza-Klein modes, whose masses depend on the size of the extra dimension. These particles have no tree-level interactions with the Standard Model, but may contribute to the loop processes, such as neutrino oscillations and proton decays. Moreover, the sixth dimension has two 3-branes connecting it, which are connected by a bulk scalar field. In the fifth dimension, a kink exists between the two branes, and we derive from this kink the localized mass of the fermions which reside on the visible (3-branes), thereby obtaining a phenomenologically plausible dark matter candidate. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.228, "title": "Longitudinal impedance and wake from XFEL undulators. Impact on current-enhanced SASE schemes", "abstract": "  In this article we derive longitudinal impedance and wake function for an undulator setup with arbitrary undulator parameter, taking into account a finite transverse size of the electron bunch. Earlier studies considered a line density-distribution of electrons instead. We focus our attention on the long-wavelength asymptote (compared with resonance wavelength), at large distance of the electron bunch from the undulator entrance compared to the overtaking length, and for large vacuum-chamber size compared to the typical transverse size of the field. These restrictions define a parameter region of interest for practical applications. We calculate a closed expression for impedance and wake function that may be evaluated numerically in the most general case. Such expression allows us to derive an analytical solution for a Gaussian transverse and longitudinal bunch shape. Finally, we study the feasibility of current-enhanced SASE schemes (ESASE) recently proposed for LCLS, that fall well-within our approximations. Numerical estimations presented in this paper indicate that impedance-induced energy spread is sufficient to seriously degrade the FEL performance. Our conclusion is in contrast with results in literature, where wake calculations for the LCLS case are given in free-space, as if the presence of the undulator were negligible. ", "machine_abstract": "We present an analytical model for the longitudinal impedance and wakefield generated by a single period of an XFEL undulator, including effects due to electron beam emittance and energy spread. We show that these effects can be significant in some cases, especially when considering schemes where the electron bunch is short compared with the wavelength (e.g., self-amplified spontaneous emission). The results are used to assess the impact of this effect on two proposed schemes at LCLS-II. In one scheme, we consider using a tapered wiggler as part of a chicane-based compressor system; in another case, we examine the use of a magnetic chicago-bars section following the undulator. For both cases, we find that the inclusion of realistic impedance and wakefields leads to modest changes in the predicted performance.     Keywords: Undulator, Wakefield, Impedance, Self-Amplified Spontaneous Emission", "paraphrased_abstract": "He is of the firm belief that this is the case. The latter has proposed a XFEL undulator, and it has been developed to estimate the total impedance and the swoon-fields of a single period. Using a thermoelectricity and a beam spread based on the flux of the electrons, we prove that these effects can be significant, especially when a scheme whose electron-tongue is very short (as in the case of spontaneous emission). We are aiming at a swish-down XFEL undulator, and a model of the effects of the self-amped swoon. This model was implemented in the XFEL undulator, using a thermoelectrical model for the longitudinal swoon and a radial model for the transverse swoon. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.158, "title": "Empirical Photometric Redshifts of Luminous Red Galaxies and Clusters in SDSS", "abstract": "  In this work I discuss the necessary steps for deriving photometric redshifts for luminous red galaxies (LRGs) and galaxy clusters through simple empirical methods. The data used is from the Sloan Digital Sky Survey (SDSS). I show that with three bands only ({\\it gri}) it is possible to achieve results as accurate as the ones obtained by other techniques, generally based on more filters. In particular, the use of the $(g-i)$ color helps improving the final redshifts (especially for clusters), as this color monotonically increases up to $z \\sim 0.8$. For the LRGs I generate a catalog of $\\sim 1.5$ million objects at $z < 0.70$. The accuracy of this catalog is $\\sigma = 0.027$ for $z \\le 0.55$ and $\\sigma = 0.049$ for $0.55 < z \\le 0.70$. The photometric redshift technique employed for clusters is independent of a cluster selection algorithm. Thus, it can be applied to systems selected by any method or wavelength, as long as the proper optical photometry is available. When comparing the redshift listed in literature to the photometric estimate, the accuracy achieved for clusters is $\\sigma = 0.024$ for $z \\le 0.30$ and $\\sigma = 0.037$ for $030 < z \\le 0.55$. However, when considering the spectroscopic redshift as the mean value of SDSS galaxies on each cluster region, the accuracy is at the same level as found by other authors: $\\sigma = 0.011$ for $z \\le 0.30$ and $\\sigma = 0.016$ for $030 < z \\le 0.55$. The photometric redshift relation derived here is applied to thousands of cluster candidates selected elsewhere. I have also used galaxy photometric redshifts available in SDSS to identify groups in redshift space and then compare the redshift peak of the nearest group to each cluster redshift (ABRIDGED). ", "machine_abstract": "We present an empirical photometric redshift technique for luminous red galaxies (LRGs) and clusters, using the Sloan Digital Sky Survey Data Release 5 (SDSS DR5). We use a sample of LRGs/clusters with spectroscopic redshifts to calibrate our method by fitting their observed colors as functions of redshift. The resulting color-redshift relations are then used to estimate photometric redshifts for all LRG/cluster candidates selected from the SDSS imaging data. Our results show that this simple approach can yield accurate photometric redshifts over most of the range 0 < z < 1.2 covered by the survey. For example, we find that the rms scatter between the estimated and true redshifts is less than 0.05(1+z), which corresponds to about 60 km/s at z = 0.6. This accuracy is comparable or better than those achieved by other methods based on template-fitting techniques.     Keywords: Empirical photometric redshifts; Luminous Red Galaxy; Cluster", "paraphrased_abstract": "We show that the method can be applied to all spheres of the sky, including z and y, as well as for the whole range of 0  z and y = 1.2, compared with the ones developed by the physico-metric method. We introduce a simple method of estimating redshifts for the galaxies of luminous red (LRGs) and clusters, using the Sloan Digital Sky Survey (SDS). The data set contains the spectral redshifts of the LRGs and clusters, and the resulting redshifts are compared with the measured colors, corresponding to the corresponding redshifts of the LRGs and clusters. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0619, "title": "Search for Heavy Neutral MSSM Higgs Bosons with CMS: Reach and Higgs-Mass Precision", "abstract": "  The search for MSSM Higgs bosons will be an important goal at the LHC. We analyze the search reach of the CMS experiment for the heavy neutral MSSM Higgs bosons with an integrated luminosity of 30 or 60 fb^-1. This is done by combining the latest results for the CMS experimental sensitivities based on full simulation studies with state-of-the-art theoretical predictions of MSSM Higgs-boson properties. The results are interpreted in MSSM benchmark scenarios in terms of the parameters tan_beta and the Higgs-boson mass scale, M_A. We study the dependence of the 5 sigma discovery contours in the M_A-tan_beta plane on variations of the other supersymmetric parameters. The largest effects arise from a change in the higgsino mass parameter mu, which enters both via higher-order radiative corrections and via the kinematics of Higgs decays into supersymmetric particles. While the variation of $\\mu$ can shift the prospective discovery reach (and correspondingly the ``LHC wedge'' region) by about Delta tan_beta = 10, we find that the discovery reach is rather stable with respect to the impact of other supersymmetric parameters. Within the discovery region we analyze the accuracy with which the masses of the heavy neutral Higgs bosons can be determined. We find that an accuracy of 1-4% should be achievable, which could make it possible in favourable regions of the MSSM parameter space to experimentally resolve the signals of the two heavy MSSM Higgs bosons at the LHC. ", "machine_abstract": "The search is performed in the context of the Minimal Supersymmetric Standard Model (MSSM) using data collected by the Compact Muon Solenoid experiment at sqrt(s) = 7 TeV, corresponding to an integrated luminosity of 5 fb-1 . The results are interpreted as limits on the production cross section times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance. In addition, upper bounds on the mass difference between the lightest CP-even Higgs boson and its heavier CP-even or CP-odd partner are derived. These results improve upon previous searches conducted by the ATLAS collaboration.     A summary of this work has been presented at:             This document contains additional information that may be useful to readers interested in reproducing our analysis or applying it to other datasets. It also includes details about how we have validated our results against those obtained independently by the ATLAS collaboration.    Introduction  The discovery of a new particle consistent with the Standard Model (SM) Higgs boson [1\u20133] has opened up a new era in particle physics. However, many open questions remain regarding the properties of this newly discovered state [4] , including whether it is part of a larger multiplet [5] . In supersymmetry [6] , each SM field has a superpartner differing only in spin statistics [7, 8] . If R-parity [9] is conserved, then all superpartners must be produced in pairs [10] . One consequence of this scenario is that there can exist more than one Higgs doublet [11] . In particular, if the lighter scalar Higgs boson observed at the LHC [12\u201318] corresponds to the lightest CP-eigenstate h0 of such a model [19, 20] , then the next-to-lightest CP-eigenstates H0 and A0 could both couple strongly to fermions [21] . Such scenarios would lead to enhanced rates for decays of these states into final states containing photons [22] .     In order to explore possible deviations from the SM predictions [23] , precise measurements of the masses and couplings of the Higgs bosons predicted by", "paraphrased_abstract": "As a result, it was possible to find a pair of Higgs bosons which were not predicted by the SM. To study possible deviations from the predictions of the SM, we have set out to investigate the properties of the new Higgs boson and the relation between them. This study has been carried out by the Compact Muon Solenoid Lab at the depth of 7 TeV, which is an integrated luminosity of five fb-1. This paper is an update of the results of the earlier search of the SM Higgs boson. The results of the search are presented in the context of the M. S. S. M. S. M., which is defined by the supersymmetry of the S. S. S. (SSM) has only one superpartner, a spin 8 = 68. This 882557105371010 1010101010101101010 210101010910102 121231220200520251091020103,23  ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2707, "title": "Concise theory of chiral lipid membranes", "abstract": "  A theory of chiral lipid membranes is proposed on the basis of a concise free energy density which includes the contributions of the bending and the surface tension of membranes, as well as the chirality and orientational variation of tilting molecules. This theory is consistent with the previous experiments [J.M. Schnur \\textit{et al.}, Science \\textbf{264}, 945 (1994); M.S. Spector \\textit{et al.}, Langmuir \\textbf{14}, 3493 (1998); Y. Zhao, \\textit{et al.}, Proc. Natl. Acad. Sci. USA \\textbf{102}, 7438 (2005)] on self-assembled chiral lipid membranes of DC$_{8,9}$PC. A torus with the ratio between its two generated radii larger than $\\sqrt{2}$ is predicted from the Euler-Lagrange equations. It is found that tubules with helically modulated tilting state are not admitted by the Euler-Lagrange equations, and that they are less energetically favorable than helical ripples in tubules. The pitch angles of helical ripples are theoretically estimated to be about 0$^\\circ$ and 35$^\\circ$, which are close to the most frequent values 5$^\\circ$ and 28$^\\circ$ observed in the experiment [N. Mahajan \\textit{et al.}, Langmuir \\textbf{22}, 1973 (2006)]. Additionally, the present theory can explain twisted ribbons of achiral cationic amphiphiles interacting with chiral tartrate counterions. The ratio between the width and pitch of twisted ribbons is predicted to be proportional to the relative concentration difference of left- and right-handed enantiomers in the low relative concentration difference region, which is in good agreement with the experiment [R. Oda \\textit{et al.}, Nature (London) \\textbf{399}, 566 (1999)]. ", "machine_abstract": "The authors present an overview of the current state-of-the-art in understanding how lipids form membranes and what determines their physical properties.  They then introduce a new theoretical framework for describing these phenomena, which they call the concise theory of chiral lipid membranes (CTCLM).  The CTCLM is based on three key concepts:  1) Lipid bilayers are composed of two interdigitated monolayers; 2) Each monolayer contains both enantiomeric forms of each lipid species; 3) Enantiomers have different molecular shapes that lead to differences in packing density within the membrane.  This model explains many experimental observations about the structure and dynamics of biological membranes without introducing any additional parameters or assumptions beyond those already used by existing models. It also provides a simple explanation for why certain types of lipids tend to be found at specific locations within cell membranes. Finally, it suggests several testable predictions that can help guide future experiments aimed at further refining our understanding of this important class of biomolecules.", "paraphrased_abstract": "The authors introduce an overview of the current state of understanding on the structure and dynamics of lipid membranes, and describe a new theory of chiral lipid membranes (CTCLM): First, a bilayer is made of two interdigitated monolayers; second, a single monolayer contains the enantiomeric forms of each lipid species; third, enantiomers have different molecular shapes, and a difference in the density of the membrane is explained. This theory explains many experimental observations about the structure and behavior of lipid membranes, without adding any new assumptions or new assumptions. In the second place, it gives a simple explanation of why some lipids are found in certain places within the membranes. The third place is proposed as a model of the chiral lipid membranes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0003, "title": "XO-2b: Transiting Hot Jupiter in a Metal-rich Common Proper Motion Binary", "abstract": "  We report on a V=11.2 early K dwarf, XO-2 (GSC 03413-00005), that hosts a Rp=0.98+0.03/-0.01 Rjup, Mp=0.57+/-0.06 Mjup transiting extrasolar planet, XO-2b, with an orbital period of 2.615857+/-0.000005 days. XO-2 has high metallicity, [Fe/H]=0.45+/-0.02, high proper motion, mu_tot=157 mas/yr, and has a common proper motion stellar companion with 31\" separation. The two stars are nearly identical twins, with very similar spectra and apparent magnitudes. Due to the high metallicity, these early K dwarf stars have a mass and radius close to solar, Ms=0.98+/-0.02 Msolar and Rs=0.97+0.02/-0.01 Rsolar. The high proper motion of XO-2 results from an eccentric orbit (Galactic pericenter, Rper<4 kpc) well confined to the Galactic disk (Zmax~100 pc). In addition, the phase space position of XO-2 is near the Hercules dynamical stream, which points to an origin of XO-2 in the metal-rich, inner Thin Disk and subsequent dynamical scattering into the solar neighborhood. We describe an efficient Markov Chain Monte Carlo algorithm for calculating the Bayesian posterior probability of the system parameters from a transit light curve. ", "machine_abstract": "We report the discovery and characterization of XO-2b, an extrasolar planet transiting its host star (HD 149026) with a period of 3.2 days. The planet is a hot Jupiter with M = 1.3 MJup and R = 0.9 RJup orbiting at a distance of only 0.04 AU from HD 149026. We find that this system has a common proper motion companion separated by ~1\u2032\u2032.5. This companion was previously identified as a metal-rich subgiant based on high-resolution spectroscopy but had not been detected photometrically before our observations. Our analysis shows that the transit depth variation observed for XO-2b can be explained if we assume that the two stars are physically associated and have nearly identical radii. If true, then the mass ratio between these two stars should be close to unity. However, we cannot rule out other scenarios such as grazing eclipses or blending effects due to nearby field stars.", "paraphrased_abstract": "The distance between them is less than 1 arc em. This, however, does not prevent the possibility of eclipses or splinters in the stars that are nearby. In this case, the differences in the two stars should be close to one. We find that this is the case with XO-2b, an extrasolar planet, a swarming Jupiter of the type Jupiter with a radius of 0.04 arc em. This planet, which is the hot Jupiter XO, has an orbit of 3.3 days. It is a hot Jupiter of 1.3 MJp and 0.9 Rjp, orbiting at a distance of only 0.04 arc from XO-2b. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1796, "title": "Iron line profiles and self-shadowing from relativistic thick accretion discs", "abstract": "  We present Fe Kalpha line profiles from and images of relativistic discs with finite thickness around a rotating black hole using a novel code. The line is thought to be produced by iron fluorescence of a relatively cold X-ray illuminated material in the innermost parts of the accretion disc and provides an excellent diagnostic of accretion flows in the vicinity of black holes. Previous studies have concentrated on the case of a thin, Keplerian accretion disc. This disc must become thicker and sub-Keplerian with increasing accretion rates. These can affect the line profiles and in turn can influence the estimation of the accretion disc and black hole parameters from the observed line profiles. We here embark on, for the first time, a fully relativistic computation which offers key insights into the effects of geometrical thickness and the sub-Keplerian orbital velocity on the line profiles. We include all relativistic effects such as frame-dragging, Doppler boost, time dilation, gravitational redshift and light bending. We find that the separation and the relative height between the blue and red peaks of the line profile diminish as the thickness of the disc increases. This code is also well-suited to produce accretion disc images. We calculate the redshift and flux images of the accretion disc and find that the observed image of the disc strongly depends on the inclination angle. The self-shadowing effect appears remarkable for a high inclination angle, and leads to the black hole shadow being completely hidden by the disc itself. ", "machine_abstract": "We present the results of our investigation into iron K\u03b1 emission lines produced by relativistically blurred reflection in X-ray binaries (XRBs). We use Monte Carlo simulations to calculate the expected profile for different values of black hole spin, inclination angle between the observer's line-of-sight and the orbital plane of the binary system, inner radius of the reflecting region on the surface of the neutron star or stellar-mass black hole, and strength of the illuminating continuum source. The effects of gravitational redshift are included as well as Doppler broadening due to Keplerian motion around the central mass. In addition we include the effect of Compton downscattering within the reflector which leads to an additional red wing being added to the line profile. Finally, we also consider the possibility that the emitting material is not distributed evenly over the entire surface of the reflector but instead forms a geometrically thin layer close to its equatorial plane. This geometry has been suggested previously based upon observations made with ASCA and BeppoSAX . Our main conclusions are:", "paraphrased_abstract": "We have been analyzing the results of a simulation of the spectral profile of XRBs as a whole, with different angular and angular variations, of the angle between the observer\u2019s line of sight and the orbital plane of the XRB, the inner radius of the reflected region on the surface of the neutron star or stellar mass, and the strength of the continuum of light. We have made the assumption that the spectral profile is not uniform across the whole surface of the XRB, but rather is only narrowing to the equatorial plane. Besides, we have included the effect of the gravity of the gravitational redshift, which has caused a further extension of the yawny of the central mass, and the effect of Compton downscattering within the XRD. In addition, we consider the possibility that the spectral material is not evenly distributed across the entire surface of the XRD, but is instead in a thin layer close to the equatorial plane. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0142, "title": "A Modest Proposal for the Astronomical Community", "abstract": "  Inspired by a recent astro-ph posting, I propose a creation of an Alternative History astro-ph archive (althistastro-ph). Such an archive would serve as a final resting place for the various telescope (and possibly other) proposals that were not successful. As we all know, from both submitting proposals and also from serving on various time allocation committees, many excellent proposals ``do not make it''. Creating such an AltHist archive would serve many goals, including venting the frustration of the authors and also providing possible amusement for the readers. These are worthy goals, but they alone would not warrant creating such an archive. The truly useful role of AltHistAstro-ph archive would be to match astronomers with unappreciated ideas with other astronomers with underutilized resources, hopefully leading in some cases to resurrection of old proposals and resulting publications in the regular astro-ph archive. Given the possible danger of a low signal-to-noise and possible confusion, a creation of a separate archive seems like a good idea, although it should be noted that low signal-to-noise is achieved on astro-ph quite often already. Finally, I include my own excellent, but rejected (twice), HST proposal, as an example of a potential AltHistAstro-ph posting. ", "machine_abstract": "The author proposes that astronomers should be required to publish their data in open access journals, and not just their results.  The author's argument is based on his experience with the journal Astronomy & Astrophysics (A&A), which he describes as an example of how astronomy can benefit by adopting more transparent practices. He also discusses some other benefits of publishing data openly, including increased citations and improved reproducibility. Finally, he presents several arguments against this proposal, such as concerns about quality control and peer review. This article was published online on September 24th 2014 at http://arxiv.org/abs/1409.5389v1 . Astronomers are often criticized for being slow to release their data publicly. In fact, many astronomical datasets remain inaccessible even after years or decades have passed since they were collected. As a result, it has become increasingly difficult to reproduce previous research findings using these data sets. To address this problem, I propose that all astronomical publications must include a statement indicating whether or not the authors plan to make their data available upon publication. If no such statement appears, then the manuscript will automatically be rejected without further consideration.", "paraphrased_abstract": "The author\u2019s argument is based on his experience with the journal Astronomy & Astrophysics, and he cites it as an example of the way in which astronomy can benefit from more openness. The author argues that in order to make this issue an official matter, the paper must contain a statement indicating that it is intended to be freely available. If it does not contain this statement, then the manuscript will be rejected without any further consideration. This article was published online on September 24, 2014 at http://arxiv.org/abs/1409.5389. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0581, "title": "Nonlocal Excitation Spectra in 2D Doped Hubbard Model", "abstract": "  Single-particle excitation spectra of the two-dimensional Hubbard model on the square lattice near half filling and at zero temperature are investigated on the basis of the self-consistent projection operator method. The method guarantees a high accuracy of the spectra with high energy and high momentum resolutions. It takes into account long-range intersite correlations as well as the strong on-site correlations. Effects of nonlocal excitations are clarified by comparing the results with those of the single-site approximation. The calculated spectra verify the quantum Monte-Carlo results for finite temperatures. The spectra at the Fermi level yield a hole-like Fermi surface in the underdoped region and an electron-like Fermi surface in the overdoped region. From a numerical analysis of the momentum dependent effective mass and self-energy, it is concluded that a marginal Fermi-liquid like state persists even at finite doping concentrations in the strongly correlated region because a van Hove singularity is pinned to the Fermi surface. It is also found that a kink structure appears in the quasiparticle energy band in the same region. The kink is shown to be caused by a mixing between the quasiparticle band and an excitation band with strong short-range antiferromagnetic correlations. These results suggest an explanation for some of the unusual properties of the normal state in high-$T_{\\rm c}$ cuprates. ", "machine_abstract": "We study the nonlocal excitation spectra for doped Mott insulators by using the dynamical cluster approximation (DCA) combined with the exact diagonalization method and the Lanczos algorithm. We show that the doping dependence of the low-energy part of the spectrum is well reproduced even at half-filling, where the system has no charge carriers. The results suggest that the low energy excitations are dominated by spin fluctuations rather than charge fluctuations. In addition to this we find that there exists an additional peak structure around the Fermi level which cannot be explained within the conventional picture based on local excitations. These findings may provide important information about the nature of electronic states near the metal-insulator transition point. PACS numbers: 71.10.Pm, 72.20.-i, 73.40.Gk  I. INTRODUCTORY REMARK The two-dimensional (2D) doped Mott insulator is one of the most interesting subjects in condensed matter physics because it can exhibit various types of novel phenomena such as high-Tc superconductivity [1] , colossal magnetoresistance [2] , and quantum Hall effect [3] . It is believed that these phenomena originate from strong electron correlations between electrons [4] . In order to understand the physical properties of strongly correlated systems theoretically, many numerical methods have been developed so far [5] - [8] . Among them, the dynamical mean-field theory [9] provides us with useful insights into the ground state properties [10] - [12] . However, since its applicability is limited only to the weak-coupling regime, it fails to describe the excited-state properties correctly [13] . On the other hand, the density matrix renormalization group [14] gives accurate results for both ground-and excited-states [15] but requires huge computational resources when applied to large clusters [16] . Therefore, it would be desirable if some efficient numerical techniques could be found to treat both ground-", "paraphrased_abstract": "Among them are the dynamical mean field theory, which is helpful in estimating the state of the earth, but which is limited to the weak coupling regime, and cannot explain the state of the excited. Besides, we have found that there are also many other effects, namely, the high-temperature superconductivity, the superconductivity of the atom, the quantum Hall effect, and the quantum Hall effect. We have studied, for the first time, the nonlocal emission spectrum of the doped Mott insulator with the density matrix renormalization, with the same results as above, but it requires enormous computational resources for the treatment of large clusters. It is expected that there are efficient numerical techniques to acquaint ourselves with the physical properties of strongly correlated systems. In this field, many numerical methods have been proposed to obtain information about the properties of the ground state and the excited state, but this has not been sufficiently investigated. Besides, we find a peak structure around the Fermi level that is not considered in the standard description, which cannot be explained in the traditional picture. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.31, "title": "The Origin of the Galaxy Mass-Metallicity Relation and Implications for Galactic Outflows", "abstract": "  (Abridged) Using cosmological hydrodynamic simulations in combination with analytic modeling, we show that the galaxy stellar mass-metallicity relation (MZR) provides strong constraints on galactic outflows across cosmic time. We compare three outflow models: No outflows, a \"constant wind\" (cw) model that emulates the popular Dekel & Silk (1986) scenario, and a ``momentum-driven wind\" (vzw) model. We find that only the vzw scaling reproduces the observed z~2 MZR's slope, amplitude, and scatter. Comparing our fully three-dimensional simulations with a simple one-zone chemical evolution model, we find that the MZR can be understood in terms of three parameters: (1) The equilibrium metallicity Z_eq=y*SFR/ACC (where y=true yield), reflecting the enrichment balance between star formation rate SFR and gas accretion rate ACC; (2) the dilution time t_d=M_g/ACC, representing the timescale for a galaxy to return to Z_eq after a metallicity-perturbing interaction; and (3) the blowout mass M_blowout, which is the galaxy stellar mass above which winds can escape its halo. When outflows with mass loading factor MLF are present, galaxies below M_blowout obey Z_eq = y/(1+MLF), while above M_blowout, Z_eq->y. Our cw model has M_blowout ~ 10^10 M_sun, which yields a sharp upturn in the MZR above this scale and a flat MZR with large scatter below it, in strong disagreement with observations. Our vzw model reproduces the observed Z_g \\propto M_*^0.3 because Z_eq \\propto MLF^-1 \\propto M_*^1/3 when MLF >> 1 (i.e. at low masses). The flattening of the MZR at M_* > 10^10.5 M_sun observed by Tremonti et al. (2004) reflects the mass scale where MLF~1, rather than a characteristic wind speed. The tight observed MZR scatter is ensured when t_d<1 dynamical time, which is only satisified at all masses and epochs in our momentum-driven wind model. ", "machine_abstract": "We present an analysis of the mass-metallicity relation (MMR) in galaxies, using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use two different methods to estimate galaxy masses -the stellar-mass-to-light ratio method and the dynamical mass method-which are both calibrated against direct measurements of galaxy masses obtained by gravitational lensing. The MMR is found to be well described by a power law with index -0.25 \u00b1 0.01 dex/log(M*), where M* denotes the total stellar mass within one effective radius Re. This slope agrees very well with previous studies based on smaller samples or less accurate estimates of galaxy masses. However, we find that our results differ significantly from those reported recently by Tremonti et al. (2004) , who claim a shallower slope of -0.16 \u00b1 0.02 dex/log(M*) (a factor of 2 difference at high masses). The origin of this discrepancy can be traced back to differences between the sample selection criteria used in these works. In particular, Tremonti et al. select their sample according to the presence/absence of emission lines in the SDSS spectra, which introduces significant biases into the resulting distribution of metallicities. Our results show no evidence for such biases: when restricting ourselves to only star-forming galaxies, as done by Tremonti et", "paraphrased_abstract": "And so, it turns out that the inclination is too great for this. We can use two different methods to measure the mass of galaxies, namely the mass-to-light ratio and the dynamic mass. We have also examined the inclination of the metallicities in the spherical universe, using the data of the Sssy Digital Sky Survey, published last June, as a supplement to the previous studies. The results of our study are well consistent with the results of Tremonti et al. (2005), who have chosen to use a lower one, and thus to use a smaller one, the MMR is not as high as the results of Tremonti et al. (2005), who are averaging the inclination of the metallicity to the maximum of the effective radius. The MMR is well described by a power law with a value of 0.25 dex/log (M*), where M* represents the mass of the stellar mass in one radius, Re. This slope is very good, and even compared with the previous work by Tremonti et al. (2005), who claim a lower slope of -0.16 dex/log (M*), a factor of 2 at high mass. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0359, "title": "The Shapes of Molecular Cloud Cores in Orion", "abstract": "  We investigate the intrinsic shapes of starless cores in the Orion GMC, using the prestellar core sample of Nutter and Ward-Thompson (2007), which is based on submillimeter SCUBA data. We employ a maximum-likelihood method to reconstruct the intrinsic distribution of ellipsoid axial ratios from the axial ratios of projected plane-of-the-sky core ellipses. We find that, independently of the details of the assumed functional form of the distribution, there is a strong preference for oblate cores of finite thickness. Cores with varying finite degrees of triaxiality are a better fit than purely axisymmetric cores although cores close to axisymmetry are not excluded by the data. The incidence of prolate starless cores in Orion is found to be very infrequent. We also test the consistency of the observed data with a uniform distribution of intrinsic shapes, which is similar to those found in gravoturbulent fragmentation simulations. This distribution is excluded at the 0.1% level. These findings have important implications for theories of core formation within molecular clouds. ", "machine_abstract": "We present the results of an investigation into the shapes and orientations of molecular cloud cores in the Orion Nebula region using near-infrared polarimetry obtained with the Gemini North telescope. We find that most (80%) of our sample are oblate spheroids, while 20% have more complex morphologies including prolate spheroids, disks, or bipolar structures. The majority of these objects show no evidence for rotation; however, we do detect significant polarization vectors aligned perpendicular to the major axes of several sources which may be due to magnetic fields. These observations suggest that many of the clouds were formed by large-scale gravitational collapse rather than rotational support. This work is supported by NASA grant NNX10AC99G. We report on the results of an investigation of the shapes and orientations of dense molecular gas clumps within the Orion Nebula region. Using near-infrared polarimetric imaging data taken at Gemini Observatory's North Telescope, we identify 80 percent of our sample as oblate spheroids. Twenty percent exhibit more complicated morphologies such as prolate spheroids or bipolar structures. Most of these objects appear to lack any internal rotation but some display polarization vectors oriented perpendicularly to their major axes suggesting the presence of magnetic fields.", "paraphrased_abstract": "\u201cThis work is supported by NASA grant N. X-10AC99. The present study is supported by NASA grant NNX-10AC99G. In this study we will describe the shapes and orientation of dense gaseous clumps in the Orion nebula. The results of our analysis show that the majority of the clumps, 80 per cent, are oblate, while a minority are complex, such as disks and bipolar clouds. The majority of these objects show no internal rotation, but several have polarizations perpendicular to the major axes, indicating that the clouds were formed not with a rotational force, but with a magnetic force. Most of these objects are without rotation, but some show significant polarizations perpendicular to the major axes, which could be due to magnetic fields. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2259, "title": "The Wiretap Channel with Feedback: Encryption over the Channel", "abstract": "  In this work, the critical role of noisy feedback in enhancing the secrecy capacity of the wiretap channel is established. Unlike previous works, where a noiseless public discussion channel is used for feedback, the feed-forward and feedback signals share the same noisy channel in the present model. Quite interestingly, this noisy feedback model is shown to be more advantageous in the current setting. More specifically, the discrete memoryless modulo-additive channel with a full-duplex destination node is considered first, and it is shown that the judicious use of feedback increases the perfect secrecy capacity to the capacity of the source-destination channel in the absence of the wiretapper. In the achievability scheme, the feedback signal corresponds to a private key, known only to the destination. In the half-duplex scheme, a novel feedback technique that always achieves a positive perfect secrecy rate (even when the source-wiretapper channel is less noisy than the source-destination channel) is proposed. These results hinge on the modulo-additive property of the channel, which is exploited by the destination to perform encryption over the channel without revealing its key to the source. Finally, this scheme is extended to the continuous real valued modulo-$\\Lambda$ channel where it is shown that the perfect secrecy capacity with feedback is also equal to the capacity in the absence of the wiretapper. ", "machine_abstract": "We consider the wiretap channel model in which an eavesdropper can obtain feedback about its observations and use it to improve future decoding attempts. We show that, for any positive integer , there exists a sequence of (2, ) codes such that each code has rate at least . This result is obtained by combining polar coding techniques with ideas from successive cancellation list decoding. The proof relies on a new converse bound for the secrecy capacity region of the wiretap channel with feedback. Finally, we present numerical results illustrating our achievability scheme. In this work, we study the wiretap channel model introduced by Wyner [1] . In particular, we focus on the case where the legitimate receiver obtains feedback about his past observations and uses it to improve his future decoding attempts. Our main contribution consists in showing that polar codes [2] combined with successive cancellation list decoding [3] achieve rates close to the secrecy capacity when used over channels with feedback. More specifically, we prove that if the number of messages grows exponentially fast as a function of the blocklength then every message can be transmitted reliably using a code whose length scales linearly with the number of messages . The remainder of this paper is organized as follows. Section II introduces some notation and definitions. Section III presents our main theorem. Section IV provides a sketch of the proof of Theorem 3.1. Finally, Section V concludes the paper.", "paraphrased_abstract": "This is the main point of this paper, and the next sections will follow the form: a first introduction and a short explanation of the topic; a second introduction to the theory of the present paper, and the second part of the introduction to the proof. The second part consists of a series of (1,2) codes, each of which has a rate of at least. This result is obtained by the combination of polar codes and successive cancellation lists, and by the proof of the secret capacity of the feedback channel. The last part, however, presents the proof of a new formulae for the secret capacity of the feedback channel. We present the study of the wire-tap channel, which is studied by Wyner, in particular by the case where the legitimate receiver receives feedback about his past observations and uses it to improve the future decoding. We illustrate that the sequence of integers consists of a series of (2,3) codes, each of which has a rate of at least. We conclude by showing that, if the number of messages increases exponentially as a function of the block length, it is possible to send each message in a uniform manner, which is proportional to the number of messages. The last results are given in numerical form, which demonstrate the validity of our argument. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0719, "title": "Gravitational Wave Signals from Chaotic System: A Point Mass with A Disk", "abstract": "  We study gravitational waves from a particle moving around a system of a point mass with a disk in Newtonian gravitational theory. A particle motion in this system can be chaotic when the gravitational contribution from a surface density of a disk is comparable with that from a point mass. In such an orbit, we sometimes find that there appears a phase of the orbit in which particle motion becomes to be nearly regular (the so-called ``stagnant motion'') for a finite time interval between more strongly chaotic phases. To study how these different chaotic behaviours affect on observation of gravitational waves, we investigate a correlation of the particle motion and the waves. We find that such a difference in chaotic motions reflects on the wave forms and energy spectra. The character of the waves in the stagnant motion is quite different from that either in a regular motion or in a more strongly chaotic motion. This suggests that we may make a distinction between different chaotic behaviours of the orbit via the gravitational waves. ", "machine_abstract": "We study gravitational wave signals from chaotic systems by using the point mass model with an accretion disk around it, which is one of the most promising candidates for gravitational wave sources in astrophysics.  We show that there are two types of gravitational waves emitted from such systems:  The first type comes from the orbital motion of the binary system and its frequency spectrum has peaks at integer multiples of the orbital frequency.  The second type comes from the spiral structure formed on the surface of the accretion disk due to tidal interaction between the central black hole and the surrounding matter.  Its frequency spectrum shows no clear peak but rather consists of many frequencies whose amplitudes decrease as their frequencies increase.  In addition we find that the amplitude of the second type of gravitational waves can be much larger than that of the first type when the spin parameter of the central black hole is large enough (a > 0.9).  This suggests that the detection rate of gravitational waves may depend strongly on the spin parameter of the source if the signal-to-noise ratio is high enough.", "paraphrased_abstract": "This explains why the amplitude of the second type of waves is far greater than that of the first type when the spin parameter of the central black hole is large (a >0.9). The second type is a spiral formed on the surface of the central black hole by the tidal interaction between the central black hole and the surrounding matter. Its frequency does not converge, but consists of many frequencies that decrease in frequency as the frequency increases. Besides, it can be shown that the amplitude of the second type is far greater than that of the first, if the spin parameter of the central black hole is large enough. We have studied the gravitational waves from chaotic systems in the point mass model, which is one of the most promising sources for gravitational waves in astrophysics. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3859, "title": "Systematics of Soft Particle Production at RHIC: Lessons from PHOBOS", "abstract": "  The PHOBOS experiment has measured the properties of particle production in heavy ion collisions between sqrt(s_NN) of 20 and 200 GeV. The dependencies of charged particle yield on energy, system size, and both longitudinal and transverse momentum have been determined over close to the full kinematic range. Identified charged particles emitted near mid-rapidity have been studied over about 2 orders of magnitude in transverse momentum. This broad data set was found to be characterized by a small number of simple scalings which factorize to a surprising degree. This study has recently been extended by the addition of new data for Cu+Cu as well as new analyses of Au+Au data, including more peripheral collisions. In addition, the exploration of global properties has been expanded with the use of new techniques, including two-particle correlations, more sensitive searches for rare events, and more detailed studies of particles emitted at very forward rapidity. The characteristics of particle production which are revealed by this extensive data set will be described along with the implications for future data from the LHC. ", "machine_abstract": "Soft particle production in relativistic heavy ion collisions is one of the most important observables for understanding the properties of strongly interacting matter produced in these reactions.  In this talk, I will present results on soft hadron and photon production obtained by the PHOBOS experiment at RHIC. The main focus will be on systematics observed as functions of collision centrality, beam energy, transverse momentum (pT), rapidity (y) and azimuthal angle (#phi). These observations are compared with theoretical predictions based on hydrodynamic models and transport calculations. Finally, we discuss how to use these measurements to extract information about the equation-of-state of nuclear matter. This work was supported by DOE grant DE-FG02-97ER41022. AMS-02 collaboration has recently reported that the positron fraction increases above 10 GeV [1] . It is generally believed that such an increase can only come from astrophysical sources like pulsars or supernova remnants [2] , but it may also have contributions from dark matter annihilation [3] . In order to understand whether there exists any contribution from dark matter annihilations, it is necessary to know precisely what happens to electrons/positrons during their propagation through space [4] . However, our knowledge of cosmic ray propagation is limited due to uncertainties associated with solar modulation effects [5] . To overcome this difficulty, we propose to measure the fluxes of primary cosmic rays using satellites [6] .", "paraphrased_abstract": "But the nature of this phenomena is still a question, and we have only recently found that the fraction of electrons is growing to ten thousand tonnes, which is in the nature of the pulsars and supernovae whose emission is high, but there is also a possibility of dark matter annihilation, so that it is necessary to know the exact conditions of the electrons and the electrons in the path of cosmic radiation. We will consider this method to be the most accurate method to investigate the state of the nuclear matter. In this talk, I will show the results of the phobos experiment performed at RHIC by a PHOBOS experiment. I will be focusing on the systematics of the correlations observed as a function of the centrality of the beam, the energy of the beam, the longitudinal momentum, the rapidity of the y-axis, and the angle of the azimuthal axis, and the method of analysis. Besides, I will also present the results of a simulation study of the distribution of positrons in relativistic heavy ions. This data is essential for the study of the properties of strongly interacting materials in such reactions. In order to determine whether dark matter annihilation has occurred, it is necessary to investigate the origin of dark matter. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4396, "title": "Report of the US long baseline neutrino experiment study", "abstract": "  This report provides the results of an extensive and important study of the potential for a U.S. scientific program that will extend our knowledge of neutrino oscillations well beyond what can be anticipated from ongoing and planned experiments worldwide. The program examined here has the potential to provide the U.S. particle physics community with world leading experimental capability in this intensely interesting and active field of fundamental research. Furthermore, this capability could be unique compared to anywhere else in the world because of the available beam intensity and baseline distances. The present study was initially commissioned in April 2006 by top research officers of Brookhaven National Laboratory and Fermi National Accelerator Laboratory and, as the study evolved, it also provided responses to questions formulated and addressed to the study group by the Neutrino Scientific Advisory Committee (NuSAG) of the U.S. DOE and NSF. The participants in the study, its Charge and history, plus the study results and conclusions are provided in this report and its appendices. A summary of the conclusions is provided in the Executive Summary. ", "machine_abstract": "The Long Baseline Neutrino Experiment (LBNE) is an international collaboration to build and operate a new accelerator-based neutrino beam facility in the United States, with a proposed location at Fermilab near Chicago, Illinois.  The LBNE will use a novel design for its proton driver that combines two existing technologies - Cyclotrons and Recirculating Linacs - into one compact system.  This report describes the status of this project as of September 2009.   It also presents some preliminary results on the physics potential of the LBNE program. The Long Baseline Neutrinos Experiment (LBNE), currently under development by an international collaboration, plans to construct a new accelerator based neutrino beam facility in northern New Mexico. The facility would be located about 1000 km away from the FermiLab site in Chicago, IL where it could take advantage of the intense NuMI neutrino beam produced there. In addition to providing high intensity neutrino beams over a wide range of energies, the facility would include a large liquid argon time projection chamber detector capable of measuring both charged current interactions and neutral current elastic scattering events. The facility would provide unique opportunities to explore fundamental questions related to neutrino masses and mixings, CP violation, and other phenomena associated with neutrino oscillations.", "paraphrased_abstract": "The project is a physics experiment, which has been proposed by an international collaboration, to construct a new nebula-based neutrino-beam in northern New Mexico. It is a novel project, the first of its kind in the world, to build a new accelerator-based neutrino-beam in North America, and it is to be built at Fermi Lab, a few thousand miles from Chicago. The nebula-beam in the Fermi Lab, where the nuMI numinium was produced, has been used for its energy. In addition to the high energy of the neutrino-beam, the facility will also be equipped with a large liquid argon detector, which will be capable of measuring both the charged current and the neutral current, a polarization of which is the nature of the sphere. This is a new project of the LBNE, which is a joint venture with an international collaboration, to build and operate a new accelerator-based neutrino-beam facility in northern New Mexico, at the FermiLab site in Chicago, where NuMI-Neo-beam can be used. The LBNE will be built using a novel and new electron-engineering technique combining two technologies, cyclotrons and recirculating litho-Linacs", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0036, "title": "A remark on the number of steady states in a multiple futile cycle", "abstract": "  The multisite phosphorylation-dephosphorylation cycle is a motif repeatedly used in cell signaling. This motif itself can generate a variety of dynamic behaviors like bistability and ultrasensitivity without direct positive feedbacks. In this paper, we study the number of positive steady states of a general multisite phosphorylation-dephosphorylation cycle, and how the number of positive steady states varies by changing the biological parameters. We show analytically that (1) for some parameter ranges, there are at least n+1 (if n is even) or n (if n is odd) steady states; (2) there never are more than 2n-1 steady states (in particular, this implies that for n=2, including single levels of MAPK cascades, there are at most three steady states); (3) for parameters near the standard Michaelis-Menten quasi-steady state conditions, there are at most n+1 steady states; and (4) for parameters far from the standard Michaelis-Menten quasi-steady state conditions, there is at most one steady state. ", "machine_abstract": "We consider a model for an enzymatic reaction network with multiple futile cycles, where each enzyme is assumed to be present at constant concentration and all reactions are reversible. We show that there exists only one positive equilibrium point if the total amount of enzymes is sufficiently large compared to the total amount of substrate molecules. This result can also be applied to other models such as those describing metabolic networks or gene regulatory systems. The proof relies on the fact that the system has a unique globally asymptotically stable equilibrium point when it is considered under mass action kinetics. In addition we prove that this equilibrium point is locally exponentially stable even though the system does not satisfy the classical Lipschitz condition. Finally, numerical simulations illustrate our results. Keywords: Enzymatic reaction networks; Mass action; Steady state analysis; Metabolic networks; Gene regulatory systems. 1 Introduction Reaction networks have been widely used to describe biochemical processes occurring inside living cells (see e.g., [1] , [4] ). These networks consist of chemical species which interact through chemical reactions. A mathematical description of these interactions leads to a set of ordinary differential equations known as the kinetic equations. For example, the Michaelis-Menten mechanism describes how an enzyme E binds reversibly to its substrate S to form a complex C before releasing product P . It consists of three elementary reactions given by  where k + i and k \u2212 i denote respectively the forward and backward rate constants associated with the ith reaction. If the concentrations of the reactants and products involved in the above scheme are denoted by [S] , [E] , [P ] and [C] then the corresponding kinetic equations read dS dt = k 2 [E][S] \u2212 k \u22121 [S], dE dt = k 3 [E][P ] \u2212 k \u22122 [E],  dC dt = k 4 [C][P ] \u2212 k \u22123 [C].  The parameters k i represent the rates of the different reactions. Note that the first two equations correspond to the formation of complexes while the last equation corresponds to their dissociation into free substrates and products.", "paraphrased_abstract": "In the following table, we consider the kinetic equations: the kinetic equations, the reaction rate, the coefficient of error, and the kinetic coefficient of error. 1 The following two equations 1 the formation of complexes and the final one 2 the dissociation of substrates and products. The following two equations: the formation of complexes, the separation of substrates and products. This result is based on the fact that, as a matter of fact, the concentration of the enzymes is always in equilibrium, and the concentration of the substrates is in equilibrium, and that this equilibrium is also locally exponential, even though the equilibrium point is not in accordance with the Lipschitz equilibrium. Moreover, we demonstrate that the system has an unusually stable equilibrium for all its components in a mass-action model, if one takes the model of a metabolic system or a gene therapy. In this study we present an enzymatic network, with a continuous course of action, in which each of the enzymes is constantly in equilibrium, and that all reactions are reversible. In this model, for example, we consider the Michaelis-Menten system: the enzyme E binds irreversibly to its substrate S, to form a complex C, and to release the free product P. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3807, "title": "Atomic data from the Iron Project. LXIV. Radiative transition rates and collision strengths for Ca II", "abstract": "  This work reports radiative transition rates and electron impact excitation rate coefficients for levels of the n= 3, 4, 5, 6, 7, 8 configurations of Ca II. The radiative data were computed using the Thomas-Fermi-Dirac central potential method in the frozen core approximation and includes the polarization interaction between the valence electron and the core using a model potential. This method allows for configuration interactions (CI) and relativistic effects in the Breit-Pauli formalism. Collision strengths in LS-coupling were calculated in the close coupling approximation with the R-matrix method. Then, fine structure collision strengths were obtained by means of the intermediate-coupling frame transformation (ICFT) method which accounts for spin-orbit coupling effects. We present extensive comparisons with the most recent calculations and measurements for Ca II as well as a comparison between the core polarization results and the \"unpolarized\" values. We find that core polarization affects the computed lifetimes by up to 20%. Our results are in very close agreement with recent measurements for the lifetimes of metastable levels. The present collision strengths were integrated over a Maxwellian distribution of electron energies and the resulting effective collision strengths are given for a wide range of temperatures. Our effective collision strengths for the resonance transitions are within ~11% from previous values derived from experimental measurements, but disagree with latter computations using the distorted wave approximation. ", "machine_abstract": "The present work is part of an ongoing effort to provide accurate atomic data for astrophysical applications, in particular those related with stellar atmospheres. The main objective has been to calculate radiative transition probabilities (A-values) and electron-ion collisional excitation rates (B-values), as well as effective collision strengths (\u03a5effs). These quantities are obtained by solving the relativistic many-body problem using large-scale configuration-interaction Dirac-Fock-Sturm calculations. In this contribution we report results on transitions among the lowest 25 levels of calcium ions up to charge state Z = 26. For each ion considered here, we have calculated A-values for all electric dipole allowed transitions between these levels. We also give B-values for all possible transitions within the same set of levels. Finally, \u03a5effs were computed for all transitions involving at least one level above n = 3. All our results can be found online through the Atomic Data Center website http://cdsarc.u-strasbg.fr/viz-bin/cat/J/A+A/.     Keywords: Collision strength, Transition probability", "paraphrased_abstract": "And we show the results in this paper, in particular in the case of calcium ions, from the lowest level to the charge of z=26. The calculations are based on the relativistic many-body problem, with the result that the equation of the two equations Dirac-Fock-Sturm solves, and the two-dimensional results of the three-dimensional model, are based on the arithmetic of the many-body inverse problem. The results of the experiments are compiled on the Atomic Data Center website: cdsarc.ustrasbg.fr.: A-values, B-values, effs. The present work is part of the ongoing effort to provide atomic data for the astrophysics, particularly those pertaining to the atmosphere of the galaxy. We present results on the transitions among the lowest five levels of calcium ion, to the charge state Z=26. For each ion, we present A-values for the electric dipole that may pass between these three levels, B-values for the transitions of all levels above n = 3. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0922, "title": "The substellar mass function in sigma Orionis. II. Optical, near-infrared and IRAC/Spitzer photometry of young cluster brown dwarfs and planetary-mass objects", "abstract": "  We investigate the mass function in the substellar domain down to a few Jupiter masses in the young sigma Orionis open cluster (3+/-2 Ma, d = 360^+70_-60 pc). We have performed a deep IJ-band search, covering an area of 790 arcmin^2 close to the cluster centre. This survey was complemented with an infrared follow-up in the HKs- and Spitzer 3.6-8.0 mum-bands. Using colour-magnitude diagrams, we have selected 49 candidate cluster members in the magnitude interval 16.1 mag < I < 23.0 mag. Accounting for flux excesses at 8.0 mum and previously known spectral features of youth, 30 objects are bona fide cluster members. Four are first identified from our optical-near infrared data. Eleven have most probable masses below the deuterium burning limit and are classified as planetary-mass object candidates. The slope of the substellar mass spectrum (Delta N / Delta M = a M^-alpha) in the mass interval 0.11 Msol M < 0.006 Msol is alpha = +0.6+/-0.2. Any opacity mass-limit, if these objects form via fragmentation, may lie below 0.006 Msol. The frequency of sigma Orionis brown dwarfs with circumsubstellar discs is 47+/-15 %. The continuity in the mass function and in the frequency of discs suggests that very low-mass stars and substellar objects, even below the deuterium-burning mass limit, may share the same formation mechanism. ", "machine_abstract": "We present optical (BVRI), near infrared (JHK) and mid-infrared (IRAC 3.6-8.0 micron; MIPS 24 micron) photometry for the known population of low-mass stars and brown dwarfs in the open star forming region Sigma Orionis. We use these data to derive spectral types and bolometric luminosities for all objects with masses below 0.1 solar masses. The resulting substellar mass function is compared to that derived by previous studies using different techniques. Our results are consistent with those obtained previously but we find evidence for an excess number of very-low mass objects at the faint end of our sample which may be due to unresolved binaries or contamination by background galaxies. This work was supported by NASA grant NAG5-12942. We thank J. Stauffer for providing us with his list of candidate members prior to publication.     Keywords: Open clusters", "paraphrased_abstract": "We thank J. Stauffer for supplying his list of members in advance. We also thank J. Stauffer for granting us a list of candidates for the study. This study was supported by NASA grant NAG5-12942. We present optical, near infrared, and mid-infrared (i.e., i.e., 3.6-8.0 microns; MIPS 24 microns). We derive the bolometric and bolometric characteristics of all the objects with mass below 0.1 solar masses. We derive from the radiance and spectral luminosity of all objects with masses below 0.1 solar masses. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3021, "title": "On the Critical Ionization Velocity Effect in Interstellar Space and Possible Detection of Related Continuum Emission", "abstract": "  Interstellar neutral hydrogen (HI) emission spectra manifest several families of linewidths whose numerical values (34, 13 & 6 km/s) appear to be related to the critical ionization velocities (CIVs) of the most abundant interstellar atomic species. Extended new analysis of HI emission profiles shows that the 34 km/s wide component, probably corresponding to the CIV for helium, is pervasive. The 34 km/s wide linewidth family is found in low-velocity (local) neutral hydrogen (HI) profiles as well as in the so-called high-velocity clouds. In addition, published studies of HI linewidths found in the Magellanic Stream, Very-High-Velocity Clouds, and Compact High-Velocity Clouds, all of which are believed to be intergalactic, have noted that typical values are of the same order. If the critical ionization velocity effect does play a role in interstellar space it may be expected to produce locally enhanced electron densities where rapidly moving neutral gas masses interact with surrounding plasma. Evidence is presented that suggests that this phenomenon is occurring in interstellar space. It manifests as a spatial association between peaks in HI structure offset with respect to peaks in high-frequency radio continuum data obtained with the Wilkinson Microwave Anisotropy Probe. ", "machine_abstract": "We have studied theoretically the effect of critical ionization velocity on the emission spectrum of interstellar space plasma, which is caused by the interaction between cosmic rays (CRs) and neutral atoms. The results show that there are two peaks at about 0.1 keV and 1 MeV in the continuum emission produced by CRs with energies below 10 GeV/nucleon interacting with hydrogen gas clouds. These peaks can be used to detect the existence of such clouds through observations. We also find that these peaks will disappear if the cloud temperature exceeds 100 K or its density decreases to less than 5 cm-3 . This work was supported by the National Natural Science Foundation of China under Grant No. 10273001.     Keywords: Cosmic ray; Hydrogen atoms; Cloud detection; Continuum emission; Critical ionization velocity. Cited by: 2         INTRODUCTION     In recent years, many experiments have been carried out to study the interactions between cosmic rays (CR) and interstellar medium (ISM). It has been found that the energy loss rate of CR particles depends strongly on their velocities [1] , so it may affect the propagation process of CRs [2] - [4] . However, most previous studies only considered the effects of Coulomb scattering [5] - [8] and nuclear collisions [9] - [11] . There were few works done on the influence of other processes [12] - [14] .     In this letter we investigate the effect of critical ionization speed on the emission spectrum of ISM plasma, which is caused mainly by the interaction between CRs and neutral atoms [15] . Our calculations show that there exist two peaks at about 0\uff0e1keV and 1MeV in the continuum emission produced when CRs with energies below10GeV/nucleon interact with hydrogen gas clouds [16] . These peaks could be detected by future X-ray telescopes [17] .  THEORY AND CALCULATIONS  Theoretical Model for Critical Ionization Speed In order to calculate the emission spectrum of ISP plasma, we first need to know how fast an ionized particle moves away from the place where it is created. According to Ref. [18] ,", "paraphrased_abstract": "The authors report that the ionization rate of CRs is strongly dependent on the velocity of their collisions with the ion, which is a very important factor in the formation of CRs. There is no evidence for the effect of Coulomb scattering, nor does it affect nuclear collisions, nor does it affect the propagation of CRs. The present paper introduces the ionization rate of CRs in the ionization rate of ionization in the ionized plasma of ionized gas, which is caused by the interaction between ionized gases and neutral gases. Moreover, there are two peaks, about 0.1 keV, and about 1 MeV, in the ionization rate of ionized gas, which reaches 0.1 keV, and which can be seen by X-ray telescopices. This work was funded by the National Natural Science Foundation of China under the project No. 10733. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1586, "title": "21-cm synthesis observations of VIRGOHI 21 - a possible dark galaxy in the Virgo Cluster", "abstract": "  Many observations indicate that dark matter dominates the extra-galactic Universe, yet no totally dark structure of galactic proportions has ever been convincingly identified. Previously we have suggested that VIRGOHI 21, a 21-cm source we found in the Virgo Cluster using Jodrell Bank, was a possible dark galaxy because of its broad line-width (~200 km/s) unaccompanied by any visible gravitational source to account for it. We have now imaged VIRGOHI 21 in the neutral-hydrogen line and find what could be a dark, edge-on, spinning disk with the mass and diameter of a typical spiral galaxy. Moreover, VIRGOHI 21 has unquestionably been involved in an interaction with NGC 4254, a luminous spiral with an odd one-armed morphology, but lacking the massive interactor normally linked with such a feature. Numerical models of NGC 4254 call for a close interaction ~10^8 years ago with a perturber of ~10^11 solar masses. This we take as additional evidence for the massive nature of VIRGOHI 21 as there does not appear to be any other viable candidate. We have also used the Hubble Space Telescope to search for stars associated with the HI and find none down to an I band surface brightness limit of 31.1 +/- 0.2 mag/sq. arcsec. ", "machine_abstract": "We present new radio continuum and H I absorption data for the dwarf irregular galaxy VIRGOHI 21, which is located at the edge of the virgo cluster. The galaxy has an optical diameter of about 1 kpc (0.3 arcmin) and shows no signs of star formation activity. We find that its neutral hydrogen mass amounts to M_HI = 2 x 10^{10} M_sol. Its total luminosity L_TOT = 3.5 x 10^{8} L_SOL corresponds to a B-band absolute magnitude MB = -18.7 mag. This value agrees well with those found by other authors for similar galaxies. From our analysis we conclude that this object may be considered as a candidate for a \"dark galaxy\". It contains only little or even no stars but still possesses a large amount of cold gas. If confirmed, it would provide further evidence for the existence of such objects.", "paraphrased_abstract": "We find that the neutral hydrogen mass in the galaxy is 2 x 1010 M. Its total luminosity is 3.5 x 108 L.SOL. This is an absolute magnitude of \u20138.0 mag. We conclude that the object may be considered a dark galaxy, and that it contains little stars or even no stars but contains a considerable amount of cold gas. Its total luminosity, LTOT 3.5 x 108 L_sOL, corresponds to an absolute magnitude of \u20136.5 mag. This value agrees with other authors\u2019 estimates of the darkness of such objects. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3854, "title": "Hadronic Gamma Rays from Supernova Remnants", "abstract": "  A gas cloud near a supernova remnant (SNR) provides a target for pp-collisions leading to subsequent gamma-ray emission through neutral pion decay. The assumption of a power-law ambient spectrum of accelerated particles with index near -2 is usually built into models predicting the spectra of very-high energy (VHE) gamma-ray emission from SNRs. However, if the gas cloud is located at some distance from the SNR shock, this assumption is not necessarily correct. In this case, the particles which interact with the cloud are those leaking from the shock and their spectrum is approximately monoenergetic with the injection energy gradually decreasing as the SNR ages. In the GLAST energy range the gamma-ray spectrum resulting from particle interactions with the gas cloud will be flatter than expected, with the cutoff defined by the pion momentum distribution in the laboratory frame. We evaluate the flux of particles escaping from a SNR shock and apply the results to the VHE diffuse emission detected by the HESS at the Galactic centre. ", "machine_abstract": "We present the results on gamma-ray emission from supernova remnants (SNRs) obtained with the Fermi Large Area Telescope (LAT). We use data collected during the first year of LAT operation to search for SNR counterparts in the GeV range and find evidence for gamma rays associated with eight Galactic SNRs, including Cas A, IC 443, W44, G349.7+0.2, G8.7-0.1, Kes 75, MSH 15-52, and RX J1713-3946. The observed fluxes are consistent with theoretical predictions based on hadronic interactions between accelerated cosmic ray particles and ambient matter. In addition we report upper limits on the gamma-ray luminosities of several other known Galactic SNRs. These results provide new insights into particle acceleration processes at work within these objects. Keywords: Supernova remnant, Fermi Large Area Telescope, Cosmic Ray", "paraphrased_abstract": "This research is supported by the Fermi Large Area Telescope (LART), a research devoted to the study of the gamma rays of cosmic rays and their associated kinetic processes. In this study we have studied the gamma rays of eight galactic LARGE STARS. These LARGE STARS are of the same age as the stars of the galaxy: Cas A, IC 443, W44, G379.8.7.0.1, G8.7.1K., MSH. 15-52, RX J1713-3946. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.152, "title": "Deconstructing dwarf galaxies: a Suprime-Cam survey of Andromeda II", "abstract": "  (Abridged) We present deep, sub-horizontal branch, multi-colour photometry of the Andromeda II dwarf spheroidal (And II dSph) taken with Subaru Suprime-Cam. We identify a red clump population in this galaxy, the first time this feature has been detected in a M31 dSph, which are normally characterized as having no significant intermediate age populations. We construct radial profiles for the various stellar populations and show that the horizontal branch has a nearly constant density spatial distribution out to large radius, whereas the reddest red giant branch stars are centrally concentrated in an exponential profile. We argue that these populations trace two distinct structural components in And II and show that this assumption provides a good match to the overall radial profile of this galaxy. The extended component dominates the stellar populations at large radius, whereas the exponential component dominates the inner few arcminutes. We show that the two components have very different stellar populations; the exponential component has an average age of $\\sim 7 - 10$ Gyrs old, is relatively metal-rich ([Fe/H] $\\sim -1$) but with a significant tail to low metallicities, and possesses a red clump. The extended component, on the other hand, is ancient ($\\sim 13$ Gyrs), metal-poor ([Fe/H] $\\sim -1.5$) with a narrower dispersion $\\sigma_{\\rm [Fe/H]} \\simeq 0.28$, and has a well developed blue horizontal branch. The extended component contains approximately three-quarters of the light of And II and its unusual density profile is unique in Local Group dwarf galaxies. This suggests that its formation and/or evolution may have been quite different to other dwarf galaxies. ", "machine_abstract": "We present the results of an optical imaging survey with Subaru/Suprime-Cam of the nearby galaxy group centered on M31, including its brightest satellite galaxy, Andromeda II (M32). We use this data to study the internal structure and stellar populations of Andromeda II in detail for the first time.  The surface brightness profile shows that Andromeda II is well described by two exponential components separated at about 1 kpc along the major axis. This double-exponential shape suggests that Andromeda II consists of two distinct components; one component has a younger age than the other. Using SSP models we find that these two components have ages of 2 Gyr and 10 Gyr respectively. In addition, there are several small knots distributed over the entire body of Andromeda II which may be associated with recent star formation activity. These knots show no clear correlation between their locations and those of globular clusters or HII regions found previously.", "paraphrased_abstract": "I will add that there are also several small knots, distributed all over the whole of Andromeda II, which may be caused by recent star-formation. This knots, however, are not obvious. In the course of this research, I will describe the internal structure and the stellar populations of Andromeda II. We have obtained the results of a visual survey of the nearby group, which consists of M31, and of its brightest satellite, Andromeda II. This survey reveals that Andromeda II has two components, one younger than the other. This double-exponential symmetry shows that Andromeda II is composed of two exponential parts, each of which is about one watt. These two components are of two wattages and ten wattages. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1504, "title": "Free Energy of Activation for the Comorosan Effect", "abstract": "  Initial reaction rate data for lactic dehydrogenase / pyruvate, lactic dehydrogenase / lactate and malic dehydrogenase / malate enzyme reactions were analyzed to obtain activation free energy changes of -329, -195 and -221 cal/mole, respectively, for rate increases associated with time-specific irradiation of the crystalline substrates prior to dissolution and incorporation in the reaction solutions. These energies, presumably, correspond to conformational or vibrational changes in the reactants or the activated complex. For the lactic dehydrogenase / pyruvate reaction, it is estimated that on the order of 10% of the irradiation energy (546 nm, 400 footcandles for 5 seconds) would be required to produce the observed reaction rate increase if a presumed photoproduct is consumed stoichiometrically with the pyruvate substrate. These findings are consistent with the proposition that the observed reaction rate enhancement involves photoproducts derived from oscillatory atmospheric gas reactions at the crystalline enzyme substrate surfaces rather than photo-excitations of the substrate molecules, per se. ", "machine_abstract": "The free energy of activation (\u0394G*) is calculated for the comorosan effect, which describes the formation of an intermediate state in the reaction between carbon dioxide and water to form carbonate ions.  The \u0394G* value obtained by this method is compared with that determined by other methods such as calorimetry or electrochemistry. It was found that these values are not consistent among themselves; however, they agree well within experimental error when the temperature dependence of the equilibrium constant is taken into account. This suggests that the discrepancy may be due to differences in the conditions under which each experiment was performed. In addition, it has been shown that the \u0394G* value depends on the nature of the solvent used in the experiments. Finally, we have proposed a mechanism for the comorosan process based on our results. The free energy of activation (\u2206G*) is calculated using the Arrhenius equation for the comorosan reaction, which describes the formation", "paraphrased_abstract": "It was then compared to the corresponding values calculated by other methods, such as calorimetry or electrochemistry. The corresponding values were found to be not univariate between the two and a half, although they remained consistent under the condition of temperature. In this way, we came up with a mechanism for the comorosan reaction. The comorosan reaction is a result of a reaction between carbon dioxide and water that is intermediate between carbon atoms and carbon dioxide. The comorosan reaction is a consequence of a comorosan reaction, which is a intermediate state between carbon dioxide and water to form carbonate ions. It has been found that the comorosan reaction is not inconsequential with the Arrhenius equation for the comorosan reaction. It is a reaction between carbon dioxide and water, in which carbon atoms form carbon ions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1095, "title": "Orbits of tori extended by finite groups and their polynomial hulls: the case of connected complex orbits", "abstract": "  Let $V$ be a complex linear space, $G\\subset\\GL(V)$ be a compact group. We consider the problem of description of polynomial hulls $\\wh{Gv}$ for orbits $Gv$, $v\\in V$, assuming that the identity component of $G$ is a torus $T$. The paper contains a universal construction for orbits which satisfy the inclusion $Gv\\subset T^\\bbC v$ and a characterization of pairs $(G,V)$ such that it is true for a generic $v\\in V$. The hull of a finite union of $T$-orbits in $T^\\bbC v$ can be distinguished in $\\clos T^\\bbC v$ by a finite collection of inequalities of the type $\\abs{z_1}^{s_1}...\\abs{z_n}^{s_n}\\leq c$. In particular, this is true for $Gv$. If powers in the monomials are independent of $v$, $Gv\\subset T^\\bbC v$ for a generic $v$, and either the center of $G$ is finite or $T^\\bbC$ has an open orbit, then the space $V$ and the group $G$ are products of standard ones; the latter means that $G=S_nT$, where $S_n$ is the group of all permutations of coordinates and $T$ is either $\\bbT^n$ or $\\SU(n)\\cap\\bbT^n$, where $\\bbT^n$ is the torus of all diagonal matrices in $\\rU(n)$. The paper also contains a description of polynomial hulls for orbits of isotropy groups of bounded symmetric domains. This result is already known, but we formulate it in a different form and supply with a shorter proof. ", "machine_abstract": "We study the orbit structure of algebraic actions of finite groups on affine toric varieties with respect to the Zariski topology, in particular we consider the action of a finite group G on an affine toric variety X associated to a normal fan $\\mathscr{N}_G$ of a rational polyhedral cone in $\\mathbb{Z}^d$ generated by vectors $v_1,...,v_n$ such that $(G\\cdot v_i)\\cap (G\\cdot v_j)\\neq \\emptyset$ for all $i,j\\in [n]$. We show that if $G$ acts faithfully on $X$ then there exists a dense open subset $U\\subset X$ which is isomorphic as a topological space to the quotient $G/G_0$ where $G_0=\\langle g\\in G:gv_i=v_i$ for some $i\\in [n]:1\\rangle$. Moreover, we prove that the closure of every orbit of this action contains a unique closed orbit. Finally, we give necessary conditions under which the action has only one closed orbit.", "paraphrased_abstract": "The orbit of this action is as follows: if it is faithfully acting on X, there exists a dense open subset of X which is isomorphic to the quotient G/G_0, where G is a triangle gin G, gv-i = v-i = v-i for some in nrangle. Moreover, we prove that each orbit of this action contains a unique closed orbit. Moreover, we prove that every orbit of this action has a unique closed orbit. We study the orbital structure of the algebraic actions of finite groups on affine toric varieties. For example, we examine the action of a finite group G in a toric X connected to the normal fan mathscrN_G_ of a rational polyhedral cone in mathbbZddd in the form of a mathbbZddd. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0914, "title": "Four quasars above redshift 6 discovered by the Canada-France High-z Quasar Survey", "abstract": "  The Canada-France High-z Quasar Survey (CFHQS) is an optical survey designed to locate quasars during the epoch of reionization. In this paper we present the discovery of the first four CFHQS quasars at redshift greater than 6, including the most distant known quasar, CFHQS J2329-0301 at z=6.43. We describe the observational method used to identify the quasars and present optical, infrared, and millimeter photometry and optical and near-infrared spectroscopy. We investigate the dust properties of these quasars finding an unusual dust extinction curve for one quasar and a high far-infrared luminosity due to dust emission for another. The mean millimeter continuum flux for CFHQS quasars is substantially lower than that for SDSS quasars at the same redshift, likely due to a correlation with quasar UV luminosity. For two quasars with sufficiently high signal-to-noise optical spectra, we use the spectra to investigate the ionization state of hydrogen at z>5. For CFHQS J1509-1749 at z=6.12, we find significant evolution (beyond a simple extrapolation of lower redshift data) in the Gunn-Peterson optical depth at z>5.4. The line-of-sight to this quasar has one of the highest known optical depths at z~5.8. An analysis of the sizes of the highly-ionized near-zones in the spectra of two quasars at z=6.12 and z=6.43 suggest the IGM surrounding these quasars was substantially ionized before these quasars turned on. Together, these observations point towards an extended reionization process, but we caution that cosmic variance is still a major limitation in z>6 quasar observations. ", "machine_abstract": "We report on four new quasars at redshifts z > 6, found in the Canada France Hawaii Telescope Legacy Survey (CFHTLS). The objects were selected as part of an ongoing survey for high-z quasars using photometric data obtained with CFHT and Spitzer Space Telescope. We present their optical to near-infrared SEDs, which are well fitted by composite quasar templates. Their luminosities range between 1.5 x 10^14 erg s-1 cm-2 and 2.1 x 10^15 erg s-1 cm-2 . These results show that there is still room for discovering very luminous quasars beyond redshift six. They also provide further evidence that supermassive black holes grew rapidly during this early phase of galaxy formation. Four quasars have been discovered at redshifts greater than 6 in the Canada France Hawaii telescope legacy survey (CFHTLS) by combining deep infrared observations taken with the Spitzer space telescope with optical data collected with the Canada France Hawaii telescope.", "paraphrased_abstract": "In this study, we present four new quasars of high z at the Red Star Survey of Canada (CFHTLS). These quasars were found in the CFHTLS by combining observations of the infrared and the optical observations of the Canada Geological Survey of the Red Star Survey. These quasars are selected from a recent survey of high-z quasars by the CFHTLS. These four quasars are the result of a joint effort of the CFHTLS and the Spitzer Space Telescope, which are both high-z.. These new quasars are very bright, i.e. 1.4 x 1014 eras. 2.1 x 1015 eras. These observations confirm that the early development of the galaxy was rapid. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2436, "title": "Observations and Modeling of Line Asymmetries in Chromospheric Flares", "abstract": "  For many years various asymmetrical profiles of different spectral lines emitted from solar flares have been frequently observed. These asymmetries or line shifts are caused predominantly by vertical mass motions in flaring layers and they provide a good diagnostics for plasma flows during solar flares. There are many controversial results of observations and theoretical analysis of plasma flows in solar chromospheric flares. The main difficulty is the interpretation of line shifts or asymmetries. For many years, methods based on bisector techniques were used but they give a reliable results only for some specific conditions and in most cases cannot be applied. The most promising approach is to use the non-LTE techniques applied for flaring atmosphere. The calculation of synthetic line profiles is performed with the radiative transfer techniques and the assumed physical conditions correspond to flaring atmosphere. I will present an overview of different observations and interpretations of line asymmetries in chromospheric flares. I will explain what we have learnt about the chromospheric evaporation in the frame of hydrodynamical models as well as reconnection models. A critical review will be done on the classical methods used to derive Doppler-shifts for optically thick chomospheric lines. In particular, details on the new approach for interpreting chromospheric line asymmetries based on the non-LTE techniques will be presented. ", "machine_abstract": "We present observations of the Mg II k line asymmetry during flares, which are compared with results obtained by numerical simulations using the RH code (Uitenbroek 2001). The observed profiles show that the blue wing is enhanced relative to the red one at all heights above the limb where we can see the flare emission. This effect is more pronounced for higher altitudes. We find that this behavior cannot be explained solely by Doppler shifts due to bulk plasma motions along the LOS. In addition, our modeling shows that the observed profile shapes cannot be reproduced without including nonthermal electron beams as an additional heating source.     Keywords: Solar flare, chromospheric lines, nonthermal electrons, radiative hydrodynamics model, RH code, Mg II k line, line asymmetry. 1 Introduction     During solar flares, intense energy release leads to rapid changes in physical conditions throughout the atmosphere of the Sun. These include temperature increases up to several million degrees Kelvin, strong magnetic fields, high densities, and large velocities. All these factors affect the shape of spectral lines emitted by different atmospheric layers. For example, it has been shown that the intensity ratio between two Fe I lines formed at different temperatures depends on the height of formation of each line (Feldman et al., 1995; Brosius & Phillips 2004) . Also, the presence of nonthermal electrons causes significant deviations from Maxwellian velocity distributions leading to asymmetric line profiles (e.g., Canfield et al. (1990) ; Doschek et al. (1991) ), while bulk flows lead to Doppler shifts of the line center position (Doschek et al., 1991; Brosius & Phillips 2004; Brosius 2009 ). Therefore, studying the temporal evolution of the line profiles provides important information about the dynamics of the flaring region. However, interpreting such data requires detailed knowledge of the underlying physics involved in the processes responsible for the observed phenomena.     In particular, the study of the Mg II h&k lines offers unique opportunities to investigate various aspects of solar flares because they form over a wide range", "paraphrased_abstract": "However, to interpret these results it is necessary to take into account the underlying physics of the processes underlying these changes. The chromospheric lines, the nonthermal electrons, the radiative hydrodynamic model, the RH code, the Mg II k line, and the asymmetrical line of the horizon, compared with the correspondingly observed line, at different temperatures. We have shown that the intensity of the lines of Fe I in the sky is proportional to the height of the limb at which they were formed. These measurements show that the blue wing of the sky is more pronounced than the red one, at all heights above the limb where the flare is visible. We further prove that the line of the horizon cannot be reproduced by using the force of the resemblance. The observed line of Mg II k is asymmetric, and the data for this line is compared with the results of numerical simulations of the RH code (Uitenbroek 2001) are very different. The resemblance of Mg II k is extremely unique and has been employed to study the resemblance of the two Fe I lines at different temperatures. These lines are formed of a variety of materials, and the temperature is higher than normal; these materials, however, produce different density distributions of the axis. Thus", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0195, "title": "Effect of temperature-dependent shape anisotropy on coercivity with aligned Stoner-Wohlfarth soft ferromagnets", "abstract": "  The temperature variation effect of shape anisotropy on the coercivity, HC(T), for the aligned Stoner-Wohlfarth (SW) soft ferromagnets, such as fcc Ni, fcc Co and bcc Fe, are investigated within the framework of Neel-Brown (N-B) analysis. An extended N-B equation is thus proposed,by introducing a single dimensionless correction function, the reduced magnetization, m(\\tao) = MS(T)/MS(0), in which \\tao = T/TC is the reduced temperature, MS(T) is the saturation magnetization, and TC is the Curie temperature. The factor, m(\\tao), accounts for the temperature-dependent effect of the shape anisotropy. The constants, H0 and E0, are for the switching field at zero temperature and the potential barrier at zero field, respectively. According to this newly derived equation, the blocking temperature above which the properties of superparamagnetism show up is described by the expression, TB = E0m^2(\\tao)/[kBln(t/t0)], with the extra correction factor m^2(\\tao). The possible effect on HC(T) and the blocking temperature, TB, attributed to the downshift of TC resulting from the finite size effect has been discussed also. ", "machine_abstract": "We study the effect of temperature dependent shape anisotropy in an exchange coupled system consisting of two identical uniaxial single domain particles, one being magnetically softer than the other and both having their easy axes parallel to each other. We show that for certain values of the parameters involved there is a significant increase in the coercive field at low temperatures compared to high temperatures. This can be understood by considering the competition between the Zeeman energy barrier due to the applied magnetic field and the thermal activation energy barrier associated with the temperature dependence of the shape anisotropy.     The model we consider consists of two identical spherical particles (with radius R) separated by a distance d along the z-axis. Each particle has its own uniaxial anisotropy constant Ks(T), where T denotes the temperature. In addition, they are also exchange-coupled through a coupling constant J. For simplicity, we assume that the anisotropy constants have the same functional form as given below,     Ks = K1 + K2 tanh[-(T/Tc)],   where Tc is some characteristic temperature scale which determines how rapidly the anisotropy changes with temperature.", "paraphrased_abstract": "... ; i.e. the temperature is the temperature. We study the effect of the temperature dependent i.e., in an interposed system, composed of two identical, spherical particles of the same radius and of the same radius, separated by a distance d along the z-axis. Each particle has a non-ionic i.e., Ks(T), Tx... where T is the temperature, and i.e., the temperature-dependent thermodynamical barrier. For the sake of simplicity, we will assume that the i.e., Ks = K1 + K2 tanh[16], Tx [16], is some characteristic temperature scale which determines the rate of the i.e., the speed of the i.e., whereas at high temperatures it is much lower. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2021, "title": "Structural fingerprinting in the transmission electron microscope: Overview and opportunities to implement enhanced strategies for nanocrystal identification", "abstract": "  This paper illustrates the prospective need for structural fingerprinting methods for nanocrystals. A review of the existing fingerprinting methods for crystal structures by means of transmission electron microscopy which work for a single setting of the specimen goniometer is given. Suggestions are made on how some of these methods could be enhanced when nanocrystals and novel instrumentation are involved, i.e. when either the kinematic or quasi-kinematic scattering approximations are sufficiently well satisfied. A novel strategy for lattice-fringe fingerprinting of nanocrystals from Fourier transforms of high-resolution phase contrast transmission electron microscopy images is briefly outlined. Nanocrystal structure specific limitations to the application of this strategy are discussed. An appeal is made to share the structural data of nanocrystals freely over the internet and infrastructure that would allow this sharing is mentioned. A shorter version of this paper has been accepted for publication in a special issue on the Structure of Nanocrystals of the Zeitschrift fuer Kristallographie, International journal for structural, physical, and chemical aspects of crystalline materials. ", "machine_abstract": "The ability to identify individual nanoparticles is essential for understanding their properties, but this task can be challenging due to the high degree of structural similarity between different types of particles. In this review we discuss recent advances in nanoparticle characterization using scanning transmission electron microscopy (STEM)-based techniques that provide information on both chemical composition and structure at atomic resolution. We also highlight some of the challenges associated with these methods as well as possible solutions. Finally, we present an overview of current applications of STEM-based approaches for identifying nanoparticles and suggest future directions for research.  Keywords: Nanoparticles, Characterization, Transmission Electron Microscopy, Scanning Transmission Electron Microscope, Atomic Resolution, Structure, Chemical Composition. The ability to identify individual nanoparticles has become increasingly important over the past decade because it allows researchers to correlate specific physical or chemical characteristics with particle size, shape, surface chemistry, crystal phase, etc., which are all known to influence the performance of many materials. However, despite significant progress made during the last few years, there remains considerable uncertainty about how best to characterize nanoparticles by combining multiple experimental parameters into one single descriptor. This problem arises mainly because nanoparticles often have similar compositions and/or structures, making them difficult to distinguish based solely on elemental analysis or conventional imaging techniques such as bright-field TEM or SEM.", "paraphrased_abstract": "The method of identifying individual particles has become more and more important in the last decade, as it allows us to analyze, by means of its physical properties, the morphology, the surface, the crystal, the mineralization, and so on. But it is still a difficult task, since the materials are often of similar composition and structure. We have recently made great strides in this field, but we are still not sure what to do. We present here some recent advances in the field of atom-specific spectroscopy and some experimental methods that provide atomic structure and chemical composition at atomic level. We discuss these methods and discuss the difficulties and the possible solutions. Besides, we have also developed some atomic-resolution spectroscopy techniques, and we present here a new method of analyzing the composition of atoms, and we also show some of the technical difficulties. We have also described some recent methods for the study of atoms by atomic-resolution. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1961, "title": "Impact of spin-zero particle-photon interactions on light polarization in external magnetic fields", "abstract": "  If the recent PVLAS results on polarization changes of a linearly polarized laser beam passing through a magnetic field are interpreted by an axion-like particle, it is almost certain that it is not a standard QCD axion. Considering this, we study the general effective interactions of photons with spin-zero particles without restricting the latter to be a pseudo-scalar or a scalar, i.e., a parity eigenstate. At the lowest order in effective field theory, there are two dimension-5 interactions, each of which has previously been treated separately for a pseudo-scalar or a scalar particle. By following the evolution in an external magnetic field of the system of spin-zero particles and photons, we compute the changes in light polarization and the transition probability for two experimental set-ups: one-way propagation and round-trip propagation. While the first may be relevant for astrophysical sources of spin-zero particles, the second applies to laboratory optical experiments like PVLAS. In the one-way propagation, interesting phenomena can occur for special configurations of polarization where, for instance, transition occurs but light polarization does not change. For the round-trip propagation, however, the standard results of polarization changes for a pseudoscalar or a scalar are only modified by a factor that depends on the relative strength of the two interactions. ", "machine_abstract": "We study the effect of spin-one and spin-two particles on the circularly polarized light propagating through an external magnetic field. We show that this effect is determined by the interaction between photons and particles with spins equal to zero, one or two only if the photon energy exceeds some threshold value which depends on the particle mass. For example, for electrons (mass m = 9.11\u00d710-31 kg) it equals to 0.5 MeV. Below this threshold there are no effects caused by higher-spin particles. The results obtained can be used as a basis for developing new methods of studying high-spin particles using optical techniques. DOI: 10.1088/1742-6596/aa6b20 I. INTRODUCTIO N The problem of describing the propagation of electromagnetic waves in matter has been studied extensively over many years [1] . In particular, the influence of various types of atoms [2] , molecules [3] , ions [4] , plasmas [5] , crystals [6] , etc., on the properties of light was investigated. However, despite numerous studies, the question about how the presence of particles with non-zero spin affects the polarization state of light remains open [7 -9] . In recent decades, interest in such problems increased significantly due to the development of quantum optics [10] . This area includes investigations into the processes occurring when high-energy photons interact with particles having different masses [11] . Such phenomena include Compton scattering [12] , pair production [13] , photo-meson production [14] , etc.. It should also be noted that these processes play an important role in astrophysics [15] , nuclear physics [16] , condensed-matter physics [17] , etc.. It follows from the above that the investigation of the impact of particles with nonzero spin on the polarization state of light is relevant both theoretically and experimentally.", "paraphrased_abstract": "The question of how light is formed, in the form of electromagnetic waves, is well known. It was studied extensively for many years, in particular by the influence of atoms, molecules, ions, ions, plasmas, crystals, etc. It was discovered that the interaction of light with particles of the lower mass - a value that depends on the mass - is about 0.5 mV. This value does not exceed the limit, however, of the higher mass particles. - In the past, research on the interactions of high-energy particles with particles of the lower mass is more or less well known, and the investigation of this is both theoretical and experimental. Besides, the investigation of the nonzero spin of a particle in the lower mass is a very important research. In the last decades, the interest in this area has increased considerably, especially in the field of quantum optics. In particular, in the field of polarization of light, there have been many studies, and in particular, in the field of polarization of a light that is in the direction of the external magnetic field. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0521, "title": "Fractionally charged excitations on frustrated lattices", "abstract": "  Systems of strongly correlated fermions on certain geometrically frustrated lattices at particular filling factors support excitations with fractional charges $\\pm e/2$. We calculate quantum mechanical ground states, low--lying excitations and spectral functions of finite lattices by means of numerical diagonalization. The ground state of the most thoroughfully studied case, the criss-crossed checkerboard lattice, is degenerate and shows long--range order. Static fractional charges are confined by a weak linear force, most probably leading to bound states of large spatial extent. Consequently, the quasi-particle weight is reduced, which reflects the internal dynamics of the fractionally charged excitations. By using an additional parameter, we fine--tune the system to a special point at which fractional charges are manifestly deconfined--the so--called Rokhsar--Kivelson point. For a deeper understanding of the low--energy physics of these models and for numerical advantages, several conserved quantum numbers are identified. ", "machine_abstract": "We study the fractional charge and statistics of elementary excitations in quantum spin systems with frustration, using exact diagonalization techniques for small clusters up to 12 sites. We find that the ground state is always gapped and has no degeneracy. The elementary excitations are fractionally charged fermions or bosons depending on whether the system is antiferromagnetic (AF) or ferromagnetic (F). In AF cases we also observe neutral fermionic excitations which carry zero electric charge but have nontrivial braiding properties. These results can be understood by mapping our models onto effective lattice gauge theories where the elementary excitations correspond to particles carrying flux quanta. Our work provides an explicit example of how fractional charges emerge naturally as topological defects in strongly correlated electronic materials. Introduction:-The discovery of high temperature superconductivity in copper oxide compounds [1] , together with other exotic phenomena such as colossal magnetoresistance [2] , non-Fermi liquid behavior [3] etc., has led to renewed interest in understanding the physics of strongly interacting electrons. One of the most important open questions concerns the nature of the elementary excitations responsible for these novel behaviors [4] . It was suggested early on [5] that the elementary excitations may be described by some kind of collective modes known as spin waves [6] . However it soon became clear [7, 8] that this description fails at low energies due to strong electron correlations. More recently there has been considerable progress towards developing theoretical descriptions based on new concepts like fractionalized quasiparticles [9] , emergent gauge fields [10] , and topological order [11] . In particular, recent experiments [12] suggest that the elementary excitations in the cuprates might indeed be described by some form of fractionalized quasiparticle [13] . This raises many interesting questions about their physical properties including their charge [14] , statistics [15] , and interactions [16] . Unfortunately, despite enormous efforts [17] , a complete microscopic theory describing all these aspects remains elusive [18] . A promising approach involves studying simplified model Hamiltonians [19, 20] whose low-energy limit captures essential features of the original problem [21] .", "paraphrased_abstract": "In recent years there has been great progress in the development of theoretical explanations of these phenomena, based on new concepts such as the fractionalized quasiparticle, the emergent gauge fields and the topological order. In particular, a recent study of the condensed-atoms of copper oxidation in a series of exotic phenomena has given rise to new questions about the nature of the elementary enigmatic properties, the charge, the statistics, the interactions, the nature of the fractional charge, and the emergence of the enigmatic charge. The fundamental properties of the elementary enigmatic properties, and their charges, are investigated, we find that the ground state is always a gaping, and in no way degenerate. This approach is an attempt to study the fractional charge and statistics of elementary enigmatical particles in the presence of the most elementary enigmatical objects, a so-called spin wave, which, though at low energies, is not possible. Besides, many recent experiments have confirmed that elementary enigmatical particles in the form of a fractionalized particle may indeed be explained by some fractionalized quasi-particle. In particular, recent experiments have shown that elementary enigmaticals in the cuprates could indeed be described by some kind of fractionalized quasi-particle. We have investigated the fractional charge and", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.275, "title": "Ultracold Thermalization of $^7$Li and $^{87}$Rb", "abstract": "  We report on measurements of cross-species thermalization inside a magnetically trapped spin-polarized mixture of $^{87}$Rb and $^7$Li atoms with both atoms in their respective low field seeking magnetic substates $|F=2,m_F=2\\right>$. Measurement of the thermalization velocity in the ultracold regime below $10 \\mu$K allows for the derivation of the absolute value of the pure triplet s-wave scattering length governing the interaction. We find $|a_{7,87}|=(59\\pm19) a_{\\rm B}$. We propose to study both species in the condensed regime to derive the sign of $a_{7,87}$. In this context, we present numerical solutions to the coupled Gross-Pitaevskii equation based on the hypothesis of a positive sign. According to the simulations, phase separation of the Li and Rb $|2,2\\right>$ clouds occurs along with a mean-field stabilization allowing for larger atom numbers of condensed $^7$Li atoms before collapse sets in. Observation of this mean-field stabilization would directly fix the sign of $a_{7,87}$. We discuss our results in the light of this proposal. ", "machine_abstract": "We report on the observation of thermalization in an ultracold mixture of fermionic lithium-7 ($^7$Li) atoms with bosonic rubidium-87 ($^{87}$Rb). The system is prepared by evaporating both species simultaneously into a crossed optical dipole trap, followed by forced radio-frequency evaporation to reach quantum degeneracy at temperatures below $T_c\\sim100\\:\\mathrm{nK}$ for each species separately. We then study the relaxation dynamics after suddenly changing the trapping potential depth while keeping the temperature constant. This leads to a sudden change in the density distribution which triggers collisional processes that redistribute energy between different modes. By measuring the time evolution of the cloud size we find that the two species thermalize within less than one second. Our results are consistent with theoretical predictions based on kinetic theory. In particular, they show that the observed thermalization rate agrees well with the expected value for elastic collisions only.", "paraphrased_abstract": "\u201cWe observe that the kinetic theory predicts that the temperature will rise less than a second with the entrainment of the atoms. We describe the formation of a mixture of the fermionic lithium atoms and the bosonic rubidium 87 atoms. The system is prepared by evaporating both species simultaneously in a crossed optical dipole, and then by evaporation at the radio frequency, to attain quantum degeneracy below the temperature of 100 K. per unit area. This results in the formation of a rapid cooling process, and the collisions between the different atoms are subsequently accelerated. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3208, "title": "Polarimetric studies of comet Hale-Bopp", "abstract": "  In the present work, the non-spherical dust grain characteristics of comet Hale-Bopp are studied using the T-matrix method and the modified power law distribution function. It is found that the observed data fits very well with the power index ($\\alpha) = - 3$. The best fit values of complex refractive index $(n, k)$ and the aspect ratio (E) at $\\alpha = - 3$ are calculated to be $(1.382, 0.035, 0.936)$ and $(1.379, 0.041,0.936)$ at $\\lambda = 0.485 \\mu m$ and $0.684\\mu m$ respectively. Kerola & Larson (K-L) analysed the same comet using the T-matrix method and the power law distribution function ($\\alpha = -3$), and found that the prolate grains can explain the observed polarization in a more satisfactory manner as compared to the other shapes. But their analysis could not reproduce the negative polarization branch beyond scattering angle $157^0$. However, the results obtained from the present work successfully generate the expected negative polarization curve beyond $157^0$ and the fitting in this case is much better than K-L's work. So it is concluded from the present study that the use of modified power law distribution function (with $\\alpha = - 3$) can fit the observed data in a better way, as compared to the power law distribution function used by previous authors. ", "machine_abstract": "The polarimetry of Comet Hale-Bopp was studied by the University of Hawaii 2 m telescope (UH2T) and the 1.5 m telescope at Mt. Wilson Observatory in 1997-1998, using broadband filters centered on 0.55 \u03bcm, 0.65 \u03bcm, 0.85 \u03bcm, and 1.25 \u03bcm.  The results show that the polarization degree is generally low for all wavelengths except near perihelion when it increases to about 10%-15% with an angle of ~40\u00b0-50\u00b0 counterclockwise relative to solar north pole. This indicates that the dust particles are aligned perpendicularly to the Sun-comet-observer line. We also find that there exists a wavelength dependence of the polarization degree which decreases as the wavelength becomes longer. These observations suggest that the size distribution of the dust grains has a power law index between -3 and -4. In addition, we found that the polarization degree shows no significant variation during the period of our observation.", "paraphrased_abstract": "The polarisation of Comet Hale-Bopp was studied by the University of Hawaii 2m telescope and the 1.1 m telescope at Mount Wilson Observatory in 1997-98 with broadband filters centered on 0.55 m, 0.65 m, 0.85 m, and 1.25 m. The polarization of Comet Hale-Bopp was determined at a distance of about 40\u00b0 - 50\u00b0 from the north pole, which indicates that the dust is oriented perpendicular to the line of the Sun-Comet. We have found that the polarization of dust is a power law of 3 and 4 in our measurement. In addition, we have found that there is no significant difference in the polarization of the dust in the measured period. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1764, "title": "Classical potential describes martensitic phase transformations between the $\\alpha$, $\\beta$ and $\\omega$ titanium phases", "abstract": "  A description of the martensitic transformations between the $\\alpha$, $\\beta$ and $\\omega$ phases of titanium that includes nucleation and growth requires an accurate classical potential. Optimization of the parameters of a modified embedded atom potential to a database of density-functional calculations yields an accurate and transferable potential as verified by comparison to experimental and density functional data for phonons, surface and stacking fault energies and energy barriers for homogeneous martensitic transformations. Molecular dynamics simulations map out the pressure-temperature phase diagram of titanium. For this potential the martensitic phase transformation between $\\alpha$ and $\\beta$ appears at ambient pressure and 1200 K, between $\\alpha$ and $\\omega$ at ambient conditions, between $\\beta$ and $\\omega$ at 1200 K and pressures above 8 GPa, and the triple point occurs at 8GPa and 1200 K. Molecular dynamics explorations of the dynamics of the martensitic $\\alpha-\\omega$ transformation show a fast-moving interface with a low interfacial energy of 30 meV/\\AA$^2$. The potential is applicable to the study of defects and phase transformations of Ti. ", "machine_abstract": "The classical interatomic potentials are used to study the martensitic transformation in Ti-based alloys. The results show that the classical potentials can reproduce the experimental lattice parameters for all three phases (alpha, beta, omega) as well as their elastic constants with reasonable accuracy. It is found that the energy barrier for the martensitic transformation decreases rapidly when temperature increases. This indicates that the martensitic transformation may be suppressed at high temperatures due to thermal fluctuations.     Keywords: Classical potentials; Martensitic transformation; Titanium alloy. 1 Introduction Titanium has been widely applied in aerospace industry because it combines low density, good corrosion resistance and excellent mechanical properties [1] . However, its poor ductility limits its application [2] , especially under extreme conditions such as high pressure or high temperature [3] . In order to improve the ductility of titanium materials, many efforts have been made by researchers [4] - [8] . Among these methods, the most promising one seems to be the introduction of second-phase particles into pure titanium matrix [9] - [11] . In fact, some commercialized titanium alloys contain small amounts of Al [12] , V [13] , Mo [14] , Fe [15] , Cr [16] , Ni [17] , Cu [18] , Sn [19] , Si [20] , Zr [21] , Nb [22] , Ta [23] , W [24] , Co [25] , Mn [26] , Ag [27] , Au [28] , Pd [29] , Pt [30] , Rh [31] , Ir [32] , Ru [33] , Os [34] , Re [35] , Sb [36] , Bi [37] , P [38] , As [39] , Ge [40] , B [41] , C [42] , N [43] , O [44] , F [45] , S [46] , Se [47] , Te [48] , I [49] , Xe [50] , Hg [51] , Gd [52] , Dy [53] , Er [54] , Yb [55] , Lu [56] , Y [57] , Sc [58] , La [59] , Ce [60] , Pr [61] , Nd [62] , Sm [63] , Eu [64] , Gd [65]", "paraphrased_abstract": "The most promising method has been the introduction of the second-phase particles in to the titanium matrix. The results of this study show that the classical potentials are capable of predicting the ductility of the material. The ductility of the titanium matrix is a result of its poor ductility. It is also the consequence of the thermal instability of the matrix. Thus, to improve the ductility of the alloys, a great deal of effort has been made in the past to improve the density, the corrosion resistance and the strength of their mechanical properties. However, it is also limited in application in very high temperatures, which makes it more difficult to achieve ductility. In recent years, there have been many attempts to make use of the material as a ductile material, but there have been few. Moreover, there are few ductile substances in commercial titanium alloys. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0229, "title": "Conformational gel analysis and graphics: Measurement of side chain rotational isomer populations by NMR and molecular mechanics", "abstract": "  Conformational gel analysis and graphics systematically identifies and evaluates plausible alternatives to the side chain conformations found by conventional peptide or protein structure determination methods. The proposed analysis determines the populations of side chain rotational isomers and the probability distribution of these populations. The following steps are repeated for each side chain of a peptide or protein: first, extract the local molecular mechanics of side chain rotational isomerization from a single representative global conformation; second, expand the predominant set of rotational isomers to include all probable rotational isomers down to those that constitute just a small percentage of the population; and third, evaluate the constraints vicinal coupling constants and NOESY cross relaxation rates place on rotational isomer populations. In this article we apply conformational gel analysis to the cobalt glycyl-leucine dipeptide and detail the steps necessary to generalize the analysis to other amino acid side chains in other peptides and proteins. For a side chain buried within a protein interior, it is noteworthy that the set of probable rotational isomers may contain one or more rotational isomers that are not identified by conventional NMR structure determination methods. In cases such as this the conformational gel graphics fully accounts for the interplay of molecular mechanics and NMR data constraints on the population estimates. The analysis is particularly suited to identifying side chain rotational isomers that constitute a small percentage of the population, but nevertheless might be structurally and functionally very significant. ", "machine_abstract": "The conformational space available to the side chains in proteins can be explored using molecular dynamics (MD) simulations, but this approach requires extensive computational resources for large systems. We have developed an alternative method that uses normal mode analysis (NMA), which has been shown previously to provide accurate predictions of protein motions at low computational cost.  In our new method we use NMA to calculate the free energy landscape associated with each side chain rotation angle. The resulting landscapes are then used as input into a statistical mechanical model that predicts the equilibrium distribution of rotamers observed experimentally. This method was tested on two small proteins containing multiple proline residues, where it correctly predicted the relative population of different rotameric states. It also accurately reproduced experimental results obtained by nuclear magnetic resonance spectroscopy (NMR). Finally, we applied this method to study the effect of mutations on the rotamer distributions of several amino acids. Our results show how changes in the local environment affect the rotamer preferences of these residues.", "paraphrased_abstract": "Then we used this method to study the effects of mutations on the rotamer distributions of several amino acids. The rotamer distributions of these amino acids showed that the local environment affects the rotamer preferences of these amino acids. To estimate the rotamer distributions of the side chains in proteins, it is possible to simulate the motion of the side chains, but the computational cost for this is insignificant. We have invented an alternative method of analysis, called Normal Mode Analysis, which has already been proven to be accurate for large systems, but the computational cost is too great for large systems. We have used NMA to calculate the free energy distribution of the side chains, and we use the result to calculate the equilibrium distribution of rotamers observed by experimental experiments. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0396, "title": "Quantum theory of exciton-photon coupling in photonic crystal slabs with embedded quantum wells", "abstract": "  A theoretical description of radiation-matter coupling for semiconductor-based photonic crystal slabs is presented, in which quantum wells are embedded within the waveguide core layer. A full quantum theory is developed, by quantizing both the electromagnetic field with a spatial modulation of the refractive index and the exciton center of mass field in a periodic piecewise constant potential. The second-quantized hamiltonian of the interacting system is diagonalized with a generalized Hopfield method, thus yielding the complex dispersion of mixed exciton-photon modes including losses. The occurrence of both weak and strong coupling regimes is studied, and it is concluded that the new eigenstates of the system are described by quasi-particles called photonic crystal polaritons, which can occur in two situations: (i) below the light line, when a resonance between exciton and non-radiative photon levels occurs (guided polaritons), (ii) above the light line, provided the exciton-photon coupling is larger than the intrinsic radiative damping of the resonant photonic mode (radiative polaritons). For a square lattice of air holes, it is found that the energy minimum of the lower polariton branch can occur around normal incidence. The latter result has potential implications for the realization of polariton parametric interactions in photonic crystal slabs. ", "machine_abstract": "We present an exact solution for the eigenstates and eigenvalues of a system consisting of a two-dimensional (2D) periodic array of semiconductor quantum dots coupled to a one-dimensional (1D) chain of identical quantum dots, which are both embedded into a 2D photonic crystal slab. The 1D chain is assumed to be driven by external laser fields at two different frequencies. We show that this structure can support bound states where photons are trapped between neighboring quantum dots along the 1D chain due to strong light-matter interaction mediated by excitons confined within each dot. These results may have important implications on future designs of optoelectronic devices based on hybrid structures combining semiconductors and photonics. In recent years there has been growing interest in developing novel optical materials and devices using nanostructures such as semiconductor quantum dots (QDs), nanowires or carbon nanotubes [1] . This research effort has led to the development of new concepts in optics including QD lasers [2] , single photon sources [3] , and QD-based solar cells [4] . In particular, QDs offer unique advantages over conventional bulk semiconductor systems because they allow controllable tuning of their electronic properties through size engineering [5] . Moreover, it was recently shown [6] that these artificial atoms can also exhibit interesting nonlinear optical effects [7, 8] . For example, when excited by intense laser pulses, QDs can generate coherent emission of multiple photons [9] . However, despite significant progress made during last decade, many fundamental questions remain unanswered about how QDs interact with electromagnetic radiation [10] .", "paraphrased_abstract": "During the last decade, great progress has been made in the field of physics, especially the optical properties of the QDs, and some of the optical phenomena are well known. Recently, however, the study has accumulated much more attention and has been led to new developments in optics, notably in the field of atoms of nanostructures, e.g., by the semiconductor atoms, by the nanowires or the carbon atoms, by the QDs. In particular, QDs have been shown to exhibit interesting nonlinear optical effects, for instance, the emitted electrons of the QDs are coherently released by the laser, and the QDs have shown the ability to emit a coherent emission of multiple electrons. In particular, it has been demonstrated that these artificial atoms can also exhibit interesting nonlinearities in their electronic properties, for instance, if they are energized by the intense light from the laser, then QDs produce coherent emitted electrons. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4655, "title": "MINOS Results, Progress and Future Prospects", "abstract": "  The MINOS long baseline experiment has been collecting neutrino beam data since March 2005 and has accumulated 3 x 10^{20} protons-on-target (POT) to date. MINOS uses Fermilab's NuMI neutrino beam which is measured by two steel-scintillator tracking calorimeters, one at Fermilab and the other 735 km downstream, in northern Minnesota. By observing the oscillatory structure in the neutrino energy spectrum, MINOS can precisely measure the neutrino oscillation parameters in the atmospheric sector. From analysis of the first year of data, corresponding to 1.27 x 10^{20} POT, these parameters were determined to be |\\Delta m^2_{32}|=2.74^{+0.44}_{-0.26} x 10^{-3} eV^2 and sin^2(2\\theta_{23})>0.87 (68% C.L.). MINOS is able to measure the neutrino velocity by comparing the arrival times of the neutrino beam in its two detectors. Using a total of 473 Far Detector events, (v-c)/c = (5.1 +/- 2.9) x 10^{-5} (68% C.L.) was measured. In addition, we report recent progress in the analysis of neutral current events and give an outline of experimental goals for the future. ", "machine_abstract": "MINOS is the name for two experiments at Fermilab that have been studying neutrinos produced by the NuMI beamline since 2003.  MINOS has measured the atmospheric neutrino mass splitting with high precision using both appearance (\u03bd\u03bc to \u03bde) and disappearance channels in its Near Detector located 1 km downstream of the production target.   The Far Detector was designed as an iron-scintillator calorimeter instrumented with 3200 photomultiplier tubes and placed 735 km away from the source point on the Soudan mine site near Minnesota.   It consists of 5 planes of 2 cm thick steel absorber plates interleaved with scintillating strips oriented perpendicularly to the direction of travel of the neutrinos.    In this talk I will present results from these experiments including recent measurements of the muon neutrino flux shape and absolute normalization made possible through the use of a new calibration system based on radioactive sources embedded within the detector itself.", "paraphrased_abstract": "In this talk, I will give the latest results from the experiments, especially relating to the measurement of the muon neutrino flux in the sphere, and the absolute normalization achieved by using the corresponding radioactive source embedded in the detector. The far detecting device was designed by MINOS, a calorimeter of iron, equipped with 3200 photomultipliers, located 735 mts from the source on the Soudan mine site in Minnesota. It is a 5 mm thick stainless steel ax with four slanting strips of steel, oriented in the direction of the muon\u2019s course. MINOS has been studying the mass shedding from NuMI\u2019s beamline since 2003. The Far Detector was placed in a dyke, a carbon meter, measuring the density of the nuon radii and the polarization of the neutrino peaks. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1089, "title": "Geochemistry of U and Th and its Influence on the Origin and Evolution of the Crust of Earth and the Biological Evolution", "abstract": "  We have investigated the migration behaviors of uranium (U) and thorium (Th) in Earth and other terrestrial planets. Theoretical models of U and Th migration have been proposed. These models suggest that the unique features of Earth are closely connected with its unique U and Th migration models and distribution patterns. In the Earth, U and Th can combine with oxidative volatile components and water, migrate up to the asthenosphere position to form an enrichment zone (EZ) of U and Th first, and then migrate up further to the crusts through magmatism and metamorphism. We emphasize that the formation of an EZ of U, Th and other heat-producing elements is a prerequisite for the formation of a plate tectonic system. The heat-producing elements, currently mainly U and Th, in the EZ are also the energy sources that drive the formation and evolution of the crust of Earth and create special granitic continental crusts. In other terrestrial planets, including Mercury, Venus, and Mars, an EZ can not be formed because of a lack of oxidative volatile components and water. For this reason, a plate tectonic system can not been developed in these planets. We also emphasize the influence of U and Th in EZ on the development and evolution of life on Earth. We propose that since the Earth and planets were born in a united solar system, there should be some common mechanisms to create the similarities and differences between them. We have tried to develop an integrated view to explain some problems in the tectonics of Earth and evolution, bio-evolution, and planetary dynamics through U and Th geochemistry. We believe that a comprehensive exploration on energy sources and their evolution is a good way to build bridges between different disciplines of science in order to better understand the Earth and planets. ", "machine_abstract": "The geochemical behavior of uranium (U) and thorium (Th), which are radioactive elements, is discussed in relation to their influence on the origin and evolution of the crust of earth as well as biological evolution.  The chemical properties of these two elements are similar; however, they have different physical characteristics that affect how they behave geologically.  Uranium has an atomic number of 92 with a half-life of 4.5 billion years while thorium has an atomic number of 90 with a half life of 1.4 billion years.   Both elements occur naturally throughout the Earth's crust but at varying concentrations depending upon the rock type.  They can be found in igneous rocks such as granite or basalt where they form minerals like uranite or thorite respectively.  These minerals may also contain other trace metals including lead, silver, gold, copper, zinc, arsenic, selenium, molybdenum, cadmium, mercury, bismuth, antimony, tellurium, cobalt, nickel, manganese, iron, vanadium, chromium, tungsten, titanium, zirconium, niobium, tantalum, hafnium, rhenium, osmium, iridium, platinum, palladium, rhodium, ruthenium, and iridium.  Uranium and thorium are also present in sedimentary rocks such as sandstone, shale, limestone, dolomite, gypsum, anhydrite, salt beds, and evaporites.  Sedimentary rocks are formed by weathering processes when water erodes...", "paraphrased_abstract": "Uranium and thorium are also found in sedimentary rocks, such as sand, shale, limestone, dolomite, anhydrite, salt beds, and evaporites. Both of them are radioactive and are present in the Earth\u2019s crust. They are naturally present in igneous rocks, such as granite or basalt, where they form minerals like uranium or thorium. They are radioactive and have the same chemical properties, but they have different physical characteristics. The chemical properties of uranium and thorium are very similar, but their physical properties are different. The atomic number of uranium is 92, with a half-life of 4.5 billion years, while thorium is 92, with a half-life of 1.4 billion years. Uranium and thorium are also present in sedimentary rocks, such as sand, rock-sand, limestone, gypsum, evaporites, salt-beds, and evaporites. They are also present in the rocks of the subterranean, such as granite and basalt, where they form minerals like uranium or thorium. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0864, "title": "Bondi accretion in the early universe", "abstract": "  This paper presents a study of quasi-steady spherical accretion in the early Universe, before the formation of the first stars and galaxies. The main motivation is to derive the basic formulas that will be used in a companion paper to calculate the accretion luminosity of primordial black holes and their effect on the cosmic ionization history.   The following cosmological effects are investigated: the coupling of the gas to the CMB photon fluid (i.e., Compton drag), Hubble expansion, and the growth of the dark matter halo seeded by the gravitational potential of the central point mass. The gas equations of motion are solved assuming either a polytropic or an isothermal equation of state. We consider the cases in which the accreting object is a point mass or a spherical dark matter halo with power-law density profile, as predicted by the theory of \"secondary infall''. Analytical solutions for the sonic radius and fitting formulas for the accretion rate are provided.   Different accretion regimes exist depending on the mass of the accreting object. If the black hole mass is smaller than 50-100 Msun, gas accretion is unaffected by Compton drag. A point mass and an extended dark halo of equal mass accrete at the same rate if M>5000 Msun, while smaller mass dark halos accrete less efficiently than the equivalent point mass. For masses M>3 x 10^4 Msun, the viscous term due to the Hubble expansion becomes important and the assumption of quasi-steady flow fails. Hence, the steady Bondi solutions transition to the time-dependent self-similar solutions for \"cold cosmological infall\". ", "machine_abstract": "We present an analysis of the Bondi accretion rate onto primordial black holes (PBHs) during the radiation-dominated era, taking into account the effects of pressure and viscosity on the gas flow. We find that for PBH masses M \u226b 10^(10) g, the Bondi radius is much larger than the Schwarzschild radius, so that the standard Bondi-Hoyle-Lyttleton formula can be used to calculate the accretion rate. For smaller mass PBHs with M < 10^(10)g, we use numerical simulations to determine the accretion rate as a function of time. The results are compared against those obtained by assuming that the accreting gas has negligible pressure or viscosity. In addition, we consider the possibility that the accreted gas may cool efficiently via bremsstrahlung emission before it reaches the central BH. Finally, we discuss how our results could affect the abundance of PBHs at different redshifts.", "paraphrased_abstract": "This analysis, in the context of the radiation-dominated period, investigates the Bondi rate on primordial black holes (PBHs), assuming that the gas has negligible pressure and viscosity. The results are compared with those of the model, which assumes a low pressure and a low viscosity. Finally, we consider the possibility that the accreted gas cools rapidly by means of bremsstrahlung before it reaches the central BH. We conclude by discussing the distribution of the Bondi accretion rate on PBHs at different redshifts. For PBHs with M10101010g, the bondi radius is much larger than the Schwarzschild radius. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2415, "title": "Multifractality and scale invariance in human heartbeat dynamics", "abstract": "  Human heart rate is known to display complex fluctuations. Evidence of multifractality in heart rate fluctuations in healthy state has been reported [Ivanov et al., Nature {\\bf 399}, 461 (1999)]. This multifractal character could be manifested as a dependence on scale or beat number of the probability density functions (PDFs) of the heart rate increments. On the other hand, scale invariance has been recently reported in a detrended analysis of healthy heart rate increments [Kiyono et al., Phys. Rev. Lett. {\\bf 93}, 178103 (2004)]. In this paper, we resolve this paradox by clarifying that the scale invariance reported is actually exhibited by the PDFs of the sum of detrended healthy heartbeat intervals taken over different number of beats, and demonstrating that the PDFs of detrended healthy heart rate increments are scale dependent. Our work also establishes that this scale invariance is a general feature of human heartbeat dynamics, which is shared by heart rate fluctuations in both healthy and pathological states. ", "machine_abstract": "The multifractal nature of the human heartbeat has been studied by many authors, but most studies have focused on short time series (typically less than one hour). In this work we study the scaling properties of long-term recordings of the electrocardiograms (ECG) of healthy subjects during sleep using wavelet transforms to calculate the singularity spectrum f(\u03b1), which characterizes the degree of multifractality present in each signal.  We find that all signals are characterized by a broad range of exponents \u03b1 indicating strong multifractality. The width \u0394\u03b1 of these spectra decreases with increasing recording length T as \u0394\u03b1~T-1/2 for T<10 hours and \u0394\u03b1~T-3/4 for T>10 hours. This indicates that the ECGs become more monofractal-like when longer recordings are considered. Finally, we show how our results can be used to detect pathological changes in the heart rate variability associated with congestive heart failure.", "paraphrased_abstract": "In this study we are examining the skewness of the human heart during sleep. We have studied the structure of the skewness of the ECG in healthy individuals at rest. The results show that the ECG is not monofractous at all. The width of the skewness decreases with the length of the recording, as a minimum of 10 hours, and as a maximum of 10 hours, the ECG becomes monofracte. The multifractality of the human heart has been studied by many authors, but they have mostly studied a short-lived series, about an hour in duration. In this study we analyze the scale of the long-term ECGs, with the use of the wavelet transform, in order to estimate the multiplicity of the signals. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0596, "title": "Phase transition in the two-component symmetric exclusion process with open boundaries", "abstract": "  We consider single-file diffusion in an open system with two species $A,B$ of particles. At the boundaries we assume different reservoir densities which drive the system into a non-equilibrium steady state. As a model we use an one-dimensional two-component simple symmetric exclusion process with two different hopping rates $D_A,D_B$ and open boundaries. For investigating the dynamics in the hydrodynamic limit we derive a system of coupled non-linear diffusion equations for the coarse-grained particle densities. The relaxation of the initial density profile is analyzed by numerical integration. Exact analytical expressions are obtained for the self-diffusion coefficients, which turns out to be length-dependent, and for the stationary solution. In the steady state we find a discontinuous boundary-induced phase transition as the total exterior density gradient between the system boundaries is varied. At one boundary a boundary layer develops inside which the current flows against the local density gradient. Generically the width of the boundary layer and the bulk density profiles do not depend on the two hopping rates. At the phase transition line, however, the individual density profiles depend strongly on the ratio $D_A/D_B$. Dynamic Monte Carlo simulation confirm our theoretical predictions. ", "machine_abstract": "We study the phase diagram and critical behavior for the two component symmetric exclusion process (TCSEP) on an infinite one-dimensional lattice, where particles can hop to nearest neighbor sites only if they are empty. We show that there is no condensation at finite density when the system has periodic boundary conditions. However, we find that the TCSEP undergoes a first-order phase transition into a condensed state as soon as it is coupled to particle reservoirs at its ends. The order parameter jumps discontinuously across this transition line which terminates at a tricritical point. In addition, we calculate exactly the current-current correlation function along the transition line using Bethe ansatz techniques. Finally, we discuss how our results may be generalized to higher dimensions. PACS numbers: 05.40.+j, 64.60.Cn, 71.10.Jk  I. INTRODUCTORY REMARK The aim of this work is to investigate the properties of a simple model of interacting particles in contact with particle reservoirs. This problem arises naturally in many physical situations such as traffic flow [1] , molecular motors [2] or granular gases [3] . Here, we consider the so-called two-component symmetric exclusion process (TCSP), i.e., a system consisting of two species of indistinguishable particles A and B evolving according to the following rules [4] : Particles of type A and B move independently on a ring of L sites by alternating between neighboring sites with rates p and q respectively. If both types of particles attempt to occupy the same site simultaneously then either the A-particle hops forward while the B-particle stays put or vice versa depending on whether p > q or p < q. Note that these processes conserve the number of each kind of particles separately but not their total number N = nA + nB. Therefore, the dynamics of the TCSP is described by the master equation", "paraphrased_abstract": "The principle is derived from a physical phenomenon called two-component-symmetric exclusion (SCEP), a system of two classes of indistinguishable particles A and B, according to the following rules: Particles of the type A and B move in unison on a ring of L radii, alternating between adjacent sites at a rate of p and q, i.e., when the two types are in agreement, and the p and q differ in the amount of a particular particle. We illustrate this with the simple model of a two-part process, the so-called two-part process, in which two particles A and B are distinguished by the same rules: they move independently on a ring of L radii, and their rates are p and q. If they try to invade the same radii, then the A radii radii radii radii are swept forward, and the B radii are borne away; we show that this process does not condense, as it is periodic, if it is connected to the particle reservoirs at its ends. It is found that the first-order phase transition occurs in the TCSEP, and the order parameter 13jm1mn, and at the bottom of this transition the order", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0799, "title": "Spin Evolution of Accreting Neutron Stars: Nonlinear Development of the R-mode Instability", "abstract": "  The nonlinear saturation of the r-mode instability and its effects on the spin evolution of Low Mass X-ray Binaries (LMXBs) are modeled using the triplet of modes at the lowest parametric instability threshold. We solve numerically the coupled equations for the three mode amplitudes in conjunction with the spin and temperature evolution equations. We observe that very quickly the mode amplitudes settle into quasi-stationary states. Once these states are reached, the mode amplitudes can be found algebraically and the system of equations is reduced from eight to two equations: spin and temperature evolution. Eventually, the system may reach thermal equilibrium and either (1) undergo a cyclic evolution with a frequency change of at most 10%, (2) evolve toward a full equilibrium state in which the accretion torque balances the gravitational radiation emission, or (3) enter a thermogravitational runaway on a very long timescale of about $10^6$ years. Alternatively, a faster thermal runaway (timescale of about 100 years) may occur. The sources of damping considered are shear viscosity, hyperon bulk viscosity and boundary layer viscosity. We vary proprieties of the star such as the hyperon superfluid transition temperature T_c, the fraction of the star that is above the threshold for direct URCA reactions, and slippage factor, and map the different scenarios we obtain to ranges of these parameters. For all our bound evolutions the r-mode amplitude remains small $\\sim 10^{-5}$. The spin frequency is limited by boundary layer viscosity to $\\nu_{max} \\sim 800 Hz [S_{ns}/(M_{1.4} R_6)]^{4/11} T_8^{-2/11}$. We find that for $\\nu > 700$ Hz the r-mode instability would be active for about 1 in 1000 LMXBs and that only the gravitational waves from LMXBs in the local group of galaxies could be detected by advanced LIGO interferometers. ", "machine_abstract": "We study nonlinear development of the r-mode instability in accreting neutron stars using numerical simulations with realistic equations of state and nuclear composition. We find that the growth rate of unstable modes is reduced by several orders of magnitude compared to previous results obtained for polytropic models, but still remains much higher than the viscous damping rates inferred from observations. The saturation amplitude of unstable modes depends on their frequency as well as on the equation of state used in our calculations.  For low-frequency modes (with frequencies below about 100 Hz) we obtain amplitudes which are consistent with those observed in SAX J1808.4-3658. However, for high-frequency modes (frequencies above 300 Hz), the predicted amplitudes exceed observational limits by more than an order of magnitude. This discrepancy may be resolved if additional dissipation mechanisms such as gravitational radiation reaction or magnetic field generation play important role during the evolution of unstable modes.", "paraphrased_abstract": "In the case of unstable modes, the density of their r-modes is restricted by the number of their frequency and, in particular, by the nature of the equation of state used in our calculations. We find that the growth rate of unstable modes is reduced by several orders of magnitude compared with that obtained for polytropic models, but it is still much higher than the evaporation coefficients inferred from observations. We have examined the development of unstable modes in accreting neutron stars in a numerical simulation based on a realistic mathematical equation of state and the nuclear composition. The amplitude of these unstable modes depends on the frequency of the variable elements and the equation of state which we use. We can calculate the amplitude of the steady state and the number of particles. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0954, "title": "Sensor Networks with Random Links: Topology Design for Distributed Consensus", "abstract": "  In a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. The signal-to-noise ratio (SNR) is usually a main factor in determining the probability of error (or of communication failure) in a link. These probabilities are then a proxy for the SNR under which the links operate. The paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. To consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. With these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. We show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost. ", "machine_abstract": "We consider the problem of designing distributed consensus algorithms in sensor networks where links are randomly generated and can be lost or added over time. We propose an algorithm that is robust to link failures, but requires only local information exchange between neighboring nodes. The proposed algorithm achieves global convergence under mild conditions on network topology. In particular, we show that if each node has at least one neighbor whose degree is greater than its own then our algorithm converges almost surely (a.s.) to the correct value. Our results also hold when there exists a small number of Byzantine nodes which may deviate arbitrarily from their prescribed behavior. Finally, numerical simulations demonstrate the effectiveness of our approach. Keywords: Sensor Networks; Distributed Consensus; Local Information Exchange; Robustness Analysis. 1 Introduction Distributed consensus problems arise naturally in many applications such as flocking [1] , formation control [2] , multi-agent coordination [3] , wireless sensor networks [4] , etc.. A typical example is the average-consensus problem: given a set of n agents connected by communication links, each agent holds some initial data xi(0) \u2208 Rm, i = 1, ..., n; it aims to compute the average x\u0304=1/n\u2211in=1xi(0). This problem was first studied by Tsitsiklis et al. [5] . They showed that if all agents have access to the same fixed directed graph G, then the average-consensus problem can be solved using a simple linear iterative scheme. However, this assumption does not always hold true since the underlying communication graphs are often random due to unreliable links [6] . In recent years, several researchers have investigated the design of distributed consensus algorithms in dynamic networks [7-10]. For instance, Olfati-Saber [7] considered the case where the communication links among agents change randomly according to independent Bernoulli processes. Under certain assumptions on the connectivity of the network, she proved that her algorithm converges almost surely (i.e., with probability one) to the desired average. Subsequently, Jadbabaie et al. [8] extended these results to undirected networks. More recently,", "paraphrased_abstract": "\u201cIn the past, several scientists have tried to develop the solution of such problems in dynamic networks. In particular, Olfati-Saber considered a case where the communication links between agents in a system change invariably by independent Bernoulli processes. In the absence of such an arrangement, the solution was found to be almost certain (i.e., of one-in-one probability) to the desired average. We present an algorithm for the solution of this problem, which is robust to the failure of links, but which only requires the exchange of information between neighboring nodes. We show that the proposed algorithm can be generalized in the presence of network topology, under mild conditions. We show that if each node has at least one neighbor whose degree is greater than its own, then the algorithm will be almost unanimous in its solution. In addition, we show that if there is an ample number of nodes, in which there are deviations from the norm, our method will be almost unanimous. It was first studied by Tsitsiklis et al., which showed that if all the agents have access to the same fixed graph G, the problem can be solved by a simple linear iterative method, but this does not always hold, because the underlying graphs are often arbitrary, because the links between them are not always steady. In our work, we study the problem of", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0924, "title": "Lower order terms in the 1-level density for families of holomorphic cuspidal newforms", "abstract": "  The Katz-Sarnak density conjecture states that, in the limit as the conductors tend to infinity, the behavior of normalized zeros near the central point of families of L-functions agree with the N -> oo scaling limits of eigenvalues near 1 of subgroups of U(N). Evidence for this has been found for many families by studying the n-level densities; for suitably restricted test functions the main terms agree with random matrix theory. In particular, all one-parameter families of elliptic curves with rank r over Q(T) and the same distribution of signs of functional equations have the same limiting behavior. We break this universality and find family dependent lower order correction terms in many cases; these lower order terms have applications ranging from excess rank to modeling the behavior of zeros near the central point, and depend on the arithmetic of the family. We derive an alternate form of the explicit formula for GL(2) L-functions which simplifies comparisons, replacing sums over powers of Satake parameters by sums of the moments of the Fourier coefficients lambda_f(p). Our formula highlights the differences that we expect to exist from families whose Fourier coefficients obey different laws (for example, we expect Sato-Tate to hold only for non-CM families of elliptic curves). Further, by the work of Rosen and Silverman we expect lower order biases to the Fourier coefficients in families of elliptic curves with rank over Q(T); these biases can be seen in our expansions. We analyze several families of elliptic curves and see different lower order corrections, depending on whether or not the family has complex multiplication, a forced torsion point, or non-zero rank over Q(T). ", "machine_abstract": "We study lower order terms in the one-level density formula for families of holomorphic  cuspidal newforms on GL(2,{\\mathbb C}). We prove that if f is such a form and g is another form with coefficients in an imaginary quadratic field K then the difference between the one-level densities of f and g vanishes to all orders at s = 1/2 when f has trivial central character or when it does not have any non-trivial zeros off the critical line. In particular this implies that the first moment of the family of forms corresponding to f agrees with the first moment of the Hecke-Maa\u00df series attached to f . This generalizes results by Iwaniec and Sarnak who proved these statements under additional hypotheses on the weight function w used in their definition of the one-level density. The proof uses ideas developed by Goldfeld and Hoffstein in their work on the Rankin-Selberg integral associated to two modular forms.", "paraphrased_abstract": "The proof applies the idea of Goldfeld and Hoffstein, who were able to use the Rankin-Selberg integral for two modular forms. We present an example of the lower order terms of the density formula for holomorphic newforms of the cuspidal group on GL(2(3, mathbb C) in which f has the smallest central character and g has no trivial zeros. The resulting statement is generalized to a generalization of the results of Iwaniec and Sarnak. The result shows that if f is such a form and g is a form with coefficients in an imaginary quadratic field K, the difference between the f and the g values vanishes to all orders at s = 1/2 when f does not have a trivial central character or if f does not have any trivial zeros at the critical line. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0263, "title": "Magnetic Excitations in La2CuO4 probed by Indirect Resonant Inelastic X-ray Scattering", "abstract": "  Recent experiments on La$_2$CuO$_4$ suggest that indirect resonant inelastic X-ray scattering (RIXS) might provide a probe for transversal spin dynamics. We present in detail a systematic expansion of the relevant magnetic RIXS cross section by using the ultrashort core-hole lifetime (UCL) approximation. We compute the scattering intensity and its momentum dependence in leading order of the UCL expansion. The scattering is due to two-magnon processes and is calculated within a linear spin-wave expansion of the Heisenberg spin model for this compound, including longer range and cyclic spin interactions. We observe that the latter terms in the Hamiltonian enhance the first moment of the spectrum if they strengthen the antiferromagnetic ordering. The theoretical spectra agree very well with experimental data, including the observation that scattering intensity vanishes for the transferred momenta ${\\bf q} = (0,0)$ and ${\\bf q} = (\\pi,\\pi)$. We show that at finite temperature there is an additional single-magnon contribution to the scattering with a spectral weight proportional to $T^3$. We also compute the leading corrections to the UCL approximation and find them to be small, putting the UCL results on a solid basis. All this univocally points to the conclusion that the observed low temperature RIXS intensity in La$_2$CuO$_4$ is due to two-magnon scattering. ", "machine_abstract": "We report on the observation and analysis of magnetic excitations in single crystals of La2CuO4 using indirect resonant inelastic x-ray scattering (RIXS). The RIXS data are compared to results obtained with neutron scattering, which show that both techniques probe similar energy scales but reveal different features due to their complementary nature. We find evidence for two distinct types of magnetic excitations at low energies: one is dispersive along the Cu-O bond direction while the other shows no dispersion within experimental resolution. These observations can be explained by considering spin waves coupled to charge fluctuations as well as localised magnons. Our study provides new insights into the low-energy physics of this prototypical cuprate compound. Introduction: Cuprates have been studied extensively over many decades because they exhibit high-temperature superconductivity [1] . Among these compounds, La2CuO4 has attracted particular attention since it was found to share several properties with more complex materials such as YBa2Cu3O7-x [2] , Bi2212 [3] or Hg1201 [4] . In recent years, there has been renewed interest in studying the electronic structure of La2CuO4 [5] - [8] . This is mainly driven by the fact that its crystal structure is simpler than those of most other cuprates [9] : it consists only of copper-oxygen layers separated by lanthanum ions. Moreover, the absence of apical oxygen atoms makes it possible to grow large high-quality single crystals [10] . Finally, the presence of only one type of charge carrier allows us to investigate the intrinsic behaviour of the material without being obscured by disorder effects [11] . The ground state of La2CuO4 is antiferromagnetic [12] . It exhibits an orthorhombic distortion below TN = 325 K [13] accompanied by a structural phase transition [14] . At lower temperatures, the system undergoes another phase transition [15] where the spins become ordered [16] . Despite extensive research [17] - [20] , the origin of the magnetism remains controversial [21] .", "paraphrased_abstract": "Then, in the lower temperature, the magnetic flux of the magnetic field becomes more and more sharp. Then, in the lower temperature, the magnetic flux is more intense and the spins are ordered. The new findings of this study provide new insights into the physics of this prototypical cuprate. Cuprates have been studied extensively for many decades, because they exhibit high temperature superconductivity. In addition, they are a little simpler than most other cuprates, because they contain only copper-oxygen layers, separated by lanthanum ions. The structure is simpler than most other cuprates, and contains only lanthanum ions. This means that they can be produced at low temperatures, and therefore are of different chemical properties. We characterized the magnetic properties of the single crystals with indirect resonant inelastic X-rays, and compared with neutron X-rays, we show that both of these spectral methods have similar dimensions, but they have different properties., which makes it possible to study the magnetic properties of the crystalline material, but without any special disorder. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3063, "title": "Search for Radiative Decays of Upsilon(1S) into eta and eta'", "abstract": "  We report on a search for the radiative decay of Upsilon(1S) to the pseudoscalar mesons eta and etaprime in 21.2 +/- 0.2 times 10^6 Upsilon(1S) decays collected with the CLEO III detector at the Cornell Electron Storage Ring (CESR). The eta meson was reconstructed in the three modes eta to gamma-gamma, eta to pi+pi-pi0 and eta to 3pi0. The etaprime meson was reconstructed in the mode etaprime to pi+ pi- eta with eta decaying through any of the above three modes, and also etaprime to gamma rho, where rho decays to pi^+ pi^-.   Five out of the seven sub-modes are found to be virtually background-free. In four of them we find no signal candidates and in one Upsilon(1S) to gamma-etaprime, etaprime to pi+ pi- eta, eta to pi+pi-pi0 there are two good signal candidates, which is insufficient evidence to claim a signal. The other two sub-modes eta to gamma-gamma and etaprime to gamma rho are background limited, and show no excess of events in their signal regions. We combine the results from different channels and obtain upper limits at the 90% C.L. which are B(Upsilon(1S) to gamma eta) < 1.0 times 10^-6 and B(Upsilon(1S) to gamma etaprime) < 1.9 times 10^-6. Our limits are an order of magnitude tighter than the previous ones and below the predictions made by some theoretical models. ", "machine_abstract": "We report on the search for radiative decays of the \\Upsilon(1S) meson to final states with one photon, one electron or muon, and an unobserved neutral particle (\\eta or \\eta'). The data sample consists of 1.0 fb-1 collected by the Belle experiment at the KEKB asymmetric-energy e+e- collider in 2007-2009. No significant signal is observed above backgrounds estimated using Monte Carlo simulation. We set upper limits on branching fractions as functions of mass hypotheses for both decay modes.  These results are combined with those obtained previously by Belle and BaBar experiments. For masses below 2 GeV/c2 , we exclude branching fraction values larger than 3 x 10-4 .  I. INTRODUCTIO N The \\Upsilon(1 S ) meson has been studied extensively since its discovery [1] because it plays important roles in understanding QCD dynamics [2] . In particular, the study of its electromagnetic properties provides information about hadronic structure [3] . In this Letter, we present our result on searches for two-body radiative decays of \\Upsilon(1 s ) to final states containing one photon, one lepton, and an unobserv ed neutral particle [4] :", "paraphrased_abstract": "In the following sentence, we present our study of the radiative decay of the -upsilon(1S) meson to the final states of one photon, one electron, and an unobserved neutral particle. In this paper, we report the results of our search for the two-body radiative decay of -upsilon(1S) meson to the final states of one photon, one electron, and an unobserved neutral particle. In addition, our results on the BaBar experiment have been compared with the results of the Belle and BaBar experiments. In this letter, we report the search for the two-body radiative decay of -upsilon(1S) meson to the final states containing one photon, one electron, and one unobserved neutral particle : we sigh, we are not sighing; we sigh, we sigh. ", "model": "bigscience bloomz", "source": "arxiv"}
