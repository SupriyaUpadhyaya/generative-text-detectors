{"source_id": 705.2519, "title": "Faithful Effective-One-Body waveforms of small-mass-ratio coalescing black-hole binaries", "abstract": "  We address the problem of constructing high-accuracy, faithful analytic waveforms describing the gravitational wave signal emitted by inspiralling and coalescing binary black holes. We work within the Effective-One-Body (EOB) framework and propose a methodology for improving the current (waveform)implementations of this framework based on understanding, element by element, the physics behind each feature of the waveform, and on systematically comparing various EOB-based waveforms with ``exact'' waveforms obtained by numerical relativity approaches. The present paper focuses on small-mass-ratio non-spinning binary systems, which can be conveniently studied by Regge-Wheeler-Zerilli-type methods. Our results include: (i) a resummed, 3PN-accurate description of the inspiral waveform, (ii) a better description of radiation reaction during the plunge, (iii) a refined analytic expression for the plunge waveform, (iv) an improved treatment of the matching between the plunge and ring-down waveforms. This improved implementation of the EOB approach allows us to construct complete analytic waveforms which exhibit a remarkable agreement with the ``exact'' ones in modulus, frequency and phase. In particular, the analytic and numerical waveforms stay in phase, during the whole process, within $\\pm 1.1 %$ of a cycle. We expect that the extension of our methodology to the comparable-mass case will be able to generate comparably accurate analytic waveforms of direct use for the ground-based network of interferometric detectors of gravitational waves. ", "machine_abstract": "We present the first results for effective-one-body (EOB) waveforms that are faithful to numerical relativity simulations over an entire inspiral, merger and ringdown phase of binary black hole systems with mass ratios q = m1/m2 as low as 1/10.  We use EOB theory in conjunction with fits to numerical-relativity data to construct accurate templates for gravitational waves emitted by such systems. The resulting template bank is used to search for signals from these sources using LIGO's second observing run data. Our analysis shows that this new class of EOB waveforms can be used to detect binary black holes out to redshifts z ~ 5 at design sensitivity of Advanced LIGO/VIRGO detectors. This represents a significant improvement compared to previous searches based on post-Newtonian approximations or phenomenological models.  The detection rate of binary black holes predicted by our analysis agrees well with current estimates obtained within the standard \u039bCDM cosmology model.", "paraphrased_abstract": "The results of our analysis are consistent with the estimations in the standard CDM cosmology. The sensitivity of the EOB waveforms is at least one thousandth of that of the advanced detectors of LIGO. The EOB waveforms are compared with the geometrical realisms for the inspiring, the merger, and the ringing phases of binary black holes. We have developed a new class of EOB waveforms which can be used to detect black holes at redshifts of z  5, as compared with the results obtained by previous studies based on the post-Newtonian approximation or phenomenological model. The results of this study show that this new class of EOB waveforms is able to detect binary black holes at redshifts of z  5. The detection rate of these black holes is significantly higher than estimates obtained in the standard cosmology. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0271, "title": "Pairing based cooling of Fermi gases", "abstract": "  We propose a pairing-based method for cooling an atomic Fermi gas. A three component (labels 1, 2, 3) mixture of Fermions is considered where the components 1 and 2 interact and, for instance, form pairs whereas the component 3 is in the normal state. For cooling, the components 2 and 3 are coupled by an electromagnetic field. Since the quasiparticle distributions in the paired and in the normal states are different, the coupling leads to cooling of the normal state even when initially $T_{paired}\\geq T_{normal}$ (notation $T_S\\geq T_N$). The cooling efficiency is given by the pairing energy and by the linewidth of the coupling field. No superfluidity is required: any type of pairing, or other phenomenon that produces a suitable spectral density, is sufficient. In principle, the paired state could be cooled as well but this requires $T_N<T_S$. The method has a conceptual analogy to cooling based on superconductor -- normal metal (SN) tunneling junctions. Main differences arise from the exact momentum conservation in the case of the field-matter coupling vs. non-conservation of momentum in the solid state tunneling process. Moreover, the role of processes that relax the energy conservation requirement in the tunneling, e.g. thermal fluctuations of an external reservoir, is now played by the linewidth of the field. The proposed method should be experimentally feasible due to its close connection to RF-spectroscopy of ultracold gases which is already in use. ", "machine_abstract": "We propose to cool fermionic atoms in optical lattices by using the pairing mechanism between two different hyperfine states, which is analogous to Cooper pair formation in superconductors. We show that this method can be used for both bosonic and fermionic systems with attractive interactions. The proposed scheme has several advantages over other methods such as evaporative cooling or sympathetic cooling.  It does not require any additional laser beams besides those needed for trapping and manipulating cold atoms. In addition it works even when there are no free particles present initially (e.g., at zero temperature). Finally we discuss how our proposal could be realized experimentally. Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging problems in atomic physics today [1] . This problem becomes particularly difficult if the initial number density of fermions is high because then elastic collisions cannot remove enough energy from the system [2] . In recent years, however, new experimental techniques have been developed [3, 4] , allowing us to trap and manipulate cold atoms on an unprecedented level [5] . These developments make it possible to study many-body phenomena [6] like superfluidity [7, 8] and Bose-Einstein condensation [9] in ultracold atomic gases. One important goal in these experiments is to reach quantum degenerate regimes where the gas consists of strongly interacting fermions [10] . However, reaching low temperatures requires efficient cooling schemes [11] . One promising approach towards achieving this goal is to use the pairing mechanism [12] . Pairs of fermions form bound states called Cooper pairs in conventional superconductors [13] . Analogously, pairs of fermions may also form bound states in ultracold atomic clouds [14] . If the interaction strength between fermions is sufficiently large, they will preferentially bind into pairs rather than remaining unpaired [15] . Therefore, cooling fermions via pairing should work well even", "paraphrased_abstract": "The latter is of high energy, but at low temperatures, and they must be cooled efficiently. In these experiments, the number of fermions is high and the number of atoms is high, but elastic collisions are not strong enough, so the effect is weak. It is important to reach the level of aquiline and spherical atoms; whereas in a spherical atom the interaction strength is very high, and it is preferential to bind in pairs, rather than combining. Then, finally, we describe how we have achieved this. The spherical atoms are arranged in the optical lattices with the resemblance of Cooper pairs, and in a spherical atom the interactions are attractive. The recently developed ionosphere has been studied to a remarkable degree; it has also been able to observe many other phenomena, such as superfluidity, and Bose-Einstein condensation. These ionospheres are of extremely large sizes and have the characteristic strength of being interconnected, and therefore are suited to being resembling pairs rather than being in two states. Our method is capable of cooling fermions at a 'quarrel-degeneracy' temperature, and can even be applied to the 'familiar'", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2584, "title": "Constraints on the Abundance of Highly Ionized Proto-Cluster Regions from the Absence of Large Voids in the Lyman Alpha Forest", "abstract": "  Energetic feedback processes during the formation of galaxy clusters may have heated and ionized a large fraction of the intergalactic gas in proto-cluster regions. When such a highly ionized hot ``super-bubble'' falls along the sightline to a background quasar, it would be seen as a large void, with little or no absorption, in the Lyman alpha forest. We examine the spectra of 137 quasars in the Sloan Digital Sky Survey, to search for such voids, and find no clear evidence of their existence. The size distribution of voids in the range 5-70 Angstrom (corresponding to physical sizes of approximately 3-35 comoving Mpc/h) is consistent with the standard model for the Lyman alpha forest without additional hot bubbles. We adapt a physical model for HII bubble growth during cosmological reionization (Furlanetto, Zaldarriaga and Hernquist 2004), to describe the expected size-distribution of hot super-bubbles at redshift around z = 3. This model incorporates the conjoining of bubbles around individual neighboring galaxies. Using the non-detection of voids, we find that models in which the volume filling factor of hot bubbles exceeds approximately 20 percent at z=3 can be ruled out, primarily because they overproduce the number of large (40-50 Angstrom) voids. We conclude that any pre-heating mechanism that explains galaxy cluster observations must avoid heating the low-density gas in the proto-cluster regions, either by operating relatively recently (z<3) or by depositing entropy in the high-density regions. ", "machine_abstract": "We use hydrodynamic simulations to study how proto-clusters grow and evolve into galaxy clusters, focusing on their baryon content at high redshifts (z > 5). We find that most of these regions are highly ionized by z = 3 due to photo-heating by UV background radiation. The resulting low neutral hydrogen fraction leads to an under-density of absorbers along the line-of-sight towards such objects compared with lower redshift observations. This effect is more pronounced for higher mass halos which have larger gas fractions than less massive ones. Using this result we derive constraints on the abundance of high-redshift proto-clusters as a function of halo mass. These results can be used to test models of structure formation and reionization. In addition they provide useful input parameters for future studies of cluster scaling relations using weak lensing techniques.     Keywords: Hydrogen ionization state, Galaxy Cluster, Reionization", "paraphrased_abstract": "These results can be used to test hydrodynamic models for the formation of galaxies and reionization. We have already obtained some data on the size of galaxies and their reionization. The result is that most of these galaxies are highly ionized at z = 3 because of the radiant radiance of the sun., due to the ultraviolet radiation. We found that the resulting low neutral hydrogen content causes the concentration of absorbers at the edge of the line of sight to be smaller than at the lower one. The resulting low neutral hydrogen mass leads to an undershoot of absorbers towards such objects as compared to lower ones. The study of hydrodynamics is carried out using the hydrodynamic simulation of a galaxy cluster, focusing on the presence of baryons in a high redshift. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0932, "title": "On the Origin of the Dichotomy of Early-Type Galaxies: The Role of Dry Mergers and AGN Feedback", "abstract": "  Using a semi-analytical model for galaxy formation, combined with a large N-body simulation, we investigate the origin of the dichotomy among early-type galaxies. We find that boxy galaxies originate from mergers with a progenitor mass ratio $n < 2$ and with a combined cold gas mass fraction $F_{\\rm cold} < 0.1$. Our model accurately reproduces the observed fraction of boxy systems as a function of luminosity and halo mass, for both central galaxies and satellites. After correcting for the stellar mass dependence, the properties of the last major merger of early-type galaxies are independent of their halo mass. This provides theoretical support for the conjecture of Pasquali et al (2007) that the stellar mass of an early-type galaxy is the main parameter that governs its isophotal shape. We argue that the observed dichotomy of early-type galaxies has a natural explanation within hierarchical structure formation, and does not require AGN feedback. Rather, we argue that it owes to the fact that more massive systems (i) have more massive progenitors, (ii) assemble later, and (iii) have a larger fraction of early-type progenitors. Each of these three trends causes the cold gas mass fraction of the progenitors of more massive early-types to be lower, so that their last major merger was dryer. Finally, our model predicts that (i) less than 10 percent of all early-type galaxies form in major mergers that involve two early-type progenitors, (ii) more than 95 percent of all boxy early-type galaxies with $M_* < 2 \\times 10^{10} h^{-1} \\Msun$ are satellite galaxies, and (iii) about 70 percent of all low mass early-types do not form a supermassive black hole binary at their last major merger. The latter may help to explain why low mass early-types have central cusps, while their massive counterparts have cores. ", "machine_abstract": "We present an analysis of the evolution of early-type galaxies (ETGs) in hydrodynamical cosmological simulations, focusing on their formation history as well as their internal structure at z = 0. We find that ETG progenitors are typically gas-rich systems with high star formation rates (SFRs), which evolve into red-sequence objects through dry mergers. In addition to this major merger channel for forming ETGs, we show that minor mergers can also contribute significantly to the growth of massive ETGs by bringing in fresh cold gas. Our results suggest that both processes play important roles in shaping the observed properties of local ETGs. This is consistent with recent observational studies showing that most massive ETGs have experienced multiple episodes of merging over cosmic time.     Keywords: galaxy evolution, galaxy morphology, galaxy scaling relations, galaxy clusters, semi-analytic models, hydrodynamics, dark matter", "paraphrased_abstract": ", they are dominated by gaseous, swarming systems. They are, therefore, of a high star formation rate, which arose through dry mergers, and which, though not the main source of their formation, have also been found to have a very significant influence on the formation of their elongated, red-sequenced objects. These results, consistent with recent studies, indicate that the most massive elongated systems have experienced many swarmings in the past. The evolution of elongated galaxies is the study of the formation of the early-type galaxies in hydrodynamic simulations, which analyzed their formation history, and their internal structure at z = 0.01. In addition to the main and minor sources of formation, minor mergers also influence the growth of the larger elongated galaxies by introducing cold gas. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0527, "title": "The Discovery of 1000 km/s Outflows in Massive Post-starburst Galaxies at z=0.6", "abstract": "  Numerical simulations suggest that active galactic nuclei (AGNs) play an important role in the formation of early-type galaxies by expelling gas and dust in powerful galactic winds and quenching star formation. However, the existence of AGN feedback capable of halting galaxy-wide star formation has yet to be observationally confirmed. To investigate this question, we have obtained spectra of 14 post-starburst galaxies at z~0.6 to search for evidence of galactic winds. In 10/14 galaxies we detect Mg II 2796,2803 absorption lines which are blueshifted by 490 - 2020 km/s with respect to the stars. The median blueshift is 1140 km/s. We hypothesize that the outflowing gas represents a fossil galactic wind launched near the peak of the galaxy's activity, a few 100 Myr ago. The velocities we measure are intermediate between those of luminous starbursts and broad absorption line quasars, which suggests that feedback from an AGN may have played a role in expelling cool gas and shutting down star formation. ", "machine_abstract": "We report the discovery of massive outflows (1000 km s-1) in two post-starburst galaxies, which are thought to be progenitors of present-day elliptical galaxies. The observations were made with Subaru/FOCAS and VLT/VIMOS using near-infrared spectroscopy covering rest-frame optical emission lines for these objects at redshift 0.6. We found that both galaxies show strong [O III]5007\u00c5/H\u03b2 ratios indicating high ionization parameters. In addition, we detected blueshifted high-velocity components in H\u03b1 profiles as well as broad wings on their H\u03b1 line profiles. These results suggest that there is an energetic ionized gas flow driven by AGN and/or supernovae activity. This work was supported by Grants-in-Aid for Scientific Research from MEXT Japan (No. 20540230). Keywords: Emission-line galaxy, Galaxy evolution, Nearby galaxy", "paraphrased_abstract": "\" In this paper, the first part of the project is a report about the ionized gas from two elliptical galaxies. They are thought to be the ancestors of the present elliptical galaxies. The results of this research are summarized. We report that there are a great number of outflows from two elliptical galaxies. The data from Subaru/FOCAS and VLT/VIMOS are very high, and they are the progenitors of modern elliptical galaxies. We observe that there are strong O III = 5007/H ratios, which are very high, and besides that there are blue-shifted components in the H line, and broad wings in the H line. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3986, "title": "Parametric Nonholonomic Frame Transforms and Exact Solutions in Gravity", "abstract": "  A generalized geometric method is developed for constructing exact solutions of gravitational field equations in Einstein theory and generalizations. First, we apply the formalism of nonholonomic frame deformations (formally considered for nonholonomic manifolds and Finsler spaces) when the gravitational field equations transform into systems of nonlinear partial differential equations which can be integrated in general form. The new classes of solutions are defined by generic off-diagonal metrics depending on integration functions on one, two and three (or three and four) variables if we consider four (or five) dimensional spacetimes. Second, we use a general scheme when one (two) parameter families of exact solutions are defined by any source-free solutions of Einstein's equations with one (two) Killing vector field(s). A successive iteration procedure results in new classes of solutions characterized by an infinite number of parameters for a non-Abelian group involving arbitrary functions on one variable. Five classes of exact off-diagonal solutions are constructed in vacuum Einstein and in string gravity describing solitonic pp-wave interactions. We explore possible physical consequences of such solutions derived from primary Schwarzschild or pp-wave metrics. ", "machine_abstract": "We present new exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors, which are generated by applying nonholonomic frame transforms (NFT) to known vacuum solutions. The NFT is constructed using an ansatz for the metric coefficients that depends on one arbitrary function of the radial coordinate only. We show how this method can be used to generate families of black hole solutions with different horizon topologies. In particular we find new rotating black ring solutions with toroidal horizons. These solutions have been obtained previously as limits of static black rings but our approach allows us to obtain them directly without any additional assumptions or approximations. Finally, we discuss some open problems related to these results. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq  I. INTRODUCTORY REMARkS The study of exact solutions to the Einstein equations has played a crucial role in understanding many aspects of general relativity. However, it is often difficult to construct such solutions because they require solving complicated nonlinear partial differential equations. This problem becomes even more challenging when considering physically interesting situations like those involving rotation and/or matter fields. Nevertheless, there exist several techniques that allow one to generate new classes of solutions starting from simpler ones. One of the most powerful methods involves transforming the original solution into another one via so-called nonholonomic frame transforms [1] . Such transformations preserve certain geometric properties of the spacetime while changing others; see [2] - [4] for reviews. For example, if the transformed solution satisfies the vacuum Einstein equations then so does the original one [5] . In this work we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to generate new exact solutions describing stationary axisymmetric spacetimes: i.e., spacetimes admitting at least two independent Killing vector fields whose orbits are closed curves [6] . Stationary axisymmetric spacetimes play an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars, planets, and black holes [7, 8]", "paraphrased_abstract": "In particular, we present rotating black rings with toroidal horizons. The results of this approach are shown in terms of the number of points. The axisymmetrical spacetimes of the star and the planet are of interest in astronomy, because they describe the exterior gravitational field of the objects that spin, such as stars, planets and black holes. In the present work, we introduce the non-holonomic transform of the Einstein equations to new exact solutions for the axisymmetrical spacetimes: that is, spacetimes admitting two separate axisymmetrical axes, whose orbits are closed curves. These are the exact solutions of the Einstein equations for stationary axisymmetrical spacetimes, i.e., spacetimes in which the axis is closed, in which the dies are closed, and the axis is closed. We present here a new solution to the Einstein equations for axisymmetrical spacetimes, with two commuting axes, in which we obtain by means of non-holonomic frame transforms (NFTs) to the vacuum solutions of Einstein, which are generated by non-holonomic frame transforms, and which preserve certain geometric properties of the spacetime but change others; see (in particular) the following for a description of the problem. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1036, "title": "Strategies for the Design of a Slide-o-Cam Transmission", "abstract": "  The optimization of the pressure angle in a cam-follower transmission is reported in this paper. This transmission is based on Slide-o-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-o-Cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. The pressure angle is a suitable performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. Two alternative design strategies are studied, namely, (i) increase the number of lobes on each cam or (ii) increase the number of cams. This device is intended to replace the current ball-screws in Orthoglide, a three-DOF parallel robot for the production of translational motions, currently under development at Ecole Centrale de Nantes for machining applications. ", "machine_abstract": "A slide-o-cam transmission is an alternative to conventional geared transmissions in which the input and output shafts are connected by means of cams that rotate on their own axes, with no gears or other mechanical elements between them.  The main advantages of this type of transmission are:  \u2022 No backlash due to tooth meshing; \u2022 High efficiency (up to 98%); \u2022 Low noise levels; \u2022 Reduced weight and volume compared to traditional gearboxes; \u2022 Easy assembly and disassembly; \u2022 Possibility of using different types of motors as inputs. This article presents some strategies for designing a slide-o-cam transmission based on the analysis of its kinematic characteristics. These strategies have been applied to develop two prototypes of slide-o-cam transmissions intended for use in electric vehicles. In addition, a mathematical model has been developed to simulate the behavior of these transmissions under various operating conditions. Finally, experimental tests were carried out to validate both the design process proposed here and the results obtained through simulation.", "paraphrased_abstract": "\u201cThere are several advantages of using a slide-o-cam\u2014it is a transmission with no teeth and no gears, and it is connected to a movable gearbox, which is in the same way as the movable gearbox. Moreover, a mathematical model has been developed to simulate the performance of this transmission under different conditions. Then, it is proposed to investigate the application of these techniques to electric cars. This article will describe the theory and practice of a slide-o-cam, which are based on the analysis of the kinematical properties of the transmission. The main advantage of this type of transmission is that it does not have toothed teeth, it is efficient and noiseless; it is not connected with gears and no mechanical elements. The design of the slide-o-cam is also an alternative to conventional geared transmissions, in which the input and output shafts are connected by means of a cam, which are rotated on their own axis. It is also easy to assembly and disassemble. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.371, "title": "Spitzer Observations of Transient, Extended Dust in Two Elliptical Galaxies: New Evidence of Recent Feedback Energy Release in Galactic Cores", "abstract": "  Spitzer observations of extended dust in two optically normal elliptical galaxies provide a new confirmation of buoyant feedback outflow in the hot gas atmospheres around these galaxies. AGN feedback energy is required to prevent wholesale cooling and star formation in these group-centered galaxies. In NGC 5044 we observe interstellar (presumably PAH) emission at 8 microns out to about 5 kpc. Both NGC 5044 and 4636 have extended 70 microns emission from cold dust exceeding that expected from stellar mass loss. The sputtering lifetime of this extended dust in the ~1keV interstellar gas, ~10^7 yrs, establishes the time when the dust first entered the hot gas. Evidently the extended dust originated in dusty disks or clouds, commonly observed in elliptical galaxy cores, that were disrupted, heated and buoyantly transported outward. The surviving central dust in NGC 5044 and 4636 has been disrupted into many small filaments. It is remarkable that the asymmetrically extended 8 micron emission in NGC 5044 is spatially coincident with Halpha+[NII] emission from warm gas. A calculation shows that dust-assisted cooling in buoyant hot gas moving out from the galactic core can cool within a few kpc in about ~10^7 yrs, explaining the optical line emission observed. The X-ray images of both galaxies are disturbed. All timescales for transient activity - restoration of equilibrium and buoyant transport in the hot gas, dynamics of surviving dust fragments, and dust sputtering - are consistent with a central release of feedback energy in both galaxies about 10^7 yrs ago. ", "machine_abstract": "We report the discovery of transient dust emission at mid-infrared wavelengths (5-20 microns) in two elliptical galaxies with active galactic nuclei (AGN). The observations were made using Spitzer Space Telescope's Infrared Array Camera and Multiband Imaging Photometer for Spitzer instruments over a period of several years. We find that the infrared luminosity is consistent with heating by AGN radiation or supernovae remnants within the central kpc region. This suggests that recent feedback energy release has been occurring in these cores. These results are important because they provide new evidence on how supermassive black holes grow through accretion onto their host galaxy centers. They also demonstrate the power of combining multiwavelength data to study the physical processes associated with nuclear activity.     Keywords: Active galactic nucleus, Galaxy evolution, Mid-infrared, Nuclear starbursts     1. Introduction     Supermassive black holes reside in the center of most massive galaxies. Their growth is thought to be fueled by gas inflow driven by gravitational torques produced during mergers and/or interactions between galaxies (Barnes & Hernquist 1996; Hopkins et al. 2006) . However, it remains unclear what happens after this fuel supply runs out. One possibility is that the black hole continues growing via radiatively inefficient accretion flows (Narayan & Yi 1994) , which may produce powerful winds and jets that can drive large-scale outflows into the surrounding interstellar medium (ISM) (Silk & Rees 1998; Di Matteo et al. 2005 ). Another possibility is that the black holes become dormant as the ISM becomes too hot to cool efficiently (Bower et al. 2006; Croton et al. 2006 ) until another merger event triggers renewed activity. Understanding the mechanisms responsible for shutting off black-hole growth will help us understand why some galaxies have large black holes while others do not.     2. Previous Work     Several studies have shown that there exists an anti-correlation between the mass of the central supermassive black hole and the stellar velocity dispersion of its host galaxy bulge (Ferrar", "paraphrased_abstract": "But it remains unknown what happens after this fuel. It is a question of what happens when the black hole is stopped up by an influx of energy, which is accompanied by strong winds and jets, which drive out huge masses of light into the interstellar medium, which is in a swarming state (Silk et al., 1999; Di Matteo et al., 2005). The findings are significant because they reveal the nature of supermassive black holes in the center of the most massive galaxies, and also show how nuclear activity is confined to a single region. It is also possible that the black hole grows into a swarming state with the help of radiating inefficient gasses, such as Narayan et al., and with the help of a powerful wind and a jet which drive large-scale influxes into the interstellar medium (ISM), e.g., Croton et al. (2005), and so on until a new merger event in a new region will be observed. In this work, we will present a study of the recurrence of a swarming black hole at the center of two elliptical galaxies with active galactic nuclei (AGN), the result of which is that", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1507, "title": "HST/ACS Coronagraphic Observations of the Dust Surrounding HD 100546", "abstract": "  We present ACS/HST coronagraphic observations of HD 100546, a B9.5 star, 103 pc away from the sun, taken in the F435W, F606W, and F814W bands. Scattered light is detected up to 14'' from the star. The observations are consistent with the presence of an extended flattened nebula with the same inclination as the inner disk. The well-known ``spiral arms'' are clearly observed and they trail the rotating disk material. Weaker arms never before reported are also seen. The inter-arm space becomes brighter, but the structures become more neutral in color at longer wavelengths, which is not consistent with models that assume that they are due to the effects of a warped disk. Along the major disk axis, the colors of the scattered-light relative to the star are \\Delta (F435W-F606W) ~ 0.0--0.2 mags and \\Delta (F435W-F814W)~0.5--1 mags. To explain these colors, we explore the role of asymmetric scattering, reddening, and large minimum sizes on ISM-like grains. We conclude each of these hypotheses by itself cannot explain the colors. The disk colors are similar to those derived for Kuiper Belt objects, suggesting that the same processes responsible for their colors may be at work here. We argue that we are observing only the geometrically thick, optically thin envelope of the disk, while the optically thick disk responsible for the far-IR emission is undetected. The observed spiral arms are then structures on this envelope. The colors indicate that the extended nebulosity is not a remnant of the infalling envelope but reprocessed disk material. ", "machine_abstract": "We present new Hubble Space Telescope (HST) Advanced Camera for Survey (ACS) coronagraphic observations in visible light and near-infrared wavelengths that reveal an extended dusty disk surrounding the Herbig Ae star HD 100546, which is known to harbor a protoplanetary disk with spiral arms. The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the central star while allowing us to detect scattered light from circumstellar material located farther away. We find evidence for two bright rings of emission separated by ~0.5\u2032\u2032 along the major axis of the disk. These features are most likely due to scattering off large grains or planetesimals orbiting close to their parent stars.     In addition, we have detected several dark gaps within these bright rings as well as fainter structures extending outward into the outer regions of the disk. Our results suggest that this system may be undergoing planet formation through gravitational interactions between larger bodies such as planets and/or planetesimals.", "paraphrased_abstract": "In the present study we present a new and detailed view of the Hubble Space Telescope ACS in visible and near-infrared wavelengths, which reveal an extended dusty disk around the Herbig Ae star HD 100546, which is known to have a protoplanetary disk with spiral arms. This disk was characterized by two bright ringlets separated by 0.5 by the main axis of the disk, indicating the presence of large grains or other objects close to the parent star. We conclude that these ringlets are formed by colliding with larger bodies such as planets or other animals. These ringlets are surrounded by dark voids and specks of light. We also see several dark holes in the dark rings, and a few faint holes that penetrate deeper into the outer regions of the disk. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0811, "title": "First CO J=6-5, 4-3 detections in local ULIRGs: the dense gas in Mrk231, and its colling budget", "abstract": "  We report on detections of the high-excitation CO J=6-5, J=4-3 lines in Mrk231, a prototypical Ultra Luminous Infrared Galaxy (ULIRG) and Seyfert 1 QSO. These observations are combined with CO J=3-2, HCN J=4-3 (this work), and CO J=2-1, J=1-0, 13CO J=2-1, HCN J=1-0 measurements taken from the literature to provide better constraints on the properties of the molecular gas in an extreme starburst/QSO in the local Universe. We find that the CO J=4-3 and J=6-5 transitions trace a different gas phase from that dominating the lower three CO transitions, with n(H_2) ~ (1-3)x10^4 cm-3 and Tk ~ (40-70) K. This phase is responsible for the luminous HCN emission, and contains most of the H2 gas mass of this galaxy. The total CO line cooling emanating from this dense phase is found similar to that of the [CII] line at 158 micron, suggesting a very different thermal balance to that seen in lower IR-luminosity galaxies, and one likely dominated by dense photon-dominated regions. Our dense \"sampling\" of the CO rotational ladder and the HCN lines enables us to produce well-constrained Spectral Line Energy Distributions (SLEDs) for the dense molecular gas in Mrk231 and compare them to those of high redshift starbursts, many of which have SLEDs that may be affected by strong lensing. Finally, we use our local molecular line excitation template to assess the capabilities of future cm and mm/sub-mm arrays in detecting CO and HCN transitions in similar systems throughout the local and distant universe. ", "machine_abstract": "We report on high-resolution observations with Herschel PACS (Poglitsch et al., 2010) and SPIRE (Griffin et al., 2010 ) that reveal for the first time the presence of cold dust emission at temperatures as low as T = 20 K in two nearby ultraluminous infrared galaxies (ULIRGs), Arp220 and Mrk 231. The observed fluxes are consistent with predictions based on models of starbursts heated by young stars. We find evidence for an additional component of cooler dust which is likely to be associated with the obscured AGN activity present in these objects. In addition we have detected the [CII] 158 \u00b5m line in both sources using PACS spectroscopy. This allows us to estimate the total mass of warm molecular hydrogen M(H2). For Arp 220 this amounts to 1.5 x 10^9 solar masses within a radius of 100 pc around the nucleus.", "paraphrased_abstract": "This is a measurement of the mass of warm molecular hydrogen, M.H., in a radius of 100 pc. In Arp220, we measure to the degree of 1109 solar masses. The observed fluxes are in accordance with models of star-shapes in young stars. We report, for the first time, in Herschel PACS and SPIRE experiments, which showed for the first time the presence of cold dust at temperatures of 20 K and above in two nearby infrared galaxies, Arp220 and Mr. 231. We find an additional component of cold dust, probably due to the obscuring of AGNs in these objects. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4238, "title": "Accurate description of optical precursors and their relation to weak-field coherent optical transients", "abstract": "  We study theoretically the propagation of a step-modulated optical field as it passes through a dispersive dielectric made up of a dilute collection of oscillators characterized by a single narrow-band resonance. The propagated field is given in terms of an integral of a Fourier type, which cannot be evaluated even for simple models of the dispersive dielectric. The fact that the oscillators have a low number density (dilute medium) and have a narrow-band resonance allows us to simplify the integrand. In this case, the integral can be evaluated exactly, although it is not possible using this method to separate out the transient part of the propagated field known as optical precursors. We also use an asymptotic method (saddle-point method) to evaluate the integral. The contributions to the integral related to the saddle-points of the integrand give rise to the optical precursors. We obtain analytic expressions for the precursor fields and the domain over which the asymptotic method is valid. When combined to obtain the total transient field, we find that the agreement between the solutions obtained by the asymptotic and the exact methods is excellent. Our results demonstrate that precursors can persist for many nanoseconds and the chirp in the instantaneous frequency of the precursors can manifest itself in beats in the transmitted intensity. Our work strongly suggests that precursors have been observed in many previous experiments. ", "machine_abstract": "We present an accurate analytical model for the description of optical precursors in ultrashort laser pulses propagating through dispersive media, which is based on the concept of nonlinear phase modulation by self-phase-modulation (SPM) and cross-phase-modulation (XPM). The proposed approach allows us to describe accurately both the temporal shape as well as the spectral content of these phenomena. We show that this new method can be used to predict the appearance of weak-field coherent optical transience (WFCOT), i.e., the generation of sub-femtosecond bursts of light with high peak power at specific wavelengths within the spectrum of the pulse. This prediction is confirmed experimentally using a Ti:Sapphire femtosecond oscillator operating at 800 nm central wavelength. Finally we demonstrate how our results are relevant for applications such as ultrafast spectroscopy or attosecond science.     Optical precursors have been observed since the early days of ultrafast optics [1\u20133] . They appear when short intense laser pulses propagate through dispersive media like glass fibers [4] , air [5] , water [6] , crystals [7, 8] , etc.. These effects were first explained theoretically by assuming that the propagation of the pulse was governed by the slowly varying envelope approximation [9] . However it has recently become clear that this assumption does not hold true anymore if one wants to explain the details of the experimental observations [10\u201312] .   In order to overcome this limitation several authors have developed more sophisticated models [13\u201319] . In particular, the so-called generalized nonlinear Schr\u00f6dinger equation (GNLSE) [20, 21] has proven very useful because it takes into account all orders of dispersion [22] , self-steepening [23] , third-order dispersion [24] , Raman scattering [25] , stimulated Brillouin scattering [26] , self-frequency shift [27] , plasma defocusing [28] , gain saturation [29] , and other higher-order effects [30] .    However, despite its successes, there still remain some discrepancies between theory and experiment [31] . For example, the GNLSE predicts that the intensity profile of the precursor should always exhibit a smooth bell-shaped structure [32]", "paraphrased_abstract": "The latter is not, however, an exalted assumption. This assumption is now being rejected in view of the fact that the propagation of the pulse is not determined by the slow varying envelope. This has recently been confirmed by some authors by the use of the so-called generalized nonlinear Schr\u00f6dinger equation. It is an expression for a series of simultaneous events, namely, self-steeping, second order, Raman scattering, equilateral Brillouin scattering, equilateral Singularity, self-frequency shift, plasma defocusing, equilateral equilaterality, equilateral saturation, equilateral saturation, and other higher order effects. The GNLSE predicts that the intensity of the precursor is always a smooth bell shape. We prove this fact in the laboratory with a Ti:Sapphire femtosecond oscillator, which operates at 800 nm central wavelength. In our work, we show that our model can be used for the description of weak field coherent optical transience (WFCOT), that is, the generation of sub-femtosecond bursts of light with high peak power at specific wavelengths of the pulse. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0194, "title": "Quantum mechanical approach to decoherence and relaxation generated by fluctuating environment", "abstract": "  We consider an electrostatic qubit, interacting with a fluctuating charge of single electron transistor (SET) in the framework of exactly solvable model. The SET plays a role of the fluctuating environment affecting the qubit's parameters in a controllable way. We derive the rate equations describing dynamics of the entire system for both weak and strong qubit-SET coupling. Solving these equation we obtain decoherence and relaxation rates of the qubit, as well as the spectral density of the fluctuating qubit's parameters. We found that in the weak coupling regime the decoherence and relaxation rates are directly related to the spectral density taken at Rabi or at zero frequency, depending on what a particular qubit's parameters is fluctuating. This relation holds also in the presence of weak back-action of the qubit on the fluctuating environment. In the case of strong back-action, such simple relationship no longer holds, even if the qubit-SET coupling is small. It does not hold either in the strong-coupling regime, even in the absence of the back-action. In addition, we found that our model predicts localization of the qubit in the strong-coupling regime, resembling that of the spin-boson model. ", "machine_abstract": "We present an exact quantum-mechanical treatment for the dynamics of open systems in which the system is coupled to many harmonic oscillators representing its surrounding environment.  We show that, under certain conditions, this model can be reduced exactly into a master equation with Lindblad form. The resulting master equations are used to study the effects of environmental fluctuations on the evolution of the density matrix describing the state of the system. In particular we consider two different models of environments corresponding to Ohmic dissipation and spin-boson interaction respectively. For both cases it is shown how the effect of the environment leads to irreversible loss of information about the initial state of the system as well as to thermalization at late times. Finally, we discuss possible applications of our results to problems such as transport through mesoscopic conductors or dissipative tunneling between localized states in disordered solids. Decoherence and relaxation processes play a crucial role in understanding the physics of open quantum systems [1, 2] . These phenomena arise when the system interacts with some external degrees of freedom (environment) whose influence cannot be neglected [3] . In recent years there has been considerable interest in developing theoretical methods capable of treating these effects beyond the perturbative regime [4] . A number of approaches have been proposed ranging from phenomenological treatments based on stochastic Schr\u00f6dinger equations [5] , to more microscopic descriptions using path integral techniques [6] or field-theoretical formulations [7, 8] . However, despite their successes, all these methods suffer from one common drawback: they do not provide any insight into the underlying physical mechanisms responsible for decoherence and relaxation; nor do they allow us to make quantitative predictions regarding the time scales involved [9] . Recently, several authors [10 -12] have suggested that the problem may be tackled within the framework of quantum mechanics itself. This idea was first put forward by Feynman [13] who showed that the statistical properties of macroscopic objects could be obtained by averaging over an ensemble of identical but microscopically distinct realizations of the same experiment. More recently, Leggett [14] introduced a method...", "paraphrased_abstract": "However, all these methods, although successful, have one characteristic drawback: they do not give us an insight into the physical mechanisms underlying the decoherence and relaxation of the system, and do not give us quantitative predictions of the time scales involved. Recent efforts have taken place to extend the understanding of the physics of open quantum systems. These are caused by the interaction of the system with the surrounding environment, which cannot be neglected. These phenomena are called \"openness\", and if they are ignored, they are irreversible. This is a key problem in the physics of open quantum systems, and has been thoroughly studied in recent years. There are many methods, such as the stochastic Schr\u00f6dinger equations, and the more microscopic explanations, such as path integrals or field theory, are now also in progress. For instance, we study the effect of the environment on the density of the system and compare them with one another. We consider two different systems, the Ohmic dissipation and the spin-boson interaction, both of which are related to the effect of the environment, and the effects of this change on the density of the system are shown, for both of them causing irreversible loss of information about the initial state of the system and to thermalisation of the system at a later time. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2317, "title": "Quasi Equilibrium Grid Algorithm: geometric construction for model reduction", "abstract": "  The Method of Invariant Grid (MIG) is an iterative procedure for model reduction in chemical kinetics which is based on the notion of Slow Invariant Manifold (SIM) [1-4]. Important role, in that method, is played by the initial grid which, once refined, gives a description of the invariant manifold: the invariant grid. A convenient way to get a first approximation of the SIM is given by the Spectral Quasi Equilibrium Manifold (SQEM) [1-2]. In the present paper, a flexible numerical method to construct the discrete analog of a Quasi Equilibrium Manifold, in any dimension, is presented. That object is named Quasi Equilibrium Grid (QEG), while the procedure Quasi Equilibrium Grid Algorithm. Extensions of the QEM notion are also suggested. The QEG is a numerical tool which can be used to find a grid-based approximation for the locus of minima of a convex function under some linear constraints. The method is validated by construction of one and two-dimensional grids for model hydrogen oxidation reaction. ", "machine_abstract": "We present an algorithm to construct the reduced basis space in the context of nonlinear problems with multiple solutions, which is based on the concept of quasi-equilibrium grid (QEG). The QEG method was originally developed by Simo and Armero as a numerical technique for solving rate-independent processes such as plasticity or damage mechanics. We show that this approach can be used to generate snapshots for constructing the reduced basis spaces associated with nonlinear problems with multiple solutions. In particular, we consider two examples arising from structural dynamics and fluid flow computations. Numerical results demonstrate that our proposed method yields accurate approximations at significantly lower computational cost than existing approaches. Keywords: Reduced Basis Method; Quasi-Equilibrium Grids; Nonlinear Problems; Model Order Reduction; Geometric Construction; Snapshot Generation. 1 Introduction. The goal of this work is to develop efficient algorithms for generating snapshots for constructing the RB spaces associated with nonlinear problems having multiple solutions. This problem arises frequently when one solves engineering applications involving complex physical phenomena such as multiphysics coupling, material failure, contact/impact, etc.. For example, in structural dynamics, it may happen that different initial conditions lead to different equilibrium states [19, 20] . Similarly, in fluid flows, there are often many steady-state solutions corresponding to different boundary conditions [7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18] . In order to solve these types of problems efficiently using the reduced basis method (RBM), it is necessary to have a good set of snapshots representing all possible solution behaviors. However, since each snapshot corresponds to a specific solution behavior, it is not easy to obtain them directly through standard finite element analysis. Therefore, various techniques have been developed over the past decade to overcome this difficulty [1, 2, 3, 4, 5, 6, 7, 9, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40", "paraphrased_abstract": "A lot of problems occur in engineering, because of the complex physical relationships that result in a failure of the system, from the re-equilibrium of the material, from the impact of the forces, from the interaction of the spheres, from the equilibrium of the gases and from the contact with the atmosphere. The problem is a common one, as one is faced with complex physical phenomena such as the multiphysics coupling, the failure of the material, the impact of the impact, and the like. The aim of this research is to create a method for constructing the RB spaces for such a nonlinear problem with multiple solutions, based on the notion of the quasi-equilibrium grid. This method has been developed by Simo and Armero, in order to solve the problems of nonlinearity, by a numerical method based on the principle of the quasi-equilibrium grid. This method has been used to create the nonlinearity of the nonlinearity of the nonlinearity of the problem with multiple solutions. The numerical method is based on the concept of the quasi-equilibrium grid. It has been described in the literature as a method for solving problems of complicated physical phenomena, such as the collision of material with itself, the contact with the environment, the interaction of metals. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2368, "title": "Regular homogeneous T-models with vacuum dark fluid", "abstract": "  We present the class of regular homogeneous T-models with vacuum dark fluid, associated with a variable cosmological term. The vacuum fluid is defined by the symmetry of its stress-energy tensor, i.e., its invariance under Lorentz boosts in a distinguished spatial direction (p_j=-\\rho), which makes this fluid essentially anisotropic and allows its density to evolve. Typical features of homogeneous regular T-models are: the existence of a Killing horizon; beginning of the cosmological evolution from a null bang at the horizon; the existence of a regular static pre-bang region visible to cosmological observers; creation of matter from anisotropic vacuum, accompanied by very rapid isotropization. We study in detail the spherically symmetric regular T-models on the basis of a general exact solution for a mixture of the vacuum fluid and dustlike matter and apply it to give numerical estimates for a particular model which illustrates the ability of cosmological T-models to satisfy the observational constraints. ", "machine_abstract": "We study the cosmological evolution in regular homogeneous T-models filled by a perfect fluid and a vacuum energy density term, which is responsible for an accelerated expansion at late times.  We find that this model can be described as a mixture between dust matter (dark matter) and radiation-like components. The latter component behaves like a relativistic gas whose pressure decreases more slowly than its energy density does. This leads to a negative effective equation-of-state parameter w eff < 0 during most of the cosmic history. However, we show that there exists a time interval where w eff > -1, so that our model cannot explain the current acceleration of the universe without introducing additional ingredients such as quintessence or modified gravity theories. Nevertheless, it may provide useful insights into the nature of dark energy. In addition, we discuss how the presence of a non-vanishing vacuum energy density affects the growth rate of structure formation on sub-horizon scales.", "paraphrased_abstract": "In addition, we discuss the effect of non-vanishing vacuum energy density on the formation of the subhorizon. We study the cosmological evolution in homogeneous T-models, filled with a fluid and a vacuum energy density, which is the cause of the current expansion of the universe. However, we show that in the long run w eff  0 is not enough to explain the recent acceleration of the universe. Moreover, we present an investigation into the nature of dark energy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1548, "title": "Retaining Black Holes with Very Large Recoil Velocities", "abstract": "  Recent numerical simulations of binary black hole mergers show the possibility of producing very large recoil velocities (> 3000 km/s). Kicks of this magnitude should be sufficient to eject the final black hole from virtually any galactic potential. This result has been seen as a potential contradiction with observations of supermassive black holes residing in the centers of most galaxies in the local universe. Using an extremely simplified merger tree model, we show that, even in the limit of very large ejection probability, after a small number of merger generations there should still be an appreciable fraction (>50%) of galaxies with supermassive black holes today. We go on to argue that the inclusion of more realistic physics ingredients in the merger model should systematically increase this retention fraction, helping to resolve a potential conflict between theory and observation. Lastly, we develop a more realistic Monte Carlo model to confirm the qualitative arguments and estimate occupation fractions as a function of the central galactic velocity dispersion. ", "machine_abstract": "We study the possibility that black holes can be retained in binary systems by gravitational radiation reaction even if their initial velocities are very large, and we find that this is possible for certain ranges of parameters.  We consider two types of binaries:  those consisting of one black hole and another compact object (such as neutron star or white dwarf), and those consisting of two black holes.   In both cases, we show how to calculate the final velocity after emission of gravitational waves using post-Newtonian approximations up to third order.  For binaries containing at least one black hole, we also use numerical relativity simulations to check our results.  Finally, we discuss some astrophysical implications of these findings. The discovery of gravitational waves has opened an exciting new window on the universe [1] . One of its most surprising features was the detection of merging black holes [2] , which were found to have masses ranging between about 4M\u2609 and 36M\u2609 [3] . This raises the question whether there exist other ways besides mergers through which black holes may form [4] . In particular, it would be interesting to know what happens when a black hole moves into a binary system composed of either another black hole or a nonblack-hole companion [5] . If the black hole's initial speed is too high, then it will escape the system before emitting enough energy via gravitational waves [6] . However, if the black hole starts out slowly but still faster than the orbital speed of the binary components, then it could potentially be captured [7, 8] .  Here, we investigate this scenario further and determine under which conditions such capture is indeed possible.", "paraphrased_abstract": "The result is that black holes in binary systems can remain in the system, even if they have a large initial velocity. This is possible with certain conditions, and we investigate it further and find out under which conditions this can be achieved. The discovery of gravitational waves opened a new vista on the universe, and its most surprising feature was the observation of merging black holes, which had masses ranging between four and 36M. Thus, the question remains whether there are other ways in which black holes can form. We consider two different classes of binary systems: one black hole and another compact object, such as a neutron star or white dwarf; and we study both these classes, with numerical simulations up to three orders of magnitude. We study two kinds of binary systems: those which consist of one black hole and another, such as a neutron star or a white dwarf, and those which consist of two black holes, and, in both cases, we calculate the final velocity after the emission of gravitational waves using post-Newtonian calculations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3236, "title": "A \"diamond-ring\" star: the unusual morphologic structure of a young (multiple?) object", "abstract": "  We have observed IRAS06468-0325 obtaining optical and infrared images through IJHKs and L' filters, K-band low-resolution spectroscopy, together with millimetre line observations of CO and CS. IRAS06468-0325 has a very unusual and enigmatic morphology with two components: a bright, close to point-like source (the diamond) and a sharp-edge ring-like structure (the ring). The source is not detected in the optical, at wavelengths shorter than the I-band. The diamond is seen in all the imaging bands observed. The ring-like structure in IRAS06468-0325 is clearly seen in the I, J, H, and Ks. It is not detected in the L'-band image. Infrared colours of the diamond are compatible with excess circumstellar emission and a young stellar nature. A strongly non-gaussian and moderately bright CO(1-0) and {13}CO(2-1) lines are seen towards IRAS06468-0325, at v_{LSR} of 30.5 km s{-1} (corresponding to a kinematic distance of 3 kpc). Very weak C{18}O(2-1) and CS(2-1) lines were detected. K-band spectra of the diamond and of the ring are similar both in the slope of the continuum and in the presence of lines supporting the idea that the ring is reflected light from the diamond. With the current data, a few different scenarios are possible to explain the morphology of this object. However, the available data seem to favour that the morphology of IRAS06468-0325 correspond to a young stellar multiple system in a transient stage where a binary co-exists with a circumbinary disc, similar to the case of GG Tau. In this case, the sharpness of the well-defined ring may be due to tidal truncation from dynamic interactions between components in a binary or multiple stellar system. IRAS06468-0325 may be an important rare case that illustrates a short-lived stage of the process of binary or multiple star formation. ", "machine_abstract": "We report on near-infrared observations of an extremely red, faint source in the field of view of the Hubble Space Telescope's Wide Field Planetary Camera 2 (WFPC2). The source is located at RA = 03h45m55s and Dec = -05d20'00\" (J2000), about 1 arcmin south-east of the open cluster NGC 6231. It was detected by WFPC2 as a point-like object with mF606W ~ 26 mag., mF814W ~ 25 mag. (Vega system) and mH ~ 20 mag.. We have obtained additional photometry for this object using the ESO NTT telescope equipped with SOFI camera. Our results show that it has very red colors (R-I > 5 mag.) and its spectrum exhibits strong absorption features characteristic of late-type stars. Its absolute magnitude M(K) = -4.3 \u00b1 0.1 suggests that we are dealing with either a brown dwarf or a low-mass star.", "paraphrased_abstract": "The rays are resolving at a wavelength of 30 m. The source is a star of the line RA = 03h45m55s and Dec = 05d20'00', about 1 arcmin south of the open cluster NGC 6231. We have obtained additional rays from the ESO NTT telescope, equipped with the SOFI rays. We report a very red, very faint light source in the rays of the WFT-planet 2 (WFT-Planet 2). It is a point-like object, with a magnitude of 606-fifty (Vega) and a height of twenty-five (Vega) m2, and a spectral value of 20 m. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3808, "title": "On Gene Duplication Models for Evolving Regulatory Networks", "abstract": "  Background: Duplication of genes is important for evolution of molecular networks. Many authors have therefore considered gene duplication as a driving force in shaping the topology of molecular networks. In particular it has been noted that growth via duplication would act as an implicit way of preferential attachment, and thereby provide the observed broad degree distributions of molecular networks.   Results: We extend current models of gene duplication and rewiring by including directions and the fact that molecular networks are not a result of unidirectional growth. We introduce upstream sites and downstream shapes to quantify potential links during duplication and rewiring. We find that this in itself generates the observed scaling of transcription factors for genome sites in procaryotes. The dynamical model can generate a scale-free degree distribution, p(k)&prop; 1/k^&gamma;, with exponent &gamma;=1 in the non-growing case, and with &gamma;>1 when the network is growing.   Conclusions: We find that duplication of genes followed by substantial recombination of upstream regions could generate main features of genetic regulatory networks. Our steady state degree distribution is however to broad to be consistent with data, thereby suggesting that selective pruning acts as a main additional constraint on duplicated genes. Our analysis shows that gene duplication can only be a main cause for the observed broad degree distributions, if there is also substantial recombinations between upstream regions of genes. ", "machine_abstract": "We present an analysis of gene duplication models in the context of evolving regulatory networks, and show that they are not sufficient to explain all observed features of these networks. We consider two types of duplications: whole-genome duplicates (WGDs) and local duplicates (LDs). WGDs occur when a complete genome is duplicated; LDs occur when only one or few genes are duplicated at once.     In both cases we study how new duplicate copies evolve after their creation by comparing them with their parent copy using sequence similarity measures. For each type of duplication event, we find that there exists a large number of duplicates which do not resemble any other existing gene more than expected under random chance. This suggests that many duplicates may be created through mechanisms different from those considered here. Furthermore, we observe that duplicates tend to have higher expression levels compared to non-duplicates. Finally, we compare our results on duplicates with previous studies on network evolution, and discuss possible extensions of this work.", "paraphrased_abstract": "In each case, we observe that in most cases there are many duplicates, which are less like those which have been produced by chance and which do not resemble the other ones at all. We also observe that duplicates are often more repressed than the other ones. We analyze the effects of gene duplication on the development of biological networks and show that they do not explain all the observable features of biological networks. We present an analysis of the emergence of gene duplications in the context of re-entranting regulatory networks and show that they are not sufficient to explain all the features of these networks. We compare the results of the study of gene duplication with the results of previous studies of network evolution, and propose new extensions. The main result is that there are many copies of the same gene, which are more similar to the one in which they have been copied. The main result is that there are many duplicates, which are not like the one already present. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1732, "title": "Spheroidal and torsional modes of quasistatic shear oscillations in the solid globe models of nuclear physics and pulsar astrophysics", "abstract": "  The past three decades of investigation on nuclear physics and pulsar astrophysics have seen gradual recognition that elastodynamic approach to the continuum mechanics of nuclear matter provides proper account of macroscopic motions of degenerate Fermi-matter constituting interior of the nuclear material objects, the densest of all known today. This paper focuses on one theoretical issue of this development which is concerned with oscillatory behavior of a viscoelastic solid globe in the regime of quasistatic, force-free, non-compressional oscillations less investigated in the literature compared to oscillations in the regime of standing shear waves. We show that in this case the problem of computing frequency and lifetime of spheroidal and torsional modes of non-radial shear vibrations damped by viscosity can be unambiguously resolved by working from the energy balance equation and taking advantage of the Rayleigh's variational method. The efficiency of this method is demonstrated by solid globe models of nuclear physics and pulsar astrophysics dealing with oscillations of a spherical mass of a viscoelastic Fermi-solid with homogeneous and non-homogeneous profiles of the bulk density, the shear modulus, and the shear viscosity. ", "machine_abstract": "We study spheroidal and torsional normal modes of quasi-static shear oscillations in the spherical shell model with incompressible liquid core, which is used to describe the dynamics of superfluid neutron stars. We show that there are two types of spheroidal modes: one type corresponds to the usual fluid-like mode, while another type describes the motion of the whole star as a rigid body. The latter type can be excited only by external perturbations. In addition we find three new families of torsional modes corresponding to different motions of the inner core. These results may have important implications for understanding the observed phenomena related to the glitch activity of pulsars.     Keywords: Quasi-static shear oscillations; Spherical shell model; Superfluidity of neutrons; Pulsar glitches; Torsional waves; Rigid-body rotation; Neutron stars; Nuclear matter; Energy density functional theory", "paraphrased_abstract": "They are a great deal more than can be said about the dynamics of pulsars. Spectra: Helix, trunk, pulsar, pulsar, pulsar swell, pulsar swell, pulsar swell, pulsar swell, pulsar swell, pulsar swell, pulsar swell, pulsar swell, pulsar swell, risible body, neutron stars, the nucleus of the nuclear, the kinetics of the nucleus of the inner core. These results can have important implications for the study of the behavior of pulsars. The spheroidal and torsional normal modes of quasi-static shear oscillations, in the model of the spherical shell, containing a liquid core, are described by us, to explain the dynamics of superfluid neutron stars. We have studied two kinds of spheroidal modes, one of which is fluid, and the other, which is a rigid body, which can only be activated by external disturbances. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.138, "title": "The Advantages of Four Dimensions for Composite Higgs Models", "abstract": "  We examine the relationship between little Higgs and 5d composite models with identical symmetry structures. By performing an \"extreme\" deconstruction, one can reduce any warped composite model to a little Higgs theory on a handful of sites. This allows us to use 4d intuition and the powerful constraints of nonlinear sigma models to elucidate obscure points in the original setup. We find that the finiteness of the Higgs potential in 5d is due to the same collective symmetry breaking as in the little Higgs. We compare a 4d and a 5d model with the same symmetry to the data. Reviewing the constraints on models related to the Minimal Composite Higgs (hep-ph/0412089), we see that it has difficulty in producing acceptable values for S, T, and m_{top} simultaneously. By contrast, in a global analysis, the Minimal Moose with custodial symmetry is viable in a large region of its parameter space and suffers from no numeric tunings. We conjecture that this result is generic for 4d and 5d models with identical symmetries. The data will less strongly constrain the little theory. ", "machine_abstract": "We study the advantages and disadvantages of composite Higgs models in four dimensions (4D) versus five dimensions (5D). In 4D, we find that there are two types of composite Higgs models with different phenomenological consequences. The first type is based on an underlying global symmetry group SU(2)L \u00d7SU(2)R \u00d7U(1)B\u2212L which leads to three Goldstone bosons after spontaneous breaking of this symmetry down to U(1)EM . This model has been studied extensively by many authors including ourselves [1\u20133] . The second type is based on an extended gauge symmetry group SU(3)C \u00d7SU(2)L \u00d7U(1)Y \u00d7Z\u2032 where Z\u2032 is a new abelian gauge factor associated with extra spatial dimension [4\u20136] . We show that both these models can be embedded into 5D theories compactified on orbifolds [7\u20139] , but they have very different properties when considered as effective 4D theories.", "paraphrased_abstract": "It is shown that both these models can be embedded into 5D theories compacted on orbifolds, but they have very different properties when compared with 5D theories. The first is based on an underlying symmetry SU(2)L SU(2)R U(1)B, which is obtained by breaking this symmetry down to U(1)EM, we have studied extensively to see the phenomenological effect of this symmetry, and the second is based on an extended symmetry SU(2)C SU(2)L U(1)Y Z, where Z is the new abelian gauge factor associated with the extra spatial dimension. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0705, "title": "On the dimension of subspaces with bounded Schmidt rank", "abstract": "  We consider the question of how large a subspace of a given bipartite quantum system can be when the subspace contains only highly entangled states. This is motivated in part by results of Hayden et al., which show that in large d x d--dimensional systems there exist random subspaces of dimension almost d^2, all of whose states have entropy of entanglement at least log d - O(1). It is also related to results due to Parthasarathy on the dimension of completely entangled subspaces, which have connections with the construction of unextendible product bases. Here we take as entanglement measure the Schmidt rank, and determine, for every pair of local dimensions dA and dB, and every r, the largest dimension of a subspace consisting only of entangled states of Schmidt rank r or larger. This exact answer is a significant improvement on the best bounds that can be obtained using random subspace techniques. We also determine the converse: the largest dimension of a subspace with an upper bound on the Schmidt rank. Finally, we discuss the question of subspaces containing only states with Schmidt equal to r. ", "machine_abstract": "We study the problem of finding an explicit formula for the dimension of the space spanned by all vectors in a given finite set $S$ whose Schatten $p$-norms are bounded above by some constant $C$, where $1<p<\\infty$ is fixed and $p>2$.  We show that this dimension can be expressed as a polynomial in $C^{1/p}$ (and hence also in the cardinality of $S$) if we assume that $S$ has no two elements which are orthogonal under any inner product on $\\mathbb{C}^n$.   This result generalizes earlier results obtained independently by M. Lustig and A. Shub and by J. Bourgain and G. Bouschler. It should be noted that our proof does not use the fact that the underlying field is complex; it works over arbitrary fields of characteristic zero. The main ingredient of our argument is a new combinatorial identity involving binomial coefficients.", "paraphrased_abstract": "\u201cTo explain this we shall have to use a new combinatorial identity based on the binomial coefficient. We are going to present the main result of our work, a new combinatorial identity, based on the Binomial Cosine. This work is generalized by M. Lustig and A. Shub, and by J. Bourgain and G. Bouschler. Our main goal is to find a new combinatorial formula, based on a new combinatorial algebra, which will be more or less the same. We find that the dimension of the space spanned by all vectors of a set of finite numbers, whose Schatten-Norms are bounded above by some constant C, where C is fixed and p is fixed, and the p-infty is fixed. We show that this dimension can be expressed in the form of a polynomial in the form of C1/pi; and thus, in the form of the cardinality of the set, if we assume that $S has no two elements orthogonal in the form of a product of mathbb-Cn. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0164, "title": "On smooth foliations with Morse singularities", "abstract": "  Let $M$ be a smooth manifold and let $\\F$ be a codimension one, $C^\\infty$ foliation on $M$, with isolated singularities of Morse type. The study and classification of pairs $(M,\\F)$ is a challenging (and difficult) problem. In this setting, a classical result due to Reeb \\cite{Reeb} states that a manifold admitting a foliation with exactly two center-type singularities is a sphere. In particular this is true if the foliation is given by a function. Along these lines a result due to Eells and Kuiper \\cite{Ku-Ee} classify manifolds having a real-valued function admitting exactly three non-degenerate singular points. In the present paper, we prove a generalization of the above mentioned results. To do this, we first describe the possible arrangements of pairs of singularities and the corresponding codimension one invariant sets, and then we give an elimination procedure for suitable center-saddle and some saddle-saddle configurations (of consecutive indices). In the second part, we investigate if other classical results, such as Haefliger and Novikov (Compact Leaf) theorems, proved for regular foliations, still hold true in presence of singularities. At this purpose, in the singular set, $Sing(\\F)$ of the foliation $\\F$, we consider {\\em{weakly stable}} components, that we define as those components admitting a neighborhood where all leaves are compact. If $Sing(\\F)$ admits only weakly stable components, given by smoothly embedded curves diffeomorphic to $S^1$, we are able to extend Haefliger's theorem. Finally, the existence of a closed curve, transverse to the foliation, leads us to state a Novikov-type result. ", "machine_abstract": "In this article we study the structure of smooth foliations on closed manifolds, which are given by submersions whose fibers have only Morse singularities (i.e., they look like graphs over their tangent spaces). We prove that such foliations can be approximated in the C 1 -topology by regular ones and give an explicit bound for the approximation error. This result is used to show that any leafwise Riemannian metric on these foliations has bounded geometry. The main tool here is the theory of harmonic maps into Hilbert spaces developed by Eells-Elworthy-Malliavin. In particular, we use the fact that every harmonic map into a Hilbert space is weakly conformal. As another application of our results, we obtain some estimates for the volume growth rate along leaves of such foliations. Finally, using the above mentioned bounds for the approximation error, we also get lower bounds for the number of critical points of generic functions on closed manifolds.", "paraphrased_abstract": "This is the foundation of our investigation, and we use the theory of harmonic maps into Hilbert spaces developed by Eells-Elworthy-Malliavin, who have shown that the harmonic maps into Hilbert spaces are weakly conformal. Moreover, the following general rule of law is that the harmonic maps into Hilbert spaces are weakly conformal. In this case, we present the metric for the growth rate of the leaf of such foliations, and we calculate the number of critical points of generic functions on closed manifolds. In particular, we use the principle of weak conformity in every harmonic map of Hilbert space. The structure of smooth foliations on closed manifolds, as we call them, is of a type that resembles graphs in their tangent spaces. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1049, "title": "Charmless Three-body Decays of B Mesons", "abstract": "  Charmless 3-body decays of B mesons are studied in the framework of the factorization approach. The nonresonant contributions arising from $B\\to P_1P_2$ transitions are evaluated using heavy meson chiral perturbation theory (HMChPT). The momentum dependence of nonresonant amplitudes is assumed to be in the exponential form $e^{-\\alpha_{NR}} p_B\\cdot(p_i+p_j)}$ so that the HMChPT results are recovered in the soft meson limit $p_i, p_j\\to 0$. In addition, we have identified another large source of the nonresonant signal in the matrix elements of scalar densities, e.g. $<K\\bar K|\\bar ss|0>$, which can be constrained from the decay $\\bar B^0\\to K_SK_SK_S$ or $B^-\\to K^-K_SK_S$. The intermediate vector meson contributions to 3-body decays are identified through the vector current, while the scalar meson resonances are mainly associated with the scalar density. Their effects are described in terms of the Breit-Wigner formalism. Our main results are: (i) All KKK modes are dominated by the nonresonant background. The predicted branching ratios of $K^+K^-K_{S(L)}$, $K^+K^-K^-$ and $K^-K_SK_S$ modes are consistent with the data within errors. (ii) Although the penguin-dominated $B^0\\to K^+K^-K_{S}$ decay is subject to a potentially significant tree pollution, its effective $\\sin 2\\beta$ is very similar to that of the $K_SK_SK_S$ mode. However, direct CP asymmetry of the former, being of order -4%, is more prominent than the latter. (iii) For $B\\to K\\pi\\pi$ decays, we found sizable nonresonant contributions in $K^-\\pi^+\\pi^-$ and $\\bar K^0\\pi^+\\pi^-$ modes, in agreement with the Belle measurements but larger than the BaBar result. ", "machine_abstract": "We present the results on charmless three-body decays of B mesons using data collected by the Belle experiment at KEKB in 2003-2007, corresponding to an integrated luminosity of 711 fb-1. We measure branching fractions for the decay modes B+ -> J/psi K*(892)0 and B- -> J/psi K*0(1430), where J/psi is reconstructed in its decay into muon pairs.  The measured values are  B+ -> J/psi: (4.6 +/- 0.7 +/- 1.1) x 10^-6; B->J/psi: (3.2 +/- 0.5 +/- 0.9) x 10^-6 , where the first errors are statistical and systematic respectively, and the second error is due to uncertainty in the number of BB events. These measurements are consistent with previous measurements but have improved precision. In addition we report new upper limits on the branching fraction for the decay mode B+ -> J/psi/phi .", "paraphrased_abstract": "And we have new, lower limits for the decay of the B-type, which is B-type. These are the same as the previous ones, but have a more accurate rigour. The branching fractions of the three-body decay of B-type B are measured, in particular for the decays of B-type and B-type B, and for the decay of J-type B, which is reconstructed by the decay of J-type B, are derived from the data of the Belle experiment at KKB, and for which an integrated luminosity of 711 fb-1 is computed. These measurements are consistent with the earlier ones, but have had better precision. We report the results of the three-body decay of B-type B (K-type B) and the decays of B-type B (K-type B) from a KEKB experiment in 2003 and 2007, in which the total luminosity of B-type B is 711 fb-1. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3271, "title": "Constraining Models of Neutrino Mass and Neutrino Interactions with the Planck Satellite", "abstract": "  In several classes of particle physics models -- ranging from the classical Majoron models, to the more recent scenarios of late neutrino masses or Mass-Varying Neutrinos -- one or more of the neutrinos are postulated to couple to a new light scalar field. As a result of this coupling, neutrinos in the early universe instead of streaming freely could form a self-coupled fluid, with potentially observable signatures in the Cosmic Microwave Background and the large scale structure of the universe. We re-examine the constraints on this scenario from the presently available cosmological data and investigate the sensitivity expected from the Planck satellite. In the first case, we find that the sensitivity strongly depends on which piece of data is used. The SDSS Main sample data, combined with WMAP and other data, disfavors the scenario of three coupled neutrinos at about the 3.5$\\sigma$ confidence level, but also favors a high number of freely streaming neutrinos, with the best fit at 5.2. If the matter power spectrum is instead taken from the SDSS Large Red Galaxy sample, best fit point has 2.5 freely streaming neutrinos, but the scenario with three coupled neutrinos becomes allowed at $2\\sigma$. In contrast, Planck alone will exclude even a single self-coupled neutrino at the $4.2\\sigma$ confidence level, and will determine the total radiation at CMB epoch to $\\Delta N_\\nu^{eff} = ^{+0.5}_{-0.3}$ ($1\\sigma$ errors). We investigate the robustness of this result with respect to the details of Planck's detector. This sensitivity to neutrino free-streaming implies that Planck will be capable of probing a large region of the Mass-Varying Neutrino parameter space. Planck may also be sensitive to a scale of neutrino mass generation as high as 1 TeV. ", "machine_abstract": "We present constraints on models of neutrino mass, mixing angles, and interactions using data from the Planck satellite experiment. We use measurements of the cosmic microwave background temperature anisotropy power spectrum to constrain cosmological parameters including the sum of the masses of all three active neutrinos ( m ), the effective number of relativistic species at recombination (), and the primordial helium abundance (). The results are consistent with previous analyses but have improved precision due to the inclusion of polarization information in addition to temperature fluctuations.     In particular we find that the total neutrino mass is constrained by Planck to be less than 0.23 eV at 95% confidence level for normal hierarchy or less than 0.28 eV for inverted hierarchy. These limits improve upon those obtained previously from WMAP7+SDSS3+HST by factors of 2.5-3.0. For comparison, these values correspond roughly to one third of an electron volt per neutrino flavor.", "paraphrased_abstract": "Using the data from the Planck experiment, we calculate the total mass of the neutrino from 0.23 eV at 90% confidence level, or 0.28 eV at the 95% confidence level. These limits are more precise than those of WMAP7 and the SDSS-3, by about a factor of two. The results are consistent with previous measurements, but they are improved by the addition of polarization and temperature variations. The total mass of the neutrino is calculated from Planck with a precision of about 0.025 eV, or 0.28 eV in the inverted hierarchy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3634, "title": "Impact of Cosmic Rays on Population III Star Formation", "abstract": "  We explore the implications of a possible cosmic ray (CR) background generated during the first supernova explosions that end the brief lives of massive Population III stars. We show that such a CR background could have significantly influenced the cooling and collapse of primordial gas clouds in minihaloes around redshifts of z ~ 15 - 20, provided the CR flux was sufficient to yield an ionization rate greater than about 10^-19 s^-1 near the center of the minihalo. The presence of CRs with energies less than approximately 10^7 eV would indirectly enhance the molecular cooling in these regions, and we estimate that the resulting lower temperatures in these minihaloes would yield a characteristic stellar mass as low as ~ 10 M_sun. CRs have a less pronounced effect on the cooling and collapse of primordial gas clouds inside more massive dark matter haloes with virial masses greater than approximately 10^8 M_sun at the later stages of cosmological structure formation around z ~ 10 - 15. In these clouds, even without CR flux the molecular abundance is already sufficient to allow cooling to the floor set by the temperature of the cosmic microwave background. ", "machine_abstract": "We study the impact of cosmic rays (CRs) on population iii star formation in minihalos, using cosmological hydrodynamic simulations with CR physics implemented self-consistently. We find that CR pressure suppresses fragmentation and increases the Jeans mass by factors of 2-5 at redshifts z = 10-20. This effect is more pronounced for higher values of the CR diffusion coefficient Dcr. The suppression of fragmentation leads to an increase in the number density of massive stars formed per halo as well as their total luminosity. For our fiducial value of Dcr = 3 \u00d7 1026 cm2 s\u22121 we obtain a factor of 4-10 enhancement over the case without CR feedback. Our results are consistent with recent observations of high-z galaxies. In addition, we show that CR feedback can explain the observed correlation between galaxy stellar masses and black hole masses. Finally, we discuss possible observational signatures of this mechanism.", "paraphrased_abstract": "And finally, we show that the feedback of CRs may explain the correlation between the masses of the galaxy and the masses of black holes. In this regard, we present the results which are based on recent observations in high-z galaxies. We show that CRs diminish the fragmentation and increase the Jeans mass by two to five, at redshifts z=10\u201320. This effect is more intense at the redshift z=20. Moreover, we show that CRs can explain the observed correlation between the masses of the galaxy and the masses of black holes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0847, "title": "Optical properties of Er3+ doped alkali-chloro phosphate glasses for optical amplifiers", "abstract": "  A new class of Erbium doped glasses with compositions xNa2O - (60-x)PbCl2 - 40P2O5 (x=0, 10, 20 and 30) were fabricated and characterized for optical properties. Absorption spectra were analyzed for important Judd-Ofelt parameters from the integrated intensities of various Er3+ glass absorption bands. Photoluminescence (PL) and its decay behavior studies were carried out for the transition 4I13/2 -> 4I15/2. A systematic correlation between the Judd-Ofelt parameter {omega}2 and the covalent nature of the glass matrix was observed, due to increased role of bridging oxygens in the glass network. The PL broadness and life times of 4I13/2_> 4I15/2 transition were typically in the range of 40-60nm and 2.13-2.5ms respectively. These glasses broadly showed high transparency, high refractive index, shorter life times and, most importantly, these glasses were found to be capable of being doped with larger concentrations of Er3+ (up to 4 wt%). Increase of Er3+ concentration resulted in the increase in PL line-widths with no significant effect of concentration quenching, indicating that these glasses are suitable for optical fibre/waveguide amplifiers. ", "machine_abstract": "Erbium-doped fiber amplifiers (EDFAs) are widely used in the field of telecommunication and sensing applications due to their high gain, low noise figure, wide bandwidths and compatibility with silica fibers. However, EDFAs have some disadvantages such as high cost and limited lifetime because they require expensive rare-earth elements like erbium ions. In this study, we report on the preparation of new glass-ceramic materials based on sodium chloride and potassium chloride phosphates containing different concentrations of Er3+ ions by melt-quenching method. The structural characterization was performed using X-ray powder diffraction analysis (XRPD), scanning electron microscopy (SEM), energy dispersive spectroscopy (EDS). Optical absorption spectra were recorded at room temperature between 300 nm and 1600 nm. Photoluminescence emission measurements were carried out under 980-nm excitation wavelength. It is found that the prepared samples show good transparency over a broad range of wavelengths up to 1600 nm.", "paraphrased_abstract": "But these types of crystals are not very stable and can be destroyed with heat. The synthesis of the crystals is conducted with X-rays of powder diffraction and electron microscopy. In particular, the crystals were characterized by the spectroscopic spectroscopy of the glass, and by the electron spectroscopic observations of the ultraviolet, the ultraviolet, the ultraviolet, the ultraviolet, and the emitted light. These crystals are commonly used in telecommunications and sensing, because they offer high power, low noise, and wide bandwidth, and are compatible with silica fibers. However, these crystals have some drawbacks, such as the cost, the time, and the limited life of the crystals, as they require expensive erbium ions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2009, "title": "Satellite Galaxies and Fossil Groups in the Millennium Simulation", "abstract": "  We use a semianalytic galaxy catalogue constructed from the Millennium Simulation to study the satellites of isolated galaxies in the LCDM cosmogony. This sample (~80,000$ bright primaries, surrounded by ~178,000 satellites) allows the characterization, with minimal statistical uncertainty, of the dynamical properties of satellite/primary galaxy systems in a LCDM universe. We find that, overall, the satellite population traces the dark matter rather well: its spatial distribution and kinematics may be approximated by an NFW profile with a mildly anisotropic velocity distribution. Their spatial distribution is also mildly anisotropic, with a well-defined ``anti-Holmberg'' effect that reflects the misalignment between the major axis and angular momentum of the host halo. The isolation criteria for our primaries picks not only galaxies in sparse environments, but also a number of primaries at the centre of ''fossil'' groups. We find that the abundance and luminosity function of these unusual systems are in reasonable agreement with the few available observational constraints. We recover the expected L_{host} \\sigma_{sat}^3 relation for LCDM models for truly-isolated primaries. Less strict primary selection, however, leads to substantial modification of the scaling relation. Our analysis also highlights a number of difficulties afflicting studies that rely on blind stacking of satellite systems to constrain the mean halo mass of the primary galaxies. ", "machine_abstract": "We present an analysis of satellite galaxies, fossil groups (FGs), and galaxy clusters in the Millennium Run simulation. We find that FGs are rare objects with only 0.5% of all halos being classified as such at z=0. The number density of FGs is consistent with observations for systems with Mvir>1013M\u2299/h. However, we also find that there exists a population of low-mass FGs which have not been observed yet but may be detectable by future surveys. In addition to their rarity, FGs show several other interesting properties compared to normal galaxy clusters: they tend to reside in more massive dark matter haloes than normal galaxy clusters; they contain fewer bright central galaxies; and they exhibit higher velocity dispersions. These results suggest that FGs can provide important constraints on models of galaxy formation and evolution. This work was supported by NASA grant NAG5-10842.", "paraphrased_abstract": "In addition to their rarity, FGs exhibit several other remarkable properties. They are usually found in a greater number of dark matter haloes than normal haloes, they contain fewer central galaxies, and they have higher velocity dispersion. This work was supported by NASA Grant NAG 510842. We study satellite galaxies, fossil groups, and galaxy clusters in the Millennium Run simulation. We find that FGs are rare, and that only 0.5% of the stars in the simulation are classified as FGs. We also find that there are some low-mass FGs that have not yet been observed, but which may be detected by future surveys. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0898, "title": "Lensing, reddening and extinction effects of MgII absorbers from z=0.4 to z=2", "abstract": "  Using a sample of almost 7000 strong MgII absorbers with 0.4 < z < 2.2 detected in the SDSS DR4 dataset, we investigate the gravitational lensing and dust extinction effects they induce on background quasars. After carefully quantifying several selection biases, we isolate the reddening effects as a function of redshift and absorber rest equivalent width, W_0. We find the amount of dust to increase with cosmic time as (1+z)^(-1.1 +/- 0.4), following the evolution of cosmic star density or integrated star formation rate. We measure the reddening effects over a factor 30 in E(B-V) and we find the dust column density to be proportional to W_0^(1.9 +/- 0.2), which provides an important scaling for theoretical modeling of metal absorbers. We also measure the dust-to-metals ratio and find it similar to that of the Milky Way.   In contrast to previous studies, we do not detect any gravitational magnification by MgII systems. We measure the upper limit \\mu<1.10 and discuss the origin of the discrepancy. Finally, we estimate the fraction of absorbers missed due to extinction effects and show that it rises from 1 to 50% in the range 1<W_0<6 Angstrom. We parametrize this effect and provide a correction for recovering the intrinsic distribution of absorber rest equivalent widths. ", "machine_abstract": "We have used the Hubble Space Telescope (HST) to study the lensing effect on background quasars by foreground galaxies associated with Mg II absorption systems at 0.4 <z abs < 2.0 in order to determine their masses and sizes. We find that most of these galaxy halos are well described by singular isothermal spheres with velocity dispersions ranging between 100 - 300 km/sec. The mean mass density within one effective radius for our sample is $\\rho_e = 1.5 \\times 10^{20} \\ M_{\\odot}/\\text{Mpc}^{3}$ which corresponds to an average halo circular velocity of $220\\ \\mathrm{km/sec}$. This value agrees very well with previous studies based on gravitational lensing measurements as well as dynamical estimates using virial theorem arguments. Using this result we can estimate the total number of dark matter particles enclosed within one effective radius of each absorber system. For all but two cases, the estimated particle numbers agree reasonably well with those expected if the absorbing gas traces the underlying dark matter distribution.", "paraphrased_abstract": "The density of these ephemeral volumes within the effective radius of each of these ephemeral masses is, to the point of enumerating the dark matter, the proportion of which is not incomparably below that of the absorbed gases. As we have shown, the mean density of the particles in an effective radius of the ephemeral bodies is about 1.7 times the mass of the halo, and therefore the diameter of the circumference is about 266 m mathmkm/s. The results are in good agreement with previous studies of the gravitational lensing of quasars by foreground galaxies containing the absorption of Mg II, at a density of 0.4 z abs  2.0, and to be calculated from these observations. This result is in good agreement with the calculations based on gravitational lensing and on the dynamical estimates based on the virial theorem. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0393, "title": "Non-saturating magnetoresistance of inhomogeneous conductors: comparison of experiment and simulation", "abstract": "  The silver chalcogenides provide a striking example of the benefits of imperfection. Nanothreads of excess silver cause distortions in the current flow that yield a linear and non-saturating transverse magnetoresistance (MR). Associated with the large and positive MR is a negative longitudinal MR. The longitudinal MR only occurs in the three-dimensional limit and thereby permits the determination of a characteristic length scale set by the spatial inhomogeneity. We find that this fundamental inhomogeneity length can be as large as ten microns. Systematic measurements of the diagonal and off-diagonal components of the resistivity tensor in various sample geometries show clear evidence of the distorted current paths posited in theoretical simulations. We use a random resistor network model to fit the linear MR, and expand it from two to three dimensions to depict current distortions in the third (thickness) dimension. When compared directly to experiments on Ag$_{2\\pm\\delta}$Se and Ag$_{2\\pm\\delta}$Te, in magnetic fields up to 55 T, the model identifies conductivity fluctuations due to macroscopic inhomogeneities as the underlying physical mechanism. It also accounts reasonably quantitatively for the various components of the resistivity tensor observed in the experiments. ", "machine_abstract": "We report on the non-saturating magnetoresistance (NSMR) effect observed for thin films with an inhomogeneous distribution of charge carriers, which is caused by their trapping at defects or impurities. We show that this NSMR can be explained within the framework of the semiclassical Boltzmann transport theory if one takes into account the spatial dependence of the scattering time. The latter was calculated using Monte Carlo simulations based on the solution of the kinetic equation for electrons interacting with phonons and static disorder potential fluctuations. Our results are compared to experimental data obtained for InAs/GaSb superlattices grown by molecular beam epitaxy. Non-saturating magnetoresistivity has been recently discovered in various materials including semiconductors [1] , metals [2] , carbon nanotubes [3] . This phenomenon manifests itself as a linear increase of resistivity \u03c1(H) \u221d H when magnetic field H increases up to several Tesla. It is believed that it originates from the presence of spatially localized states near the Fermi level [4] . In this work we study the non-saturating magnetotransport properties of semiconductor heterostructures consisting of alternating layers of GaSb/InAs. These structures were grown by molecular beam epi-taxy [5] . They have a high density of structural imperfections such as dislocations [6] and stacking faults [7] . As a result, they exhibit strong electron localization [8] leading to pronounced quantum confinement effects [9] . Figure 1 shows typical temperature dependences of resistance R xx measured along the growth direction z for different values of applied magnetic fields. One can see that all curves demonstrate metallic behavior down to low temperatures T < 2 K. At higher temperatures there appears a weak insulating-like behavior due to thermally activated hopping between localized states [10] . For small magnetic fields [ Fig.  1(a) ] the resistance decreases monotonically upon cooling indicating the dominance of elastic scattering processes [11] . However, for larger fields [Figs. 1(c),1(d)] the resistance exhibits non-monotonic behavior with a minimum around 10-20 K followed by", "paraphrased_abstract": "As the crystalline structure is formed of alternating layers of InAs and GaSb, a high density of the structural defects is visible, such as dislocations, cracks, and stacked faults, which are not at all metallic in the slightest extent. They exhibit strong localization of the electrons, leading to the effect of the quantum confinement. In this study we have examined the non-saturating magnetoresistance of thin materials made of semiconductors, of metals, of carbon nanotubes, of alternating layers of GaSb and of GaSb, and in which the two layers of GaSb and of GaSb were grown by molecular beam epitaxy. In this way we have investigated the non-saturating magnetoresistance of thin, inhomogeneous materials, and of which the structures are composed of alternating layers of GaSb and InAs, which have high density of dislocations and brittle interlocking layers, and that, in this way, there is a strong localization of the electrons at the Fermi level, and we have a definite quantum entropy. The results of our study are compared with those of InAs and GaSb superlattices, grown by molecular beam epitaxy. The non-saturating magnetotransport property is described by the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1084, "title": "Vacuum Energy and Renormalization on the Edge", "abstract": "  The vacuum dependence on boundary conditions in quantum field theories is analysed from a very general viewpoint. From this perspective the renormalization prescriptions not only imply the renormalization of the couplings of the theory in the bulk but also the appearance of a flow in the space of boundary conditions. For regular boundaries this flow has a large variety of fixed points and no cyclic orbit. The family of fixed points includes Neumann and Dirichlet boundary conditions. In one-dimensional field theories pseudoperiodic and quasiperiodic boundary conditions are also RG fixed points. Under these conditions massless bosonic free field theories are conformally invariant. Among all fixed points only Neumann boundary conditions are infrared stable fixed points. All other conformal invariant boundary conditions become unstable under some relevant perturbations. In finite volumes we analyse the dependence of the vacuum energy along the trajectories of the renormalization group flow providing an interesting framework for dark energy evolution. On the contrary, the renormalization group flow on the boundary does not affect the leading behaviour of the entanglement entropy of the vacuum in one-dimensional conformally invariant bosonic theories. ", "machine_abstract": "We study vacuum energy in quantum field theory with boundary conditions that break conformal invariance, such as Dirichlet or Neumann boundary conditions. We show how to renormalize this quantity using zeta function regularization techniques. In particular we find that for any number of dimensions there is an infinite set of counterterms which must be included when computing the vacuum energy density at zero temperature. This result has implications for Casimir effect calculations where one considers two parallel plates separated by some distance. The presence of these additional terms can lead to significant changes in the results obtained previously. Finally we consider the case of fermions coupled to scalar fields and compute the vacuum expectation value of the stress-energy tensor. For certain values of the coupling constant it turns out that the vacuum state becomes unstable due to spontaneous symmetry breaking. Vacuum energy plays an important role in many areas of physics including cosmology [1] , black hole thermodynamics [2] , and condensed matter systems [3] . It also appears in various contexts within string theory [4] . In recent years much progress has been made towards understanding the nature of vacuum fluctuations in quantum field theories (QFTs) [5] - [8] . However most work done so far has focused primarily on QFTs defined on flat space-time manifolds without boundaries [9] - [11] . Recently however there have been several attempts to understand vacuum fluctuations in QFTs defined on curved backgrounds [12] - [14] . Another interesting problem involves studying vacuum fluctuations in QFT's defined on spaces with boundaries [15] - [17] . Such problems are relevant for example in the context of Casimir effects [18] - [20] .", "paraphrased_abstract": "And so we study the kinetic energy of vacuums in quantum field theory, as a fusion of a flat, space-time, and a flat background, and now we study a few different ways to study the fluctuations in QFTs in a curved background. This is particularly relevant to the Casimir effect. In recent years much has been done to investigate the nature of the fluctuations in QFTs, both flat and curved, and there have been attempts to understand the fluctuations in the curved background of QFTs. This work has shown how to normalize the density of a vacuum energy density, and we have renormalized it with zeta function regularization. For instance, in the case of fermions attached to scalar fields, we have found the value of the vacuum expectation as a tensor. This result is of great interest for Casimir effects. In the last part of the paper, we have examined the fluctuations of the vacuum in QFTs, and we present a method of renormalization, using zeta function regularization. We consider the vacuum energy in quantum field theory with boundary conditions, such as Dirichlet and Neumann boundary conditions, and in particular we find an infinite number of counter-terms that must be included in the calculation of the vacuum density at a certain temperature. This is very relevant in the case", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.046, "title": "Origin of magnetoelectric behavior in BiFeO$_3$", "abstract": "  The magnetoelectric behavior of BiFeO$_3$ has been explored on the basis of accurate density functional calculations. The structural, electronic, magnetic, and ferroelectric properties of BiFeO$_3$ are predicted correctly without including strong correlation effect in the calculation. Moreover, the experimentally-observed elongation of cubic perovskite-like lattice along the [111] direction is correctly reproduced. At high pressure we predicted a pressure-induced structural transition and the total energy calculations at expanded lattice show two lower energy ferroelectric phases, closer in energy to the ground state phase. Band-structure calculations show that BiFeO$_3$ will be an insulator in A- and G-type antiferromagnetic phases and a metal in other magnetic configurations. Chemical bonding in BiFeO$_3$ has been analyzed using various tools and electron localization function analysis shows that stereochemically active lone-pair electrons at the Bi sites are responsible for displacements of the Bi atoms from the centro-symmetric to the noncentrosymmetric structure and hence the ferroelectricity. A large ferroelectric polarization (88.7 $\\mu$C/cm$^{2}$) is predicted in accordance with recent experimental findings. The net polarization is found to mainly ($>$ 98%) originate from Bi atoms. Moreover the large scatter in experimentally reported polarization values is due to the large anisotropy in the spontaneous polarization. ", "machine_abstract": "The origin of the magnetoelectric effect (ME) is investigated by first-principles calculations for bulk and surface structures of BiFeO$_{3}$, which has been recently discovered to be ferroelectric with antiferromagnetic ordering at room temperature.  The calculated results show that the ME coupling constant is strongly dependent on the oxygen octahedral tilt angle $\\theta_{oct}$; it increases rapidly as $\\theta_{oct}$ decreases below $11^{\\circ} \\sim 13^{\\circ}$. This result suggests that the large ME response observed experimentally can be attributed mainly to the rotation of oxygen octahedra around the c-axis. In addition, we find that the contribution from the Fe magnetic moment plays an important role in enhancing the ME effect when $\\theta_{oct}$ becomes small enough. Finally, we discuss possible origins of the discrepancy between our theoretical prediction and experimental observation. We also present some suggestions for further experiments.", "paraphrased_abstract": "It is subsequently examined whether the effect of the magnetic field can be explained by the equilateral relation of the oxygen octahedra around the c-axis. It is shown that the proportion of the magnetic field is positively influenced by the degree of equilaterality of the atoms, whose equilateralities are in excess of those of the atoms. Besides, it is shown that the contribution of the Fe magnetic moment can be significant in enhancing the equilateral relations of the atoms, when equilateralities are sufficiently small. It is suggested that the large energy and magnetic pulses observed in experimental tests can be explained by the rotation of the atoms about the axis. We conclude by discussing some possible reasons for the difference between our model and the experimental observation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3592, "title": "The evolution of interstellar clouds in a streaming hot plasma including heat conduction", "abstract": "  To examine the evolution of giant molecular clouds in the stream of a hot plasma we performed two-dimensional hydrodynamical simulations that take full account of self-gravity, heating and cooling effects and heat conduction by electrons. We use the thermal conductivity of a fully ionized hydrogen plasma proposed by Spitzer and a saturated heat flux according to Cowie & McKee in regions where the mean free path of the electrons is large compared to the temperature scaleheight. Significant structural and evolutionary differences occur between simulations with and without heat conduction. Dense clouds in pure dynamical models experience dynamical destruction by Kelvin-Helmholtz (KH) instability. In static models heat conduction leads to evaporation of such clouds. Heat conduction acting on clouds in a gas stream smooths out steep temperature and density gradients at the edge of the cloud because the conduction timescale is shorter than the cooling timescale. This diminishes the velocity gradient between the streaming plasma and the cloud, so that the timescale for the onset of KH instabilities increases, and the surface of the cloud becomes less susceptible to KH instabilities. The stabilisation effect of heat conduction against KH instability is more pronounced for smaller and less massive clouds. As in the static case more realistic cloud conditions allow heat conduction to transfer hot material onto the cloud's surface and to mix the accreted gas deeper into the cloud. ", "machine_abstract": "We study the thermal instability and fragmentation of an initially uniform, isothermal cloud embedded in a hot plasma flow with constant velocity shear using two-dimensional hydrodynamic simulations. The initial temperature difference between the cloud and its surroundings leads to the development of Rayleigh-Taylor instabilities at the cloud boundary which grow into mushroom-like structures that are unstable against Kelvin-Helmholtz modes as well. We find that these two types of instabilities can lead to the formation of filaments and clumps within the cloud on different timescales depending on the strength of the magnetic field. In addition we show that heat conduction has only minor effects on the cloud's morphology but significantly influences the cloud's internal structure by smoothing out density fluctuations. Finally, we discuss how our results may be used to explain observations of cometary globules and other dense molecular clouds. Subject headings: Instability -Conductivity -Magnetic fields -Clouds", "paraphrased_abstract": "But what does the elasticity of the cloud depend on, and how much is the magnetic field, of the atmosphere, and the density? What does the heat conduction? What does the elasticity of the cloud depend on? What does the density depend on? We study the thermal instability and the collapse of an undifferentiated, uniform cloud surrounded by a hot plasma flow with constant velocity. We study the origin of Rayleigh-Taylor instabilities at the boundary of the cloud, which develops into mushroom-like structures that are not compatible with the Kelvin-Helmholtz modes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2027, "title": "Measuring Electric Fields From Surface Contaminants with Neutral Atoms", "abstract": "  In this paper we demonstrate a technique of utilizing magnetically trapped neutral Rb-87 atoms to measure the magnitude and direction of stray electric fields emanating from surface contaminants. We apply an alternating external electric field that adds to (or subtracts from) the stray field in such a way as to resonantly drive the trapped atoms into a mechanical dipole oscillation. The growth rate of the oscillation's amplitude provides information about the magnitude and sign of the stray field gradient. Using this measurement technique, we are able to reconstruct the vector electric field produced by surface contaminants. In addition, we can accurately measure the electric fields generated from adsorbed atoms purposely placed onto the surface and account for their systematic effects, which can plague a precision surface-force measurement. We show that baking the substrate can reduce the electric fields emanating from adsorbate, and that the mechanism for reduction is likely surface diffusion, not desorption. ", "machine_abstract": "We report on the measurement of electric fields produced by surface contaminants using neutral atoms as probes. The technique is based on measuring the Stark shift in atomic resonance lines due to an applied electric field, and has been used previously for studying electric fields near surfaces such as those found at liquid helium temperatures or in high vacuum environments.  We have extended this method to measure electric fields over a wide range of temperatures (4 K - 300 K) and pressures (10-6 Torr - atmospheric pressure). In addition we demonstrate that it can be used to study electric fields generated by charged particles trapped close to surfaces. This work opens up new possibilities for probing electric fields in many different systems including biological samples where conventional techniques are limited. Measurement of electric fields produced by charged particle traps using neutral atoms: A novel probe of local electrostatic potentials. Measuring electric fields produced by surface contaminant... Neutral atoms provide a unique tool for investigating electric fields because they respond directly to the vector potential associated with electromagnetic fields. Here we use this property to measure electric fields produced by surface contamination. Our approach relies on observing the Stark splitting of atomic energy levels when exposed to external electric fields. Previous experiments have demonstrated this effect in low temperature and ultra-high vacuum conditions1-5 but here we show how these measurements may also be performed under more typical laboratory conditions.", "paraphrased_abstract": "In this work, we demonstrate that we can measure electric fields produced by the resonant atoms of the surface of the material. We demonstrate that this is also possible in the case of the resonant atoms of the surface. Neutral atoms are a novel probe of local electric fields. We have already studied electric fields near the surface, such as at liquid helium temperatures and vacuum. We have also investigated the effect of electric fields on the atoms of the surface. Our approach relies on observing the Stark splitting of the atoms in the presence of electric fields. This phenomenon has been previously used to study electric fields near surfaces, such as at liquid helium or at high vacuums. Here we apply this method to the measurement of electric fields of surface contaminants. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2704, "title": "Gas-grain chemistry in cold interstellar cloud cores with a microscopic Monte Carlo approach to surface chemistry", "abstract": "  AIM: We have recently developed a microscopic Monte Carlo approach to study surface chemistry on interstellar grains and the morphology of ice mantles. The method is designed to eliminate the problems inherent in the rate-equation formalism to surface chemistry. Here we report the first use of this method in a chemical model of cold interstellar cloud cores that includes both gas-phase and surface chemistry. The surface chemical network consists of a small number of diffusive reactions that can produce molecular oxygen, water, carbon dioxide, formaldehyde, methanol and assorted radicals. METHOD: The simulation is started by running a gas-phase model including accretion onto grains but no surface chemistry or evaporation. The starting surface consists of either flat or rough olivine. We introduce the surface chemistry of the three species H, O and CO in an iterative manner using our stochastic technique. Under the conditions of the simulation, only atomic hydrogen can evaporate to a significant extent. Although it has little effect on other gas-phase species, the evaporation of atomic hydrogen changes its gas-phase abundance, which in turn changes the flux of atomic hydrogen onto grains. The effect on the surface chemistry is treated until convergence occurs. We neglect all non-thermal desorptive processes. RESULTS: We determine the mantle abundances of assorted molecules as a function of time through 2x10^5 yr. Our method also allows determination of the abundance of each molecule in specific monolayers. The mantle results can be compared with observations of water, carbon dioxide, carbon monoxide, and methanol ices in the sources W33A and Elias 16. Other than a slight underproduction of mantle CO, our results are in very good agreement with observations. ", "machine_abstract": "We present results for the chemical composition and temperature structure of two different models of dense molecular clouds, which are based on detailed microphysical calculations including gas-grain interactions. The first model is an isolated spherical core that collapses under its own gravity; it has been evolved up to densities of 10^8 cm^{-3}. In this case we find that grain-surface reactions play only a minor role because they occur mainly at low temperatures where the density is too small to allow efficient freeze-out onto grains. However, these processes can be important if the collapse proceeds faster than predicted by standard theory (e.g., due to magnetic fields). We also study the evolution of a protostellar envelope surrounding a newly formed star. Here we find that the formation of complex organic molecules such as methanol or formaldehyde requires high densities and relatively warm temperatures. This suggests that these species may not be abundant enough to explain their observed abundances in dark clouds unless additional sources of heating exist.", "paraphrased_abstract": "- Then we study the morphology of a protostellar envelope around a newly formed star. We found that the synthesis of complex organic molecules, such as methanol and formaldehyde, requires high density and relatively high temperatures, and this implies that they may not be abundant in the dark clouds, unless the heating is incorporated. We subsequently investigated the chemistry and the temperature of two different models of dense clouds, both of which are based on a detailed investigation of gas-grain interactions. The first one is a solid core, which collapses under the gravity of its own gravity; the second one is a solid core of 10 cm-3. Its density is small, since the density is too small to allow for efficient freeze-off of grains. However, the processes of the surface are very important, since they occur only at low temperatures, where the density is too small for efficient freeze-off. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1856, "title": "Transition region features observed with Hinode/EIS", "abstract": "  Two types of active region feature prominent at transition region temperatures are identified in Hinode/EIS data of AR 10938 taken on 2007 January 20. The footpoints of 1 MK TRACE loops are shown to emit strongly in emission lines formed at log T=5.4-5.8, allowing the temperature increase along the footpoints to be clearly seen. A density diagnostic of Mg VII yields the density in the footpoints, with one loop showing a decrease from 3x10^9 cm^-3 at the base to 1.5x10^9 cm^-3 at a projected height of 20 Mm. The second feature is a compact active region transition region brightening which is particularly intense in O V emission (log T=5.4) but also has a signature at temperatures up to log T=6.3. The Mg VII diagnostic gives a density of 4x10^10 cm^-3, and emission lines of Mg VI and Mg VII show line profiles broadened by 50 km/s and wings extending beyond 200 km/s. Continuum emission in the short wavelength band is also found to be enhanced, and is suggested to be free-bound emission from recombination onto He^+. ", "machine_abstract": "We present the first results on transition region (TR) lines obtained by the Extreme Ultraviolet Imaging Spectrometer (EIS), aboard the Solar-B satellite, in active regions and quiet Sun areas. The EIS data are compared to those taken simultaneously by the EUV Imaging Telescope (EIT). We find that TR line intensities show significant enhancements over their photospheric values at temperatures between 10 5 K and 2 \u00c2 10 6 K. In addition, we observe strong spatial variations in the intensity ratios among different TR lines. These observations suggest that there is an abundance variation across the solar surface for some elements such as Fe xii, Fe xiii, Si vii, and Mg ix. This work was supported by NASA grants NNG05GE73G and NNX07AQ85G. Hinode is a Japanese mission developed and launched by ISAS/JAXA, collaborating with NAOJ as domestic partner, NASA and STFC (UK) as international partners. Scientific operation of the Hinode mission is conducted by the Hinode science team organized at ISAS/JAXA.", "paraphrased_abstract": "The Hinode mission is conducted by the Hinode Science Institute, the coordinating organization of which is the ISAS Institute. The project is an experimental mission of the Japanese Institute of Astronomy and is financed by the Japanese government. It is operated by the Jno-Jno-Jn, the NAOJ is the domestic partner, and NASA is the international partner. The EIS-ESR is the first of its kind on the planet and in the sky. The EIS-ESR is compared with the EIT-ESR data from the Solar-B satellite in active and a quiet position. The EIS-ESR data are compared with the EIT-ESR data on the Earth. We find that the ratio of the three TRS lines is significant and more than the photospheric value at temperature of 10 5 K. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1055, "title": "Probing the Structure of Gamma-Ray Burst Jets with Steep Decay Phase of their Early X-ray Afterglows", "abstract": "  We show that the jet structure of gamma-ray bursts (GRBs) can be investigated with the tail emission of the prompt GRB. The tail emission which we consider is identified as a steep-decay component of the early X-ray afterglow observed by the X-ray Telescope onboard Swift. Using a Monte Carlo method, we derive, for the first time, the distribution of the decay index of the GRB tail emission for various jet models. The new definitions of the zero of time and the time interval of a fitting region are proposed. These definitions for fitting the light curve lead us an unique definition of the decay index, which is useful to investigate the structure of the GRB jet. We find that if the GRB jet has a core-envelope structure, the predicted distribution of the decay index of the tail has a wide scatter and has multiple peaks, which cannot be seen for the case of the uniform and the Gaussian jet. Therefore, the decay index distribution tells us the information on the jet structure. Especially, if we observe events whose decay index is less than about 2, both the uniform and the Gaussian jet models will be disfavored according to our simulation study. ", "machine_abstract": "We present an analysis of early X-ray afterglow data for eight gamma-ray bursts (GRBs) in which we find evidence that they are associated with relativistic jets viewed off-axis, and show how this can be used to probe jet structure. We use our model to predict the late-time behaviour of these GRB afterglows and compare it with observations made by Swift/XRT. The results suggest that the majority of GRB jets have a structured energy distribution, with most of the kinetic energy contained within a narrow cone along the jet axis. This is consistent with theoretical expectations based on models where GRBs result from the collapse of massive stars into black holes or neutron stars.  Introduction  Gamma-ray bursts (GRBs; see Piran 2004 , Gehrels et al. 2009 ) are brief flashes of high-energy radiation lasting typically 10 s but ranging up to several hundred seconds. They were first detected over 50 years ago (Klebesadel et al. 1973; Strong et al. 1974) , but despite extensive observational efforts there remain many open questions about them. In particular, what powers the emission? What causes the observed diversity between different bursts? The standard fireball model (see e.g., Rees & Meszaros 1992; Sari 1997; Piran 1999; Wijers 2001; Kumar & Zhang 2015) provides one explanation for the prompt phase of GRB emission. It involves the dissipation of kinetic energy stored in a relativistically expanding shell of plasma produced during some catastrophic event such as the merger of two compact objects or the collapse of a massive star. However, this model cannot explain all aspects of GRB phenomenology. For example, it does not account for the wide range of durations seen across the population of GRBs (e.g., Nakar 2007), nor do current models provide any satisfactory explanation for why only a small fraction of collapsing stars produce observable GRBs (e. g., Bromm & Loeb 2006) . Furthermore, the lack of detection of optical counterparts to short-duration GRBs has led to suggestions that at least some", "paraphrased_abstract": "The first GRBs were detected 50 years ago (Klebesadel et al., 1973, and Strong et al., 1974), but the observation of these GRBs remained a mystery. This is particularly the case, when compared with the theory of a collapse of massive stars into black holes or neutron stars. In a general fireball model (ref. Rees and Meszaros 1992, Sari 1997, Piran 1999, Wijers 2001, Kumar et al.), the principle of which is to dissipate the kinetic energy in a kinetic shell, in the course of a catastrophic event, for example, the merger of two compact objects or the collapse of a huge star. In this model, we assume the emission of an intense gas, and we present an example of the occurrence of such an intense gaseous gas. The present model of GRBs is a simple and straightforward one: it assumes the dissipation of the kinetic energy in a sphere of kinetic energy, which is surrounded by a narrow cone on the jet axis. This is in agreement with the theory, which explains GRBs as a result of the collapse of giant stars into black holes or neutron stars. Our present model demonstrates that GRBs are most frequent and organized, with a", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2838, "title": "Kirillov-Reshetikhin conjecture : the general case", "abstract": "  We prove the Kirillov-Reshetikhin (KR) conjecture in the general case : for all twisted quantum affine algebras we prove that the characters of KR modules solve the twisted Q-system and we get explicit formulas for the character of their tensor products (the untwisted simply-laced case was treated by Najakima, and the untwisted case by the author). The proof is uniform and provides several new developments for the representation theory of twisted quantum affine algebras, including twisted Frenkel-Reshetikhin q-characters (expected by Frenkel-Reshetikhin and Frenkel-Mukhin). We also prove the twisted T-system. As an application we get explicit formulas for the twisted q-characters of fundamental representations for all types, including the formulas for types D_4^{(3)}, E_6^{(2)} conjectured by Reshetikhin. We prove the formulas for KR modules in types A_n^{(2)} and D_4^{(3)} conjectured by Kuniba-Suzuki. Eventually our results imply the conjectural branching rules [HKOTT] to the quantum subalgebra of finite type. ", "machine_abstract": "We prove that the Kirillov-Reshitikhin conjecture holds for any semisimple Lie algebra over an arbitrary field, and in particular we show that the number of irreducible representations of the quantum group is equal to the dimension of its Cartan subalgebra.  We also give some applications of this result to the representation theory of finite groups. The Kirillov-Reshetkhin Conjecture states that if $\\mathfrak{g}$ is a simple complex Lie algebra with root system $R = \\Phi(\\mathfrak{g})$ then there exists a bijection between the set of isomorphism classes of irreducible representations of the quantized enveloping algebra $U_q(\\mathfrak{g}^{0})$ (where $q$ is not a root of unity), and the set of dominant weights $\\lambda \\in P^{+}(R)$ such that $|\\lambda| \\leq q - 1$.  The proof uses results on the classification of indecomposable tilting modules due to Donkin, Erdmann, Mathieu and Schmidt; these are combined with techniques developed by Lusztig and Kashiwara to obtain our main theorem.", "paraphrased_abstract": "It uses the results of the classification of indecomposable tilting modules from Donkin, Erdmann, Mathieu, and Schmidt to obtain our main theorem. We use the result to prove that the Kirillov-Reshetkhin conjecture is true for any simple Lie algebra, if the root is a simple complex Lie algebra, whose root is a phi (mathfrakg) then there is a bijection between the set of isomorphism classes of the irreducible representations of the quantized enveloping algebra U = Phi (mathfrakg), if the p is not a root of unity, and the set of dominant weights lambda in P+(R) leq leq q leq q leq q leq q leq q leq q leq q leq q leq q leq q - 1. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3822, "title": "The Cut-off Covering Spectrum", "abstract": "  We introduce the $R$ cut-off covering spectrum and the cut-off covering spectrum of a complete length space or Riemannian manifold. The spectra measure the sizes of localized holes in the space and are defined using covering spaces called $\\delta$ covers and $R$ cut-off $\\delta$ covers. They are investigated using $\\delta$ homotopies which are homotopies via grids whose squares are mapped into balls of radius $\\delta$.   On locally compact spaces, we prove that these new spectra are subsets of the closure of the length spectrum. We prove the $R$ cut-off covering spectrum is almost continuous with respect to the pointed Gromov-Hausdorff convergence of spaces and that the cut-off covering spectrum is also relatively well behaved. This is not true of the covering spectrum defined in our earlier work which was shown to be well behaved on compact spaces. We close by analyzing these spectra on Riemannian manifolds with lower bounds on their sectional and Ricci curvature and their limit spaces. ", "machine_abstract": "The cut-off covering spectrum is the set of all functions in $L^2(\\mathbb{R})$ whose Fourier transforms are supported on $[-1,1]$ and have absolute value at most 1 everywhere.  It was introduced by David Hilbert as an example of a non-trivial closed subspace of $L^2$ that does not contain any nonconstant entire analytic function.   In this talk we will discuss some recent results about the structure of the cut-off covering spectrum. The first part of our discussion concerns the problem of determining whether or not a given bounded linear operator on $L^2$ has finite rank.    We show how to use the theory of Toeplitz operators with matrix symbols developed by Sarason and others to give a complete solution to this problem for certain classes of operators including those arising from multiplication by polynomials.   For other classes of operators such as those coming from multiplication by trigonometric polynomials it turns out that there exist examples which do not fall into one of these categories but still have finite rank.     The second part of our discussion concerns a theorem due to Joram Lindenstrauss concerning the existence of extremal points in the unit ball of the space of continuous functions on the circle with absolutely convergent Fourier series.   This theorem can be viewed as a generalization of the classical result of Carleson stating that every point in the unit ball of $H^1_0(T)$ (the Hardy space of order 1)  is an extreme point of the unit ball of $H^1_o(T)$ where $T = [0,2\\pi]$.   By using techniques similar to those used in the proof of the original theorem of Carleson we prove a version of Lindenstrauss' theorem for the space of continuous functions...", "paraphrased_abstract": "This is a generalization of Carleson's original theory, which says that all points in the unit ball of \"H\" (the Hardy space of order 1) are points in the \"H\" ball of \"H\" (hardy space of order 1), where T is a point of \"T\" where T = 0 or 2 pi. In the second part of our talk we will discuss a theorem of Joram Lindenstrauss that the infinite space of the continuous sphere is surrounded by extremal points. We present a theory of Toeplitz operators with a matrix of symbols developed by Sarason and others and show that we can solve the problem for some of the classes of the sphere, including those of the sphere. Moreover, there exist classes of spheres that are not in one of these classes, but still have a finite number. The first problem is to determine whether a given linear sphere in a sphere is finite. We show that for certain spheres, including those arising from polynomials, we have some examples which are not in one of these categories, but nevertheless finite. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1149, "title": "Distortion of Gravitational-Wave Packets Due to their Self-Gravity", "abstract": "  When a source emits a gravity-wave (GW) pulse over a short period of time, the leading edge of the GW signal is redshifted more than the inner boundary of the pulse. The GW pulse is distorted by the gravitational effect of the self-energy residing in between these shells. We illustrate this distortion for GW pulses from the final plunge of black hole (BH) binaries, leading to the evolution of the GW profile as a function of the radial distance from the source. The distortion depends on the total GW energy released and the duration of the emission, scaled by the total binary mass, M. The effect should be relevant in finite box simulations where the waveforms are extracted within a radius of <~ 100M. For characteristic emission parameters at the final plunge between binary BHs of arbitrary spins, this effect could distort the simulated GW templates for LIGO and LISA by a fraction of 0.001. Accounting for the wave distortion would significantly decrease the waveform extraction errors in numerical simulations. ", "machine_abstract": "We study the gravitational self-force acting on a wave packet in curved spacetime, and show that it causes its distortion.  We consider two types of wavepackets: one is constructed by superposing plane waves with different frequencies; another is made up of spherical waves emitted at various angles around an isolated source point. In both cases we find that the force acts as if there were additional sources located behind the packet's center-of-mass worldline. The effect can be understood intuitively using the concept of \"gravitational memory\". Our results are relevant for understanding how gravitational waves propagate through space-time. They also provide new insights into the problem of gravitational radiation reaction. Introduction - A fundamental question about gravitational waves (GWs) concerns how they evolve over time when propagating through curved space-time [1] . This issue has been studied extensively within the framework of linearized gravity theory [2] , where GWs are treated as small perturbations of flat Minkowski background geometry [3] . In this work we focus on the effects due to gravitational self-coupling [4] . These arise because each part of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity [5] . As such, the total force acting upon any given portion of a GW depends not only on the local curvature but also on the entire history of the wave [6] . It turns out that these forces cause significant distortions of the wave packets [7, 8] . For example, the shape of a plane-wave packet changes during propagation so that its peak moves away from the direction of motion [9] . Similar behavior was found for spherical wave packets [10] .", "paraphrased_abstract": "I will describe this in a few sentences: We are concerned with the self-conductivity of the gravitational waves, a resemblance of the Minkowski space. The fundamental question of gravitational waves (GWs) is: how they change over time in a curved space, as well as the consequences of their bending and bending. We will consider two spherical waves, one forming of plane waves with different frequencies; another consists of spherical waves, emitted from a point at an angle, around an isolated point. These results have the effect of bending the waves in a curved space, and they provide new insights into the question of radiation reactions. In this study we are analyzing the gravitational self-stifling of a wave packet, and we show that this self-stiffness of the wave is a cause of distortion; a plane wave, for example, changes shape, so that its crest moves away from the direction of its motion, as in a sphere. We examine the effect of gravitational self-separation in a curved space and find that it does not only depend on the local curvature, but on the whole history of the wave. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1088, "title": "Extended Comment on \"One-Range Addition Theorems for Coulomb Interaction Potential and Its Derivatives\" by I. I. Guseinov (Chem. Phys. Vol. 309 (2005), pp. 209 - 213)", "abstract": "  Addition theorems are principal tools that express a function $f (\\bm{r} \\pm \\bm{r}')$ in terms of products of other functions that only depend on either $\\bm{r}$ or $\\bm{r}'$. The best known example of such an addition theorem is the Laplace expansion of the Coulomb potential which possesses a characteristic two-range form. Guseinov [Chem. Phys. {\\bf 309}, 209 - 213 (2005)] derived one-range addition theorems for the Coulomb potential via the limit $\\beta \\to 0$ in previously derived one-range addition theorems for the Yukawa potential $\\exp \\bigl(-\\beta | \\bm{r}-\\bm{r}'| \\bigr) /| \\bm{r}-\\bm{r}'|$. At first sight, this looks like a remarkable achievement, but from a mathematical point of view, Guseinov's work is at best questionable and in some cases fundamentally flawed. One-range addition theorems are expansions in terms of functions that are complete and orthonormal in a given Hilbert space, but Guseinov replaced the complete and orthonormal functions by nonorthogonal Slater-type functions and rearranged the resulting expansions. This is a dangerous operation whose validity must be checked. It is shown that the one-center limit $\\bm{r}' = \\bm{0}$ of Guseinov's rearranged Yukawa addition theorems as well as of several other addition theorems does not exist. Moreover, the Coulomb potential does not belong to any of the Hilbert spaces implicitly used by Guseinov. Accordingly, one-range addition theorems for the Coulomb potential diverge in the mean. Instead, these one-range addition theorems have to interpreted as expansions of generalized functions in the sense of Schwartz that converge weakly in suitable functionals. ", "machine_abstract": "We have recently shown that the one-range addition theorems derived in our previous work are valid not only for the Coulomb interaction potential but also its derivatives, such as the nuclear attraction potential or the exchange potential.     In this comment we show how these results can be used to derive new addition theorems for the nuclear attraction potential and the exchange potential. These new addition theorems are useful when calculating matrix elements between atomic orbitals with different angular momenta. We illustrate their application using examples involving hydrogenic wave functions. Finally, we discuss some possible extensions of these results. DOI: 10.1063/1.2055316     This is an extended version of a comment published in ChemPhysChem. DOI: 10.1002/cphc.201500420         One-range addition theorems play important roles in many areas of physics including quantum chemistry [1-3], molecular physics [4], condensed matter physics [5], etc.. They provide simple expressions for evaluating matrix elements of various potentials between two arbitrary wavefunctions. For example, they allow us to calculate matrix elements of the Coulomb interaction potential between any pair of atomic orbital basis sets without having to perform complicated numerical integrations [6]. Recently, we showed that the same approach could be applied to other types of potentials [7-9].", "paraphrased_abstract": "It is an extension of a comment that was made in the journal ChemPhys. It is a summary of the previous work and a discussion of some of the new additions that were made in the previous work. This is an extension of the comments made in ChemPhysChem. It is a summary of a work published in ChemPhys Chemistry. It is a summary of the results of the previous work on the Coulomb interaction potential and its derivatives. It is useful to use these new additions for calculating matrix elements between atomic orbitals with different angular moments. We have recently shown that the one-range additions made in the previous work are valid not only for the Coulomb interaction potential but also for its derivatives, such as the nuclear attraction potential or the exchange potential. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0643, "title": "Spectral method for matching exterior and interior elliptic problems", "abstract": "  A spectral method is described for solving coupled elliptic problems on an interior and an exterior domain. The method is formulated and tested on the two-dimensional interior Poisson and exterior Laplace problems, whose solutions and their normal derivatives are required to be continuous across the interface. A complete basis of homogeneous solutions for the interior and exterior regions, corresponding to all possible Dirichlet boundary values at the interface, are calculated in a preprocessing step. This basis is used to construct the influence matrix which serves to transform the coupled boundary conditions into conditions on the interior problem. Chebyshev approximations are used to represent both the interior solutions and the boundary values. A standard Chebyshev spectral method is used to calculate the interior solutions. The exterior harmonic solutions are calculated as the convolution of the free-space Green's function with a surface density; this surface density is itself the solution to an integral equation which has an analytic solution when the boundary values are given as a Chebyshev expansion. Properties of Chebyshev approximations insure that the basis of exterior harmonic functions represents the external near-boundary solutions uniformly. The method is tested by calculating the electrostatic potential resulting from charge distributions in a rectangle. The resulting influence matrix is well-conditioned and solutions converge exponentially as the resolution is increased. The generalization of this approach to three-dimensional problems is discussed, in particular the magnetohydrodynamic equations in a finite cylindrical domain surrounded by a vacuum. ", "machine_abstract": "We consider the problem of finding an approximate solution to the following boundary value problem:   \\begin{cases}     -\\Delta u = f & \\text{in } \\Omega,\\\\     u = 0 & \\text{on } \\partial\\Omega.   \\end{cases} Here $\\Omega$ is a bounded domain in $R^n$ with smooth boundary $\\partial\\Omega$.  We assume that $f$ belongs to $L^2(\\Omega)$ and satisfies some additional conditions which are specified below. The main goal of this work is to develop efficient numerical methods for solving (1). In particular we will focus on spectral methods based on the use of spherical harmonics expansions. These methods have been successfully applied to various types of partial differential equations arising in mathematical physics. However their application to the problem under consideration has not yet been studied systematically. Here we present several new results concerning the convergence analysis of these methods as well as their implementation details. Our approach allows us to obtain optimal order error estimates without any regularity assumptions about the exact solution or its derivatives.", "paraphrased_abstract": "Here, we will study a numerical method of a spatial resolution, based on spherical harmonics, that is, a spectral method based on a spherical harmonics expansion. These methods have already been successfully applied to partial differential equations in mathematics. However, they have not yet been applied to the problem we are studying. In particular, we will present a spectral solution of the equation of u=0 which is based on the spherical harmonics expansion of a spherical harmonics expansion, which has been well developed and is applied to many partial differential equations. We consider the following problem: the u=0 = f & text in Omega, u = 0 and text in Omega. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0703, "title": "Transient behavior of surface plasmon polaritons scattered at a subwavelength groove", "abstract": "  We present a numerical study and analytical model of the optical near-field diffracted in the vicinity of subwavelength grooves milled in silver surfaces. The Green's tensor approach permits computation of the phase and amplitude dependence of the diffracted wave as a function of the groove geometry. It is shown that the field diffracted along the interface by the groove is equivalent to replacing the groove by an oscillating dipolar line source. An analytic expression is derived from the Green's function formalism, that reproduces well the asymptotic surface plasmon polariton (SPP) wave as well as the transient surface wave in the near-zone close to the groove. The agreement between this model and the full simulation is very good, showing that the transient \"near-zone\" regime does not depend on the precise shape of the groove. Finally, it is shown that a composite diffractive evanescent wave model that includes the asymptotic SPP can describe the wavelength evolution in this transient near-zone. Such a semi-analytical model may be useful for the design and optimization of more elaborate photonic circuits whose behavior in large part will be controlled by surface waves. ", "machine_abstract": "We study the scattering properties of surface plasmons (SPs) by a single subwavelength groove in an optically thick metal film, which is excited by a normally incident plane wave with circular polarization. The SP field distribution and its decay length are calculated using the Green's function method for different values of the groove depth hg and width wg. It is found that when the groove depth increases to a certain value, the SP field decays exponentially along both directions away from the groove center instead of being confined within the groove region as before. This phenomenon can be explained by the fact that the SP mode becomes leaky once it scatters into radiation modes through the groove aperture. We also find that there exists another critical groove depth beyond which no SP field can exist inside or outside the groove due to total internal reflection between the groove walls. These results provide useful information on how to design efficient plasmonic devices based on grooves.", "paraphrased_abstract": "- we found out how to design effective plasmonic devices. The distribution of the scattering fields and the decay length was estimated by the Green's Function, for different values of the depth hg and width wg. We found that the radiance and the decay length of the SP field was increasing with the height of the groove, while the light passed through the groove and was no longer condensed in the groove. The results indicated that the optically thick octane film was excited by a radiance polarized by a circular polarization. This phenomenon was explained by the fact that the SP mode emitted radiation and became leaky through the groove, so that the beam of the SP becomes leaking. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0238, "title": "Radio Astrometric Detection and Characterization of Extra-Solar Planets: A White Paper Submitted to the NSF ExoPlanet Task Force", "abstract": "  The extraordinary astrometric accuracy of radio interferometry creates an important and unique opportunity for the discovery and characterization of exo-planets. Currently, the Very Long Baseline Array can routinely achieve better than 100 microarcsecond accuracy, and can approach 10 microarcsecond with careful calibration. We describe here RIPL, the Radio Interferometric PLanet search, a new program with the VLBA and the Green Bank 100 m telescope that will survey 29 low-mass, active stars over 3 years with sub-Jovian planet mass sensitivity at 1 AU. An upgrade of the VLBA bandwidth will increase astrometric accuracy by an order of magnitude. Ultimately, the colossal collecting area of the Square Kilometer Array could push astrometric accuracy to 1 microarcsecond, making detection and characterizaiton of Earth mass planets possible.   RIPL and other future radio astrometric planet searches occupy a unique volume in planet discovery and characterization parameter space. The parameter space of astrometric searches gives greater sensitivity to planets at large radii than radial velocity searches. For the VLBA and the expanded VLBA, the targets of radio astrometric surveys are by necessity nearby, low-mass, active stars, which cannot be studied efficiently through the radial velocity method, coronagraphy, or optical interferometry. For the SKA, detection sensitivity will extend to solar-type stars. Planets discovered through radio astrometric methods will be suitable for characterization through extreme adaptive optics.   The complementarity of radio astrometric techniques with other methods demonstrates that radio astrometry can play an important role in the roadmap for exoplanet discovery and characterization. ", "machine_abstract": "The National Science Foundation (NSF) has recently formed an \"Exoplanet Task Force\" with the goal of identifying key science goals for future space missions in exoplanet research, including radio astrometry.  In this white paper we present our vision on how such a mission could be designed to meet these goals. We argue that a dedicated radio telescope is needed to detect and characterize extrasolar planets using their radio emission. The proposed instrument would have unprecedented sensitivity at decimeter wavelengths, allowing it to detect planetary mass companions around nearby stars as well as directly measure the masses of known giant planet systems. This will enable us to answer fundamental questions about the formation and evolution of planetary systems. Keywords: Radio astronomy, Extrasolar planet detection, Planetary system characterization, Space mission concept development. 1 Introduction   The discovery of more than 1000 extra-solar planets over the past decade has revolutionized our understanding of planetary systems beyond our own solar system. However, many important questions remain unanswered regarding the origin and evolution of these systems. For example, what are the physical characteristics of most of these newly discovered planets? How do they form? What happens when two or more planets interact gravitationally? Are there other Earth-like worlds orbiting Sun-like stars within reachable distances?  Answering these questions requires detailed observations of individual planets, which can only be achieved by direct imaging techniques. Unfortunately, current ground-based observatories cannot achieve high enough angular resolution to resolve the majority of close-in planets due to atmospheric turbulence effects.   To overcome this limitation, NASA's Kepler satellite was launched in 2009 to search for transiting planets around bright stars. Although Kepler has been extremely successful, its primary focus is on detecting large planets in short orbits. It does not provide any information on the orbital inclination angle of detected planets, nor does it allow for precise measurements of planet radii and masses. Furthermore, because of its relatively small field-of-view, Kepler misses out on discoveries made outside of its target fields.", "paraphrased_abstract": "The discovery of more than a thousand extrasolar planets during the last decade has dramatically reshaped our understanding of the origin and evolution of planets beyond our solar system. However, it is still unknown whether such planets have originated in other places than in our own solar system. In view of this, NASA's Kepler satellite was launched in 2009. The goal of the Kepler mission was to find large planets in short orbits, but the instrument did not provide the right angle for the detection of large, indeterminate planets, nor did it enable the precise measurements of radii and masses. This project was also very unsuccessful in detecting large planets in small orbits, and, consequently, it was not able to identify any of these large, nearby stars. To answer these questions, it was necessary to establish a dedicated radio telescope, and this instrument would have the power to detect and characterize the radii and masses of the great stars, and to measure their size. The NSF has recently formed a \"Exoplanet Task Force,\" which is charged with identifying the key scientific goals for future exoplanet observations, and with the radio astronomy. We will discuss in detail the operation of this instrument and describe the steps taken by it to achieve these goals. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1448, "title": "Structures in the Universe and Origin of Galaxies", "abstract": "  The analysis of images (of obtained in various ranges of the lengths of waves) of luminous objects in the Universe by means of a method of multilevel dynamic contrasting led author to the conclusions: a) the structures of all observable galaxies represents a complicated constructions which have the tendency to self-similarity and made of separate (basic) blocks, which are a coaxially tubular structures and a cartwheel-like structures; b) the majority of observable objects in the Universe are luminous butt-ends of almost invisible (of almost completely transparent) of filamentary formations which structures are seen only near to their luminous butt-ends; c) the result of analysis of images of cosmic objects show the structure of many pairs of cooperating galaxies point to opportunity of their formation at butt-ends generated in a place of break of the similar filament; d) the interacting galaxies (M 81 and M 82) show they are butt-ends of sawed off of two branches of a treelike filament and their interaction is coming out through this filament; e) as our Universe is in dynamics the processes of formation of stars, galaxies and their congestions can go presently by means of a fracturing of filaments with a corresponding diameters and of the time for their such formation is necessary much less, than along existing standard model. ", "machine_abstract": "The book is divided into three parts, each part containing several chapters. The first part deals with cosmological models and their predictions for large-scale structures observed today. In this section we will discuss how galaxies form and evolve within these models. We will also introduce some basic concepts such as dark matter halos, galaxy biasing etc., which are important to understand the formation of large scale structure. The second part discusses observational techniques used to study the distribution of galaxies on different scales. Here we will describe various surveys that have been carried out over past few decades using ground-based telescopes or space based missions like Hubble Space Telescope (HST). Finally, third part describes statistical methods commonly used to analyze data obtained by observing the universe. This includes topics ranging from correlation functions to power spectrum analysis. The main goal of this course is to provide an introduction to modern astrophysics. It covers many aspects of theoretical physics and observational astronomy including general relativity, quantum mechanics, nuclear physics, particle physics, stellar evolution, black holes, supernovae, quasars, gamma-ray bursts, pulsar, gravitational waves, cosmic microwave background radiation, big bang nucleosynthesis, inflationary cosmology, dark energy, dark matter, baryonic acoustic oscillations, primordial fluctuations, galaxy clusters, supermassive black holes, active galactic nuclei, starburst galaxies, infrared galaxies, radio galaxies, interacting galaxies, merging galaxies, elliptical galaxies, lenticular galaxies, spiral galaxies, irregular galaxies, dwarf galaxies, blue compact dwarfs, Lyman-break galaxies, high-z quasars, distant red galaxies, high-redshift galaxies, intergalactic medium, interstellar medium, Milky Way Galaxy, Local Group of Galaxies, Virgo Cluster of Galaxies, Coma Cluster of Galaxies, Perseus Cluster of Galaxies, Abell Clusters of Galaxies, Large Scale Structure of the Universe, Cosmic Web, Supercluster-void network, Dark Matter Halos, Biased Growth of Structures, Observational Techniques, Statistical Methods, Cosmological Parameters, Future Directions...", "paraphrased_abstract": "The aim of the book is to provide a general introduction to the world of astronomy. It covers general relativity, quantum mechanics, nuclear physics, particle physics, stellar evolution, black holes, supernovae, quasars, gamma rays, pulsars, gravitational waves, cosmic microwave background radiation, astronomy in general, cosmic acoustics, primordial fluctuations, galaxy clusters, supermassive black holes, supermassive black holes, active galaxy nuclei, active galaxies, active globular nuclei, active galaxy clusters, supermassive black holes, active galaxy nuclei, interacting galaxies, interacting radio galaxies, interacting galaxies, interacting galaxies, interacting galaxies, spiral galaxies, elliptic galaxies, elliptic galaxies, spiral galaxies, irregular galaxies, dwarf galaxies, high-z quasars, Lyman schisars, Lyman schisars, z quasars, high-z quasars, liman schisars, blue quasars, lyman schisars, high-z ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2629, "title": "Families of holomorphic bundles", "abstract": "  The first goal of the article is to solve several fundamental problems in the theory of holomorphic bundles over non-algebraic manifolds: For instance we prove that stability and semi-stability are Zariski open properties in families when the Gauduchon degree map is a topological invariant, or when the parameter manifold is compact. Second we show that, for a generically stable family of bundles over a K\\\"ahler manifold, the Petersson-Weil form extends as a closed positive current on the whole parameter space of the family. This extension theorem uses classical tools from Yang-Mills theory developed by Donaldson (e.g. the Donaldson functional and the heat equation for Hermitian metrics on a holomorphic bundle). We apply these results to study families of bundles over a K\\\"ahlerian manifold $Y$ parameterized by a non-K\\\"ahlerian surface $X$, proving that such families must satisfy very restrictive conditions. These results play an important role in our program to prove existence of curves on class VII surfaces. ", "machine_abstract": "In this article, we study families of holomorphic vector bundles on complex algebraic varieties. We prove that the set of isomorphism classes of such families is naturally an affine scheme over the base variety and give explicit equations for it in terms of Chern classes.  In particular, if the base variety has dimension one then these schemes are reduced to points corresponding to the rank and degree of each bundle in the family.  We also show how our results can be used to construct moduli spaces of stable vector bundles with fixed determinant. The main result of this article was announced by J. P. Serre at the conference \"Algebraic geometry and number theory\" held in Paris in June 2005 (see [Ser] ). Families of holomorphic vector bundles have been studied extensively since the work of Grothendieck [Gro1] . They play important roles both in algebraic geometry and mathematical physics; see e.g., [Bri1,  Bri2,   Bri3,   ...", "paraphrased_abstract": "The result of this work was announced by J. P. Serre, the co-organizer of the conference \u201cAlgebraic geometry and Number Theory\u201d held in Paris, June 2005. The study of families of holomorphic vectors has been devoted to the study of Grothendieck, and they play a key role in the field of algebraic geometry and mathematical physics. We have analyzed the holomorphic classes of such families in the simplest algebraic varieties, and we show that the class of isomorphisms of such families is affine with the base variety, and gives an exact solution to the equations in Chern classes, which we consider to be affine with the Chern classes, as well as of the moduli spaces of a stable bundle. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1717, "title": "Relation between exchange-only optimized potential and Kohn-Sham methods with finite basis sets; solution of a paradox", "abstract": "  Arguments showing that exchange-only optimized effective potential (xOEP) methods, with finite basis sets, cannot in general yield the Hartree-Fock (HF) ground state energy, but a higher one, are given. While the orbital products of a complete basis are linearly dependent, the HF ground state energy can only be obtained via a basis set xOEP scheme in the special case that all products of occupied and unoccupied orbitals emerging from the employed orbital basis set are linearly independent from each other. In this case, however, exchange potentials leading to the HF ground state energy exhibit unphysical oscillations and do not represent a Kohn-Sham (KS) exchange potential. These findings solve the seemingly paradoxical results of Staroverov, Scuseria and Davidson that certain finite basis set xOEP calculations lead to the HF ground state energy despite the fact that within a real space (or complete basis) representation the xOEP ground state energy is always higher than the HF energy. Moreover, whether or not the occupied and unoccupied orbital products are linearly independent, it is shown that basis set xOEP methods only represent exact exchange-only (EXX) KS methods, i.e., proper density-functional methods, if the orbital basis set and the auxiliary basis set representing the exchange potential are balanced to each other, i.e., if the orbital basis is comprehensive enough for a given auxiliary basis. Otherwise xOEP methods do not represent EXX KS methods and yield unphysical exchange potentials. ", "machine_abstract": "We show that the exchange-only optimized potentials (OEPs) are not equivalent to the Kohn-Sham (KS) method in general, even if one uses an exact density functional for the exchangecorrelation energy. We demonstrate this by solving analytically the OEPs for two simple model systems using Gaussian-type orbitals as basis functions. The results obtained within both approaches differ significantly. In particular, we find that the KS approach yields incorrect values for the total energies of these systems. This is due to the fact that the KS equations do not have solutions corresponding to all possible densities which can be generated by the given basis sets. On the other hand, the OEP formalism always provides unique solutions for any given density matrix. Our analysis shows also how to resolve the apparent paradox arising when trying to apply the OEP formalism to the case where only a limited number of basis functions is used.", "paraphrased_abstract": "We show that OEPs are not equivalent to the general Kohn-Sham method of equivocation, although they are used in precise terms for the exchange of energy. Hence, the OEPs, on the contrary, are always unique in every instance. The OEPs in this work are based on the Gaussian-style orbitals of Gaussian-type orbitals. The results of this analysis, however, are not consistent. The results of the OEPs are in some cases incorrect, in particular when applied to two simple model systems with Gaussian orbitals, which are derived from Gaussian orbitals. We illustrate this by analysing the OEPs of two simple models, corresponding to Gaussian orbitals, to solve the equivalence. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1551, "title": "Beryllium in Ultra-Lithium-Deficient Halo Stars - The Blue Straggler Connection", "abstract": "  There are nine metal-deficient stars that have Li abundances well below the Li plateau that is defined by over 100 unevolved stars with temperatures above 5800 K and values of [Fe/H] $<$ $-$1.0. Abundances of Be have been determined for most of these ultra-Li-deficient stars in order to investigate the cause of the Li deficiencies. High-resolution and high signal-to-noise spectra have been obtained in the Be II spectral region near 3130 \\AA for six ultra-Li-deficient stars with the Keck I telescope and its new uv-sensitive CCD on the upgraded HIRES. The spectrum synthesis technique has been used to determine Be abundances. All six stars are found to have Be deficiencies also. Two have measurable - but reduced - Be and four have only upper limits on Be. These results are consistent with the idea that these Li- and Be-deficient stars are analogous to blue stragglers. The stars have undergone mass transfer events (or mergers) which destroy or dilute both Li and Be. The findings cannot be matched by the models that predict that the deficiencies are due to extra-mixing in a subset of halo stars that were initially rapid rotators, with the possible exception of one star, G 139-8. Because the ultra-Li-deficient stars are also Be-deficient, they appear to be genuine outliers in population of halo stars used to determine the value of primordial Li; they no longer have the Li in their atmospheres that was produced in the Big Bang. ", "machine_abstract": "We report the detection of beryllium (Be) lines in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240.  These are the first detections of Be in metal-poor halo stars with [Fe/H] < -2.5 dex. We find that these stars have high surface gravities for their temperatures, indicating they may be blue stragglers or other evolved objects. In addition to the Be features at 4131 \u00c5 and 4130 \u00c5 we also see evidence for an unidentified feature near 3970 \u00c5 which is likely due to C+N+O. This work was supported by NASA grant NAG5-9998. Keywords: Beryllium; Blue straggler; Metal poor star; Ultracool dwarf. 1. Introduction. The discovery of extremely low-mass stars has opened up new avenues into understanding how planets form around very cool dwarfs. However, there remains much uncertainty about the formation process itself as well as the chemical composition of such systems. One important aspect of this problem involves determining whether or not terrestrial planet formation can occur within the habitable zone of ultracool dwarfs. To address this question it will be necessary to determine if the atmospheres of these stars contain significant amounts of heavy elements like carbon, nitrogen, oxygen, sulfur, sodium, potassium, magnesium, aluminum, silicon, calcium, titanium, iron, nickel, cobalt, copper, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It should be noted that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars.", "paraphrased_abstract": "It is important to determine whether terrestrial planets can form within the zone of these warm dwarfs. To find out, we must first determine whether the atmosphere of these stars contains considerable amounts of heavy elements: carbon, nitrogen, oxygen, sulfur, sodium, potassium, magnesium, magnesium, silicon, calcium, titanium, copper, nickel, cobalt, copper, zinc, acryl, copper, lead, mercury, lead, uranium, thorium, plutonium, and plutonium. We report here the detection of beryllium lines in two ultra-low-mattering stars, CS 2892-052 and HE 0107-5240, in two ultra-low-mattering stars: CS 2892-052 and HE 0107-5240, which are the first to be discovered in a halo, and have a diameter of three hundred meters. Moreover, we find an object near the 4131-meter mark, which could be attributed to C+N+O, which may be attributed to the co-energetic reaction of the C+N+O. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0234, "title": "A p-adic quasi-quadratic point counting algorithm", "abstract": "  In this article we give an algorithm for the computation of the number of rational points on the Jacobian variety of a generic ordinary hyperelliptic curve defined over a finite field of cardinality $q$ with time complexity $O(n^{2+o(1)})$ and space complexity $O(n^2)$, where $n=\\log(q)$. In the latter complexity estimate the genus and the characteristic are assumed as fixed. Our algorithm forms a generalization of both, the AGM algorithm of J.-F. Mestre and the canonical lifting method of T. Satoh. We canonically lift a certain arithmetic invariant of the Jacobian of the hyperelliptic curve in terms of theta constants. The theta null values are computed with respect to a semi-canonical theta structure of level $2^\\nu p$ where $\\nu >0$ is an integer and $p=\\mathrm{char}(\\F_q)>2$. The results of this paper suggest a global positive answer to the question whether there exists a quasi-quadratic time algorithm for the computation of the number of rational points on a generic ordinary abelian variety defined over a finite field. ", "machine_abstract": "We present an efficient algorithm for computing the number of points on curves over finite fields with odd characteristic, which is based on the use of quadratic reciprocity laws and the Hasse-Weil bound in the form of Weierstrass functions. The main idea behind our approach consists in reducing the problem to that of finding the number of solutions of certain systems of linear equations modulo primes of small norm. This allows us to obtain a very fast running time by using standard techniques such as modular reduction or lattice basis reduction algorithms. In particular we show how this method can be used to compute efficiently the number of rational points on hyperelliptic curves defined over prime fields. Finally, we give some numerical examples illustrating the efficiency of our algorithm compared to other known methods. Keywords: Computational algebraic geometry, Number theory, Finite field arithmetic, Prime numbers, Hyperelliptic curve cryptography, Quadratic reciprocity law", "paraphrased_abstract": "It is the main idea of the proposed method to reduce the problem of linear equations to the simple modulo of the prime. This reduces the problem to the simple modulo of the prime, and the method can be used for fast computations with the standard reducing methods, the modular reductions and the lattice-based reductions. In particular, we show how the method can be used for the calculation of the number of rational points on hyperelliptic curves on prime fields. We also present an efficient algorithm to compute the number of rational points on hyperelliptic curves over prime fields. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0053, "title": "A Global Approach to the Theory of Special Finsler Manifolds", "abstract": "  The aim of the present paper is to provide a global presentation of the theory of special Finsler manifolds. We introduce and investigate globally (or intrinsically, free from local coordinates) many of the most important and most commonly used special Finsler manifolds: locally Minkowskian, Berwald, Landesberg, general Landesberg, $P$-reducible, $C$-reducible, semi-$C$-reducible, quasi-$C$-reducible, $P^{*}$-Finsler, $C^{h}$-recurrent, $C^{v}$-recurrent, $C^{0}$-recurrent, $S^{v}$-recurrent, $S^{v}$-recurrent of the second order, $C_{2}$-like, $S_{3}$-like, $S_{4}$-like, $P_{2}$-like, $R_{3}$-like, $P$-symmetric, $h$-isotropic, of scalar curvature, of constant curvature, of $p$-scalar curvature, of $s$-$ps$-curvature. The global definitions of these special Finsler manifolds are introduced. Various relationships between the different types of the considered special Finsler manifolds are found. Many local results, known in the literature, are proved globally and several new results are obtained. As a by-product, interesting identities and properties concerning the torsion tensor fields and the curvature tensor fields are deduced. Although our investigation is entirely global, we provide; for comparison reasons, an appendix presenting a local counterpart of our global approach and the local definitions of the special Finsler spaces considered. ", "machine_abstract": "In this article, we present an approach for studying special Finsler manifolds by using global methods and techniques in Riemannian geometry. We introduce some new concepts such as geodesic convexity, strongly convexity, and strictly convexity on special Finsler manifolds. Then, we prove that every special Finsler manifold is locally geodesically convex at any point. Finally, we show that if a special Finsler manifold admits a strictly convex function then it has constant flag curvature. Keywords: Geodesic convexity, Finsler metric, Flag curvature. Mathematics Subject Classification (2010): 53C20, 53C25, 53A10. In this article, we study special Finsler manifolds with global methods and techniques in the theory of Riemannian geometry. First, we define geodesic convexity and strongly convexity on special Finslser manifolds. Then, under certain conditions, we prove that every Finsler manifold is locally  geodesically convex at each point. Next, we give necessary and sufficient conditions for a special Finsler manifold to have constant flag curvature.", "paraphrased_abstract": "A number of Riemannian-based methods and procedures have been introduced to the subject. In this work we present an approach for studying special Finsler-type manifolds by a new method of Riemannian geometry. We introduce several new terms, geodesic convexity, strongly convexity, and strict convexity, to special Finsler-type manifolds. Then we prove that every special Fins-type manifold is locally convex at any point. Finally, we prove that if a special Fins-type manifold admits a strictly convex function, then it has a constant flag-curvet. In this work, we present an approach to studying special Fins-type manifolds, using global techniques and methods of Riemannian geometry. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.077, "title": "High Resolution X-Ray Imaging of the Center of IC342", "abstract": "  We presented the result of a high resolution (FWHM~0.5'') 12 ks Chandra HRC-I observation of the starburst galaxy IC342 taken on 2 April 2006. We identified 23 X-ray sources within the central 30' x 30' region of IC342. Our HRC-I observation resolved the historical Ultraluminous X-ray sources (ULX), X3, near the nucleus into 2 sources, namely C12 and C13, for the first time. The brighter source C12, with L(0.08-10keV)=(6.66\\pm0.45)\\times10^{38}ergs^-1, was spatially extended (~82 pc x 127 pc). From the astrometric registration of the X-ray image, C12 was at R.A.=03h:46m:48.43s, decl.=+68d05m47.45s, and was closer to the nucleus than C13. Thus we concluded that source was not an ULX and must instead be associated with the nucleus. The fainter source C13, with L(0.08-10keV)=(5.1\\pm1.4) x 10^{37}ergs^-1 was consistent with a point source and located $6.51'' at P.A. 240 degree of C12.   We also analyzed astrometrically corrected optical Hubble Space Telescope and radio Very Large Array images, a comparison with the X-ray image showed similarities in their morphologies. Regions of star formation within the central region of IC342 were clearly visible in HST H alpha image and this was the region where 3 optical star clusters and correspondingly our detected X-ray source C12 were observed. We found that a predicted X-ray emission from starburst was very close to the observed X-ray luminosity of C12, suggesting that nuclear X-ray emission in IC342 was dominated by starburst. Furthermore, we discussed the possibility of AGN in the nucleus of IC342. Although our data was not enough to give a firm existence of an AGN, it could not be discarded. ", "machine_abstract": "We present high resolution X-ray imaging and spectroscopy of the central region of the galaxy cluster Abell S1063 (IC342). The data were obtained with Chandra ACIS-S3 in August 2002, for an exposure time of 50 ks. We detect two bright point sources at the center of the cluster which are associated to AGNs. Their luminosities range between 1043 erg/sec and 1044 erg/sec in the 0.5-10 keV energy band. In addition we find evidence that there is diffuse emission around these sources. This emission has a temperature of about 3 keV and its spectrum can be fitted by a thermal plasma model with solar abundance ratios. From our analysis it appears that this gas may have been heated up recently due to shocks produced during mergers or interactions among subclusters. Finally, we also detected extended soft emission surrounding both AGN candidates. Its origin could be either thermal bremsstrahlung radiation from hot gas or inverse Compton scattering off relativistic electrons.", "paraphrased_abstract": "But we saw a faint twilight emission around the two AGNs. It was ten times greater than that at the tenth keV zenith, and the wavelength was ten times higher than that at the second keV zenith, and the total energy was only ten times higher than the average energy of ten thousand years old. Moreover, we saw a thin spectroradium in the vicinity of these two AGNs. It was detected by the X-ray spectroradiometer of the central region of the Abells, at the center of the cluster. We found two bright point sources of light, which are associated with AGNs, and their brightness is of about 1044 erg/s in the 0.5 kV band. The emission was of the temperature of three keV, and we can estimate its density by the thermal density of the sun and the ray abundance of the sun. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1763, "title": "Landau (\\Gamma,\\chi)-automorphic functions on \\mathbb{C}^n of magnitude \\nu", "abstract": "  We investigate the spectral theory of the invariant Landau Hamiltonian $\\La^\\nu$ acting on the space ${\\mathcal{F}}^\\nu_{\\Gamma,\\chi}$ of $(\\Gamma,\\chi)$-automotphic functions on $\\C^n$, for given real number $\\nu>0$, lattice $\\Gamma$ of $\\C^n$ and a map $\\chi:\\Gamma\\to U(1)$ such that the triplet $(\\nu,\\Gamma,\\chi)$ satisfies a Riemann-Dirac quantization type condition. More precisely, we show that the eigenspace $ {\\mathcal{E}}^\\nu_{\\Gamma,\\chi}(\\lambda)=\\set{f\\in {\\mathcal{F}}^\\nu_{\\Gamma,\\chi}; \\La^\\nu f = \\nu(2\\lambda+n) f}$; $\\lambda\\in\\C,$ is non trivial if and only if $\\lambda=l=0,1,2, ...$. In such case, ${\\mathcal{E}}^\\nu_{\\Gamma,\\chi}(l)$ is a finite dimensional vector space whose the dimension is given explicitly. We show also that the eigenspace ${\\mathcal{E}}^\\nu_{\\Gamma,\\chi}(0)$ associated to the lowest Landau level of $\\La^\\nu$ is isomorphic to the space, ${\\mathcal{O}}^\\nu_{\\Gamma,\\chi}(\\C^n)$, of holomorphic functions on $\\C^n$ satisfying $$ g(z+\\gamma) = \\chi(\\gamma) e^{\\frac \\nu 2 |\\gamma|^2+\\nu\\scal{z,\\gamma}}g(z), \\eqno{(*)} $$ that we can realize also as the null space of the differential operator $\\sum\\limits_{j=1}\\limits^n(\\frac{-\\partial^2}{\\partial z_j\\partial \\bar z_j} + \\nu \\bar z_j \\frac{\\partial}{\\partial \\bar z_j})$ acting on $\\mathcal C^\\infty$ functions on $\\C^n$ satisfying $(*)$. ", "machine_abstract": "We introduce the notion of Landau (\\Gamma, \\chi)-automorphic functions of magnitude $\\nu$ for any integer $\\nu \\geq 0$ and show that they are holomorphic sections of certain line bundles over the complex flag manifold $\\mathbb{C}^{N} / \\Lambda^{(\\nu)}_{\\mathbb{N}} = \\mathrm{SL}_N(\\mathbb{C})/\\mathrm{Sp}_{2N - N_0}(\\mathbb{C})$.  We also prove an analogue of the Riemann-Roch theorem in this setting which allows us to compute the dimension of these spaces explicitly as a function of $\\nu$. Finally we give some examples of explicit bases for these spaces. This is joint work with Jens Franke. The results presented here were obtained while I was at the University of Hamburg under the supervision of Prof. Dr.-Ing. Henning Samtleben. In this article we study automorphic forms on the complex flag manifold $\\mathrm{SL}_N(\\mathbb{Z})/\\mathrm{Sp}^{2N-N_0}(\\mathbb{Z})$ where $N$ denotes the number of rows of the matrix representation of the group element. These automorphic forms can be viewed as holomorphic sections of certain vector bundles over the flag manifold whose fibers are given by homogeneous polynomials of degree $\\nu$ in $n$ variables. For each such polynomial there exists a unique irreducible representation of the Lie algebra $\\mathfrak{sl}_n$ into the space of endomorphisms of the corresponding fiber. Using this correspondence between representations and vector bundles one obtains a decomposition of the flag bundle into line bundles. Our main result shows how to construct holomorphic sections of these line bundles using theta series associated to the underlying lattice $\\Lambda_{(\\nu)}$.", "paraphrased_abstract": "This work is a joint work with Jens Franke. We have studied automorphic forms on the complex flag manifold (NamathbbZ) / Namathbz, where N is the number of rows in the matrix of the element group. We introduce the notion of Landau (Gamma, Chi) automorphic functions of nu$ in the space of an integer nugeq0. We have found that these automorphic forms are holomorphic sections of certain strands of vectors, over the flag manifold, whose fibers are represented by homogeneous polynomials of nu$ in the space of an inverse. In the case of a polynomial, there is a unique and irreducible representation of the Lie algebra mathfraksln into the space of endomorphisms of the corresponding strand. We introduce the concept of Landau, Gamma, chi, which is equivalent to a number of digits in nugeq 0. We also present an analogue of the Riemann-Roch theorem which allows us to determine the dimensions of these spaces as a function of nu", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3878, "title": "A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA Networks with Delay Constraints", "abstract": "  A game-theoretic framework is used to study the effect of constellation size on the energy efficiency of wireless networks for M-QAM modulation. A non-cooperative game is proposed in which each user seeks to choose its transmit power (and possibly transmit symbol rate) as well as the constellation size in order to maximize its own utility while satisfying its delay quality-of-service (QoS) constraint. The utility function used here measures the number of reliable bits transmitted per joule of energy consumed, and is particularly suitable for energy-constrained networks. The best-response strategies and Nash equilibrium solution for the proposed game are derived. It is shown that in order to maximize its utility (in bits per joule), a user must choose the lowest constellation size that can accommodate the user's delay constraint. Using this framework, the tradeoffs among energy efficiency, delay, throughput and constellation size are also studied and quantified. The effect of trellis-coded modulation on energy efficiency is also discussed. ", "machine_abstract": "We consider the problem of energy-efficient modulation for downlink transmissions over code-division multiple-access (CDMA) networks, where each user has an individual delay constraint and is equipped with a rechargeable battery that can be charged by harvesting ambient radio-frequency signals. We formulate this problem as a noncooperative game between users competing for limited power resources under their own constraints on transmission rates and delays. In particular, we show how to compute Nash equilibria of such games using convex optimization techniques. Our numerical results demonstrate significant gains in terms of both network throughput and energy efficiency compared to conventional schemes based on fixed-rate transmissions. The proposed approach also allows us to quantify tradeoffs among different performance metrics, including average packet delay, outage probability, and energy consumption per bit. This work was supported by NSF grants CNS-0932040 and CCF-0939370.     Code available at http://arxiv.org/abs/1206.5481", "paraphrased_abstract": "There is a problem of energy efficient modulation of downlinks on a CDMA network, in which each user has a set of delays and a rechargeable battery that can be charged by collecting the ambient radio frequency. We investigate the efficiency of this strategy and the efficiency of energy efficient transmissions with fixed rates, in a network which, as a rule, has its own limits and constraints. This method, in addition, gives us the ability to quantify the trade-off between delay and outage and energy consumption per bit. We present results that show considerable gains in both throughput and energy efficiency compared with conventional transmissions, namely fixed rates. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3704, "title": "Variation of fundamental constants: theory and observations", "abstract": "  Review of recent works devoted to the variation of the fundamental constants is presented including atomic clocks, quasar absorption spectra, and Oklo natural nuclear reactor data. Assuming linear variation with time we can compare different results. From the quasar absorption spectra:   $\\dot{\\mu}/\\mu=(1 \\pm 3) \\times 10^{-16}$ yr$^{-1}$. A combination of this result and the atomic clock results gives the best limt on variation of $\\alpha$: $\\dot{\\alpha}/\\alpha=(-0.8 \\pm 0.8) \\times 10^{-16}$ yr$^{-1}$. The Oklo natural reactor gives the best limit on the variation of $m_s/\\Lambda_{QCD}$ where $m_s$ is the strange quark mass.   Huge enhancement of the relative variation effects happens in transitions between close atomic, molecular and nuclear energy levels. We suggest several new cases where the levels are very narrow. Large enhancement of the variation effects is also possible in cold atomic and molecular collisions near Feshbach resonance. Massive bodies (stars or galaxies) can also affect physical constants. They have large scalar charge $S$ proportional to number of particles which produces a Coulomb-like scalar field $U=S/r$. This leads to a variation of the fundamental constants proportional to the gravitational potential, e.g. $\\delta \\alpha/ \\alpha = k_\\alpha \\delta (GM/ r c^2)$. We compare different manifestations of this effect.The strongest limit   $k_\\alpha +0.17 k_e= (-3.5\\pm 6) \\times 10^{-7}$. ", "machine_abstract": "The variation in the values of physical constants is one of the most important problems for modern physics, cosmology and astrophysics. The main goal of this work was to study the possible time variations of some fundamental constants (fine structure constant \u03b1, gravitational constant G, proton-to-electron mass ratio \u03bc) using different theoretical approaches as well as observational data on distant objects.  In our previous works we have shown that the observed redshifts of quasars can be explained by varying fine-structure constant with cosmic time. We also found that the evolution of the universe may lead to the change of other fundamental constants such as gravitational constant or proton-to-electron mass ratio. Here we present new results obtained within the framework of these models. First, we show how the model based on varying fine-structure constant explains the recent measurements of quasar absorption lines at high redshift performed by Webb et al. (1999) . Second, we discuss the possibility of explaining the Hubble parameter H0 = 72 \u00b1 8 km s-1 Mpc-1 measured recently by Freedman et al. (2001) using the same approach. Finally, we consider another class of models where all three fundamental constants vary simultaneously.", "paraphrased_abstract": "It was our main aim to study the possible changes in some of the fundamental constants (such as the fine structure constant, the gravitational constant G, the proton-to-electron mass ratio ) in the time course of a cosmic object. This was based on the theory that a change in the fine structure constant could be explained by the varying fine structure constant over time. We also found that the evolution of the universe may lead to the change of other fundamental constants, such as the gravitational constant or the proton-to-electron mass ratio. Then we will examine a new class of models, where all three are changed at once. This class of models is called the \"coherent\" class. We will explain here how the varying fine structure constant explained the recent observations of quasar absorption lines at high redshifts by Webb et al. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0758, "title": "Entangling Independent Photons by Time Measurement", "abstract": "  A quantum system composed of two or more subsystems can be in an entangled state, i.e. a state in which the properties of the global system are well defined but the properties of each subsystem are not. Entanglement is at the heart of quantum physics, both for its conceptual foundations and for applications in information processing and quantum communication. Remarkably, entanglement can be \"swapped\": if one prepares two independent entangled pairs A1-A2 and B1-B2, a joint measurement on A1 and B1 (called a \"Bell-State Measurement\", BSM) has the effect of projecting A2 and B2 onto an entangled state, although these two particles have never interacted or shared any common past[1,2]. Experiments using twin photons produced by spontaneous parametric down-conversion (SPDC) have already demonstrated entanglement swapping[3-6], but here we present its first realization using continuous wave (CW) sources, as originally proposed[2]. The challenge was to achieve sufficiently sharp synchronization of the photons in the BSM. Using narrow-band filters, the coherence time of the photons that undergo the BSM is significantly increased, exceeding the temporal resolution of the detectors. Hence pulsed sources can be replaced by CW sources, which do not require any synchronization[6,7], allowing for the first time the use of completely autonomous sources. Our experiment exploits recent progress in the time precision of photon detectors, in the efficiency of photon pair production by SPDC with waveguides in nonlinear crystals[8], and in the stability of narrow-band filters. This approach is independent of the form of entanglement; we employed time-bin entangled photons[9] at telecom wavelengths. Our setup is robust against thermal or mechanical fluctuations in optical fibres thanks to cm-long coherence lengths. ", "machine_abstract": "We propose an experiment to entangle two independent photons in the time domain, using only linear optical elements and single-photon detectors.  The scheme is based on measuring the arrival times of the photons at different locations with respect to each other. We show that this measurement can be used to generate entanglement between the photons without any post-selection or feed-forward operations. This method may find applications for quantum communication networks where it would allow one to distribute entangled states over large distances. Entanglement plays a central role in many areas of physics ranging from condensed matter systems [1] , atomic gases [2] , and trapped ions [3] to quantum information processing [4] . In particular, entanglement has been shown to be essential for quantum teleportation [5] , superdense coding [6] , quantum key distribution [7] , and quantum computing [8] . In recent years there have been several proposals to create entanglement between distant particles [9] - [11] . However, most schemes require either nonlinear interactions [12] , which are difficult to implement experimentally [13] , or postselection [14] , which introduces additional noise into the system [15] . Recently, we proposed a new scheme [16] to produce entanglement between remote particles using only linear optics [17] and single photon detection [18] . Our approach relies on performing measurements on the arrival times of the particles at different locations [19] . Here we present detailed calculations showing how our proposal works as well as its experimental feasibility [20] .  Figure 1 shows a schematic diagram of our setup. Two identical sources emit pairs of photons (red) towards Alice's station A and Bob's station B respectively [21] . Each source consists of a pulsed laser [22] generating pairs of photons via spontaneous parametric down-conversion [23] . These photons travel through separate paths until they reach stations A and B [24] . At these stations, Alice and Bob perform measurements on their respective photons [25] . They measure the arrival times tA and tB [26] of...", "paraphrased_abstract": "The system consists of two identical beams which are radiated towards Alice\u2019s station A and Bob\u2019s station B, respectively. Each beam is a pulsed laser which produces two light waves via a receptive parametric down-conversion, which in turn generates a series of light waves which, as a result, reach their respective stations A and B. At each station Alice and Bob take measurements on their respective photons. These measurements can be used to form entanglements between the two particles, and this method can be employed for distributing the entanglements across large distances. In recent years there have been several attempts to create entanglements between distant particles, including condensed matter, atoms, and trapped ions, and quantum computing. The use of entanglement in various fields of physics has been shown to be essential in the operation of quantum communication, superdense coding, quantum key distribution, and quantum computing. In this we propose an experiment to disentangle two independent particles in the time domain, using only linear optics and single photon detectors. Our experiment consists in measuring the arrival times of the two particles in the same direction, with different times. We show that this measure is able to disentangle the two particles without post-selection or feedforward. We are working on a new experiment, involving only linear optic", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0211, "title": "Red Sequence Cluster Finding in the Millennium Simulation", "abstract": "  We investigate halo mass selection properties of red-sequence cluster finders using galaxy populations of the Millennium Simulation (MS). A clear red sequence exists for MS galaxies in massive halos at redshifts z < 1, and we use this knowledge to inform a cluster-finding algorithm applied to 500 Mpc/h projections of the simulated volume. At low redshift (z=0.4), we find that 90% of the clusters found have galaxy membership dominated by a single, real-space halo, and that 10% are blended systems for which no single halo contributes a majority of a cluster's membership. At z=1, the fraction of blends increases to 22%, as weaker redshift evolution in observed color extends the comoving length probed by a fixed range of color. Other factors contributing to the increased blending at high-z include broadening of the red sequence and confusion from a larger number of intermediate mass halos hosting bright red galaxies of magnitude similar to those in higher mass halos. Our method produces catalogs of cluster candidates whose halo mass selection function, p(M|\\Ngal,z), is characterized by a bimodal log-normal model with a dominant component that reproduces well the real-space distribution, and a redshift-dependent tail that is broader and displaced by a factor ~2 lower in mass. We discuss implications for X-ray properties of optically selected clusters and offer ideas for improving both mock catalogs and cluster-finding in future surveys. ", "machine_abstract": "We present an algorithm for finding galaxy clusters using only photometric redshifts and no spectroscopic information, based on identifying overdensities of galaxies with similar colours. We apply this method to the Millennium simulation (Springel et al., 2005) and compare our results against those obtained by applying the same technique to mock catalogues constructed from semi-analytic models of galaxy formation within dark matter haloes. The comparison shows that we are able to recover most of the true cluster population at z < 1.5, but find significant contamination due to projection effects beyond this redshift. This is mainly caused by the fact that the colour-magnitude relation becomes less tight as one moves towards higher redshifts. Our analysis also reveals that there exists a large number of spurious detections which can be removed by imposing additional constraints such as requiring all candidate members to have magnitudes brighter than some threshold value or by restricting ourselves to systems whose total luminosity exceeds a certain limit.", "paraphrased_abstract": "This analysis shows that a large number of false detections exist, which can be removed by imposing additional constraints, such as the requirement that all candidates of the test must be brighter than a threshold value or to restrict the range of system brightness to one or two peaks. Our approach is based on a method for finding a cluster with only the photometric redshift and no spectral information, and on a selection of a sufficient number of galaxies with similar colors. We present a method for the discovery of clusters, based on the analysis of semianalytical models of galaxy formation in haloes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0556, "title": "Effective conservation of energy and momentum algorithm using switching potentials suitable for molecular dynamics simulation of thermodynamical systems", "abstract": "  During a crossover via a switching mechanism from one 2-body potential to another as might be applied in modeling (chemical) reactions in the vicinity of bond formation, energy violations would occur due to finite step size which determines the trajectory of the particles relative to the potential interactions of the unbonded state by numerical (e.g. Verlet) integration. This problem is overcome by an algorithm which preserves the coordinates of the system for each move, but corrects for energy discrepancies by ensuring both energy and momentum conservation in the dynamics. The algorithm is tested for a hysteresis loop reaction model with an without the implementation of the algorithm. The tests involve checking the rate of energy flow out of the MD simulation box; in the equilibrium state, no net rate of flows within experimental error should be observed. The temperature and pressure of the box should also be invariant within the range of fluctuation of these quantities. It is demonstrated that the algorithm satisfies these criteria. ", "machine_abstract": "We present an effective method to conserve the total energy and linear momentum in molecular dynamics (MD) simulations by introducing two types of potentials: one is used during the MD run, while another is only used when calculating the forces on particles at each time step. The latter type of potentials are switched off after being calculated so that they do not affect the subsequent MD trajectories. We show how this scheme can be implemented into existing MD codes with minimal modifications. In addition, we demonstrate its effectiveness through several examples including liquid argon, water clusters, and carbon nanotubes. Our results indicate that our new scheme conserves both energy and momentum very well even though it does not require any additional computational cost compared to conventional schemes. This work was supported by the National Natural Science Foundation of China under Grants No. 10874145 and No. 10934011 . Keywords: Energy-momentum conservation; Switching potentials; Molecular dynamics", "paraphrased_abstract": "This work was supported by the National Natural Science Foundation of China through Grant No. 10874145 and No. 10934011, respectively. The enzymatic reaction, which has a vanishing energy, is the vanishing momentum. This new method of retaining momentum, is very useful in the kinetic theory. Two different kinds of potentials are used in the simulations, one for the formation of the particles, and the other for the calculation of the forces on the particles. The second kind is switched off after it is calculated. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2025, "title": "On the Performance Evaluation of Encounter-based Worm Interactions Based on Node Characteristics", "abstract": "  An encounter-based network is a frequently disconnected wireless ad-hoc network requiring nearby neighbors to store and forward data utilizing mobility and encounters over time. Using traditional approaches such as gateways or firewalls for deterring worm propagation in encounter-based networks is inappropriate. Because this type of network is highly dynamic and has no specific boundary, a distributed counter-worm mechanism is needed. We propose models for the worm interaction approach that relies upon automated beneficial worm generation to alleviate problems of worm propagation in such networks. We study and analyze the impact of key mobile node characteristics including node cooperation, immunization, on-off behavior on the worm propagations and interactions. We validate our proposed model using extensive simulations. We also find that, in addition to immunization, cooperation can reduce the level of worm infection. Furthermore, on-off behavior linearly impacts only timing aspect but not the overall infection. Using realistic mobile network measurements, we find that encounters are non-uniform, the trends are consistent with the model but the magnitudes are drastically different. Immunization seems to be the most effective in such scenarios. These findings provide insight that we hope would aid to develop counter-worm protocols in future encounter-based networks. ", "machine_abstract": "In this work, we propose an encounter based worm interaction model to evaluate the performance of different node characteristics in terms of their ability to detect and prevent worms spreading over mobile ad hoc networks (MANETs). We consider two types of nodes with distinct capabilities for detecting and preventing worms: normal nodes that are vulnerable to infection by worms but can detect them using signature detection techniques; and immune nodes which have no vulnerability to worm infections but can prevent worm propagation through quarantine mechanisms. The proposed model is used to study how these two types of nodes interact when they meet each other during network operation. In particular, our results show that: 1) Immune nodes play a significant role in reducing the number of infected nodes as well as the total number of encounters between susceptible and infectious nodes; 2) Immune nodes should be deployed at strategic locations within MANETs; 3) Immune nodes should not only focus on quarantining infectious nodes but also on isolating suspicious nodes; 4) Immune nodes should use both signature detection and quarantine mechanisms simultaneously to achieve better performance against worm propagation; 5) Immune nodes should adopt dynamic quarantine strategies instead of static ones since static quarantine may lead to unnecessary isolation of legitimate nodes.", "paraphrased_abstract": "In this study, we investigate the relationship of two kinds of worms to each other and investigate how these two types interact. We consider the two types of worms as vulnerable to worms, but which detect them by means of signatures; and the two types of worms that are resistant to worms but which can withstand worms through their quarantine mechanisms. Immune worms play a major role in reducing the number of worms and the number of encounters between worms; 3) Immune worms are placed at strategic locations in MANETs; 3) Immune worms are placed at a strategic site in which they are not only quarantined, but isolate suspicious worms; 4) Immune worms are used to combine with signature-detaining and quarantine in order to reduce worm spread; 6) Immune worms are regarded as dynamically and dynamically, for static quarantine may lead to unnecessary isolation of legitimate worms. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0846, "title": "Resonating singlet valence plaquettes", "abstract": "  We consider the simplest generalizations of the valence bond physics of SU(2) singlets to SU(N) singlets that comprise objects with N sites -- these are SU(N) singlet plaquettes with N=3 and N=4 in three spatial dimensions. Specifically, we search for a quantum mechanical liquid of such objects -- a resonating singlet valence plaquette phase that generalizes the celebrated resonating valence bond phase for SU(2) spins. We extend the Rokhsar-Kivelson construction of the quantum dimer model to the simplest SU(4) model for valence plaquette dynamics on a cubic lattice. The phase diagram of the resulting quantum plaquette model is analyzed both analytically and numerically. We find that the ground state is solid everywhere, including at the Rokhsar-Kivelson point where the ground state is an equal amplitude sum. By contrast, the equal amplitude sum of SU(3) singlet triangular plaquettes on the face centered cubic lattice is liquid and thus a candidate for describing a resonating single valence plaquette phase, given a suitably defined local Hamiltonian. ", "machine_abstract": "We study the ground state properties and excitations in the spin-1/2 Heisenberg antiferromagnet on the square lattice with nearest-neighbor interactions only, using exact diagonalization (ED) for small clusters up to 12 sites as well as density-matrix renormalization group (DMRG) calculations for larger systems. We find that the ground-state energy per site is lower than the classical value by about 0.25J, where J denotes the exchange coupling constant between neighboring spins. The magnetic susceptibility shows Curie-Weiss behavior at high temperatures but decreases rapidly below T = 2J/3. This indicates strong quantum fluctuations which are also reflected in the low-temperature dependence of the specific heat. In addition we observe an unusual peak structure in the spin-spin correlation function S(q). For q along the principal axes of the Brillouin zone this peak has its maximum at q = \u03c0 while it shifts towards smaller values when approaching the diagonals.", "paraphrased_abstract": "The magnetic field has the Curie-Weiss pattern at high temperatures, but declines rapidly below the value of two-thirds J. This indicates that there are significant quantum fluctuations in the low temperature sensitivity. The magnetic susceptibility to the magnetic field is of Curie-Weiss order at high temperatures, but decreases rapidly below two-thirds J. This indicates strong quantum fluctuations which are also reflected in the low temperature specific heat. We are examining the energy per site at the Heisenberg antiferromagnet, in the square ring, with nearest neighbours, to 12 sites, with ED as an initial diagonal, for small clusters of 12 sites, and DMRG as an extended DMRG for larger systems. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2783, "title": "Nonlinear Evolution of Baryon Acoustic Oscillations", "abstract": "  We study the nonlinear evolution of the baryon acoustic oscillations (BAO) in the dark matter power spectrum and correlation function using renormalized perturbation theory (RPT). In a previous paper we showed that RPT successfully predicts the damping of acoustic oscillations; here we extend our calculation to the enhancement of power due to mode-coupling. We show that mode-coupling generates additional oscillations that are out of phase with those in the linear spectrum, leading to shifts in the scales of oscillation nodes defined with respect to a smooth spectrum. When Fourier transformed, these out of phase oscillations induce percent-level shifts in the acoustic peak of the two-point correlation function. We present predictions for these shifts as a function of redshift; these should be considered as a robust lower limit to the more realistic case that includes in addition redshift distortions and galaxy bias. We show that these nonlinear effects occur at very large scales, leading to a breakdown of linear theory at scales much larger than commonly thought. We discuss why virialized halo profiles are not responsible for these effects, which can be understood from basic physics of gravitational instability. Our results are in excellent agreement with numerical simulations, and can be used as a starting point for modeling BAO in future observations. To meet this end, we suggest a simple physically motivated model to correct for the shifts caused by mode-coupling. ", "machine_abstract": "We study the nonlinear evolution of baryonic acoustic oscillations (BAO) in an expanding universe using numerical simulations with N-body particles and hydrodynamic gas dynamics. We find that BAOs are suppressed by nonlinear gravitational clustering, but they can be recovered by applying a Wiener filter to the density field at each redshift. The reconstructed power spectrum agrees well with linear theory predictions on large scales where the effect is negligible. On small scales, however, we observe significant deviations between the two spectra due to nonlinear effects such as shell crossing and virialization. These results suggest that it may be possible to use BAOs as standard rulers for cosmological distance measurements even when the underlying matter distribution becomes highly nonlinear. In this work, we present our main findings below.  1 Introduction  Baryons play important roles in galaxy formation through their interactions with dark matter. For example, observations show that galaxies form around peaks of the primordial density fluctuations which grow into massive halos via gravitational instability. Therefore, understanding how baryons evolve in time and space is crucial for studying galaxy formation processes.  In recent years, there has been growing interest in measuring the large-scale structure of the Universe using baryonic tracers like neutral hydrogen or stars. One promising method involves tracing the spatial distribution of these objects back in time using spectroscopic surveys. This technique allows us to measure the statistical properties of the cosmic web, including its geometry and topology, over a wide range of redshifts.   The most prominent feature observed in the measured correlation functions of various types of baryonic tracers is known as \"baryonic acoustic oscillation\" (BAO). It refers to periodic wiggles seen in the power spectrum of the tracer population caused by sound waves propagating through the early universe before decoupling [see e.g., 1] . Since the amplitude of the BAO signal depends only weakly on the physical state of the medium, it provides a robust way to probe the expansion history of the universe independent of other cosmological parameters [2] . Recently, several groups have reported detections of the BAO signature in the correlation function of Lyman", "paraphrased_abstract": "There is a feature, in the measured correlation of the baryonic tracers known as \u201cbaryonic acoustic oscillations\u201d that is known as \u201cbaryonic acoustic oscillations\u201d (BAO) that is peculiar to the tracing of baryonic tracers, such as neutral hydrogen or stars. The tracing of baryonic acoustic oscillations by the traces of light, such as the waves which travel through the early universe, is a signal that enables us to determine the expansion history of the universe independent of other parameters. The reconstructed power spectrum agrees with the linear theories, where the reconstructed power spectrum agrees with the underlying structure, but the nonlinearity of the distribution is observable. It is proposed that the spectral properties of the cosmic web, the geometry and topology, may be studied over a wide range of redshifts. We show that the nonlinearity of the BAOs is suppressed by nonlinear clustering, but we can recover it by applying a Wiener filter to the density field at each redshift. The reconstructed power spectrum agrees with the linear theory, but on the small scale it is possible to observe considerable deviations in the power spectrum, due to nonlinearity of shell-crossing and virialization. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1162, "title": "The multiple viewpoints as approach to information retrieval within collaborative development context", "abstract": "  Nowadays, to achieve competitive advantage, the industrial companies are considering that success is sustained to great product development. That is to manage the product throughout its entire lifecycle. Achieving this goal requires a tight collaboration between actors from a wide variety of domains, using different software tools producing various product data types and formats. The actors' collaboration is mainly based on the exchange /share product information. The representation of the actors' viewpoints is the underlying requirement of the collaborative product development. The multiple viewpoints approach was designed to provide an organizational framework following the actors' perspectives in the collaboration, and their relationships. The approach acknowledges the inevitability of multiple integration of product information as different views, promotes gathering of actors' interest, and encourages retrieved adequate information while providing support for integration through PLM and/or SCM collaboration. In this paper, a multiple viewpoints representation is proposed. The product, process, organization information models are discussed. A series of issues referring to the viewpoints representation are discussed in detail. Based on XML standard, taking electrical connector as an example, an application case of part of product information modeling is stated. ", "machine_abstract": "The main goal of this research is the study and analysis of how different users can contribute in an open source software project, by sharing their knowledge through comments on issues or pull requests (PRs). The authors propose a new method for analyzing user participation based on the number of views that each comment receives during its life cycle. This work also presents a novel technique for identifying relevant comments using text mining techniques applied over the textual content of PRs. Finally, it proposes a ranking algorithm which takes into account both the relevance score assigned to each comment and the number of times they have been viewed. In order to evaluate these proposals, we conducted experiments with data collected from GitHub repositories related to two well-known open-source projects. We found out that our proposed methods are able to identify relevant comments more accurately than other state-of-the-art approaches. Moreover, we observed that the proposed ranking algorithm improves the results obtained when only considering the relevance scores.", "paraphrased_abstract": "He then analyzed the contents of the GitHub repository, and compared it with the contents of the PRs. Moreover, he found that the proposed method, when applied to the text of the PRs, has better results than the other methods. To investigate this work, he conducted experiments with GitHub data and the content of a few popular projects. This work presented a new method to determine the number of views received by each comment, and a new method for estimating the number of views of each comment. He also developed a new method to estimate the number of views received during the life of the comment, by means of text mining techniques. The authors presented a new method for estimating the number of views of each comment, based on the number of views received. The method, in turn, improved the results obtained by only analyzing the relevance score. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3584, "title": "A Search for Planets Transiting the M Dwarf Debris Disk Host, AU Microscopii", "abstract": "  We present high cadence, high precision multi-band photometry of the young, M1Ve, debris disk star, AU Microscopii. The data were obtained in three continuum filters spanning a wavelength range from 4500\\AA to 6600\\AA, plus H$\\alpha$, over 28 nights in 2005. The lightcurves show intrinsic stellar variability due to starspots with an amplitude in the blue band of 0.051 magnitudes and a period of 4.847 days. In addition, three large flares were detected in the data which all occur near the minimum brightness of the star. We remove the intrinsic stellar variability and combine the lightcurves of all the filters in order to search for transits by possible planetary companions orbiting in the plane of the nearly edge-on debris disk. The combined final lightcurve has a sampling of 0.35 minutes and a standard deviation of 6.8 millimags (mmag). We performed Monte Carlo simulations by adding fake transits to the observed lightcurve and find with 95% significance that there are no Jupiter mass planets orbiting in the plane of the debris disk on circular orbits with periods, P $\\le 5$ days. In addition, there are no young Neptune-like planets (with radii 2.5$\\times$ smaller than the young Jupiter) on circular orbits with periods, P $\\le 3$ days. ", "machine_abstract": "We report on our search for planets transiting the debris disk host star AU Mic (M3V). We observed two transit events with Spitzer Space Telescope's IRAC camera at 3.6 and 4.5 microns in 2009-2010 using the same technique as described by Charbonneau et al. (2005) . The first event was detected only in one bandpass while the second event showed no evidence of a transit signal above the noise level. Using these data we place upper limits on the radius R p /R s < 0.11 and mass m sin i < 1.1 MJup for any planet orbiting AU Mic within an orbital period P = 2 days. These results are consistent with previous studies that have found few or none giant planets around low-mass stars like AU Mic.     Keywords: Planetary systems - Formation, evolution, structure; Exoplanet detection techniques - Transit photometry", "paraphrased_abstract": "The authors report about their search for the star AU Mic. They describe a search for the planets of the debris disk host star AU Mic (M3V). They describe two transits of AU Mic with the Spitzer Space Telescope, at 3.6 and 4.5 microns, which we observed in 2009 and 2010, using the same method as Charbonneau et al. (2005). The first transit was observed only in one band, the second was not observed in any band. These results are consistent with previous observations, which have found no giant planets around low-mass stars like AU Mic. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2605, "title": "Mapping the Cosmological Confidence Ball Surface", "abstract": "  We present a new technique to compute simultaneously valid confidence intervals for a set of model parameters. We apply our method to the Wilkinson Microwave Anisotropy Probe's (WMAP) Cosmic Microwave Background (CMB) data, exploring a seven dimensional space (tau, Omega_DE, Omega_M, omega_DM, omega_B, f_nu, n_s). We find two distinct regions-of-interest: the standard Concordance Model, and a region with large values of omega_DM, omega_B and H_0. This second peak in parameter space can be rejected by applying a constraint (or a prior) on the allowable values of the Hubble constant. Our new technique uses a non-parametric fit to the data, along with a frequentist approach and a smart search algorithm to map out a statistical confidence surface. The result is a confidence ``ball'': a set of parameter values that contains the true value with probability at least 1-alpha. Our algorithm performs a role similar to the often used Markov Chain Monte Carlo (MCMC), which samples from the posterior probability function in order to provide Bayesian credible intervals on the parameters. While the MCMC approach samples densely around a peak in the posterior, our new technique allows cosmologists to perform efficient analyses around any regions of interest: e.g., the peak itself, or, possibly more importantly, the 1-alpha confidence surface. ", "machine_abstract": "We present an algorithm for mapping the cosmological confidence ball surface, which is defined as the set of all points in parameter space that are consistent with current observations at some specified level of statistical significance.  The algorithm uses Monte Carlo Markov Chains to generate samples from the posterior probability distribution function (PDF) and then calculates the likelihoods of these samples using a modified version of CosmoMC.   We apply this method to two different data sets - WMAP7+BAO+H0 and Planck+WP+highL+BAO+H0+JLA - and find that our results agree well with those obtained by other authors who have used similar techniques.  In addition we show how one can use this technique to calculate the volume enclosed within the cosmological confidence ball. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.", "paraphrased_abstract": "We also show how one can calculate the volume of the cosmological confidence ball. The work was performed under the auspices of the Lawrence Livermore National Laboratory, under the contract DE-AC52-07NA27344. We introduce an algorithm for calculating the surface of the cosmological confidence ball, which is a set of all points in the parameter space consistent with the present observations. The algorithm uses Monte Carlo Markov Chains to generate samples from the posterior probability distribution function (PDF), and then calculates the likelihoods of these samples with a modified version of CosmoMC. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.223, "title": "Strings on AdS_2 and the High-Energy Limit of Noncritical M-Theory", "abstract": "  Noncritical M-theory in 2+1 dimensions has been defined as a double-scaling limit of a nonrelativistic Fermi liquid on a flat two-dimensional plane. Here we study this noncritical M-theory in the limit of high energies, analogous to the \\alpha'\\to\\infty limit of string theory. In the related case of two-dimensional Type 0A strings, it has been argued that the conformal \\alpha'\\to\\infty limit leads to AdS_2 with a propagating fermion whose mass is set by the value of the RR flux. Here we provide evidence that in the high-energy limit, the natural ground state of noncritical M-theory similarly describes the AdS_2\\times S^1 spacetime, with a massless propagating fermion. We argue that the spacetime effective theory in this background is captured by a topological higher-spin extension of conformal Chern-Simons gravity in 2+1 dimensions, consistently coupled to a massless Dirac field. Intriguingly, the two-dimensional plane populated by the original nonrelativistic fermions is essentially the twistor space associated with the symmetry group of the AdS_2\\times S^1 spacetime; thus, at least in the high-energy limit, noncritical M-theory can be nonperturbatively described as a \"Fermi liquid on twistor space.\" ", "machine_abstract": "We study string theory in anti-de Sitter space-time (AdS) with two dimensions, which is dual to noncritical M-theory at high energies. We show that this duality can be understood as an equivalence between certain two-dimensional conformal field theories (CFTs). The CFT corresponding to strings on AdS_2 has central charge c = 3l/2G, where l is the radius of curvature of AdS_2 and G is Newton's constant. This CFT contains a free boson X_0 and a fermion $\\psi$; we find that the partition function $Z(\\beta)$ for these fields satisfies the following functional equation:  $$Z(\\beta) = Z(1/\\beta) \\exp\\left[-\\frac{\\beta^2}{2}\\ln\\left(1-e^{-2\\pi i/3}\\right)\\right].$$ Here $1/\\beta$ plays the role of the coupling constant $\\lambda$ in the worldsheet action. In particular, when $1/\\beta \\to 0$, the partition function becomes $Z(0) = 1$, while it vanishes exponentially fast if $1/\\beta \\to \\infty$.", "paraphrased_abstract": "\u201cThe main function of the sub-field of AdS is that of a free boson X and a fermion psi; we have shown that the sub-field of AdS is a partition function which is a function of the division function Z(beta) \u2013 e5442ln44pi4434pi44344525535555pi4444444444444444444444444556722 23 555533right3>42564642343344156,54510101106344n514551056645", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3045, "title": "A possible explanation why the Theta+ is seen in some experiments and not in others", "abstract": "  To understand the whole set of positive and null data on the Theta+(1530)-production, we suggest the hypothesis that multiquark hadrons are mainly generated from many-quark states, which emerge either as short-term hadron fluctuations, or as hadron remnants in hard processes. This approach allows us to describe both non-observation of the Theta+ in current null experiments and peculiar features of its production in positive experiments. Further, we are able to propose new experiments that might be decisive for the problem of the Theta+ existence. Distributions of the Theta+ in such experiments can give important information both on higher Fock components of conventional hadrons and about structure and hadronization properties of hadron remnants produced in hard processes. We also explain that description of multiquark hadrons may require a modified form of the constituent quark model, with quark masses and couplings being intermediate between their values for the familiar constituent quarks and the current ones. ", "machine_abstract": "The theta + (\u03b8+) effect has been observed by many authors, but it was never explained satisfactorily.  In this work we propose an explanation for its origin based on the fact that \u03b8+ appears only when there are two or more particles with opposite spin states present at the same time. We show how to calculate the probability amplitude of finding such pairs using Feynman diagrams. The results obtained agree well with those reported experimentally. This article is available from: http://arxiv.org/abs/1401.5189. The theta + (\u03b8+ ) effect [1] , which consists of a small increase in the intensity of the scattered beam as compared to the unscattered one, has been observed by several authors [2] - [4] . However, no satisfactory explanation has yet been given for its origin [5] . In this work we propose an interpretation for the appearance of \u03b8+ based on the fact that it occurs only if there are simultaneously two or more particles with different spins [6] . To demonstrate our hypothesis, we use Feynman's diagrammatic method [7] to calculate the probability amplitudes corresponding to these processes [8] .", "paraphrased_abstract": "The effect of the theta- (+) is usually a small increase in the intensity of a reflected beam compared with the unscathed one. However, no clear explanation is given for the origin of the phenomenon. We propose an interpretation of the phenomenon of +, based on the fact that + appears only when two or more particles have different spins. We show that we can calculate the amplitude of these processes by means of the Feynman diagrams. We show how to calculate the probability of such pairs by calculating the amplitude of the probability. The result is agreeable with those of the experimental experiment. The + effect has been observed by many authors, but its origin is not fully explained. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0963, "title": "Nova Geminorum 1912 and the Origin of the Idea of Gravitational Lensing", "abstract": "  Einstein's early calculations of gravitational lensing, contained in a scratch notebook and dated to the spring of 1912, are reexamined. A hitherto unknown letter by Einstein suggests that he entertained the idea of explaining the phenomenon of new stars by gravitational lensing in the fall of 1915 much more seriously than was previously assumed. A reexamination of the relevant calculations by Einstein shows that, indeed, at least some of them most likely date from early October 1915. But in support of earlier historical interpretation of Einstein's notes, it is argued that the appearance of Nova Geminorum 1912 (DN Gem) in March 1912 may, in fact, provide a relevant context and motivation for Einstein's lensing calculations on the occasion of his first meeting with Erwin Freundlich during a visit in Berlin in April 1912. We also comment on the significance of Einstein's consideration of gravitational lensing in the fall of 1915 for the reconstruction of Einstein's final steps in his path towards general relativity. ", "machine_abstract": "The gravitational lensing effect was first predicted by Einstein in 1915, but it took another 50 years before the first confirmed observation could be made.  In this talk I will describe how one particular event - Nova Geminorum 1912 - played an important role in motivating Einstein to develop his theory of general relativity.   The talk is based on my recent book \"The First Lensing Event\" (Cambridge University Press).   Gravitational lensing occurs when light passes close to a massive object such as a galaxy or cluster of galaxies.  This causes the light rays to bend towards each other so that distant objects appear magnified and distorted.    In 1919, Einstein published his famous field equations which described gravity as curvature in space-time caused by matter/energy.  These equations were able to explain many phenomena including the perihelion precession of Mercury's orbit around the Sun.   However, there remained some puzzles about these equations.  For example, they did not predict any bending of light passing near the Sun.  Einstein realised he needed to modify his original field equations if he wanted them to account for gravitational lensing effects.  He introduced what we now call the cosmological constant into his new set of equations.  However, despite considerable effort over several decades, no-one has been able to measure the value of this constant with sufficient accuracy to test whether Einstein's prediction is correct.    This talk describes how astronomers finally managed to observe gravitational lensing in 1979 using observations of a distant quasar known as Q0957+561A-B.  It also explains why the discovery of gravitational lenses led to the awarding of the Nobel Prize in Physics to Roger Blandford and Roman Vishniac in 1997.", "paraphrased_abstract": "When light passes near a large object, such as a galaxy or a cluster of galaxies, it bends towards each other and magnifies distant objects. The gravitational lensing phenomenon was first predicted by Einstein in 1915, but it took another 50 years before the first observation was verified. This talk will show how, in 1979, astronomers successfully managed to discover the gravitational lensing phenomenon by using the observations of a distant quasar, Q-0957+561B. The talk will explain how astronomers finally managed to find out by means of a distant quasar named Q-0957+561B, the quasar of which was named Q-0957+561B. It will explain how the gravitational lensing phenomenon was first predicted by Einstein in 1915, but it took another fifty years to find out. Einstein understood that he had to modify the equations of the original field, to account for the gravitational lensing. The new equations contained what is now called the cosmological constant. But none of them could be measured with sufficient precision to establish whether Einstein's predictions were correct. The first observations of gravitational lensing in the year 1919 were made in the famous field equations of the field, which describe the movement of matter and energy in space. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0191, "title": "Intricate Knots in Proteins: Function and Evolution", "abstract": "  A number of recently discovered protein structures incorporate a rather unexpected structural feature: a knot in the polypeptide backbone. These knots are extremely rare, but their occurrence is likely connected to protein function in as yet unexplored fashion. Our analysis of the complete Protein Data Bank reveals several new knots which, along with previously discovered ones, can shed light on such connections. In particular, we identify the most complex knot discovered to date in human ubiquitin hydrolase, and suggest that its entangled topology protects it against unfolding and degradation by the proteasome. Knots in proteins are typically preserved across species and sometimes even across kingdoms. However, we also identify a knot which only appears in some transcarbamylases while being absent in homologous proteins of similar structure. The emergence of the knot is accompanied by a shift in the enzymatic function of the protein. We suggest that the simple insertion of a short DNA fragment into the gene may suffice to turn an unknotted into a knotted structure in this protein. ", "machine_abstract": "The authors present an overview of the role that knots play in proteins, with particular emphasis on their function and evolution.  They discuss how protein knots are formed by covalent bonds between amino acids (the building blocks of proteins) as well as non-covalent interactions such as hydrogen bonding.  The authors also describe how different types of knots can be classified based upon their topology.   Finally they explain why it is important to study knots in proteins since these structures may have evolved for specific functions or because they provide stability against proteolysis (breakdown into smaller peptides).    This article was originally published on BioMed Central. It has been re-posted here under Creative Commons License 3.0. Protein knots are intriguing structural motifs found within many naturally occurring polypeptides. These knotted conformations result from noncovalent interactions among residues along the backbone chain combined with covalent cross-linkages at certain positions. In this review we summarize our current understanding about the formation mechanisms of various knot topologies observed in nature. We then highlight recent advances made toward characterizing the functional roles played by protein knots.", "paraphrased_abstract": "They describe the formation of knots of many naturally occurring polypeptides. These knots have formed in the backbone, and the fusion of covalent bonds is found in the top of some sulfonyl residues. In the present article, we summarize the present understanding of the formation of knots in the proteins, with special attention to the function of the knots and to the evolution of them. The authors then describe how the knots in proteins are formed, both by covalent bonds between amino acids (the building blocks of proteins), and by hydrogen bonds. The authors also give an account of the classification of knots by topology. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0287, "title": "The Wide Field Spectrograph (WiFeS)", "abstract": "  This paper describes the Wide Field Spectrograph (WiFeS) under construction at the Research School of Astronomy and Astrophysics (RSAA) of the Australian National University (ANU) for the ANU 2.3m telescope at the Siding Spring Observatory. WiFeS is a powerful integral field, double-beam, concentric, image-slicing spectrograph designed to deliver excellent thoughput, wavelength stability, spectrophotometric performance and superb image quality along with wide spectral coverage throughout the 320-950 nm wavelength region. It provides a 25x38 arcsec. field with 0.5 arcsec. sampling along each of twenty five 38X1 arcsec slitlets. The output format is optimized to match the 4096x4096 pixel CCD detectors in each of two cameras individually optimized for the blue and the red ends of the spectrum, respectively. A process of \"interleaved nod-and-shuffle\" will be applied to permit quantum noise-limited sky subtraction. Using VPH gratings, spectral resolutions of 3000 and 7000 are provided. The full spectral range is covered in a single exposure at R=3000, and in two exposures in the R=7000 mode. The use of transmissive coated optics, VPH gratings and optimized mirror coatings ensures a throughput (including telescope atmosphere and detector) > 30% over a wide spectral range. The concentric image-slicer design ensures an excellent and uniform image quality across the full field. To maximize scientific return, the whole instrument is configured for remote observing, pipeline data reduction, and the accumulation of calibration image libraries. ", "machine_abstract": "The WiFeS instrument is an integral field spectrograph for the Australian National University 2.3m telescope at Siding Spring Observatory, Australia.  It has been in operation since 2005 and was designed to provide high quality optical spectroscopy over a wide range of wavelengths with minimal overheads.   The WiFeS instrument consists of two cameras that are mounted on a common baseplate which sits inside a vacuum chamber attached to the Cassegrain focus of the ANU 2.3m telescope.    Each camera contains a lenslet array that produces a set of images across its focal plane.  These images can be combined into a single data cube using software developed by Dopita et al. (2007).    This data cube provides information about both spatial position and wavelength along each line-of-sight through the object being observed.  In addition to this capability, WiFeS also offers several other advantages including:  - High throughput due to the use of dichroic beam splitters and fibre optic coupling between the lenses and detectors.  - Flexible observing modes ranging from fully automated observations to manual control via remote desktop interface.  - Fast readout times allowing multiple targets to be observed simultaneously or rapid cycling between different targets during a night's observation run.  - Low maintenance requirements as it does not require any moving parts within the vacuum enclosure.  - Excellent stability thanks to the design of the optics bench and the thermal isolation provided by the vacuum enclosure.  - Large format detector arrays providing excellent signal-to-noise ratio even under poor seeing conditions.  - Robust mechanical design ensuring long-term reliability...", "paraphrased_abstract": "Besides, it has many other advantages: - High throughput due to dichroic beam splitters, - fiber optic coupling between the lenses and the detectors; - Optical resolving, - stable operation; - Adaptable spectrometers for a wide range of wavelengths; - Flexible monitoring modes: automatic observation; remote control; - fast reading times, allowing multiple images to be observed simultaneously, or - rapid cycling of different objects in a night's observation. - Strong mechanical design ensuring long-term reliability... The WiFeS instrument is a field spectrometer for the Siding Spring Observatory, Australia, the 2.3m telescope of the Australian National University, a field spectrometer of the 2.3m telescope, which was built in 2005 and is used in the field for spectroscopy of the 2.3m telescope. The instrument contains two cameras, mounted on a common plate, in a vacuum chamber attached to the Cassegrain focus of the ANU telescope, a pair of lenses, which are arranged in a single plane, which are combined into a single data-cube in the form of a square cube, which provides information about the position and the wavelength of the object in which it is observed. The software of this system is Dopita et al. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0559, "title": "Formation and Collapse of Quiescent Cloud Cores Induced by Dynamic Compressions", "abstract": "  (Abridged) We present numerical hydrodynamical simulations of the formation, evolution and gravitational collapse of isothermal molecular cloud cores. A compressive wave is set up in a constant sub-Jeans density distribution of radius r = 1 pc. As the wave travels through the simulation grid, a shock-bounded spherical shell is formed. The inner shock of this shell reaches and bounces off the center, leaving behind a central core with an initially almost uniform density distribution, surrounded by an envelope consisting of the material in the shock-bounded shell, with a power-law density profile that at late times approaches a logarithmic slope of -2 even in non-collapsing cases. The resulting density structure resembles a quiescent core of radius < 0.1 pc, with a Bonnor-Ebert-like (BE-like) profile, although it has significant dynamical differences: it is initially non-self-gravitating and confined by the ram pressure of the infalling material, and consequently, growing continuously in mass and size. With the appropriate parameters, the core mass eventually reaches an effective Jeans mass, at which time the core begins to collapse. Thus, there is necessarily a time delay between the appearance of the core and the onset of its collapse, but this is not due to the dissipation of its internal turbulence as it is often believed. These results suggest that pre-stellar cores may approximate Bonnor-Ebert structures which are however of variable mass and may or may not experience gravitational collapse, in qualitative agreement with the large observed frequency of cores with BE-like profiles. ", "machine_abstract": "We present numerical simulations to study the formation, evolution, and collapse of quiescent cloud cores induced by dynamic compressions in turbulent molecular clouds. We find that these cloud cores are formed through shock compression at intersections between shocks driven into the clouds by supersonic turbulence. The cloud core masses range from 0.1 M\u2299 to 1 M\u2299 with typical sizes of about 1000 AU. These cloud cores have low internal velocities (< 2 km s-1) but can be accelerated up to 10 km s-1 during their lifetimes due to gravitational interactions with other dense clumps within the same clouds. Most of them evolve quasi-statically for several free-fall times before collapsing dynamically on time scales ranging from one to ten free-fall times. Our results suggest that such cloud cores may represent an important source of prestellar objects in star-forming regions. Keywords: Turbulence, Star Formation", "paraphrased_abstract": "It was also found that the cloud forming process was a form of compression at the intersection of supersonic shocks, which in the tropics of the clouds brought on the eddy at the eddy. It was a low eddy, at a speed of a few miles per second, and it could be accelerated up to ten times per hour. We have developed the simulations to study the eddy and the eddy of the cloud forming processes induced by the eddy of the clouds. We find that the eddy is formed by the eddy of the clouds from the eddy of the eddy of the supersonic cloud. The cloud forming process is governed by a series of shocks that are driven to the clouds by a supersonic turbulence. The eddy of the cloud consists of about 0.1 to 0.1 M., whose size is usually a thousand AU. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1365, "title": "Multi-wavelength analysis of 18um-selected galaxies in the AKARI/IRC monitor field towards the North Ecliptic Pole", "abstract": "  We present an initial analysis of AKARI 18um-selected galaxies using all 9 photometric bands at 2-24um available in the InfraRed Camera (IRC), in order to demonstrate new capabilities of AKARI cosmological surveys. We detected 72 sources at 18um in an area of 50.2 arcmin^2 in the AKARI/IRC monitor field towards the North Ecliptic Pole (NEP). From this sample, 25 galaxies with probable redshifts z>~ 0.5 are selected with a single colour cut (N2-N3>0.1) for a detailed SED analysis with ground-based BVRi'z'JK data. Using an SED radiative transfer model of starbursts covering the wavelength range UV -- submm, we derive photometric redshifts from the optical-MIR SEDs of 18um-selected galaxies. From the best-fit SED models, we show that the IRC all-band photometry is capable of tracing the steep rise in flux at the blue side of the PAH 6.2um emission feature. This indicates that the IRC all-band photometry is useful to constrain the redshift of infrared galaxies, specifically for dusty galaxies with a less prominent 4000A break. Also, we find that the flux dip between the PAH 7.7 and 11.2um emission feature is recognizable in the observed SEDs of galaxies at z~1. By using such a colour anomaly due to the PAH and silicate absorption features, unique samples of ULIRGs at z~1, `silicate-break' galaxies, can be constructed from large cosmological surveys of AKARI towards the NEP, i.e. the NEP-Deep and NEP-Wide survey. This pilot study suggests the possibility of detecting many interesting galaxy properties in the NEP-Deep and Wide surveys, such as a systematic difference in SEDs between high- and low-z ULIRGs, and a large variation of the PAH inter-band strength ratio in galaxies at high redshifts. [abridged] ", "machine_abstract": "We have carried out multi-wavelength observations for a sample of infrared (IR) selected galaxies with AKARI and other telescopes to investigate their physical properties, such as dust temperature T d , luminosity L IR , star formation rate SFR, stellar mass M * . The main results are summarized below.  We found that most of our targets show red colors at optical wavelengths indicating old ages and/or low metallicities. In addition, we detected strong polycyclic aromatic hydrocarbon emission features at 6.2, 7.7, 8.6, 11.3 um which indicate active star-formation activities. By fitting the observed spectral energy distributions (SEDs), we derived the following parameters;  - Dust temperatures range between 30 K and 60 K. - Luminosities range between 10^10 and 10^12 Lsun. - Star formation rates range between 0.1 and 100 Msun yr-1. - Stellar masses range between 10^9 and 10^11 Msun.", "paraphrased_abstract": "Moreover, there was evidence of a strong polycyclic aromatic hydrocarbon signature at 6.2, 7.7, 8.6, and 11.3 um, which indicated active formation of stars. The main results are given below. The data collected in the optical spectrum of the galaxy have been taken from the AKARI and other telescopes, to investigate their physical properties, such as the temperature T d, the luminosity L d, the star formation rate SFR, the mass of M. *.... ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1705, "title": "On the nature of the short duration GRB 050906", "abstract": "  We present deep optical and infrared observations of the short duration GRB 050906. Although no X-ray or optical/IR afterglow was discovered to deep limits, the error circle of the GRB (as derived from the Swift BAT) is unusual incontaining the relatively local starburst galaxy IC328. This makes GRB 050906 a candidate burst from a soft-gamma repeater, similar to the giant flare from SGR 1806-20. The probability of chance alignment of a given BAT position with such a galaxy is small (<1%), although the size of the error circle (2.6 arcminute radius) is such that a higher-z origin can't be ruled out. Indeed, the error circle also includes a moderately rich galaxy cluster at z=0.43, which is a plausible location for the burst given the apparent preference that short GRBs have for regions of high mass density. No residual optical or infrared emission has been observed, either in the form of an afterglow or later time emission from any associated supernova-like event. We discuss the constraints these limits place on the progenitor of GRB 050906 based on the expected optical signatures from both SGRs and merging compact object systems. ", "machine_abstract": "We report on optical and near-infrared observations made with the Hubble Space Telescope (HST) in response to an alert issued by Swift/BAT for GRB050906, which was detected at 07:47:06 UT on September 6, 2005. The prompt emission lasted about 1 s and had a peak flux density of ~1 Crab. We find that this burst is similar to other short-duration gamma-ray bursts observed previously; it has no detectable afterglow down to limits of R > 26 mag at t = 0.5 hr and JHK > 23 mag at t = 2 days. However, we do detect a bright source coincident with the position of the X-ray afterglow reported by Swift/XRT. This object shows significant variability over timescales ranging from minutes to hours. It also exhibits strong H-alpha absorption lines characteristic of young stellar objects. These properties are consistent with those expected if the source were a massive star forming region within our Galaxy.", "paraphrased_abstract": "This object has a considerable radiance and a remarkable H-alpha absorption, which are characteristic of young stellar objects. In our analysis we will present optical and near-infrared X-rays taken by the Hubble Space Telescope, in response to a remark made by Swift/BAT at a certain hour in September 2005. We find that the burst was remarkably similar to other bursts, which, while they have no radiance beyond the limit of R 26 mag at t = 0.5 hr, and J HK > 23 mag at t = 2 days, have no afterglow. The spectra of the light, however, is quite variable, ranging from minutes to hours. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1903, "title": "Growth window and possible mechanism of millimeter-thick single-walled carbon nanotube forests", "abstract": "  Our group recently reproduced the water-assisted growth method, so-called \"super growth\", of millimeter-thick single-walled carbon nanotube (SWNT) forests by using C2H4/ H2/ H2O/ Ar reactant gas and Fe/ Al2O3 catalyst. In this current work, a parametric study was carried out on both reaction and catalyst conditions. Results revealed that a thin Fe catalyst layer (about 0.5 nm) yielded rapid growth of SWNTs only when supported on Al2O3, and that Al2O3 support enhanced the activity of Fe, Co, and Ni catalysts. The growth window for the rapid SWNT growth was narrow, however. Optimum amount of added H2O increased the SWNT growth rate but further addition of H2O degraded both the SWNT growth rate and quality. Addition of H2 was also essential for rapid SWNT growth, but again, further addition decreased both the SWNT growth rate and quality. Because Al2O3 catalyzes hydrocarbon reforming, Al2O3 support possibly enhances the SWNT growth rate by supplying the carbon source to the catalyst nanoparticles. The origin of the narrow window for rapid SWNT growth will also be discussed. ", "machine_abstract": "We report the growth of millimeter-thick vertically aligned single-walled carbon-nanotube forests on SiO 2 /Si substrates by chemical vapor deposition using Fe nanoparticles as catalysts at temperatures between 700 \u00b0C and 850 \u00b0C in Ar/H 2 . The diameter distribution is centered around 1 nm, with an average tube length of about 10 \u03bcm. We find that the density of the grown forests increases linearly with time up to a maximum value of 3 \u00d7 10 11 cm -2 , which corresponds to a volume filling factor of 0.7 for the tubes. This high density can be explained by considering the balance between the nucleation rate and the growth rate of individual tubes. In addition, we show that the vertical alignment of the tubes is maintained over large areas (1 cm   2 ) without any noticeable defects or cracks. These results suggest that these thick forests are promising candidates for applications such as field emitters and transparent electrodes.     Carbon nanotubes have attracted considerable attention because they exhibit unique physical properties [1]. However, it has been difficult to grow high-quality carbon nanotubes due to their extremely small diameters [2]. Recently, several groups reported the growth of vertically aligned carbon nanotube arrays [3]\u2013[6]. Although these studies demonstrated the potential use of carbon nanotubes in various fields [7, 8], most of them were limited to thin films less than 100 nm thick [9]. To fully exploit the advantages offered by carbon nanotubes, it is necessary to develop techniques capable of producing thicker films [10].", "paraphrased_abstract": "It was reported that carbon nanotubes were fabricated in many fields and are of remarkable beauty. But the use of carbon nanotubes has not been able to be realized in the world, and there is still the need to produce thicker films. Recently several groups reported the development of vertically oriented carbon nanotubes on SiO2 and SiO2 inorganic carbon, which is only a fraction of a millimeter thick. The dimensions were given as the center of the diameter, and the length of the tubes was about 10 m. These measurements exhibited remarkable strength, and showed that the density of the stacked trees increased in time to a value of three  10 11 cm2, which corresponds to a volume of 0.7. These numbers indicated the advantages of carbon nanotubes for use in different applications. In order to realize full use of carbon nanotubes, it is necessary to have a means of manufacturing thicker layers. We have recently reported the growth of vertically aligned carbon nanotubes on the SiO 2/Si substrate at a temperature of 700 \u00b0C and 850 \u00b0C. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2121, "title": "Quantum Electrodynamics of qubits", "abstract": "  Systematic description of a spin one-half system endowed with magnetic moment or any other two-level system (qubit) interacting with the quantized electromagnetic field is developed. This description exploits a close analogy between a two-level system and the Dirac electron that comes to light when the two-level system is described within the formalism of second quantization in terms of fermionic creation and annihilation operators. The analogy enables one to introduce all the powerful tools of relativistic QED (albeit in a greatly simplified form). The Feynman diagrams and the propagators turn out to be very useful. In particular, the QED concept of the vacuum polarization finds its close counterpart in the photon scattering off a two level-system leading via the linear response theory to the general formulas for the atomic polarizability and the dynamic single spin susceptibility. To illustrate the usefulness of these methods, we calculate the polarizability and susceptibility up to the fourth order of perturbation theory. These {\\em ab initio} calculations resolve some ambiguities concerning the sign prescription and the optical damping that arise in the phenomenological treatment. We also show that the methods used to study two-level systems (qubits) can be extended to many-level systems (qudits). As an example, we describe the interaction with the quantized electromagnetic field of an atom with four relevant states: one S state and three degenerate P states. ", "machine_abstract": "The quantum electrodynamic (QED) effects on the dynamics and decoherence of semiconductor-based qubit systems are investigated in this work. The QED effect is described by an effective interaction between the qubit system and its environment, which consists of vacuum fluctuations of the electromagnetic field as well as thermal photons at finite temperature. We show that the QED effect can be treated perturbatively for typical experimental parameters. In particular, we find that the spontaneous emission rate of excitons into free space modes increases with increasing number N of electrons involved in the qubit state. This leads to faster relaxation times T 1 , but also to stronger pure dephasing rates T 2 . For realistic values of N = 10 \u2212 100, however, these effects remain small compared to other sources of relaxation such as phonon scattering or electron-electron interactions.     Introduction     Quantum information processing has attracted considerable attention over recent years due to its potential applications in various fields ranging from communication technology [1] to metrology [2] . Semiconductor-based solid-state devices have been proposed as promising candidates for realizing scalable quantum computers [3] . Among them, excitonic states in semiconductors [4] represent one of the most important classes of physical objects suitable for storing and manipulating quantum information [5] . However, it turns out that exciton-exciton interactions [6] lead to rapid decay processes [7, 8] , so that only few excitations may be stored coherently within each individual device [9] . To overcome this problem, several proposals have been made recently [10] - [13] based on hybrid structures consisting of different materials [14] - [16] .   In this Letter, we investigate how the quantum electrodynamic (or radiative) coupling [17] affects the dynamics of semiconductor-based qubit sys-tems. As shown schematically in Fig. 1(a) , our model includes two types of environments surrounding the qubit system: First, there exist vacuum fluctuations of the electromagnetic fields inside the cavity [18] , leading to spontaneous emission of excitons into free-space modes [19, 20] . Second, there exists a bath of thermal photons [21]", "paraphrased_abstract": "We have presented a new theory for the study of quantum electrodynamics and the effect of radiative couplings on the dynamical and decomposition of semiconductors. We have been examining in detail in Fig. 1(a) in a manner similar to that of a crystal-based qubit system. The system is composed of two types of environments, one of which is vacuum, and is thus expelled into a free-space mode. The second, by way of thermal light, is a bath of photons. We will consider here the effect of the QED by adjusting the energy balance between the system and the environment, which is in turn influenced by vacuum waves, which give rise to excitations in a free-space mode. As a result, the relaxation time is longer than with other sources of relaxation, such as phonon scattering or electron-electron interaction. We have recently found a method for obtaining and implementing a QED, which is, if you take into account the effect of a QED in a quantum computer, this is the result of an effective interaction between the system and the environment, and this is constituted by a vacuum, a vacuum, and a thermal bath, and these are at room temperature. This means that the QED effect is quite strong for a real N of N, which is about the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1118, "title": "Asynchronous games: innocence without alternation", "abstract": "  The notion of innocent strategy was introduced by Hyland and Ong in order to capture the interactive behaviour of lambda-terms and PCF programs. An innocent strategy is defined as an alternating strategy with partial memory, in which the strategy plays according to its view. Extending the definition to non-alternating strategies is problematic, because the traditional definition of views is based on the hypothesis that Opponent and Proponent alternate during the interaction. Here, we take advantage of the diagrammatic reformulation of alternating innocence in asynchronous games, in order to provide a tentative definition of innocence in non-alternating games. The task is interesting, and far from easy. It requires the combination of true concurrency and game semantics in a clean and organic way, clarifying the relationship between asynchronous games and concurrent games in the sense of Abramsky and Melli\\`es. It also requires an interactive reformulation of the usual acyclicity criterion of linear logic, as well as a directed variant, as a scheduling criterion. ", "machine_abstract": "We study the computational complexity of games with imperfect information, where players have no knowledge about each other's moves and actions are not necessarily alternating between them. We show that such games can be solved in polynomial time if they satisfy certain conditions on their structure (e.g., being bipartite). In particular, we prove this for all perfect-information games played by two or more players who do not know which player has made what move at any given point during play. Our results also apply to multiplayer versions of some well-known combinatorial problems like the traveling salesman problem. Games with imperfect information arise naturally when modeling situations involving multiple agents whose decisions cannot always be observed directly. For example, consider a group of people playing a board game together; it is often impossible to tell exactly who moved last because there may be several pieces involved in one move. Another common situation occurs in distributed systems, where different processes communicate indirectly via messages sent over an unreliable network.", "paraphrased_abstract": "The problem of the game of swarming information arises naturally from the existence of a group of agents whose decisions cannot always be readily seen directly, for example, a group of people playing a board game, where it is hard to tell the precise position of each player in the game, as there are often many pieces. Our research has also been applied to the computational complexity of the games with swarming information, in which a player does not know in advance what direction each of his actions are taking, and in which the actions of all players are not necessarily alternating. We show that such games can be solved in a polynomial time if they have the same structure (e.g., they are bipartite). In particular, we show that all games with swarming information are invariably bipartite, provided that there are certain conditions to make them so. For example, in a group of people playing a game together, it is often difficult to determine who is the first, because there may be several moves. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1416, "title": "Anatomy of Malicious Singularities", "abstract": "  As well known, the b-boundaries of the closed Friedman world model and of Schwarzschild solution consist of a single point. We study this phenomenon in a broader context of differential and structured spaces. We show that it is an equivalence relation $\\rho $, defined on the Cauchy completed total space $\\bar{E}$ of the frame bundle over a given space-time, that is responsible for this pathology. A singularity is called malicious if the equivalence class $[p_0]$ related to the singularity remains in close contact with all other equivalence classes, i.e., if $p_0 \\in \\mathrm{cl}[p]$ for every $p \\in E$. We formulate conditions for which such a situation occurs. The differential structure of any space-time with malicious singularities consists only of constant functions which means that, from the topological point of view, everything collapses to a single point. It was noncommutative geometry that was especially devised to deal with such situations. A noncommutative algebra on $\\bar{E}$, which turns out to be a von Neumann algebra of random operators, allows us to study probabilistic properties (in a generalized sense) of malicious singularities. Our main result is that, in the noncommutative regime, even the strongest singularities are probabilistically irrelevant. ", "machine_abstract": "We present an anatomical study on the structure and evolution of malicious singularities in complex networks, which are defined as nodes that have a disproportionately large number of connections to other nodes. We show how these nodes can be identified by their topological properties using statistical mechanics methods. In particular we find that the degree distribution of such nodes follows a power law with exponent 2.5 \u00b1 0.1 for all studied real-world networks ranging from social networks like Facebook or Twitter to biological networks like metabolic pathways or protein-protein interaction networks. This finding is consistent across different network sizes and types. The results presented here provide new insights into the organization principles of complex systems and may help to identify key players within them. Complex networks play a crucial role in many fields including physics, biology, sociology, computer science, engineering, economics, etc., where they represent interactions between entities [1] . A common feature of most complex networks is the presence of so-called \"hubs\" -highly connected nodes [2] , whose removal often has dramatic effects [3] . In this work we focus on identifying hubs in complex networks based solely on their topological features. To do so, we use statistical mechanics techniques [4] to analyze the degree distributions of several real world networks [5] . Our analysis reveals that the degree distribution of hubs follows a power-law [6] with exponent 2.5\u00b10.1 independently of the size and type of the considered network (see Fig.  1 ). Interestingly, our findings are also valid when considering only the largest component of each network [7, 8] . These results suggest that the observed scaling behavior is not due to finite-size effects but rather reflects some fundamental property of complex networks [9] .", "paraphrased_abstract": "I have studied the structure and evolution of malicious singularities in complex networks, and we have studied them, in particular, by their topological properties, and the results are consistent with all network sizes and types. Complex networks play a key role in many fields, from physics, biology, sociology, computer science, engineering, economics and so on, in which there are interactions among entities, and they are characterized by so-called hubs, which are in excess of one another, and which can be removed by a great effort, often with dramatic results. We present an anatomical study of the formation and evolution of malicious singularities in complex networks, focusing on the topological properties, and showing that the distribution of these nodes is based solely on their topological properties, and this is the case for all the studied networks, from social networks like Facebook or Twitter to biological networks like metabolic networks or protein\u2013protein interactions. The results show that the scale of these nodes is not a result of a finite size, but a reflection of a fundamental property of complex networks. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2955, "title": "Rational points on certain elliptic surfaces", "abstract": "  Let $\\mathcal{E}_{f}:y^2=x^3+f(t)x$, where $f\\in\\Q[t]\\setminus\\Q$, and let us assume that $\\op{deg}f\\leq 4$. In this paper we prove that if $\\op{deg}f\\leq 3$, then there exists a rational base change $t\\mapsto\\phi(t)$ such that on the surface $\\cal{E}_{f\\circ\\phi}$ there is a non-torsion section. A similar theorem is valid in case when $\\op{deg}f=4$ and there exists $t_{0}\\in\\Q$ such that infinitely many rational points lie on the curve $E_{t_{0}}:y^2=x^3+f(t_{0})x$. In particular, we prove that if $\\op{deg}f=4$ and $f$ is not an even polynomial, then there is a rational point on $\\cal{E}_{f}$. Next, we consider a surface $\\cal{E}^{g}:y^2=x^3+g(t)$, where $g\\in\\Q[t]$ is a monic polynomial of degree six. We prove that if the polynomial $g$ is not even, there is a rational base change $t\\mapsto\\psi(t)$ such that on the surface $\\cal{E}^{g\\circ\\psi}$ there is a non-torsion section. Furthermore, if there exists $t_{0}\\in\\Q$ such that on the curve $E^{t_{0}}:y^2=x^3+g(t_{0})$ there are infinitely many rational points, then the set of these $t_{0}$ is infinite. We also present some results concerning diophantine equation of the form $x^2-y^3-g(z)=t$, where $t$ is a variable. ", "machine_abstract": "We study the rational points on some families of K3 surfaces and abelian surfaces over finite fields, especially those with Picard number one or two.  We prove that for any prime power $q$ there are infinitely many primes $p$ such that the Fermat curve $F_q(x)=x^{q+1}-1$ has no solutions in $\\mathbb{F}_p$ (Theorem 1).  This is done by showing that if $p\\neq q+1$ then the equation $x^{q+1}-1=y^p$ always has at least one solution in $\\mathbb{F}_{p}^{\\times}$.   As an application we show how to construct infinite families of supersingular primes (Corollary 2).   In Theorem 3 we give necessary conditions for the existence of a point of order $m_1m_2$ on an abelian surface $A=E_1\\times E_2$ where $E_i$ are ordinary elliptic curves defined over a field $K$ of characteristic zero.   ...", "paraphrased_abstract": "Moreover, the equator can be used to create the infinitely many supersingular primes. We will consider some arbitrary families of K3 and the abelian surfaces, in particular the equator A. The Abelian surface A.E.1 =E2 = E... Then the equation x-q+1 = y-p = a solution of Mathbb-F. p=3 (18). This solution, in fact, is a solution of Mathb-F. ptimes. As an example, we show how to construct infinite families of supersingular primes (Corollary 1). We prove that there are infinitely many primes p such that the Fermat curve F-q (x) = x-q+1=y-p, always has at least one solution in Mathb-F. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0351, "title": "FIRST-based survey of Compact Steep Spectrum sources, V. Milliarcsecond-scale morphology of CSS objects", "abstract": "  Multifrequency VLBA observations of the final group of ten objects in a sample of FIRST-based compact steep spectrum (CSS) sources are presented. The sample was selected to investigate whether objects of this kind could be relics of radio-loud AGNs switched off at very early stages of their evolution or possibly to indicate intermittent activity. Initial observations were made using MERLIN at 5 GHz. The sources have now been observed with the VLBA at 1.7, 5 and 8.4 GHz in a snapshot mode with phase-referencing. The resulting maps are presented along with unpublished 8.4-GHz VLA images of five sources. Some of the sources discussed here show a complex radio morphology and therefore a complicated past that, in some cases, might indicate intermittent activity. One of the sources studied - 1045+352 - is known as a powerful radio and infrared-luminous broad absorption line (BAL) quasar. It is a young CSS object whose asymmetric two-sided morphology on a scale of several hundred parsecs, extending in two different directions, may suggest intermittent activity. The young age and compact structure of 1045+352 is consistent with the evolution scenario of BAL quasars. It has also been confirmed that the submillimetre flux of 1045+352 can be seriously contaminated by synchrotron emission. ", "machine_abstract": "We present the results of our study on milliarcsec (mas) scale radio morphologies for a sample of compact steep spectrum (CSS) objects using Very Long Baseline Array (VLBA). The VLBA observations were carried out at 5 GHz in full polarization mode with an angular resolution of 0.5 mas and sensitivity better than 100 microJy beam-1 . We have detected all but one source in our sample which is unresolved by the VLBA. Our main findings are as follows:  1. All the resolved sources show core-jet structure except two sources where we see only jet emission.  2. Most of the jets appear to be straight or slightly bent towards north-east direction.  3. There appears to be no correlation between the linear size and the position angle of the jet axis.  4. A comparison of the observed flux density ratio between the core and the first component shows that most of these sources lie below the theoretical limit predicted by the standard model of relativistic beaming.", "paraphrased_abstract": "We have investigated the properties of a sub-middle class of crystalline SSS objects, with the aid of the Very Long Baseline Array, in a full-polarization mode, with an angular resolution of a meter and sensitivity of a thousand tens of joules. Our primary conclusions are as follows: 1) All the detected sources show the core-jet structure, except for two that have only jets visible. 2) The jets appear to be straight or slightly bent to the north. 3. There is no correlation between the linear size and the angle of the axis of the jet. Moreover, the ratio of flux density between the core and the first component of the source shows that the density of these sources lies at the theoretical limit of the standard relativistic beaming. The result is as follows: ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2535, "title": "Sunyaev-Zel'dovich profiles and scaling relations: modelling effects and observational biases", "abstract": "  We use high-resolution hydrodynamic re-simulations to investigate the properties of the thermal Sunyaev-Zel'dovich (SZ) effect from galaxy clusters. We compare results obtained using different physical models for the intracluster medium (ICM), and show how they modify the SZ emission in terms of cluster profiles and scaling relations. We also produce realistic mock observations to verify whether the results from hydrodynamic simulations can be confirmed. We find that SZ profiles depend marginally on the modelled physical processes, while they exhibit a strong dependence on cluster mass. The central and total SZ emission strongly correlate with the cluster X-ray luminosity and temperature. The logarithmic slopes of these scaling relations differ from the self-similar predictions by less than 0.2; the normalization of the relations is lower for simulations including radiative cooling. The observational test suggests that SZ cluster profiles are unlikely to be able to probe the ICM physics. The total SZ decrement appears to be an observable much more robust than the central intensity, and we suggest using the former to investigate scaling relations. ", "machine_abstract": "We present the results of our analysis on simulated galaxy clusters, which are used to study how different physical processes affect SZ observables (profiles and scaling relations). We use hydrodynamical simulations performed with GADGET-2 code in order to investigate the effect of: gas clumping, AGN feedback, cooling flows, mergers, triaxiality and orientation of the cluster relative to observer's line-of-sight. The main conclusions are:  - Gas clumpiness has an important impact on SZ observable quantities such as integrated Comptonization parameter Y500 or pressure profile P(r), especially at small radii.  - Cooling flow regions have lower values for Y500 than expected by self-similar model predictions due to their low temperature and density compared to other parts of the cluster.  - Mergers can significantly change the shape of the pressure profile leading to higher central pressures and steeper slopes towards outer parts.  - Triaxiality affects both the amplitude and slope of the pressure profile depending on its orientation wrt. the observer's line-of-sigh.  - Inclination angle between the major axis of the cluster and the observer's line-ofsight is one of the most significant factors affecting the observed properties of galaxy clusters.", "paraphrased_abstract": "We consider, for instance, the effect of a gas on the elasticity of the compton in Y500, the rate of Compton entrainment, and the rate of Pressure P (r), whose values vary considerably in the lower regions of the cluster, and in the higher regions, in the lower regions of Y500, for the same reason, that they have lower temperature and density than in the other regions of the cluster. This results in an important result for the SZ observables, namely, the Pressure P (r) and the Profile of the Universe. This result was derived from hydrodynamics of the GADGET-2 code. Using this code, we examined the effect of gas clumping, AGN feedback, cooling flows, mergers, and triaxiality in the Pressure Profile. This result was expressed in the following terms: gas clumping has an important effect on the SZ observables, including the Y500, Y500, and the Pressure Profile of P(r), especially at small radii. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4557, "title": "TRUFAS, a wavelet based algorithm for the rapid detection of planetary transits", "abstract": "  Aims: We describe a fast, robust and automatic detection algorithm, TRUFAS, and apply it to data that are being expected from the CoRoT mission. Methods: The procedure proposed for the detection of planetary transits in light curves works in two steps: 1) a continuous wavelet transformation of the detrended light curve with posterior selection of the optimum scale for transit detection, and 2) a period search in that selected wavelet transformation. The detrending of the light curves are based on Fourier filtering or a discrete wavelet transformation. TRUFAS requires the presence of at least 3 transit events in the data. Results: The proposed algorithm is shown to identify reliably and quickly the transits that had been included in a standard set of 999 light curves that simulate CoRoT data. Variations in the pre-processing of the light curves and in the selection of the scale of the wavelet transform have only little effect on TRUFAS' results. Conclusions: TRUFAS is a robust and quick transit detection algorithm, especially well suited for the analysis of very large volumes of data from space or ground-based experiments, with long enough durations for the target-planets to produce multiple transit events. ", "machine_abstract": "We present TRUFAS (Transit Recognition Using Fast Approximate Signatures), an efficient and robust transit search pipeline that uses fast approximate signatures to detect periodic signals in time series data. The method is particularly well suited for detecting short duration transits with high signal-to-noise ratio. We demonstrate its performance on simulated light curves generated by the Exoplanet Transit Database as well as real Kepler light curves. Our results show that TRUFAS can achieve higher efficiency than other algorithms while maintaining low false positive rates.     Keywords: Transiting planet, Wavelets, Time-series analysis, False positives reduction, Planetary system characterization         1 Introduction     Planets are detected indirectly through their gravitational effects upon their host stars. These effects include changes in stellar radius or luminosity caused by the passage of planets across the line-of-sight between the star and Earth. This phenomenon is known as a transit event. In order to characterize exoplanet systems it is necessary to identify these events efficiently and accurately. However, this task has been made more difficult due to the large number of false positives produced by systematic noise sources such as instrumental artifacts and astrophysical phenomena like eclipsing binaries and pulsating stars.     To date there have been several methods developed specifically for identifying transit-like features within astronomical time series data. Some examples include: Box Least Squares (BLS) [1] , BLS+ [2] , TrES [3] , TAP [4] , EXOTRANS [5] . While each of these techniques performs reasonably well under certain conditions they all suffer from one common drawback; they require significant computational resources when searching for multiple transit candidates simultaneously. For example, the most widely used technique, Box Least Squares, requires O(N3) operations where N is the length of the time series being analyzed [6] . As a result, many of these techniques cannot be applied directly to current and future surveys which will produce enormous amounts of data [7][8][9] .     In recent years wavelet transforms have become increasingly popular for analyzing astronomical time series data [10][", "paraphrased_abstract": "The transit of the planet is indirectly determined by the gravitational influence of the host star. The effect is a change in the size or luminosity of the star, which causes a shift in the ray of light from the star to the earth, which is called a transit event. Among these techniques there are a number of methods, and all of them have a common disadvantage: they require significant computational resources in searching for a number of transits simultaneously. These methods have in particular been developed in the light of the astronomical time series, in which the time series are characterized by a single, large-scale event, and the result is that many of these data are not easily analyzed by the astronomical time series, because the time series are extremely large. The result is that many of these methods are not suitable for analyzing current and future surveys, which will yield huge amounts of data. This is why it is necessary to identify these events in a timely manner. For example, the method of the Least Squares is based on O(N3) operations, where N is the length of the study period, and in the case of the Least Squares it is based on the O(N3) value of the length of the study period. In this regard, our study demonstrates that Least Squares is superior to all other methods in detecting short transits, with high signal-to", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.132, "title": "Accretion vs colliding wind models for the gamma-ray binary LS I +61 303: an assessment", "abstract": "  LS I +61 303 is a puzzling Be/X-ray binary with variable gamma-ray emission at up TeV energies. The nature of the compact object and the origin of the high-energy emission are unclear. One family of models invokes particle acceleration in shocks from the collision between the B-star wind and a relativistic pulsar wind, while another centers on a relativistic jet powered by accretion. Recent high-resolution radio observations showing a putative \"cometary tail\" pointing away from the Be star near periastron have been cited as support for the pulsar-wind model. We wish here to carry out a quantitative assessment of these competing models for this extraordinary source. We apply a 3D SPH code for dynamical simulations of both the pulsar-wind-interaction and accretion-jet models. The former yields a description of the shape of the wind-wind interaction surface. The latter provides an estimation of the accretion rate. The results allow critical evaluation of how the two distinct models confront the data in various wavebands under a range of conditions. When one accounts for the 3D dynamical wind interaction under realistic constraints for the relative strength of the B-star and pulsar winds, the resulting form of the interaction front does not match the putative \"cometary tail\" claimed from radio observations. On the other hand, dynamical simulations of the accretion-jet model indicate that the orbital phase variation of accretion power includes a secondary broad peak well away from periastron, thus providing a plausible way to explain the observed TeV gamma ray emission toward apastron. We conclude that the colliding-wind model is not clearly established for LS I +61 303, while the accretion-jet model can reproduce many key characteristics of the observed TeV gamma-ray emission. ", "machine_abstract": "We present new results on the nature and geometry of the compact object in the gamma-ray binary system LS I +61 303, based on observations with the INTEGRAL satellite. We find that the source is variable at all wavelengths studied here (radio to hard X-rays), but shows no evidence for orbital modulation or eclipses. The X-ray spectrum can be described by either a power law model or thermal bremsstrahlung emission; both are consistent with previous studies. In addition we report the detection of pulsations in the radio band which have been previously reported only once before. These pulsations show up as periodic intensity variations in our data set, and their periodicity has been confirmed using two independent methods. Using these results together with those obtained from optical photometry and spectroscopy, we conclude that this source most likely contains a neutron star accreting matter from its companion Be-star via Roche lobe overflow.", "paraphrased_abstract": "The X-ray spectrum is described by either a power law or a thermal bremsstrahlung spectrum, both of which are confirmed by independent experiments. We have also observed the detection of radio pulses in the band which were only previously observed and which have been confirmed by two independent tests. In addition, we have derived the occurrence of pulses in the radio band which have been previously only observed once before by the researchers. These pulses show up as periodic patterns in our data, and this pattern has been confirmed by two independent measurements. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4313, "title": "Projectile Fragmentation of $^{86}$Kr at 64 MeV/nucleon", "abstract": "  We measured fragmentation cross sections produced using the primary beam of $^{86}$Kr at 64 MeV/nucleon on $^9$Be and $^{181}$Ta targets. The cross sections were obtained by integrating the momentum distributions of isotopes with 25<Z<36 measured using the RIPS fragment separator at RIKEN. The cross-section ratios obtained with the $^{181}$Ta and $^{9}$Be targets depend on the fragment masses, contrary to the simple geometrical models. We compared the extracted cross sections to EPAX; an empirical parameterization of fragmentation cross sections. Predictions from current EPAX parameterization severely overestimate the production cross sections of very neutron-rich isotopes. Attempts to obtain another set of EPAX parameters specific to the reaction studied here, to extrapolate the neutron-rich nuclei more accurately have not been very successful, suggesting that accurate predictions of production cross sections of nuclei far from the valley of stability require information of nuclear properties which are not present in EPAX. ", "machine_abstract": "The projectile fragmentation of 86Kr at 64MeV/nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a beam intensity of 1nAe. The main results are as follows:  - A total number of about 10000 events have been recorded for this experiment.  - The charge distribution is peaked around Z=40, but shows also a large contribution between 30 and 40 charges units (see fig.1 ). This indicates that the fragments produced by the break-up of 86Kr are not only light particles like neutrons or protons, but contain many intermediate mass fragments too.   - The angular distributions show two peaks corresponding to forward and backward emission respectively (see fig.2 ).  - The energy spectra present a maximum around 10-12 MeV/u which corresponds to the most probable kinetic energy per nucleon of the emitted fragments (see fig.3 ). - The isotopic composition of the fragments is shown on figure 4 . It can be seen that there is no significant difference between the fragment production in the forward hemisphere and in the backward one.", "paraphrased_abstract": "I have a system for the study of the fragmentation of 86Kr at 64MeV/nucleons. Its scale is a symmetrical equilateral curve and shows a strong convective phase. The kinetic energy is estimated to be about ten-nucleons, which is the most probable energy for the decay of the emitted fragments. This indicates that the fragments formed by the equilateral decay of 86Kr are not merely light particles like neutrons or protons, but are also intermediate mass fragments. 2 The charge distribution is centered around z=40, but shows a great amount of equilateral charge between 30 and 40. Figure 4: The isotopic composition of the fragments. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0453, "title": "Fractal analysis for sets of non-differentiability of Minkowski's question mark function", "abstract": "  In this paper we study various fractal geometric aspects of the Minkowski question mark function $Q.$ We show that the unit interval can be written as the union of the three sets $\\Lambda_{0}:=\\{x:Q'(x)=0\\}$, $\\Lambda_{\\infty}:=\\{x:Q'(x)=\\infty\\}$, and $\\Lambda_{\\sim}:=\\{x:Q'(x)$ does not exist and $Q'(x)\\not=\\infty\\}.$ The main result is that the Hausdorff dimensions of these sets are related in the following way. $\\dim_{H}(\\nu_{F})<\\dim_{H}(\\Lambda_{\\sim})= \\dim_{H} (\\Lambda_{\\infty}) = \\dim_{H} (\\mathcal{L}(h_{\\mathrm{top}}))<\\dim_{H}(\\Lambda_{0})=1.$   Here, $\\mathcal{L}(h_{\\mathrm{top}})$ refers to the level set of the   Stern-Brocot multifractal decomposition at the topological entropy   $h_{\\mathrm{top}}=\\log2$ of the Farey map $F,$ and $\\dim_{H}(\\nu_{F})$ denotes the Hausdorff dimension of the measure of maximal entropy of the dynamical system associated with $F.$ The proofs rely partially on the multifractal formalism for Stern-Brocot intervals and give non-trivial applications of this formalism. ", "machine_abstract": "In this work, we study the fractal dimension and Hausdorff measure of the set of points where  is not differentiable in . We show that these measures are equal to one half times the corresponding values on the unit interval with Lebesgue measure.  This result generalizes previous results by Jarn\u00edk (for ) and Marstrand (for ). The proof relies on an explicit formula for the Fourier transform of , which was obtained recently by Fej\u00e9r and Riesz. In addition, we prove that the same holds true if instead of the Lebesgue measure on the unit interval, we consider any other probability measure whose support has positive length. Finally, we give some applications of our main theorem to the theory of lacunary series. Let be the space of all complex-valued functions defined on the real line equipped with the topology of uniform convergence on compact subsets of . Let denote the class of all infinitely differentiable functions on such that their derivatives satisfy the H\u00f6lder condition with exponent greater than or equal to 1 at every point of the domain.  For each integer let us define the operator as follows:  where denotes the th derivative of .  It can easily be seen that is continuous linear functional on .", "paraphrased_abstract": "This is a continuous linear function of.... In the following table, we define the operator as follows: where denotes the th derivative of... Let be the space of all complex-valued functions arranged along the real line, with the topology of uniform convergence at the compact subsets of... Let be the space of all complex-valued functions, with the topology of uniform convergence at all points of the domain... Let be the space of all infinitely differentiable functions, whose derivatives satisfy the H\u00f6lder condition with an exponent greater than or equal to 1 at every point of the domain. Moreover, we prove that this is true even if we do not consider the Lebesgue measure at the unit interval. This proves that the same holds for if we use any other measure of probability that has a positive length. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2675, "title": "Generic Gravitational Wave Signals from the Collapse of Rotating Stellar Cores: A Detailed Analysis", "abstract": "  We present detailed results from performing general relativistic (GR) simulations of stellar core collapse to a proto-neutron star, using a microphysical equation of state (EoS) as well as an approximate description of deleptonization during the collapse phase. We show that for a wide variety of rotation rates and profiles the gravitational wave (GW) burst signals from the core bounce are of a generic type, already known as Type I in the literature. In addition, for most models the characteristic frequency of the GW burst signal lies in a narrow range around approximately 718 Hz. In our systematic study, using both GR and Newtonian gravity, we identify, individually quantify, and discuss in detail the micro- and macrophysical mechanisms leading to this result, i.e. the effects of rotation, the EoS, and deleptonization. We also discuss the detectability prospects of such GW burst signals by GW detectors, and infer that such a generic type of signal templates will likely facilitate a more efficient search in current and future detectors of both interferometric and resonant type. ", "machine_abstract": "We present an analysis of gravitational wave signals produced by rotating stellar cores collapsing to black holes, using numerical relativity simulations performed with the Einstein Toolkit. We find that generic collapse models produce characteristic features in their gravitational waveforms which can be used as templates for matched filtering searches. These include (i) a precursor signal, (ii) a ringdown phase and (iii) a post-merger phase. The precursor is associated with the formation of a shock front at the surface of the core; it has been observed previously but its properties have not yet been studied systematically. In this work we show how these properties depend on the rotation rate of the progenitor star. For rapidly-rotating progenitors, the precursor signal contains multiple peaks whose frequencies are determined primarily by the spin frequency of the central object prior to merger. This suggests that such precursors could be detected even if they were emitted by binary neutron stars or black hole-neutron star systems where no information about the pre-merger spins would be available.", "paraphrased_abstract": "We study the resonant waves of a rotating star falling towards black holes, using the Einstein method. The results show that resonant waves of the star rising to black holes have a specific onset time, which is defined by the onset time of the central object before the merger. The onset time is the time when the resonant waves are most strongly emitted by the star that has not yet formed. The onset time is a period of time that is divided into four phases: a onset time, a ring-down phase, and a resonant time. In our experiments, we study the nature of the kinetic signal from the star that collapses in the black hole, a state of which has been described, but which has not been thoroughly studied. We show how the resonant wave is affected by the speed of the star before resolving. This is a type of asymmetrical signal, whose origin is the initial state of the core, and whose properties have not been studied to a certain extent. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2626, "title": "Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in hypre and PETSc", "abstract": "  We describe our software package Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as a stand-alone serial library, as an external package to PETSc (``Portable, Extensible Toolkit for Scientific Computation'', a general purpose suite of tools for the scalable solution of partial differential equations and related problems developed by Argonne National Laboratory), and is also built into {\\it hypre} (``High Performance Preconditioners'', scalable linear solvers package developed by Lawrence Livermore National Laboratory). The present BLOPEX release includes only one solver--the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\\it hypre} provides users with advanced high-quality parallel preconditioners for linear systems, in particular, with domain decomposition and multigrid preconditioners. With BLOPEX, the same preconditioners can now be efficiently used for symmetric eigenvalue problems. PETSc facilitates the integration of independently developed application modules with strict attention to component interoperability, and makes BLOPEX extremely easy to compile and use with preconditioners that are available via PETSc. We present the LOBPCG algorithm in BLOPEX for {\\it hypre} and PETSc. We demonstrate numerically the scalability of BLOPEX by testing it on a number of distributed and shared memory parallel systems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron workstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition and {\\it hypre} multigrid preconditioning. We test BLOPEX on a model problem, the standard 7-point finite-difference approximation of the 3-D Laplacian, with the problem size in the range $10^5-10^8$. ", "machine_abstract": "We present Block Locally Optimal Precondioner for the solution of large sparse linear systems arising from discretized partial differential equations using Krylov subspace methods.  The preconditioners are constructed by solving local eigenvalue problems on small subdomains, which can be done efficiently with parallel iterative solvers such as GMRES or BiCGSTAB. We show that these preconditioners have good convergence properties when used within an inexact Newton-Krylov-Schwarz method to solve nonlinear PDEs. In addition we demonstrate how this approach is implemented in two popular libraries, PETSc and hypre. Finally, we provide numerical results showing the effectiveness of our proposed algorithm. Keywords: Nonlinear PDE, Inexact Newton Method, Parallel Iterative Solver, Preconditioning, Subdomain Decomposition. 1 Introduction. The goal of this work is to develop efficient algorithms for solving large-scale nonlinear partial differential equations (PDEs). These algorithms will use domain decomposition techniques combined with parallel iterative solving strategies to construct effective preconditioners for the resulting linear system. Our focus here is on developing new approaches for constructing locally optimal block diagonal preconditioners based on the Schur complements associated with overlapping Schwarz methods. This type of preconditioner has been shown to perform well for many types of applications including convection-diffusion-reaction equations [19, 20] , Stokes flow [3, 4] , Maxwell's equations [6] , elasticity [7, 8] , and incompressible Navier-Stokes flows [9, 10] . However, there remain several challenges related to their construction and application. First, it is difficult to compute accurate approximations to the Schur complement matrices due to the presence of singularities at corners and edges of the computational domains. Second, the number of unknowns per subdomain increases rapidly as the problem size grows making direct factorization impractical. Third, the condition numbers of the Schur complement matrices grow exponentially fast with respect to the number of subdomains. Fourth, the cost of applying the preconditioner may become prohibitively expensive if one", "paraphrased_abstract": "The main purpose of this study is to devise a new, effective method for the solving of large, complex partial differential equations (PDEs) by the numerical method of Newton. We introduce a method for the construction of a local optimal block diagonal with Schur complements, which are complemented by overlapping Schwarz functions. We present an approach for the solution of large, sparse, linear systems of a discrete, partial differential equations in Krylov subspace, and for a solution of these equations. We show that the block diagonal preconditioners have good convergence properties in the case of linear systems whose eigenvalues are small and are easily solved with parallel, iterative iterations, for example in the case of a GMRES or BiCGSTAB, but it is shown that the preconditioners are effective and good convergence with the method. We are especially interested in the development of new algorithms for solving large-scale partial differential equations in the Krylov subspace. We are working on the construction of such algorithms in a precise way using the Schur complements, overlapping the Schwarz methods. We are also presenting the application of the technique to the problems of a large sparse linear system in the form of a discrete partial differential equation. Then, in an inexact Newton-Krylov-Schwarz method, we are ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2874, "title": "4U 0115+63 from RXTE and INTEGRAL Data: Pulse Profile and Cyclotron Line Energy", "abstract": "  We analyze the observations of the transient X-ray pulsar 4U 0115+63 with the RXTE and INTEGRAL observatories in a wide X-ray (3-100 keV) energy band during its intense outbursts in 1999 and 2004. The energy of the fundamental harmonic of the cyclotron resonance absorption line near the maximum of the X-ray flux from the source (luminosity range 5x10^{37} - 2x10^{38} erg/s) is ~11 keV. When the pulsar luminosity falls below ~5x10^{37} erg/s, the energy of the fundamental harmonic is displaced sharply toward the high energies, up to ~16 keV. Under the assumption of a dipole magnetic field configuration, this change in cyclotron harmonic energy corresponds to a decrease in the height of the emitting region by ~2 km, while other spectral parameters, in particular, the cutoff energy, remain essentially constant. At a luminosity ~7x10^{37} erg/s, four almost equidistant cyclotron line harmonics are clearly seen in the spectrum. This suggests that either the region where the emission originates is compact or the emergent spectrum from different (in height) segments of the accretion column is uniform. We have found significant pulse profile variations with energy, luminosity, and time. In particular, we show that the profile variations from pulse to pulse are not reduced to a simple modulation of the accretion rate specified by external conditions. ", "machine_abstract": "We have analyzed the pulse profiles in two energy bands (3-20 keV, 20-100 keV) for 4U0115+63 using data obtained by the Rossi X-ray Timing Explorer (RXTE). The results show that there is no significant difference between these two energy bands except at phase 0.7-0.9 where we see an excess emission above 100 keV. We also find that this source shows a double peaked profile with a dip around phase 0.5 which can be explained as due to absorption effects on the line-of-sight. In addition, we have used data from the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), which has detected cyclotrons lines near 30 keV and 60 keV respectively. Using our model parameters derived from the RXTE data analysis, we are able to reproduce both the observed pulse profiles and the cyclotrons line energies simultaneously.", "paraphrased_abstract": "Then, we analyzed the radiance profiles in two ten-kilovolt bands (i.e., three to ten-kilovolt bands) of the four cyclotrons, which we have obtained from the Rossi X-ray telegraph. This telegraph showed a double peak and a dip in the peak, which is attributed to the absorption of the light in the line. The spectral recurrence of this cyclotron is also shown in the third ten-kilovolt band. The cyclotrons are also present. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2592, "title": "Incoherent non-Fermi liquid scattering in a Kondo lattice", "abstract": "  One of the most notorious non-Fermi liquid properties of both archetypal heavy-fermion systems [1-4] and the high-Tc copper oxide superconductors [5] is an electrical resistivity that evolves linearly with temperature, T. In the heavy-fermion superconductor CeCoIn5 [5], this linear behaviour was one of the first indications of the presence of a zero-temperature instability, or quantum critical point. Here, we report the observation of a unique control parameter of T-linear scattering in CeCoIn5, found through systematic chemical substitutions of both magnetic and non-magnetic rare-earth, R, ions into the Ce sub-lattice. We find that the evolution of inelastic scattering in Ce1-xRxCoIn5 is strongly dependent on the f-electron configuration of the R ion, whereas two other key properties -- Cooper-pair breaking and Kondo-lattice coherence -- are not. Thus, T-linear resistivity in CeCoIn5 is intimately related to the nature of incoherent scattering centers in the Kondo lattice, which provides insight into the anomalous scattering rate synonymous with quantum criticality [7]. ", "machine_abstract": "We study the incoherent transport properties of a strongly correlated electron system, namely the periodic Anderson model (PAM) with strong hybridization between conduction and localized electrons. We show that this PAM can be mapped onto an effective spinless fermion model which is equivalent to a disordered Fermi gas at low temperatures. The resulting density of states exhibits a pseudogap structure near the chemical potential. In addition we find that the resistivity shows a linear temperature dependence below some characteristic temperature T*. This behavior is consistent with recent experiments on heavy-fermion compounds such as Yb$_{1}$-xYb$x$Cu$_2$Si$2$O$_{7-x}$. Finally, we discuss how our results are related to previous theoretical work based on the dynamical mean-field theory. PACS numbers: 71.27.+a, 72.25.+f, 74.78.+h", "paraphrased_abstract": "A, B, C, C, D, D,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3295, "title": "Computer simulation of model cohesive powders: influence of assembling procedure and contact laws on low consolidation states", "abstract": "  The structure and mechanical properties of a simple two-dimensional model of a cohesive powder are investigated by molecular dynamics simulations. Micromechanical ingredients involve elasticity, friction, a short range attraction and, possibly, rolling resistance (RR) in contacts. The microstructure of the cohesive packing varies according to the assembling procedure, from rather densely packed if isolated particles are directly compressed to much looser if the formation of large aggregates is allowed prior to compression. A crucial parameter is the ratio P*= Pa/F0 of applied pressure P, acting on grains of diameter $a$, to maximum tensile contact force F0. At low P* the final structure depends on the level of velocity fluctuations at the early stages of cluster aggregation. With RR the coordination number approaches 2 in the limit of low initial velocities or large rolling friction. The force network generally comprises small hyperstatic clusters with forces of the order of F0, joined by nearly unstressed, barely rigid arms. As P* grows, it quickly rearranges into force chain-like patterns. Density correlations witness a fractal structure, with dimension Df, up to some density-dependent blob size. WIth RR Df coincides with the ballistic aggregation result, despite a possibly different connectivity. Possible effects of some parameters on material strength are evoked. ", "machine_abstract": "The aim of this work is to study the effect of different assembly procedures, as well as different contact laws between particles in powder compaction processes using computer simulations. The results show that the packing density depends strongly on the applied pressure during the assembly process. In addition, it was found that the densification behavior can be influenced by changing the contact law parameters. Finally, we have shown how the microstructure evolves with increasing pressure for two different types of particle contacts. Powder compaction has been studied extensively over many years due to its importance in various industrial applications such as pharmaceutical tablets or metal parts produced via hot pressing techniques [1] . However, despite these efforts there are still open questions regarding the underlying mechanisms governing the densification behavior under external loads [2] . In order to understand the complex phenomena occurring inside compacted materials, several experimental studies were performed [3] , which revealed important information about the evolution of the microstructure [4] . Nevertheless, experiments cannot provide detailed insight into the local interactions taking place at the grain scale [5] . Therefore, numerical methods based on discrete element models (DEM) [6] became an alternative approach to investigate the mechanical properties of granular systems [7, 8] . These DEM-based approaches allow one to track individual grains within large assemblies and thus enable us to obtain valuable insights into the micromechanical response [9] . For example, recent investigations showed that the macroscopic stress-strain curve obtained from compression tests can be reproduced accurately if realistic inter-particle contact laws are used [10] .", "paraphrased_abstract": "This has made it possible to study the complex chemistry of the particles, and to determine the underlying mechanism of the microstructure of the particles. Powder is widely used for the manufacture of various products, including pharmaceutical tablets, metal parts produced by hot pressing, but also a lot of questions remain. So it is necessary to use numerical techniques based on discrete element models to investigate the micromechanical behavior of the particles, a technique that combines the two dimensionality of individual grains with the size of individual grains and thus gives us a good understanding of the microstructure of the particles. However, the experimental results show that the density of the powder depends strongly on the pressure applied during the assembly, and that the density of the powder increases with the increasing pressure. The results show that the density of the powder increases with the pressure applied. Besides, the density of the powder is influenced by the contact conditions. The results also show that the density of the powder is influenced by the contact law. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0528, "title": "Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless Networks", "abstract": "  This paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity L. Throughput capacity L is not achievable under 802.11. This paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. We show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3L/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690L (0.740L) when the source nodes are many hops away. We conjecture that 3L/4 is also the upper bound for general networks. When all links have equal length, 2L/3 can be shown to be the upper bound for general networks. Our simulations show that 802.11 networks with random topologies operated with AODV routing can only achieve throughputs far below the upper bounds. Fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. Indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously. ", "machine_abstract": "In this work, we study the throughput capacity region for multi-hop wireless networks with multiple transmitters and one receiver (MISO-MHWN). We first show that the MISO-MHWN is equivalent to an interference channel where each transmitter has only one antenna but can communicate simultaneously with all receivers in the network. Then, by using the concept of degrees-of-freedom (DoF), which characterizes how many parallel data streams are supported at high signal-to-noise ratio (SNR) regime, we derive outer bounds on the DoF region of the MISO-MHWNS. Finally, based on these results, we propose a novel transmission scheme called \"Interference Alignment\" (IA) to achieve the optimal DoF region. The proposed IA scheme exploits both spatial multiplexing gain as well as multiuser diversity gain. In particular, it allows different users to transmit their signals over non-overlapping time-frequency resources while maintaining full spatial reuse among them.", "paraphrased_abstract": "Then we introduce a new transmission method called interferenceal allocation, which combines spatial multiplexing with multiuser diversity. It combines the multiplexing and multiuser diversity, and combines the multiuser diversity of all users. The results are analyzed and then proposed a new transmission method based on interferenceal allocation. Then, by comparing the degrees of freedom (DOF) with the signal-to-noise ratio, we estimate the DoF region of the MISO-MHWNs. We first prove that MISO-MHWN is like a signal interference channel, where each transmitter has only one antenna and communicates simultaneously with all receivers in the network. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0619, "title": "Towards the optimal window for the 2MASS dipole", "abstract": "  A comparison of the 2MASS flux dipole to the CMB dipole can serve as a method to constrain a combination of the cosmological parameter Omega_m and the luminosity bias of the 2MASS survey. For this constraint to be as tight as possible, it is necessary to maximize the correlation between the two dipoles. This can be achieved by optimizing the survey window through which the flux dipole is measured. Here we explicitly construct such a window for the 2MASS survey. The optimization in essence reduces to excluding from the calculation of the flux dipole galaxies brighter than some limiting magnitude K_min of the near-infrared K_s band. This exclusion mitigates nonlinear effects and shot noise from small scales, which decorrelate the 2MASS dipole from the CMB dipole. Under the assumption of negligible shot noise we find that the optimal value of K_min is about five. Inclusion of shot noise shifts the optimal K_min to larger values. We present an analytical formula for shot noise for the 2MASS flux dipole, to be used in follow-up work with 2MASS data.   The misalignment angle between the two dipoles is a sensitive measure of their correlation: the higher the correlation, the smaller the expectation value of the angle. A minimum of the misalignment is thus a sign of the optimal gravity window. We model analytically the distribution function for the misalignment angle and show that the misalignment estimated by Maller et al. is consistent with the assumed underlying model (though it is greater than the expectation value). We predict with about 90% confidence that the misalignment will decrease if 2MASS galaxies brighter than K_min = 5 mag are excluded from the calculation of the flux dipole. This prediction has been indirectly confirmed by the results of Erdogdu et al. (ABRIDGED) ", "machine_abstract": "We present an analysis of the Galactic latitude distribution of stars in the 2MASS survey, and find that it is well described by a superposition of two Gaussians with different widths. The narrower Gaussian has a mean latitude of $-2^{\\circ}$ , while the broader one peaks at $+3^{\\circ}$ . We show that this double-Gaussian model can be used to explain the observed latitude dependence of the number density fluctuations as measured by the power spectrum of the 2MASS all-sky map.     In addition we demonstrate how our results are consistent with previous studies on the subject. Finally, using the derived parameters of the double-Gaussian model, we calculate the expected value of the quadrupole moment of the CMB temperature anisotropy induced by the Galaxy's gravitational potential. This calculation shows that the quadrupole moment predicted by the model agrees very well with recent measurements made by WMAP satellite.", "paraphrased_abstract": "In conclusion, by using the derived parameters of the double-Gaussian model, we calculate the expected temperature anisotropy of the irradiated mass of the Galaxy, which is induced by the gravitational potential of the Galaxy. We find that the results of this study are in accordance with previous work. We have also calculated the distribution of the Galactic latitudes in the survey of 2MASS and show that this distribution is well explained by two different Gaussian models of the two spheres. The narrower Gaussian has a mean latitude of -1.8circ, and the broad one is at -3.6circ. We show that this double Gaussian model can be used to explain the observed latitude dependence of the power spectrum of the 2MASS map of the sky. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0737, "title": "Orbifoldes speciales et classification bimeromorphe des varietes kaehleriennes compactes", "abstract": "  This is a sequel to [Ca01]=math.AG/0110051. We define the bimeromorphic {\\it category} of geometric orbifolds. These interpolate between (compact K\\\" ahler) manifolds and such manifolds with logarithmic structure. These geometric orbifolds are considered from the point of view of their geometry, and thus equipped with the usual invariants of varieties: morphisms and bimeromorphic maps, differential forms, fundamental groups and universal covers, fields of definition and rational points. The most elementary properties, directly adapted from the case of varieties without orbifold structure, are established here. The arguments of [Ca01] can then be directly adapted to extend the main structure results to this orbifold category. We hope to come back to deeper aspects later. The motivation is that the natural frame for the theory of classification of compact K\\\" ahler (and complex projective) manifolds includes at least the category of orbifolds, as shown in [Ca01] by the fonctorial decomposition of {\\it special} manifolds as tower of orbifolds with either $\\kappa_+=-\\infty$ or $\\kappa=0$, and also, seemingly, by the minimal model program, in which most proofs work only after the adjunction of a \"boundary\".   Also, fibrations enjoy in the bimeromorphic category of geometric orbifolds extension properties not satisfied in the category of varieties without orbifold structure, permitting to express invariants of the total space from those of the generic fibre and of the base. For example, the natural sequence of fundamental groups is exact there; also the total space is special if so are the generic fibre and the base. This makes this category suitable to lift properties from orbifolds having either $\\kappa_+=-\\infty$ or $\\kappa=0$ to those which are special. ", "machine_abstract": "Dans cet article, nous donnons une nouvelle preuve de la classification bimeromorphee des varietes kahleriennes compactes en utilisant les orbifolds speciaux. Nous montrons que si X est un orbifold special alors le groupe fondamental de X s'identifie au groupe fondamental d'une variete kahlerienne compacte Y . _SPLIT_ En particulier, on obtient que tout orbifold special admet une resolution symplectique. On montre aussi que toute application holomorphe entre deux orbifolds speciaux induit par passage aux groupes fondamentaux une application holomorphe sur leurs resolutions respectives. Enfin, on donne quelques exemples d'orbifolds speciaux qui ne sont pas resolubles. Keywords: Orbifoldes sp\u00e9ciaux, Vari\u00e9t\u00e9s K\u00e4hleriennes Compactes, Groupes fondamentaux, R\u00e9solutions symplectiques. 1 Introduction Soit X un espace analytique complexe connexe muni d'un feuilletage holomorphe F . Un tel objet sera appel\u00e9 dans ce texte un orbifold sp\u00e9cial (voir [V]). Dans cette note, nous allons donner une nouvelle d\u00e9monstration du th\u00e9or\u00e8me suivant d\u00fb \u00e0 Verbitsky : Th\u00e9or\u00e8me 0.1. (Verbitsky) Tout orbifold sp\u00e9cial admet une r\u00e9solution symplectique.", "paraphrased_abstract": "A tel objet sera appel\u00e9e dans ce texte un orbifold sp\u00e9cial (voir V). Dans cette note, nous allons donner une nouvelle d\u00e9monstration du th\u00e9or\u00e8me suivant d\u00fb \u00e0 Verbitsky, Th\u00e9or\u00e8me 0.1 (Verbitsky). Tout orbifold sp\u00e9cial admet une resolution symplectique. Si si X est un orbifold sp\u00e9cial, alors le groupe fondamental de X identifie au groupe fondamental de d'une kahlerienne compacte Y. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.043, "title": "Anonymity in the Wild: Mixes on unstructured networks", "abstract": "  As decentralized computing scenarios get ever more popular, unstructured topologies are natural candidates to consider running mix networks upon. We consider mix network topologies where mixes are placed on the nodes of an unstructured network, such as social networks and scale-free random networks. We explore the efficiency and traffic analysis resistance properties of mix networks based on unstructured topologies as opposed to theoretically optimal structured topologies, under high latency conditions. We consider a mix of directed and undirected network models, as well as one real world case study -- the LiveJournal friendship network topology. Our analysis indicates that mix-networks based on scale-free and small-world topologies have, firstly, mix-route lengths that are roughly comparable to those in expander graphs; second, that compromise of the most central nodes has little effect on anonymization properties, and third, batch sizes required for warding off intersection attacks need to be an order of magnitude higher in unstructured networks in comparison with expander graph topologies. ", "machine_abstract": "We study anonymity systems that operate over unstructured networks, such as Tor and Mixminion. We show how to construct mixes with provable guarantees against traffic analysis attacks by using only local information about the topology of the underlying network. Our construction is based on mixing small groups of users together at each node along their paths through the system. This approach allows us to achieve high levels of anonymity while maintaining low latency for most messages. In addition, we present several extensions to our basic scheme which allow it to be deployed more easily in practice. Finally, we evaluate our system both analytically and experimentally, showing that it achieves good performance under realistic conditions. Anonymity systems are used to protect user privacy when sending or receiving data over public communication channels. These systems typically consist of a set of nodes (called mixes) connected via some anonymous communication channel. Each message entering the system is encrypted multiple times before being sent out again; this process is called \"mixing\". The goal of these systems is to prevent attackers from linking senders and receivers of messages within the system. However, if all messages go through exactly the same sequence of mixes then they can still be linked using statistical techniques known as \"traffic analysis\". Traffic analysis has been shown to compromise the security of many existing anonymity systems including Tor [1] , Crowds [2] , Onion Routing [3] , Freenet [4] , and Mixminion [5] . To overcome this problem, researchers have proposed various approaches [6] - [8] . In this work, we focus on anonymity systems operating over unstructured networks [9]- [11] . Unstructured networks differ from traditional peer-to-peer networks [12] because there is no global knowledge available regarding the structure of the network. Instead, each node maintains only partial information about its immediate neighbors. For example, in the case of Tor [13] , each node knows only the identity of its direct neighbors but not those of other nodes further away. As another example, in Mixminion [14] , each node knows only whether two given nodes are directly connected or not", "paraphrased_abstract": "A privacy-protecting network is a communication system for the private communication of people and data, which consists of a series of nodes connected in an anonymous manner, in the form of a network. Anonymous networks differ from traditional peer-to-peer networks in that the network is unknown to every node in its path, and no one knows anything about the structure of the network. We have a knowledge of the topology of the network at every node, but there is no knowledge of the identities of the neighbors, as in Tor, for example, Tor is not known, but only the identities of the nearest neighbors. And for a similar purpose, we have a description of some extensions to our basic design that make it more convenient for us to implement. In this paper, we will discuss the study of the privacy of anonymity systems that operate on unstructured networks, such as Tor, and MixMirror. These systems, which are generally used for sending and receiving messages, are composed of a set of nodes connected by some unsecured channel. In these systems, messages are encrypted for a long time and sent out repeatedly. This process is called \u201cmixing\u201d. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1965, "title": "SDSS J080531.84+481233.0: An Unresolved L Dwarf/T Dwarf Binary", "abstract": "  SDSS J080531.84+481233.0 is a peculiar L-type dwarf that exhibits unusually blue near-infrared and mid-infrared colors and divergent optical (L4) and near-infrared (L9.5) spectral classifications. These peculiar spectral traits have been variously attributed to condensate cloud effects or subsolar metallicity. Here I present an improved near-infrared spectrum of this source which further demonstrates the presence of weak CH4 absorption at 1.6 micron but no corresponding band at 2.2 micron. It is shown that these features can be collectively reproduced by the combined light spectrum of a binary with L4.5 and T5 components, as deduced by spectral template matching. Thus, SDSS J080531.84+481233.0 appears to be a new low-mass binary straddling the L dwarf/T dwarf transition, an evolutionary phase for brown dwarfs that remains poorly understood by current theoretical models. The case of SDSS J080531.84+481233.0 further illustrates how a select range of L dwarf/T dwarf binaries could be identified and characterized without the need for high angular resolution imaging or radial velocity monitoring, potentially alleviating some of the detection biases and limitations inherent to such techniques. ", "machine_abstract": "We report the discovery and characterization of an unresolved binary system consisting of two late-type dwarf stars, SDSS J080531.8+481233.0AB (hereafter referred to as J0805+4812). The primary component is classified as a T6p dwarf star with a mass of ~70 MJup while its companion has been identified as a cool brown dwarf candidate with a temperature between 1000-2000 K. We have used high-resolution near-infrared spectroscopy obtained at Gemini Observatory in order to confirm that both components are gravitationally bound. Our analysis shows that this object is one of the most massive known binaries composed by two low-mass objects. This work was supported by NASA's Astrophysics Data Analysis Program under award NNX10AD20G issued through the Space Telescope Science Institute, which is operated by AURA for NASA under contract NAS 5-26555. In addition we acknowledge support from NSF grant AST-0908816.     We present the discovery and characterization of a new unresolved binary system made up of two late type dwarfs, SDSSJ080531.8 + 481233.0 AB (hereafter J0805+4812; see Figure 1 ). The primary component is classified spectroscopically as a T6p star with a mass of about 70 M Jup , while its companion has been tentatively identified as a cool brown-dwarf candidate with temperatures ranging from 1000 - 2000K . Using high resolution infrared spectroscopy taken at Gemini Observatory , we show that these two objects are gravitationally bound . This makes it one of the most massive systems ever found containing two low-mass objects .", "paraphrased_abstract": "The discovery and characterisation of a new system consists of two dwarfs of late type, SYB 00435 and SYB 00436 (see Figure 1), of the type T6p and SYB 00436 respectively, and is of a size of about 70 MJ, while its companion is a cool brown dwarf, whose temperature is around 2000 K. The gravity of these two dwarfs is, therefore, remarkably high, which makes this system one of the most massive in the world. Observed by the Gemini Observatory, they are both gravitationally bound. We describe the new symbol, containing two dwarfs of the same type, J080531.8 and 481233.0 \u2013 J0805 and 481234. We show how the two dwarfs are gravitationally bound by the gemini telescope, and show that they are both gravitationally bound. The results are remarkable. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1148, "title": "Aspects of stochastic resonance in reaction-diffusion systems: The nonequilibrium-potential approach", "abstract": "  We analyze several aspects of the phenomenon of stochastic resonance in reaction-diffusion systems, exploiting the nonequilibrium potential's framework. The generalization of this formalism (sketched in the appendix) to extended systems is first carried out in the context of a simplified scalar model, for which stationary patterns can be found analytically. We first show how system-size stochastic resonance arises naturally in this framework, and then how the phenomenon of array-enhanced stochastic resonance can be further enhanced by letting the diffusion coefficient depend on the field. A yet less trivial generalization is exemplified by a stylized version of the FitzHugh-Nagumo system, a paradigm of the activator-inhibitor class. After discussing for this system the second aspect enumerated above, we derive from it -through an adiabatic-like elimination of the inhibitor field- an effective scalar model that includes a nonlocal contribution. Studying the role played by the range of the nonlocal kernel and its effect on stochastic resonance, we find an optimal range that maximizes the system's response. ", "machine_abstract": "We study the effects of noise on spatially extended systems by using an extension of the concept of nonequilibrium potential (NEP). We show that NEPs can be used to characterize different types of stochastic resonances, such as those observed for excitable and bistable systems near their respective Hopf bifurcations. In particular we find that the presence of noise enhances the amplitude of oscillations in both cases but with very different mechanisms. For excitable systems this is due to the fact that noise increases the probability of crossing the threshold between two stable states; while for bistable systems it occurs because noise induces transitions between these states. Finally, we discuss how our results are related to previous studies based on other approaches. Stochastic resonance has been studied extensively during recent years [1] . It refers to the phenomenon whereby weak signals can be enhanced or detected more easily when they are embedded into a noisy background [2] . In many physical situations, however, one needs to consider not only the effect of external noise sources but also internal fluctuations arising from the dynamics itself [3] . This problem becomes particularly relevant if the signal-to-noise ratio is small [4] , which may occur either because the signal is intrinsically weak or because its intensity is comparable to the level of intrinsic noise [5] . Moreover, even though the signal is strong enough so that it could be clearly distinguished without any additional noise [6] , there might still exist some optimal amount of noise that maximizes the detection efficiency [7, 8] .", "paraphrased_abstract": "In this connection, we consider the effects of noise on spatially extended systems. We use the concept of non-equilibrium potential (NEP), to characterize different kinds of stochastic resonances, such as those that are observed for example in the oscillations of the excitable and the bistable, as well as in the oscillations of the bistable. The results of this study are related to the results of some other experiments based on other methods. We examine the effect of noise on spatially extended systems by means of a variation of the principle of the non-equilibrium potential, or the effect of the noise on the oscillation of the oscillation. The effects of noise are particularly relevant when the signal to noise ratio is very small, whether it is intrinsically weak or because it is resolvingly with a similar intensity. Moreover, although the signal is strong enough to be easily distinguished, it may not be sufficiently strong to make it clear, but there may be some suitable amount of noise that maximizes the detection efficiency. We study the effects of noise on the spatially extended systems by applying the concept of the non-equilibrium potential (NEP). In particular, we find that noise increases the frequency of oscillations in both cases, but in different ways: for the excitation the noise increases the probability of crossing the threshold between two stable states, while for the bistability the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0538, "title": "Radio Through X-ray Spectral Energy Distributions of 38 Broad Absorption Line Quasars", "abstract": "  We have compiled the largest sample of multiwavelength spectral energy distributions (SEDs) of Broad Absorption Line (BAL) quasars to date, from the radio to the X-ray. We present new Spitzer MIPS (24, 70, and 160 micron) observations of 38 BAL quasars in addition to data from the literature and public archives. In general, the mid-infrared properties of BAL quasars are consistent with those of non-BAL quasars of comparable luminosity. In particular, the optical-to-mid-infrared luminosity ratios of the two populations are indistinguishable. We also measure or place upper limits on the contribution of star formation to the far-infrared power. Of 22 (57%) upper limits, seven quasars have sufficiently sensitive constraints to conclude that star formation likely contributes little (<20%) to their far-infrared power. The 17 BAL quasars (45%) with detected excess far-infrared emission likely host hyperluminous starbursts with L_fir,SF=10^{13-14} L_sun. Mid-infrared through X-ray composite BAL quasar SEDs are presented, incorporating all of the available photometry. Overall, we find no compelling evidence for inherent differences between the SEDs of BAL vs. non-BAL quasars of comparable luminosity. Therefore a ``cocoon'' picture of a typical BAL quasar outflow whereby the wind covers a large fraction of the sky is not supported by the mid-infrared SED comparison with normal quasars, and the disk-wind paradigm with a typical radio-quiet quasar hosting a BAL region remains viable. ", "machine_abstract": "We present the radio through X-ray spectral energy distributions (SEDs) for 38 quasars with broad absorption lines in their optical spectra, selected from the Sloan Digital Sky Survey Data Release 5 quasar catalog and observed by Chandra and/or XMM-Newton. We find that these sources are typically characterized by steep radio to infrared continua, weak or absent emission lines at ultraviolet wavelengths, and strong soft excesses below 1 keV. The majority of our sample show evidence for significant intrinsic reddening as indicated by the presence of deep UV troughs and high values of the Balmer decrement. In addition, we detect several objects which exhibit extremely flat radio-to-X-ray slopes indicative of relativistic beaming effects. These results suggest that BAL quasars represent an important phase in the evolution of luminous active galactic nuclei during which they undergo rapid changes in physical conditions within their central regions. This is supported by recent theoretical models suggesting that BAL outflows may play an important role in regulating black hole growth via feedback processes.     Keywords: Active Galactic Nuclei", "paraphrased_abstract": "\" In our study, we present a selection of quasars with broad absorption lines from the Sloan Digital Sky Survey, which we took from the quasars of the Sloan Digital Sky Survey, and which we observed by Chandra and XMM-Newton. These quasars are of an important phase of the luminous active galactic nuclei, which undergoes rapid physical changes in their central regions, and can play a central role in black hole development. In addition, we found several objects with extremely flat slopes, indicating the influence of relativistic beaming. We find the quasars with a broad spectrum of absorption in their optical spectra, with the usual steep-wave radiation in the middle of the horizon, with a weak or absent emission in the ultraviolet, with weak equivalences below 1 keV. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0196, "title": "Atomic force microscopy (AFM) study of thick lamellar stacks of phospholipid bilayers", "abstract": "  We report an Atomic Force Microscopy (AFM) study on thick multi lamellar stacks of approx. 10 mum thickness (about 1500 stacked membranes) of DMPC (1,2-dimyristoyl-sn-glycero-3-phoshatidylcholine) deposited on silicon wafers. These thick stacks could be stabilized for measurements under excess water or solution. From force curves we determine the compressional modulus B and the rupture force F_r of the bilayers in the gel (ripple), the fluid phase and in the range of critical swelling close to the main transition. AFM allows to measure the compressional modulus of stacked membrane systems and values for B compare well to values reported in the literature. We observe pronounced ripples on the top layer in the Pbeta' (ripple) phase and find an increasing ripple period Lambda_r when approaching the temperature of the main phase transition into the fluid Lalpha phase at about 24 C. Metastable ripples with 2Lambda_r are observed. Lambda_r also increases with increasing osmotic pressure, i.e., for different concentrations of polyethylene glycol (PEG). ", "machine_abstract": "The structure and dynamics of lipid membranes are important for many biological processes, such as cell division or protein transport across the membrane. In this work we use atomic force microscopy to investigate the structural properties of stacked layers of phospholipids in water. We find that these structures form spontaneously on mica surfaces at room temperature within minutes after adding the lipids into solution. The height profiles show that the thicknesses of the individual layers vary between 1 nm and 2 nm depending on their composition. By analyzing the lateral diffusion coefficients of single molecules with respect to time, we can determine whether they are mobile or immobile. Our results indicate that the mobility is strongly dependent on the number of layers present in each stack. For example, while most of the molecules in one layer diffuse freely over large distances, those in two layers exhibit only small displacements perpendicular to the surface. This behavior suggests that the mobility decreases significantly when more than one layer forms.", "paraphrased_abstract": "At room temperature, the water, which is in liquid form, has a thickness of 1 to 2 nm. Depending on the composition, the thickness of the individual layers may vary between one nm and two nm. The density of individual molecules in the stacks is determined by the number of layers in the stack, and by examining the lateral diffusion coefficients of the molecules over time. In the case of two layers, the density of the molecules is much smaller, and the movement of the molecules in two layers is proportional to the thickness of the layer. We can study the structure of the lipid membrane, the structure of which is important for many biological processes, for example, the division of cells, the transfer of proteins. In this study, we have applied atomic force microscopy to study the cellular structure and dynamics of the lipid membrane. We found that these structures erupted in the presence of lipids at room temperature within minutes after idling the lipids. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0556, "title": "Ballistic Graphene Nanoribbon MOSFETs: a full quantum real-space simulation study", "abstract": "  A real-space quantum transport simulator for carbon nanoribbon (CNR) MOSFETs has been developed. Using this simulator, the performance of carbon nanoribbon (CNR) MOSFETs is examined in the ballistic limit. The impact of quantum effects on device performance of CNR MOSFETs is also studied. We found that 2D semi-infinite graphene contacts provide metal-induced-gap-states (MIGS) in the CNR channel. These states would provide quantum tunneling in the short channel device and cause Fermi level pining. These effects cause device performance degradation both on the ON-state and the OFF-state. Pure 1D devices (infinite contacts), however, show no MIGS. Quantum tunneling effects are still playing an important role in the device characteristics. Conduction due to band-to-band tunneling is accurately captured in our simulations. It is important in these devices, and found to dominate the off-state current. Based on our simulations, both a 1.4nm wide and a 1.8nm wide CNR with channel length of 12.5nm can outperform ultra scaled Si devices in terms of drive current capabilities and electrostatic control. Although subthreshold slopes in the forward-bias conduction are better than in Si transistors, tunneling currents are important and prevent the achievement of the theoretical limit of 60mV/dec. ", "machine_abstract": "We present the first fully self-consistent, atomistic quantum transport calculations for ballistic graphene nanoribbons (GNRs) with realistic band structure and electrostatic potential profiles using nonequilibrium Green's function formalism in combination with density functional theory (DFT). We show that the GNRs' electronic properties are strongly dependent on their widths as well as edge structures. The calculated current-voltage characteristics reveal several interesting features such as negative differential resistance at low bias voltages due to resonant tunneling through localized states near the Fermi level. In addition, we find that the presence of hydrogen passivation layers can significantly enhance the device performance by suppressing the backscattering effect caused by defects or impurities along the edges.     Keywords: Ballistic transport, Graphene nanoribbon, Nonequilibrium Green's functions, Density functional theory, Quantum transport calculation. 1 Introduction     Graphene is an emerging material which has attracted considerable attention recently because it exhibits unique physical properties [1] . It consists of carbon atoms arranged into a honeycomb lattice where each carbon atom forms covalent bonds with three neighboring carbons [2] . Due to its two-dimensional nature, graphene shows high carrier mobility [3] , thermal conductivity [4] , mechanical strength [5] , optical transparency [6] , and flexibility [7] . These remarkable properties make graphene promising candidates for future nanoelectronic devices [8] .   Graphene nanoribbons (G-NR), i.e., strips of graphene with finite width [9] , have been proposed as building blocks for various applications including transistors [10] , interconnects [11] , photodetectors [12] , solar cells [13] , sensors [14] , etc.. Compared to conventional silicon-based electronics [15] , GNRs offer many advantages [16] : they exhibit higher electron mobilities [17] ; they allow better control over the charge carriers [18] ; they provide more design freedom [19] ; and they enable new functionality [20] . However, there still exist some challenges associated with practical realization of GNRs [21] . For example, the fabrication process requires precise control of ribbon width [22] and edge roughness [23] . Moreover, the electrical properties of GNRs depend sensitively", "paraphrased_abstract": "We present the first accurate and accurate computations of a quantum transport of graphene on the basis of the size of the tr-di-ltcs, and the electrostatic potential of the graphene bands, based on the non-requilibrium green\u2019s function, and on the basis of density functional theory. Graphene is a new material that has recently received considerable attention because of its unique properties: it is composed of carbon atoms, arranged in a honeycomb, in which each atom binds with the neighboring carbons, and its structure is two-dimensional, with high mobility, thermal conductivity, thermal strength, optical transparency, and flexibility. Moreover, the electrical properties of graphene depend on the width of the layer, the thickness of the material, the degree of decomposition of the material, and the degree of decomposition of the material, which we assume by arithmetic. We consider the GNRs as a construction for various electronic devices, such as transistors, interconnects, solar cells, sensors, and sensors, among others. Moreover, we consider that the material of GNRs must be precisely controlled, in particular, by the thickness of the layer, the edge, and the thickness of the material. Moreover, the electrical properties of the GNRs are sensitive to the size and roughness of the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2351, "title": "The Use of ITIL for Process Optimisation in the IT Service Centre of Harz University, exemplified in the Release Management Process", "abstract": "  This paper details the use of the IT Infrastructure Library Framework (ITIL) for optimising process workflows in the IT Service Centre of Harz University in Wernigerode, Germany, exemplified by the Release Management Process. It is described, how, during the course of a special ITIL project, the As-Is-Status of the various original processes was documented as part of the process life cycle and then transformed in the To-Be-Status, according to the ITIL Best Practice Framework. It is also shown, how the ITIL framework fits into the four-layered-process model, that could be derived from interviews with the universities IT support staff, and how the various modified processes interconnect with each other to form a value chain. The paper highlights the final results of the project and gives an outlook on the future use of ITIL as a business modelling tool in the IT Service Centre of Harz University. It is currently being considered, whether the process model developed during the project could be used as a reference model for other university IT centres. ", "machine_abstract": "The purpose of this study is to examine how an organisation can use Information Technology Infrastructure Library (ITIL) as a process optimisation tool within its service centre environment. The research focuses on the release management process at the IT service centre of Harz University and examines whether it complies with all relevant aspects of the ITIL framework. A case study approach was used to collect data through interviews and document analysis. This article presents the results of the investigation into the current state of the release management process at Harz University's IT service centre. It also provides recommendations for improving the process based on the findings. Keywords: information technology infrastructure library, process optimization, release management process, university, IT service centre, case study method. 1 Introduction Today, universities are increasingly relying on their computer systems to support teaching and research activities. As such, they need reliable services that meet user requirements [1] . However, many universities have experienced problems related to poor quality software releases [2] , which may lead to system failures or even loss of data [3] . In order to avoid these issues, organisations should implement appropriate processes [4] . One widely accepted set of guidelines for managing information technology (IT) services is provided by the Information Technology Infrastructure Library [5] . According to [6] , ITIL has been adopted by more than 80% of Fortune 500 companies worldwide since 2003.", "paraphrased_abstract": "This article describes the current state of the release-processing at Harz University. It discusses the issue of the release process at the university's IT service centre. It also presents the recommendations for improving the release-processing by means of the case study. Besides, ITIL is widely adopted by most large organizations and has been used by more than 80% of the Fortune 500 companies. Moreover, in many universities, for example, the software has not been released on time and has not been properly regulated. To avoid this, they need to establish suitable processes. In this study, we examine the remittance management at the IT service center of Harz University and test whether it is in accordance with all the requirements of the ITIL. The aim of this study is to find out whether the release management system of the university can be used as a process optimisation tool for its service center. The study focuses on the release management process in the IT service center of Harz University and studies whether it complies with all the requirements of the ITIL. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.153, "title": "A study of the $p d \\to p d \\eta$ reaction", "abstract": "  A study of the $p d \\to p d \\eta$ reaction in the energy range where the recent data from Uppsala are available, is done in the two-step model of $\\eta$ production including the final state interaction. The $\\eta -d$ final state interaction is incorporated through the solution of the Lippmann Schwinger equation using an elastic scattering matrix element, $T_{\\eta d \\to \\eta d}$, which is required to be half off-shell. It is written in a factorized form, with an off-shell form factor multiplying an on-shell part given by an effective range expansion up to the fourth power in momentum. The parameters of this expansion have been taken from an existing recent relativistic Faddeev equation solution for the $\\eta NN$ system corresponding to different $\\eta-N$ scattering amplitudes. Calculations have also been done using few body equations within a finite rank approximation (FRA) to generate $T_{\\eta d \\to \\eta d}$. The $p-d$ final state interaction is included in the spirit of the Watson-Migdal prescription by multiplying the matrix element by the inverse of the Jost function. The $\\eta-d$ interaction is found to be dominant in the region of small invariant $\\eta -d$ mass, $M_{\\eta d}$. The $p-d$ interaction enhances the cross section in the whole region of $M_{\\eta d}$, but is larger for large $M_{\\eta d}$. We find nearly isotropic angular distributions of the proton and the deuteron in the final state. All the above observations are in agreement with data. The production mechanism for the entire range of the existing data on the $p d \\to p d \\eta$ reaction seems to be dominated by the two-step model of $\\eta$ production. ", "machine_abstract": "We have studied the $^{p}_{d}$ elastic scattering at low energies in order to extract information on the nucleon-nucleon interaction and its off-shell behavior.  We used an effective Lagrangian approach with one-boson exchange potentials for the mesons, which are constrained by experimental data. The results obtained show that the inclusion of the pion-exchange potential is essential to reproduce correctly the phase shifts up to about 100 MeV/c.  The analysis performed shows also that the contribution coming from the rho-meson exchange gives rise to a repulsive effect in the nuclear force, while the omega-meson exchange produces attractive effects. Finally we have calculated the cross section for the process $$^{p}_{d} \\to ^{p}_d\\eta$$ using the same formalism as before. Our results agree well with those reported recently by other authors. In this work we present some new results concerning the calculation of the cross sections for the processes $$^{p}_{d} \\rightarrow ^{p}_d \\eta$$  and $$^{p}_{d} \\leftarrow ^{p}_d \\gamma$$ . These reactions can be considered as complementary probes of the nucleonnucleon interaction since they involve different spin states (scalar versus vector) and different orbital angular momenta (zero versus one).", "paraphrased_abstract": "\u201cWe have now performed experiments on the elastic scattering at low energies in order to gather some information on the nucleon-nucleon interaction, and its off-shell behavior. In this work, we present new results concerning the calculation of cross-sections for the processes of \u201cpd\u201d and \u201cpd\u201d to \u201cpd\u201d and \u201cpd\u201d to pd eta\u201d and \u201cpd\u201d to pd gamma\u201d. This can be considered complementary probes of the nucleon-nucleon interaction, since the two processes involve different spins (scalar versus vector) and different orbital angular momenta (zero versus one). Then, calculating the cross-section of the process \u2018pd\u2019 to pd eta\u2019, we used a Lagrangian approach with pion exchanging potentials, which were constrained by the experimental data. The results showed that the inclusion of the pion exchanging potential was necessary to accurately describe the phase shifts up to about 100 MeV/c. The results showed that the rho-meson exchange is detrimental to the nuclear force, while the omega exchange is attractive. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1006, "title": "Sharp $L^p$-estimates for maximal operators associated to hypersurfaces in $\\bR^3$ for $p>2.$", "abstract": "  We study the boundedness problem for maximal operators $\\M$ associated to smooth hypersurfaces $S$ in 3-dimensional Euclidean space. For $p>2,$ we prove that if no affine tangent plane to $S$ passes through the origin and $S$ is analytic, then the associated maximal operator is bounded on $L^p(\\RR^3)$ if and only if $p>h(S),$ where $h(S)$ denotes the so-called height of the surface $S.$ For non-analytic finite type $S$ we obtain the same statement with the exception of the exponent $p=h(S).$ Our notion of height $h(S)$ is closely related to A. N. Varchenko's notion of height $h(\\phi)$ for functions $\\phi$ such that $S$ can be locally represented as the graph of $\\phi$ after a rotation of coordinates.   Several consequences of this result are discussed. In particular we verify a conjecture by E.M. Stein and its generalization by A. Iosevich and E. Sawyer on the connection between the decay rate of the Fourier transform of the surface measure on $S$ and the $L^p$-boundedness of the associated maximal operator $\\M$, and a conjecture by Iosevich and Sawyer which relates the $L^p$-boundedness of $\\M$ to an integrability condition on $S$ for the distance function to tangential hyperplanes, in dimension three.   In particular, we also give ess. sharp uniform estimates for the Fourier transform of the surface measure on $S,$ thus extending a result by V.N. Karpushkin from the analytic to the smooth setting and implicitly verifying a conjecture by V.I. Arnol'd in our context. ", "machine_abstract": "In this article, we prove sharp Lp estimates for the maximal operator associated with the surface measure on a closed convex surface in R3 when p>2.  We also show that these results are optimal by constructing examples where the bounds cannot be improved. The main tool used is an extension theorem due to Coifman and Weiss which allows us to reduce matters to studying the case of surfaces contained in planes. This reduction leads naturally to consideration of certain singular integral operators whose kernels have been studied extensively over the past several decades. In particular, we use recent work of David Jerison and Michael Loss concerning the behavior of such operators near their singularities. Finally, we apply our results to obtain new information about the regularity properties of solutions to the Dirichlet problem for Laplace's equation on bounded domains in R3. Our results can be viewed as extensions of earlier work done by Kenig and Stein who proved similar results for the unit sphere in R4.", "paraphrased_abstract": "The main tool used is an extension theorem by Coifman and Weiss, which reduces the situation to the case of the surface of a plane. This reduction leads naturally to the consideration of certain integral operators, of which the most recent work has been devoted. This method is based on recent work of David Jerison and Michael Loss. We draw herein from the study of the case of the surfaces of a plane. We show here the effects of these operators on the regularity of solutions to the Dirichlet problem of the equation of Laplace on bounded domains. This article is based on the work of Kenig and Stein, who showed these results for the unit sphere of R3. The main tool we use is the extension theorem of Coifman and Weiss, which we can apply to the case of a surface contained in a plane, and this reduced case opens the way to a variety of singular integral operators, the kernel of which has been thoroughly studied in recent decades. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4382, "title": "Analysis of long range order", "abstract": "  A first principles analysis of order-disorder transition in alloys shows that ordering energy is a function of temperature due to thermal vibrations. The inter-nuclear potential energy term converges if zero point vibrations are incorporated and this method can replace the Ewald sum method. Core energy contributions to the ordering energy are stored exclusively in superlattice lines. The effect of electron-phonon interactions on ordering energy is of the same order of magnitude as ordering energy near transition temperatures and cannot be ignored. Ising model and variants are incorrect in explaining alloy phase transitions as they ignore the role of electron-phonon interactions without justification. A theoretical formalism that incorporates the Debye-Waller Factor component of electron-phonon interactions in electronic structure calculations already exists and must be adopted when modeling temperature dependent phenomena. It is suggested that DWF correction will account substantially for the discrepancy between experimental and theoretical ordering energy in Ni3V. Thermal vibrations alter magnetic ordering energy at finite temperatures. The role of electron-phonon interactions in alloy and magnetic phase transitions cannot be ignored and must be incorporated in all models. This will also ensure consistency with x-ray and electron diffraction (alloy transitions) and neutron diffraction (magnetic transitions) results. An isotope effect is predicted for (magnetic) phase transitions if the transition temperature is below Debye temperature. Recent observations of an isotope effect in magnetic phase transitions confirm our above conclusions and imply that the role of electron-phonon interactions must be incorporated in all theories and models of magnetism to avoid contradictions. ", "machine_abstract": "The concept of long-range order is central to the understanding of many physical phenomena, including phase transitions and critical behavior in statistical mechanics.  In this article we present an analysis of the mathematical foundations for the study of such systems.   We begin by introducing the notion of a measure-preserving transformation on a probability space (X, \\Sigma, \\mu) as well as that of a measurable partition of X into subsets with equal probabilities.    Next, we define the entropy H(T) associated with T as the supremum over all possible values of the Shannon information content of any finite collection of sets belonging to some fixed partition P of X.   Finally, we introduce the Kolmogorov-Sinai entropy hKS(T), which measures the rate at which information can be produced by observing the evolution of the system under consideration.   The main result presented here shows how these two quantities are related through the use of conditional entropies.", "paraphrased_abstract": "This was the first work to introduce the concept of a measure-preserving transformation of a probability space (X, Sigma, mu), which was the measurable division of X into subsets of equal probability. Then, we introduce the entropy H(T) associated with T, which is the dominant value of the Shannon information content of any finite set of units belonging to the partition of X into a fixed P. The main result showed that the relationship between these two quantities was made up by means of conditional entropies., i.e., entropy H(T)  is the absolute value of all possible values of Shannon information content of any infinite set of sets belonging to a fixed partition of X. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0645, "title": "Discovery of extreme asymmetry in the debris disk surrounding HD 15115", "abstract": "  We report the first scattered light detection of a dusty debris disk surrounding the F2V star HD 15115 using the Hubble Space Telescope in the optical, and Keck adaptive optics in the near-infrared. The most remarkable property of the HD 15115 disk relative to other debris disks is its extreme length asymmetry. The east side of the disk is detected to ~315 AU radius, whereas the west side of the disk has radius >550 AU. We find a blue optical to near-infrared scattered light color relative to the star that indicates grain scattering properties similar to the AU Mic debris disk. The existence of a large debris disk surrounding HD 15115 adds further evidence for membership in the Beta Pic moving group, which was previously argued based on kinematics alone. Here we hypothesize that the extreme disk asymmetry is due to dynamical perturbations from HIP 12545, an M star 0.5 degrees (0.38 pc) east of HD 15115 that shares a common proper motion vector, heliocentric distance, galactic space velocity, and age. ", "machine_abstract": "We report on new observations made with the Hubble Space Telescope (HST) that reveal an extremely asymmetric distribution of dust around the young star HD 15115, located at a distance of about 150 light-years away in the constellation Pictor.  The images show two bright lobes extending to distances greater than 100 AU and separated by more than 50 AU along the major axis of the system's circumstellar disk. We interpret these features as evidence for recent planet formation activity within this protoplanetary disk. This is one of only three known cases where such structures have been observed directly; all other examples are inferred indirectly through modeling or scattered-light imaging techniques. These results provide important constraints on models of planet formation and migration. In addition, we find no evidence for any significant amount of cold dust beyond 250 AU from the central star. Our findings suggest that the outer regions of protoplanetary disks may be cleared out much faster than previously thought.", "paraphrased_abstract": "And since there is no cold dust beyond the central star, there is no evidence of a significant amount of cold air. Moreover, there is no evidence of any significant cold dust, at least up to the central star. We report on new observations made by the Hubble Space Telescope (HST) in the vicinity of the young star HD 15115, which is located about 150 light-years from Earth in the constellation Pictor. The observations show two lobes of light extending as far as 100 AU and separated by a distance of over 50 AU along the circumstellar disk. These observations provide a strong basis for the modeling of planet formation and migration. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.224, "title": "Spitzer observations of a 24 micron shadow: Bok Globule CB190", "abstract": "  We present Spitzer observations of the dark globule CB190 (L771). We observe a roughly circular 24 micron shadow with a 70 arcsec radius. The extinction profile of this shadow matches the profile derived from 2MASS photometry at the outer edges of the globule and reaches a maximum of ~32 visual magnitudes at the center. The corresponding mass of CB190 is ~10 Msun. Our 12CO and 13CO J = 2-1 data over a 10 arcmin X 10 arcmin region centered on the shadow show a temperature ~10 K. The thermal continuum indicates a similar temperature for the dust. The molecular data also show evidence of freezeout onto dust grains. We estimate a distance to CB190 of 400 pc using the spectroscopic parallax of a star associated with the globule. Bonnor-Ebert fits to the density profile, in conjunction with this distance, yield xi_max = 7.2, indicating that CB190 may be unstable. The high temperature (56 K) of the best fit Bonnor-Ebert model is in contradiction with the CO and thermal continuum data, leading to the conclusion that the thermal pressure is not enough to prevent free-fall collapse. We also find that the turbulence in the cloud is inadequate to support it. However, the cloud may be supported by the magnetic field, if this field is at the average level for dark globules. Since the magnetic field will eventually leak out through ambipolar diffusion, it is likely that CB190 is collapsing or in a late pre-collapse stage. ", "machine_abstract": "We report the detection of an infrared dark cloud (IRDC) in the vicinity of the open cluster NGC 6334, using data obtained with Spitzer Space Telescope's Infrared Array Camera (IRAC). The IRDC is associated with the molecular cloud complex G327.3+0.6 and has been identified as Bok globule CB190 by Clemens & Barvainis (1988) . We find that this object exhibits a prominent 24 micron shadow which may be caused by absorption against bright mid-infrared emission from nearby protostars or young stellar objects. This feature suggests that the cloud contains dense cores at different evolutionary stages. Using near-infrared extinction mapping we identify two candidate starless cores within the cloud. These are located near the center of the cloud where the 24 micron shadow is most pronounced. Our analysis shows that these cores have masses between 0.5 Msun to 1 Msun and radii ranging from 1000 AU to 3000 AU .", "paraphrased_abstract": "As you know, the first part of the study was a complete analysis of the objects of the NGC, a clear, open cluster, which is also a member of the dark cloud complex G327.3-0.3, which, according to the astronomical classification of the Bok globule, is a Bok globule, the identification of which is by Clemens and Barvainis. The second part of the analysis was to establish two starless cores in the dark cloud. The two cores are located at the centre of the cloud, where the 24-micron darkness is most prominent. Its mass is between 0.5 Msun to 1 Msun and their radii from a thousand AU to a million AU. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3915, "title": "Testing Explanations of the $B\\to\\phi K^*$ Polarization Puzzle", "abstract": "  $B\\to\\phi K^*$ ($\\btos$) is three separate decays, one for each polarization of the final-state vector mesons (one longitudinal, two transverse). It is observed that the fraction of transverse decays, $\\fT$, and the fraction of longitudinal decays, $\\fL$, are roughly equal: $\\fTfL \\simeq 1$, in opposition to the naive expectation that $\\fT \\ll \\fL$. If one requires a single explanation of all polarization puzzles, two possibilities remain within the standard model: penguin annihilation and rescattering. In this paper we examine the predictions of these two explanations for $\\fTfL$ in $\\btod$ decays. In $B \\to \\rho\\rho$ decays, only $\\bd \\to \\rho^0\\rho^0$ can possibly exhibit a large $\\fTfL$. In B decays related by U-spin, we find two promising possibilities: (i) $B^+ \\to K^{*0} \\rho^+$ ($\\btos$) and $B^+ \\to \\Kbar^{*0} K^{*+}$ ($\\btod$) and (ii) $\\bs \\to K^{*0} \\Kbar^{*0}$ ($\\btos$) and $\\bd \\to \\Kbar^{*0} K^{*0}$ ($\\btod$). The measurement of $\\fTfL$ in these pairs of decays will allow us to test penguin annihilation and rescattering. Finally, it is possible to distinguish penguin annihilation from rescattering by performing a time-dependent angular analysis of $\\bd \\to \\Kbar^{*0} K^{*0}$. ", "machine_abstract": "We present new measurements of the longitudinal polarization fraction $f_L$ in B meson decays to $\\phi K^*$ final states, using data collected by the Belle experiment at KEKB with an integrated luminosity of 711 fb-1. We measure $f_L = 0.65 \\pm 0.04(stat) \\pm 0.03(syst),$ which is consistent with previous results and indicates that there are no large contributions from physics beyond the Standard Model.     The measurement was performed on $^{B}_{s \\to \\phi K^*}$ events where one or both kaons were identified as charged pions (K+/-). This method allows for a precise determination of the decay time distribution without relying on Monte Carlo simulation techniques. Using this technique we find that the measured value of $f_L$ does not depend strongly on the choice of fitting range used to extract it. In addition, we perform a study of systematic uncertainties associated with our analysis procedure.", "paraphrased_abstract": "This technique allows the measurement of decay time without using Monte Carlo simulations. Moreover, it can be examined the systematic uncertainties associated with our analysis. We have taken measurements of the longitudinal polarization fraction in B mesons decaying to phi K. This is in accordance with the previous results and suggests that physics does not contribute a great deal. The measured value is 0.65 pm 0.04(stat) and 0.03(syst) in B mesons, and these are consistent with previous measurements, and show that there are no substantial contributions from physics outside the Standard Model. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0174, "title": "Reparametrization Invariance, the controversial extraction of $\\alpha$ from $B\\to\\pi\\pi$ and New Physics", "abstract": "  The extraction of the weak phase $\\alpha$ from $B\\to\\pi\\pi$ decays has been controversial from a statistical point of view, as the frequentist vs. bayesian confrontation shows. We analyse several relevant questions which have not deserved full attention and pervade the extraction of $\\alpha$. Reparametrization Invariance proves appropriate to understand those issues. We show that some Standard Model inspired parametrizations can be senseless or inadequate if they go beyond the minimal Gronau and London assumptions: the single weak phase $\\alpha$ just in the $\\Delta I=3/2$ amplitudes, the isospin relations and experimental data. Beside those analyses, we extract $\\alpha$ through the use of several adequate parametrizations, showing that there is no relevant discrepancy between frequentist and bayesian results. The most relevant information, in terms of $\\alpha$, is the exclusion of values around $\\alpha\\sim \\pi/4$; this result is valid in the presence of arbitrary New Physics contributions to the $\\Delta I=1/2$ piece. ", "machine_abstract": "We present an overview of our recent work on extracting information about physics beyond the Standard Model (SM) from B-decay data.  We discuss how to extract the weak phase angle $\\Delta\\alpha = \\alpha_s - \\alpha_W$ in the SM from measurements of CP asymmetries in decays such as $B \\to \\pi \\pi$ using reparametrization invariance.  This method is based on the fact that the decay amplitudes for these processes are invariant under certain transformations which leave their physical properties unchanged.   The extracted value of $\\Delta\\alpha$ can then be compared with theoretical predictions obtained within specific extensions of the SM.  We also show how this approach allows one to test whether new physics contributions to the decay amplitude violate or preserve flavor symmetry.  Finally we comment on some of the difficulties associated with applying this technique experimentally. A more detailed discussion of these issues will appear elsewhere. [1]  [2] .  [3]  [4]", "paraphrased_abstract": "The method is used to calculate the weak phase angle DAlpha-alpha-W, which is the measure of the phase ratios of decays of the B-decay, and to calculate the decay amplitudes, as well as to compare them with the theoretical predictions obtained by extension of the SM. We discuss some of the difficulties in applying this method in experimental practice. Then, we describe how to extract the weak phase angle DAlpha-alpha = Alpha-W in the SM from the measurements of the CP asymmetries in decays of the B-decay, such as B- to pi-pi, with the re-invariance method. We discuss the method and the results of the experiment, which are described in detail in a separate article. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2494, "title": "The effect of stellar-mass black holes on the structural evolution of massive star clusters", "abstract": "  We present the results of realistic N-body modelling of massive star clusters in the Magellanic Clouds, aimed at investigating a dynamical origin for the radius-age trend observed in these systems. We find that stellar-mass black holes, formed in the supernova explosions of the most massive cluster stars, can constitute a dynamically important population. If a significant number of black holes are retained (here we assume complete retention), these objects rapidly form a dense core where interactions are common, resulting in the scattering of black holes into the cluster halo, and the ejection of black holes from the cluster. These two processes heat the stellar component, resulting in prolonged core expansion of a magnitude matching the observations. Significant core evolution is also observed in Magellanic Cloud clusters at early times. We find that this does not result from the action of black holes, but can be reproduced by the effects of mass-loss due to rapid stellar evolution in a primordially mass segregated cluster. ", "machine_abstract": "We study how the presence of a central supermassive black hole (SMBH) affects the dynamical evolution and mass loss rate of an open cluster with N = 10, 000 stars in virial equilibrium. We use direct N-body simulations to follow the long-term evolution of this system for up to 1 Gyr. The SMBH is modeled as a point particle that interacts gravitationally only with the most bound particles of each star cluster orbiting it. Our results show that the presence of the SMBH causes significant changes in the structure of the open cluster over time. In particular, we find that:  -The open cluster loses about half its initial total mass within 0.5 Gyr due to tidal stripping by the SMBH.  -The open cluster becomes more spherical after being tidally stripped by the SMBH. -The open cluster develops a core-halo structure where the density profile follows a power law at large radii but has a flat distribution near the center.", "paraphrased_abstract": "The density profile of the sphere of a SMBH is a point particle, which is bound gravitationally to the most bound particles of all the stars in its orbit. Our model is based on the recurrence of N-body simulations to follow the evolution of this system up to a Gyr. We find that the presence of the SMBH affects the structure and structure of an open cluster with a mass of 10, 000 stars. Our analysis shows that the SMBH is a significant influence on the morphology and structure of the SMBH, and that the SMBH possesses significant effects on the structure of the SMBH. For example, the SMBH is a point particle which interacts gravitationally only with the most bounding particles of each star cluster. The SMBH is a central supermassive black hole and the mass of the SMBH is 0.085\u00b0. The SMBH is a point particle, which in general interacts gravitationally only with the most bound particles of the star clusters orbiting it. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1788, "title": "A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA Networks with Delay QoS Constraints", "abstract": "  A game-theoretic framework is used to study the effect of constellation size on the energy efficiency of wireless networks for M-QAM modulation. A non-cooperative game is proposed in which each user seeks to choose its transmit power (and possibly transmit symbol rate) as well as the constellation size in order to maximize its own utility while satisfying its delay quality-of-service (QoS) constraint. The utility function used here measures the number of reliable bits transmitted per joule of energy consumed, and is particularly suitable for energy-constrained networks. The best-response strategies and Nash equilibrium solution for the proposed game are derived. It is shown that in order to maximize its utility (in bits per joule), a user must choose the lowest constellation size that can accommodate the user's delay constraint. This strategy is different from one that would maximize spectral efficiency. Using this framework, the tradeoffs among energy efficiency, delay, throughput and constellation size are also studied and quantified. In addition, the effect of trellis-coded modulation on energy efficiency is discussed. ", "machine_abstract": "In this work, we propose an energy-efficient modulation scheme for downlink transmission over code division multiple access (CDMA) networks by exploiting the inherent multiuser diversity and delay requirements. We formulate the problem as a noncooperative game between users competing for limited power resources under their individual delay constraints. The proposed algorithm is shown to converge to Nash equilibrium points that are Pareto optimal solutions to the formulated optimization problems. Numerical results show that our approach can significantly improve system performance compared to existing schemes. In particular, it achieves higher data rates while maintaining low outage probabilities at different signal-to-noise ratios.     Keywords: Code Division Multiple Access, Noncooperative Games, Power Allocation, Energy Efficiency, Multiuser Diversity, Delay Constraint. 1 Introduction     With the rapid growth of wireless communication systems such as mobile phones and personal digital assistants, there has been increasing interest in developing efficient resource allocation algorithms to maximize network capacity or minimize total transmit power consumption [1] . For example, [2] considers joint subcarrier and bit allocations among users in orthogonal frequency-division multiplexing (OFDM)-based broadband wireless networks using Lagrangian relaxation techniques; [3] proposes a distributed algorithm based on dual decomposition theory to solve the sum-power minimization problem subject to rate constraints in OFDMA cellular networks; [4] develops a low-complexity iterative water-filling algorithm to optimize the tradeoff between spectral efficiency and fairness in multi-cell OFDMA networks. However, these works do not consider user-specific delay requirements which may be important in some applications like voice communications. To address this issue, [5] presents a cross-layer design framework where packet scheduling decisions are made jointly across physical layer, MAC layer, and application layer according to both channel conditions and end-to-end delay requirements. [6] studies the problem of maximizing the weighted sum-rate of all users in a single-cell uplink scenario with per-user delay constraints. It shows that the resulting optimization problem is NP-hard and then solves it via convex programming methods. Although these works have considered various aspects of resource allocation in wireless networks, they", "paraphrased_abstract": "It has been a fact that since mobile phones and other personal assistants have become commonplace in the world, a lot of interest has arisen in calculating the optimal allocation of resources and energy. The authors of these works, however, have not considered the issue of the number of users, and the delay requirements that must be met for the transmission of data. Nevertheless, they have not addressed the underlying delay requirements of the user or the communication. This has not been addressed, however, in the case of the telephone. To address this, they have introduced a cross-layer model, where the transmission of packets is made together in the physical layer, the MAC layer, and the application layer, according to the channel condition and the end-to-end delay. The results show that our approach can greatly improve the system\u2019s performance in a given case, in particular, the higher rate of data, the lower the probability of outage. This is an example of the efficient transmission of packets among the users of the mobile phone and the personal assistants, and it is an example of a joint distribution of packets and bits among the users of the wireless network with the help of a Lagrangian relaxation technique, the method of the mixed converging and the combined converging strategy, with the result that the resulting optimization problem is very difficult, and the solution is solved with convex programming. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0429, "title": "Bayesian reconstruction of the cosmological large-scale structure: methodology, inverse algorithms and numerical optimization", "abstract": "  We address the inverse problem of cosmic large-scale structure reconstruction from a Bayesian perspective. For a linear data model, a number of known and novel reconstruction schemes, which differ in terms of the underlying signal prior, data likelihood, and numerical inverse extra-regularization schemes are derived and classified. The Bayesian methodology presented in this paper tries to unify and extend the following methods: Wiener-filtering, Tikhonov regularization, Ridge regression, Maximum Entropy, and inverse regularization techniques. The inverse techniques considered here are the asymptotic regularization, the Jacobi, Steepest Descent, Newton-Raphson, Landweber-Fridman, and both linear and non-linear Krylov methods based on Fletcher-Reeves, Polak-Ribiere, and Hestenes-Stiefel Conjugate Gradients. The structures of the up-to-date highest-performing algorithms are presented, based on an operator scheme, which permits one to exploit the power of fast Fourier transforms. Using such an implementation of the generalized Wiener-filter in the novel ARGO-software package, the different numerical schemes are benchmarked with 1-, 2-, and 3-dimensional problems including structured white and Poissonian noise, data windowing and blurring effects. A novel numerical Krylov scheme is shown to be superior in terms of performance and fidelity. These fast inverse methods ultimately will enable the application of sampling techniques to explore complex joint posterior distributions. We outline how the space of the dark-matter density field, the peculiar velocity field, and the power spectrum can jointly be investigated by a Gibbs-sampling process. Such a method can be applied for the redshift distortions correction of the observed galaxies and for time-reversal reconstructions of the initial density field. ", "machine_abstract": "We present an overview of our recent work on Bayesian methods for reconstructing the large scale structures in the universe using galaxy redshift surveys. We discuss how to formulate this problem as a statistical inference task with priors that encode physical information about the underlying matter distribution. The posterior probability density function is then evaluated by applying Bayes' theorem together with Markov Chain Monte Carlo (MCMC) sampling techniques. In particular we focus on two different approaches which are based either on Gibbs sampling or Metropolis-Hastings algorithm. Finally we describe some applications of these methods to simulated data sets. This research was supported by NSF grant AST-0707763. Cosmology has been revolutionized over the past decade by precision measurements of the cosmic microwave background anisotropies made by WMAP [1] , PLANCK [2] and other experiments [3] . These observations have provided strong evidence for the existence of dark energy [4] and have led to tight constraints on many parameters describing the physics of the early universe [5] . However, despite their successes there remain several open questions regarding fundamental aspects of the standard model of cosmology [6] . One such question concerns the nature of dark matter [7, 8] : what is its particle content? What is its mass? How does it interact with ordinary matter? Answering these questions requires detailed knowledge of the spatial distribution of dark matter throughout space and time [9] . Unfortunately direct detection experiments [10] cannot provide this information because they only measure the gravitational effects of dark matter particles [11] . Instead one must rely on indirect probes like galaxy clustering [12] , weak lensing [13] and 21 cm emission [14] .", "paraphrased_abstract": "As a result of these observations, cosmologists have gained new insights into the nature of dark matter and have developed several new conditions for a more accurate model of the physics of the early universe. But the methods of direct detection are not sufficient, for they only measure the gravity of the dark matter, which, in turn, are dependent on the indirect detection of dark matter. This research was supported by NSF grant AST-0707763. This research was supported by the NSF grant AST-0707763; we have presented our latest studies on the Bayesian method of reconstructing the large structure of the universe by the observation of the cosmic microwave background from WMAP and of PLANCK. The results have been proved to be correct, and the physics of the early universe has been severely restricted. The question of the nature of dark matter, and of its composition, has been well investigated; but there are still many questions of fundamental importance, such as: what is the content of the dark matter, and what is the mass? How does it interact with ordinary matter? ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3106, "title": "A Surprising Reversal of Temperatures in the Brown-Dwarf Eclipsing Binary 2MASS J05352184-0546085", "abstract": "  The newly discovered brown-dwarf eclipsing binary 2MASS J05352184-0546085 provides a unique laboratory for testing the predictions of theoretical models of brown-dwarf formation and evolution. The finding that the lower-mass brown dwarf in this system is hotter than its higher-mass companion represents a challenge to brown-dwarf evolutionary models, none of which predict this behavior. Here we present updated determinations of the basic physical properties of 2M0535-05, bolstering the surprising reversal of temperatures with mass in this system. We compare these measurements with widely used brown-dwarf evolutionary tracks, and find that the temperature reversal can be explained by some models if the components of 2M0535-05 are mildly non-coeval, possibly consistent with dynamical simulations of brown-dwarf formation. Alternatively, a strong magnetic field on the higher-mass brown dwarf might explain its anomalously low surface temperature, consistent with emerging evidence that convection is suppressed in magnetically active, low-mass stars. Finally, we discuss future observational and theoretical work needed to further characterize and understand this benchmark system. ", "machine_abstract": "We report on an unexpected reversal of temperatures between two components of a brown-dwarf eclipsing binary system, which we have discovered using infrared photometry and spectroscopy obtained with Spitzer Space Telescope (Werner et al., 2004) and Gemini Observatory (Gemini North telescope). The primary component is cooler than its secondary by about 300 K at optical wavelengths but warmer by about 100 K at near-infrared wavelengths. We find that this temperature inversion can be explained if both stars are irradiated by their mutual accretion disk. This finding suggests that the disks around young low-mass objects may be more complex than previously thought.     Keywords: Accretion Disk, Inverse P-Cygni profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric variability, Stellar radius, Temperature inversion, Young star         1 Introduction     An important goal for understanding how planets form is to determine what happens during the earliest stages of planet formation when protoplanetary disks surround young stellar systems. One key question concerns whether or not these disks evolve into planetary systems like our own solar system. To answer such questions it will be necessary to study individual examples of young circumstellar disks as they evolve over time. However, because most young stars are deeply embedded within dense molecular clouds, direct observations of the inner regions of these disks are difficult. Fortunately, some young stars are surrounded by optically thin dusty envelopes that allow us to probe the physical conditions near the central object through scattered light. These so-called transitional disks show evidence of clearing out large amounts of material inside several AU of the central star while still retaining significant quantities of gas farther away (Strom et al., 1989; Skrutskie et al., 1990; Calvet et al., 2002; Muzerolle et al., 2003; Sicilia-Aguilar et al., 2006; Espaillat et al., 2007) .     A number of studies suggest that the outer edges of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby", "paraphrased_abstract": "A very important goal in analyzing the formation of planets is to ascertain how protoplanetary disks grow into the planetary systems. One important task of this research is to find out whether the disks grow into planetary systems like ours. However, the young stars are surrounded by dense, densely populated clouds, and there is no direct observation of their innermost layers. One of the unforeseen findings of the present work is the reversal of temperatures between two components of a brown dwarf eclipsing the central star, which we have found by observing its temperature at the wavelengths of the optical and the ultraviolet (Werner et al., 2004). Our study was carried out by observing the inward profiles of a pair of stars by the same accretion disk. It was found that the first component was cooler than the second by about 300 K at the optical wavelengths, while the latter was about 100 K at the ultraviolet. The results show that the disks around the young stars are more complex than previously thought. We propose an infrared investigation of the temperature of two different components of a brown dwarf binary system, which we have investigated with the Spitzer Space Telescope (Werner et al., 2004). In this study we have observed that a few ethereal disks were forming on the horizon, separating out the matter between", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3135, "title": "Anomalous structural and mechanical properties of solids confined in quasi one dimensional strips", "abstract": "  We show using computer simulations and mean field theory that a system of particles in two dimensions, when confined laterally by a pair of parallel hard walls within a quasi one dimensional channel, possesses several anomalous structural and mechanical properties not observed in the bulk. Depending on the density $\\rho$ and the distance between the walls $L_y$, the system shows structural characteristics analogous to a weakly modulated liquid, a strongly modulated smectic, a triangular solid or a buckled phase. At fixed $\\rho$, a change in $L_y$ leads to many reentrant discontinuous transitions involving changes in the number of layers parallel to the confining walls depending crucially on the commensurability of inter-layer spacing with $L_y$. The solid shows resistance to elongation but not to shear. When strained beyond the elastic limit it fails undergoing plastic deformation but surprisingly, as the strain is reversed, the material recovers completely and returns to its original undeformed state. We obtain the phase diagram from mean field theory and finite size simulations and discuss the effect of fluctuations. ", "machine_abstract": "We study the elasticity of thin films with periodic microstructure, which are constrained to lie on an underlying substrate. We show that such systems can exhibit anomalously large values for their Young's moduli as well as Poisson ratios. The origin of these effects is traced back to the presence of phonon soft modes associated with the periodicity along the film normal direction. These results have implications for the design of novel materials with tailored elastic properties.     In recent years there has been growing interest in understanding how confinement affects the physical behavior of matter at the nanoscale [1] . This problem arises naturally when considering thin films or nanowires embedded within bulk materials; however it also applies more generally whenever a system is restricted to occupy only part of its available phase space [2] . For example, this situation occurs frequently during crystal growth where defects may be introduced into the lattice structure by impurities [3] , or when studying colloidal suspensions [4] .   In this work we consider the case of a thin film with periodic microstructure, whose thickness h lies between two length scales L and d (see Fig 1) . Here L represents the typical size of the unit cell while d denotes the characteristic spacing between adjacent layers; both quantities are assumed to be much smaller than the in-plane dimensions of the sample. Such structures arise commonly in nature, e.g., in layered compounds like graphite [5] , transition metal dichalcogenides [6] , and hexagonal boron nitride [7] . They are also used extensively in technological applications ranging from photovoltaics [8] to optoelectronics [9] .     Figure 1: Schematic illustration of our model geometry. A thin film with periodic microstructures is confined to lie on top of a rigid substrate.", "paraphrased_abstract": "\u201cIn a thin film with periodic microstructures a rigid material is usually used. This problem arises naturally with thin films and nanowires embedded in the bulk materials; however, it is also a general case when a system is restricted to part of its space in its phase space. In this case, the thin film has periodic microstructures and is confined to a solid substrate. We study the elasticity of thin films with periodic microstructures confined to a rigid substrate. We show that such systems are extraordinarily large in Young's moduli and Poisson ratios. These results have implications for the design of new materials with elastic properties. We have found a case of a thin film with periodic microstructures, whose thickness is h, between two length scales L and d (see Fig. 1). L is the typical size of the cell, d is the characteristic spacing between the layers; both are much smaller than the in-plane dimensions of the sample. In this case, we have a thin film with periodic microstructures, which is bound to lie on a rigid substrate. We have been studying this problem in recent years, as well as in a thin film, or in a nanowire, forming part of the bulk of materials. This is particularly the case when the materials are confined to occupy only part of the available phase space. This is", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1182, "title": "An Optical Source Catalog of the North Ecliptic Pole Region", "abstract": "  We present a five (u*,g',r',i',z') band optical photometry catalog of the sources in the North Ecliptic Pole (NEP) region based on deep observations made with MegaCam at CFHT. The source catalog covers about 2 square degree area centered at the NEP and reaches depths of about 26 mag for u*, g', r' bands, about 25 mag for i' band, and about 24 mag for z' band (4 sigma detection over an 1 arcsec aperture). The total number of cataloged sources brighter than r'= 23 mag is about 56,000 including both point sources and extended sources. From the investigation of photometric properties using the color-magnitude diagrams and color-color diagrams, we have found that the colors of extended sources are mostly (u*-r') < 3.0 and (g'-z') > 0.5. This can be used to separate the extended sources from the point sources reliably, even for the faint source domain where typical morphological classification schemes hardly work efficiently. We have derived an empirical color-redshift relation of the red sequence galaxies using the Sloan Digital Sky Survey data. By applying this relation to our photometry catalog and searching for any spatial overdensities, we have found two galaxy clusters and one nearby galaxy group. ", "machine_abstract": "We present an optical source catalog for the north ecliptic pole region (NEPR) based on data obtained with the Palomar Observatory Sky Survey (POSS-II). The NEPR is defined as the area within 10 degrees in right ascension and declination centered at RA = 20 h 00 m , Dec = +85 deg . We have used POSS-II plates taken between 1950 and 1990 to produce this catalog, which contains over 1 million sources down to B J = 22 mag . The photometric calibration was performed using Landolt standard stars observed during the same nights that the sky survey plates were exposed. Photometry has been carried out by means of aperture photometry techniques. Magnitudes are given in the Johnson system. In addition we provide proper motions for all objects brighter than B J = 18 mag . This catalog will be useful for studies related to galactic structure and evolution.     Keywords: Palomar Observatory Sky Survey", "paraphrased_abstract": "The calibrations were performed by means of photometric calculations, and the numbers are given in the Johnson system. The data were obtained from the Palomar Observatory Sky Survey between 1950 and 1990. We have used POSS-II telescopes, which have a catalogue of more than 1 million sources, down to Bj. 22 mag., in which the NEPR is defined as the area at 10 degrees in right ascension and declination at RA = 20 h00 m, and at +85 deg. We have presented a source database for the north pole, based on data from the Palomar Observatory Sky Survey, collected by the Palomar Observatory in the year 1950, based on the observations from Landolt standards, observable on the same nights that the POSSII was taken. The size of the objects is calculated using the method of the Johnson system. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0326, "title": "On generalized entropy measures and pathways", "abstract": "  Product probability property, known in the literature as statistical independence, is examined first. Then generalized entropies are introduced, all of which give generalizations to Shannon entropy. It is shown that the nature of the recursivity postulate automatically determines the logarithmic functional form for Shannon entropy. Due to the logarithmic nature, Shannon entropy naturally gives rise to additivity, when applied to situations having product probability property. It is argued that the natural process is non-additivity, important, for example, in statistical mechanics, even in product probability property situations and additivity can hold due to the involvement of a recursivity postulate leading to a logarithmic function. Generalizations, including Mathai's generalized entropy are introduced and some of the properties are examined. Situations are examined where Mathai's entropy leads to pathway models, exponential and power law behavior and related differential equations. Connection of Mathai's entropy to Kerridge's measure of \"inaccuracy\" is also explored. ", "machine_abstract": "We present an approach to the analysis of metabolic networks based on information-theoretic concepts, in particular Shannon's entropy measure. We show that this concept can be extended by considering not only single metabolites but also pairs or higher-order tuples of them as elementary units for measuring entropy. This leads us to define so-called pathway entropies which are used to quantify how much uncertainty is associated with different parts of the network. The proposed method allows one to identify those parts of the network where most of the uncertainty resides. In addition we introduce a novel way of visualizing metabolic networks using these new entropy-based quantities. Finally, we demonstrate our approach by applying it to two examples taken from biochemistry literature. Metabolic networks play important roles in many biological processes such as cell growth and development [1] . They consist of chemical reactions transforming various compounds into each other [2] , e.g., glucose molecules are transformed into energy-rich adenosine triphosphate (ATP) molecules via glycolysis [3] . The study of metabolic networks has been attracting increasing interest over recent years [4] - [8] . One reason for this growing interest lies in their potential use as drug targets [9] . Another motivation comes from the fact that they provide valuable insights into cellular metabolism [10] . For example, the identification of key enzymes involved in certain diseases may help to develop drugs against these diseases [11] . Furthermore, metabolic networks have been shown to exhibit scale-free properties [12] similar to those observed in social systems [13] . These findings suggest that there might exist common principles underlying both types of networks [14] . In order to understand the functioning of metabolic networks better, several mathematical models have been developed [15] - [17] . Amongst others, stoichiometric approaches [18] try to describe all possible states of a given metabolic system mathematically. However, due to the high number of degrees of freedom inherent in such models [19] , it becomes difficult to analyze large metabolic networks [20] . Therefore, alternative methods have been suggested [21] - [23] .", "paraphrased_abstract": "\u201cIn order to understand the function of metabolic networks, several mathematical models have been developed, especially stoichiometric, which, while preserving the degree of freedom, make it difficult to model large metabolic networks. For example, it has been found that metabolic networks are similar to social networks, in that they can be identified by a certain number of parameters and in a single instance of a single metabolic unit. Then, using Shannon\u2019s entropy, we show that it is possible to extend the definition of entropy by introducing not only metabolites but also by higher-order tuples. We thus give a general overview of metabolic networks, and, first of all, a novel analysis of the role of entropy in the analysis of metabolic networks. In this regard, it is found that the study of metabolic networks is becoming more and more important. Besides, they have been shown to exhibit in the network a scale-free characteristic [5] and similarity to social systems. There are also some equivalences, equivalences, and other characteristics which may be common in both types of networks. Thus, the following explanation of entropy is provided, and a new method is proposed to analyze the networks. The proposed method is also illustrated by two examples taken from the literature. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1918, "title": "Massive CP$^1$ theory from a microscopic model for doped antiferromagnets", "abstract": "  A path-integral for the t-J model in two dimensions is constructed based on Dirac quantization, with an action found originally by Wiegmann (Phys. Rev. Lett. {\\bf 60}, 821 (1988); Nucl. Phys. B323, 311 (1989)). Concentrating on the low doping limit, we assume short range antiferromagnetic order of the spin degrees of freedom. Going over to a local spin quantization axis of the dopant fermions, that follows the spin degree of freedom, staggered CP$^1$ fields result and the constraint against double occupancy can be resolved. The staggered CP$^1$ fields are split into slow and fast modes, such that after a gradient expansion, and after integrating out the fast modes and the dopant fermions, a CP$^1$ field-theory with a massive gauge field is obtained that describes generically incommensurate coplanar magnetic structures, as discussed previously in the context of frustrated quantum antiferromagnets. Hence, the possibility of deconfined spinons is opened by doping a colinear antiferromagnet. ", "machine_abstract": "We propose an explicit microscopic model for the recently proposed massive CP$^1$ theory, which is based on the idea that the Higgs field arises as a collective mode in strongly correlated systems with spontaneously broken symmetry.  We show how this can be realized by considering a system of fermions coupled to bosonic degrees of freedom via a four-fermion interaction term and derive the effective action for the Higgs field. The resulting Higgs potential has two degenerate minima corresponding to the vacuum expectation values $\\langle H \\rangle = \\pm v_0$ where $v_0 \\sim J$ (the strength of the coupling between the fermions and bosons) and $J > 0$ is determined self-consistently within our approach. In addition we find that there are no massless excitations associated with the Higgs field. Finally, we discuss possible experimental realizations of such a scenario using cold atoms or trapped ions. Introduction - Recently it was suggested [1] that the Standard Model Higgs field may arise as a collective mode in a class of strongly correlated systems with spontaneusly broken symmetry. This proposal relies on the observation [2] that the Higgs mechanism can be understood as arising from the condensation of bound states of fermions into bosonic particles [3] . It follows then that if one could engineer a system of fermions interacting with some other degree(s) of freedom so that these bound states condense, then one would have spontaneous breaking of gauge symmetry and hence a Higgs-like state. A concrete realization of this idea was given in Ref. [4] , where it was shown that the Higgs field emerges naturally when one considers a system of fermions with spin-1/2 coupled to bosonic degrees", "paraphrased_abstract": "\u201cWe have been arguing for a long time that the standard model of the Higgs field is an organization in a group of strongly correlated systems with a spontaneously broken symmetry. The idea of the Higgs system is based on the observation that it is a collective mode in a class of strongly correlated systems, whose symmetry is spontaneously broken. The fact that there is a collective mode in the Higgs field, according to the theory, can be proved by the observation that the Higgs mechanism is constituted by the condensation of bound states of fermions into a field of bosonic particles, and the result is that there is spontaneously broken symmetry and a Higgs-like state. We present a detailed analysis of the recent giant CP2, which is based on the idea that the Higgs field is a collective mode in strongly correlated systems, and this idea can be explained by the fact that the Higgs field can be obtained by a system of fermions, which, as a result of the four-fermion term, is proportional to the degree of freedom in which it is expressed. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2375, "title": "Connecting LHC, ILC, and Quintessence", "abstract": "  If the cold dark matter consists of weakly interacting massive particles (WIMPs), anticipated measurements of the WIMP properties at the Large Hadron Collider (LHC) and the International Linear Collider (ILC) will provide an unprecedented experimental probe of cosmology at temperatures of order 1 GeV. It is worth emphasizing that the expected outcome of these tests may or may not be consistent with the picture of standard cosmology. For example, in kination-dominated quintessence models of dark energy, the dark matter relic abundance can be significantly enhanced compared to that obtained from freeze out in a radiation-dominated universe. Collider measurements then will simultaneously probe both dark matter and dark energy. In this article, we investigate the precision to which the LHC and ILC can determine the dark matter and dark energy parameters under those circumstances. We use an illustrative set of four benchmark points in minimal supergravity in analogy with the four LCC benchmark points. The precision achievable together at the LHC and ILC is sufficient to discover kination-dominated quintessence, under the assumption that the WIMPs are the only dark matter component. The LHC and ILC can thus play important roles as alternative probes of both dark matter and dark energy. ", "machine_abstract": "We propose to connect the Large Hadron Collider (LHC) with an International Linear Collider (ILC), which will be built in Japan or Korea, by using quintessence as dark energy.  We show that this connection is possible if we assume that the universe has been expanding at a constant rate for about 10 billion years since its birth. The expansion speed of the universe can then be determined by measuring the time delay between signals emitted simultaneously from distant galaxies observed today and those emitted from them when they were closer together during their youthful days. This measurement would provide us with information on how much dark energy there was back then. If our proposal turns out to be correct, it may lead to new insights into the nature of dark energy. In addition, it could also shed light on why the universe looks so different now than what it looked like just after its birth. It might even help explain why the universe appears to have evolved in such a way that life forms are able to exist here on Earth.", "paraphrased_abstract": "It can be argued that the expansion of the universe is caused by the tidal energy of the universe. We say that it can be proved by analyzing the distance between the distant galaxies and the distant ones that have been emitted by them during their young days. This tidal energy would then be calculated and calculated by measuring the time delay between the signals of distant galaxies when they were still young. We calculate the distance between the signals emitted by them from distant galaxies when they were younger. The calculation would be based on the distance between the distant galaxies and the distant ones when they were still young. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1832, "title": "Near and Mid-IR Photometry of the Pleiades, and a New List of Substellar Candidate Members", "abstract": "  We make use of new near and mid-IR photometry of the Pleiades cluster in order to help identify proposed cluster members. We also use the new photometry with previously published photometry to define the single-star main sequence locus at the age of the Pleiades in a variety of color-magnitude planes.   The new near and mid-IR photometry extend effectively two magnitudes deeper than the 2MASS All-Sky Point Source catalog, and hence allow us to select a new set of candidate very low mass and sub-stellar mass members of the Pleiades in the central square degree of the cluster. We identify 42 new candidate members fainter than Ks =14 (corresponding to 0.1 Mo). These candidate members should eventually allow a better estimate of the cluster mass function to be made down to of order 0.04 solar masses.   We also use new IRAC data, in particular the images obtained at 8 um, in order to comment briefly on interstellar dust in and near the Pleiades. We confirm, as expected, that -- with one exception -- a sample of low mass stars recently identified as having 24 um excesses due to debris disks do not have significant excesses at IRAC wavelengths. However, evidence is also presented that several of the Pleiades high mass stars are found to be impacting with local condensations of the molecular cloud that is passing through the Pleiades at the current epoch. ", "machine_abstract": "We present new near-infrared photometric data for the Pleiades open cluster obtained with the Infrared Survey Facility (IRSF) on the South African Astronomical Observatory's 1-m telescope at Sutherland in November 2005. The observations were made using the IRSF/SIRIUS camera which has simultaneous JHK s imaging capability over an 8 x8 field-of-view. We have also used archival 2MASS data to supplement our sample. Our results are presented as colour-magnitude diagrams (CMDs), where we show that there is good agreement between our photometry and previous studies. Using these CMDs, we identify several previously unreported candidate members of the Pleiades based upon their location relative to theoretical pre-main sequence evolutionary tracks. These candidates include two objects located near the substellar boundary, one of which appears to be a brown dwarf member of the Pleiades.", "paraphrased_abstract": "The data were collected with the IRSF/SIRIUS camera, which was capable of measuring eight dimensions over a field of view of 8 inches. The results of this study were presented in the form of colour-magnitude diagrams (CMDs) where we can show that our method is in good agreement with other studies. The results of our research are shown as a coloured scale, a measure which is consistent with the previous study. Two objects lie at the edge of the substellar system, and one of them is supposed to be a brown dwarf. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.205, "title": "Black Holes and Large N Species Solution to the Hierarchy Problem", "abstract": "  We provide the perturbative and non-perturbative arguments showing that theories with large number of species of the quantum fields, imply an inevitable hierarchy between the masses of the species and the Planck scale, shedding a different light on the hierarchy problem. In particular, using the black hole physics, we prove that any consistent theory that includes N number of the Z_2-conserved species of the quantum fields of mass \\Lambda, puts a lower bound on the Planck mass, which in large N limit is given by N\\Lambda^2. An useful byproduct of this proof is that any exactly conserved quantum charge, not associated with a long-range classical field, must be defined maximum modulo N, bounded by the the ratio of the Planck to the unit charge masses squared. For example, a continuous global U(1) `baryon number' symmetry, must be explicitly broken by gravity, at least down to a Z_N subgroup, with N bounded by the ratio of the Planck to baryon masses squared. The same constraint applies to any discrete gauge symmetry, as well as to other quantum-mechanically detectable black hole charges that are associated with the massive quantum hair of the black hole. We show that the gravitationally-coupled N-species sector that solves the gauge hirearchy problem, should be probed by LHC. ", "machine_abstract": "The Standard Model (SM) is an extremely successful theory, but it has some problems that need to be addressed before we can claim to have found \"theory of everything\".  The SM does not explain why there are three generations of quarks and leptons with different masses or how neutrinos acquire mass. It also fails to provide any explanation for dark matter in our universe.   In this talk I will discuss my recent work on addressing these issues by introducing new particles into the SM which interact only gravitationally. These new particles are called gravitons and they carry spin-2 quantum number like graviton in general relativity. We show that if gravitons exist then their interactions with other fields lead to generation dependent masses for fermions as well as small Majorana masses for neutrinos. Gravitons also contribute to the energy density of the universe leading to possible solution to the problem of missing dark matter. Finally, we argue that gravitons may play important role in understanding hierarchy between Planck scale and electroweak scale.", "paraphrased_abstract": "It is shown that if gravitons exist, they interact with other forces, and so their mass is determined by fermions, and neutrinos are small Majorana masses. Gravitons also contribute to the energy of the universe, leading to the possibility of a solution to the problem of missing dark matter. The SM is an extremely successful theory, but it has some problems that must be overcome before it can be considered \"theory of everything\". The SM does not explain why there are three generations of quarks and leptons, or how neutrinos gain mass. The SM does not explain how the light in our universe has been lost. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3462, "title": "The host galaxies of strong CaII QSO absorption systems at z<0.5", "abstract": "  We present new imaging and spectroscopic observations of the fields of five QSOs with very strong intervening CaII absorption systems at redshifts z<0.5 selected from the Sloan Digital Sky Survey. Recent studies of these very rare absorbers indicate that they may be related to damped Lyman alpha systems (DLAs). In all five cases we identify a galaxy at the redshift of the CaII system with impact parameters up to ~24 kpc. In four out of five cases the galaxies are luminous (L ~L*), metal-rich (Z ~Zsun), massive (velocity dispersion, sigma ~100 km/s) spirals. Their star formation rates, deduced from Halpha emission, are high, in the range SFR = 0.3 - 30 Msun/yr. In our analysis, we paid particular attention to correcting the observed emission line fluxes for stellar absorption and dust extinction. We show that these effects are important for a correct SFR estimate; their neglect in previous low-z studies of DLA-selected galaxies has probably led to an underestimate of the star formation activity in at least some DLA hosts. We discuss possible links between CaII-selected galaxies and DLAs and outline future observations which will help clarify the relationship between these different classes of QSO absorbers. ", "machine_abstract": "We have obtained deep optical spectra for eight QSOs with known redshifts in the range 0.4-0.5, and searched them for intervening Ca II absorbers using the equivalent width (EW) method. We find that all eight QSOs show strong Ca II absorptions associated with their own galaxy halos. The EWs are found to be correlated with the luminosities of the QSOs themselves. This correlation is consistent with previous results on Mg II absorbers but not with those on C IV absorbers. In addition we find that there exists an anti-correlation between the EWs and the impact parameters of the absorber-galaxy pairs. These results suggest that the Ca II absorbers may arise mainly from gaseous disks surrounding massive elliptical galaxies rather than from galactic winds driven by starburst activity.         Keywords: Absorber-galaxy pairs; Galaxy halo; Quasar; Redshift", "paraphrased_abstract": "The absorption of galaxy pairs, galaxy halo, quasar, and redshift are found in all eight QSOs, with known redshifts of 0.4-0.5. We investigate the absorption of the halo and the halo by using the equivalent-width method. The results are consistent with the earlier results on the Mg II halo but not with the previous results on the C IV halo. These results show that the Ca II halo originates from gaseous disks surrounded by massive elliptic galaxies, rather than from the galactic winds triggered by the starry activity of the galaxy. The result is as follows: the halo contains strong halo halo and quasar halo, and ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1808, "title": "Tests of Bayesian Model Selection Techniques for Gravitational Wave Astronomy", "abstract": "  The analysis of gravitational wave data involves many model selection problems. The most important example is the detection problem of selecting between the data being consistent with instrument noise alone, or instrument noise and a gravitational wave signal. The analysis of data from ground based gravitational wave detectors is mostly conducted using classical statistics, and methods such as the Neyman-Pearson criteria are used for model selection. Future space based detectors, such as the \\emph{Laser Interferometer Space Antenna} (LISA), are expected to produced rich data streams containing the signals from many millions of sources. Determining the number of sources that are resolvable, and the most appropriate description of each source poses a challenging model selection problem that may best be addressed in a Bayesian framework. An important class of LISA sources are the millions of low-mass binary systems within our own galaxy, tens of thousands of which will be detectable. Not only are the number of sources unknown, but so are the number of parameters required to model the waveforms. For example, a significant subset of the resolvable galactic binaries will exhibit orbital frequency evolution, while a smaller number will have measurable eccentricity. In the Bayesian approach to model selection one needs to compute the Bayes factor between competing models. Here we explore various methods for computing Bayes factors in the context of determining which galactic binaries have measurable frequency evolution. The methods explored include a Reverse Jump Markov Chain Monte Carlo (RJMCMC) algorithm, Savage-Dickie density ratios, the Schwarz-Bayes Information Criterion (BIC), and the Laplace approximation to the model evidence. We find good agreement between all of the approaches. ", "machine_abstract": "Gravitational wave astronomy is an emerging field that will provide new insights into the universe and its fundamental laws through observations of gravitational waves emitted by merging black holes, neutron stars or other compact objects in distant galaxies.  In this work we present several tests of different techniques used to select between competing models describing the observed data. We consider two examples where the signal-to-noise ratio (SNR) of the detected signals are low enough so that it becomes difficult to distinguish between different physical scenarios using standard frequentist hypothesis testing methods. The first example considers the problem of distinguishing between binary black hole systems with spin aligned versus anti-aligned with their orbital angular momentum vector. The second example considers the problem of determining whether a given source has been emitting gravitational radiation continuously over time as opposed to being active only during short bursts. For both cases we compare results obtained using three different model selection methods: Akaike's information criterion (AIC), Bayes factors computed via nested sampling (NS-BF), and the Deviance Information Criterion (DIC).", "paraphrased_abstract": "In this paper we present some of the various methods of estimating the data that we have been observing. We examine two examples in which the SNR of the signals are so low that it becomes difficult to differentiate between the different physical scenarios by the usual frequentist hypothesis. The first is the case of determining the ratio between binary black holes that have the spin aligned or anti-aligned according to the angular momentum vector of their orbital angular momentum vectors, the second case is the case of determining whether a source of gravitational energy has been continually generating gravitational radiation or merely active in short bursts. The astronomy of gravitational waves is a field that is emerging, and will provide new insights into the universe and its fundamental laws, by observing gravitational waves emitted by merging black holes, neutron stars, and other compact objects in distant galaxies. In this work, we compare the results of several different methods to find the right models that are suitable for the observations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3786, "title": "New smooth hybrid inflation", "abstract": "  We consider the extension of the supersymmetric Pati-Salam model which solves the b-quark mass problem of supersymmetric grand unified models with exact Yukawa unification and universal boundary conditions and leads to the so-called new shifted hybrid inflationary scenario. We show that this model can also lead to a new version of smooth hybrid inflation based only on renormalizable interactions provided that a particular parameter of its superpotential is somewhat small. The potential possesses valleys of minima with classical inclination, which can be used as inflationary paths. The model is consistent with the fitting of the three-year Wilkinson microwave anisotropy probe data by the standard power-law cosmological model with cold dark matter and a cosmological constant. In particular, the spectral index turns out to be adequately small so that it is compatible with the data. Moreover, the Pati-Salam gauge group is broken to the standard model gauge group during inflation and, thus, no monopoles are formed at the end of inflation. Supergravity corrections based on a non-minimal Kaehler potential with a convenient choice of a sign keep the spectral index comfortably within the allowed range without generating maxima and minima of the potential on the inflationary path. So, unnatural restrictions on the initial conditions for inflation can be avoided. ", "machine_abstract": "We propose a new inflationary model, which is based on the idea that there are two scalar fields in nature and they interact with each other through their potentials. We call this model \"smooth hybrid inflation\" (SHI). In SHI, one field plays the role of the waterfall field while another field acts as an inflaton. The potential for our model has three parameters; we show how these parameters can be fixed by using observational data such as WMAP7+BAO+H0. Our results indicate that the tensor-to-scalar ratio r=0.0055+0.0005-0.0004 at 95% CL. This value is consistent with recent observations. Moreover, we find that the spectral index ns=1.0145+0.0025-0.0024 at 68% CL. Therefore, our model predicts values for both nS and r within 1 sigma error bars of current experimental bounds.", "paraphrased_abstract": "A spectral index ns=1.1010+0.007 - 0.0022 at 5% ccc. Our model predicts ns and r in a single sigma error. We propose a new inflationary model based on the idea that two scalar fields exist in nature and interact in their potentials. We call this model \"smooth hybrid inflation\" and it is the first of its kind in the world. In this model, one field is the waterfall and the other is the inflaton. In our study, the value of r is 0.058+0.006+0.002+0.004 at the highest point of the spectrum. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0007, "title": "Polymer Quantum Mechanics and its Continuum Limit", "abstract": "  A rather non-standard quantum representation of the canonical commutation relations of quantum mechanics systems, known as the polymer representation has gained some attention in recent years, due to its possible relation with Planck scale physics. In particular, this approach has been followed in a symmetric sector of loop quantum gravity known as loop quantum cosmology. Here we explore different aspects of the relation between the ordinary Schroedinger theory and the polymer description. The paper has two parts. In the first one, we derive the polymer quantum mechanics starting from the ordinary Schroedinger theory and show that the polymer description arises as an appropriate limit. In the second part we consider the continuum limit of this theory, namely, the reverse process in which one starts from the discrete theory and tries to recover back the ordinary Schroedinger quantum mechanics. We consider several examples of interest, including the harmonic oscillator, the free particle and a simple cosmological model. ", "machine_abstract": "The present work is devoted to the study of quantum mechanics in polymer chains, which are modeled as one-dimensional systems with nearest-neighbor interactions between particles.  We consider two different models for such systems:  The first model describes an ensemble of interacting fermions on a chain; it can be viewed as a generalization of the Hubbard model (which corresponds to spinless fermions) to include spin-dependent hopping amplitudes and repulsive interaction terms.  In this case we show that there exists a critical value U_c of the interaction strength above which the ground state energy per particle becomes negative.  This result implies that the ground state of the system undergoes a phase transition at U = U_c from a metallic-like phase into an insulating phase.   The second model considered here consists of bosons moving along a ring subject to periodic boundary conditions.  Here we prove rigorously that the ground-state energy per particle converges to zero when the number N of particles tends to infinity.  Moreover, we provide upper bounds on the rate of convergence towards the limit.  These results imply that the ground state of our system exhibits superfluid behavior.", "paraphrased_abstract": "The second model is a boson, which moves along a ring, which is subject to periodic boundary conditions. The result is that the energy per particle approaches zero, when N is equal to infinity. The second model, meanwhile, is a set of particles moving along a ring, subject to periodic boundary conditions. This is the case where the interaction strength of the particle is at a critical value. This means that the energy per particle approaches zero when the number of particles tends to infinity. This means that the ground state of our system is superfluous. We present two models for such systems: the first is an ensemble of fermions interacting on a chain, and the second is a generalization of the Hubbard model (which is a complete synthesis of spinless fermions) to include spin-dependent hopping amplitudes and repellent interaction terms. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1129, "title": "SW Sextantis stars: the dominant population of CVs with orbital periods between 3-4 hours", "abstract": "  [Abridged] We present time-series optical photometry of five new CVs identified by the Hamburg Quasar Survey. The eclipses observed in HS 0129+2933, HS 0220+0603, and HS 0455+8315 provided very accurate orbital periods of 3.35129827(65), 3.58098501(34), and 3.56937674(26) h, respectively. HS 0805+3822 shows grazing eclipses and has a likely orbital period of 3.2169(2) h. Time-resolved optical spectroscopy of the new CVs (with the exception of HS 0805+3822) is also presented. Radial velocity studies provided an orbital period of 3.55 h for HS 1813+6122, which allowed us to identify the observed photometric signal at 3.39 h as a negative superhump wave. The spectroscopic behaviour clearly identifies these new CVs as new SW Sextantis stars. These new additions increase the number of known SW Sex stars to 35. Almost 40 per cent of the total SW Sex population do not show eclipses, invalidating the requirement of eclipses as a defining characteristic of the class and the models based on a high orbital inclination geometry alone. On the other hand, the predominance of orbital periods in the narrow 3-4.5 h range is becoming more pronounced. In fact, almost half the CVs which populate the 3-4.5 h period interval are definite members of the class. These statistics are confirmed by our results from the Hamburg Quasar Survey CVs. Remarkably, 54 per cent of the Hamburg nova-like variables have been identified as SW Sex stars with orbital periods in the 3-4.5 h range. The observation of this pile-up of systems close to the upper boundary of the period gap is difficult to reconcile with the standard theory of CV evolution, as the SW Sex stars are believed to have the highest mass transfer rates among CVs. Finally, we review the full range of common properties that the SW Sex stars exhibit. ", "machine_abstract": "We present an analysis of all known short-period cataclysmic variables (CVs) in which we find that most systems have orbital periods longer than 3 hrs and are dominated by SW Sex stars, while those with shorter periods tend to be AM Her binaries. We show that this dichotomy is consistent with theoretical predictions for the evolution of CVs driven by angular momentum loss via gravitational radiation. The observed distribution of orbital periods can also be explained if there exists a minimum period below which no CVs exist due to magnetic braking. This result has important implications on our understanding of how CVs evolve towards shorter orbital periods. Cataclysmic Variables (CVs), interacting binary star systems consisting of a white dwarf primary accreting matter from its low-mass companion through Roche lobe overflow, are among the best studied classes of close binary stars. They provide unique opportunities to study many aspects of astrophysics such as stellar structure and evolution, mass transfer processes, nuclear burning at high temperatures, and relativistic effects near compact objects. In particular, they offer insights into the formation mechanisms of both single and double degenerate white dwarfs, the progenitors of Type Ia supernovae.", "paraphrased_abstract": "It is an exciting discovery that has given us a chance to study a wide range of astrophysics, from the structure of stars to the mass transfer, from the nuclear fire to the atoms at a high temperature, and from relativistic effects in compact objects. Among these are the Cataclysmic Variables, a category of close-bound binary stars, which, together with the White Dwarf, accrets matter from its low-mass companion, through the Roche lobe, and the result is a model of the formation of white Dwarfs and the progenitor of Type Ia supernovae. This is an explanation of the distribution of orbital periods and explains the apparent difference between the observed and the observed orbital periods. This paradox is in agreement with the theoretical prediction of the evolution of CVs resulting from the loss of angular momentum by gravitational radiation. The observable distribution of orbital periods can also be explained by the existence of a minimum period of magnetic braking. This result is important for the study of how the CVs evolve towards a shorter orbital period. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0572, "title": "Mixed Meson Masses with Domain-Wall Valence and Staggered Sea Fermions", "abstract": "  Mixed action lattice calculations allow for an additive lattice spacing dependent mass renormalization of mesons composed of one sea and one valence quark, regardless of the type of fermion discretization methods used in the valence and sea sectors. The value of the mass renormalization depends upon the lattice actions used. This mixed meson mass shift is an important lattice artifact to determine for mixed action calculations; because it modifies the pion mass, it plays a central role in the low energy dynamics of all hadronic correlation functions. We determine the leading order, $\\mathcal{O}(a^2)$, and next to leading order, $\\mathcal{O}(a^2 m_\\pi^2)$, additive mass shift of \\textit{valence-sea} mesons for a mixed lattice action with domain-wall valence fermions and rooted staggered sea fermions, relevant to the majority of current large scale mixed action lattice efforts. We find that on the asqtad improved coarse MILC lattices, this additive mass shift is well parameterized in lattice units by $\\Delta(am)^2 = 0.034(2) -0.06(2) (a m_\\pi)^2$, which in physical units, using $a=0.125$ fm, corresponds to $\\Delta(m)^2 = (291\\pm 8 \\textrm{MeV})^2 -0.06(2) m_\\pi^2$. In terms of the mixed action effective field theory parameters, the corresponding mass shift is given by $a^2 \\Delta_\\mathrm{Mix} = (316 \\pm 4 \\textrm{MeV})^2$ at leading order plus next-to-leading order corrections including the necessary chiral logarithms for this mixed action calculation, determined in this work. Within the precision of our calculation, one can not distinguish between the full next-to-leading order effective field theory analysis of this additive mixed meson mass shift and the parameterization given above. ", "machine_abstract": "We present results for the masses of light mesons made up of quarks in different flavors, using domain-wall valence fermions on N f = 2 + 1 flavor staggered sea configurations generated by MILC collaboration at lattice spacing $a \\sim 0.12$ fm.  We use two methods to extract the masses:  The first method is based on fitting the correlation functions directly to exponentials.  In this case we find that the mixed meson masses are systematically lower than those obtained from single exponential fits to the corresponding correlators.  This effect can be explained as an artifact due to excited state contamination.  To remove it, we fit the data simultaneously to three exponentials (two ground states and one excited state) and obtain consistent values for all the masses within statistical errors.  For the second method, we calculate the effective masses of the mixed mesons and then perform a linear extrapolation to infinite time separation.  Again, our results agree well with each other.  Finally, we compare our results with experimental measurements and previous calculations performed with Wilson-type fermions.", "paraphrased_abstract": "Moreover, we compare our results with experimental measurements and with previous measurements of Wilson fermions. We present the mass of the light mesons of quarks in different colors of a domain, based on valence fermions on the surface of the domain, and with the same spacing as the lattices, in a ratio of 0.2 to 0. This effect, presumably due to the excited state, is explained by the fact that the mass of the mixed mesons is systematically lower than those of the single calamity. For the second method, we compute the mass of the mixed mesons and give an indefinite calculation of the time. Then, we compute the mass of the mixed mesons and, after that, give an indefinite linear extrapolation of the total mass, and conclude that our results are consistent with the one of the experimental measurements. We present the mass of light mesons made of quarks in different flavors, grouped in domains, in which the density of the domains, f = 2 f 2 is 0.12 fm. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3041, "title": "ATLAS sensitivity to Wtb anomalous couplings in top quark decays", "abstract": "  We study the sensitivity of the ATLAS experiment to Wtb anomalous couplings in top pair production with semileptonic decay, pp -> t tbar -> W+ b W- bbar, with one of the W bosons decaying leptonically and the other hadronically. Several observables are examined, including the W helicity fractions and new quantities recently introduced, such as the ratios of helicity fractions and some angular asymmetries defined in the W rest frame. The dependence on anomalous couplings of all these observables has been previously obtained. In this work we show that some of the new observables also have smaller systematic uncertainties than the helicity fractions, with a dependence on anomalous couplings similar or stronger than for helicity fractions. Consequently, their measurement can significantly improve the limits on anomalous couplings. Moreover, the most sensitive measurements can be combined. In this case, the precision achieved in the determination of Wtb anomalous couplings can be of a few percent in the semileptonic channel alone. ", "machine_abstract": "We present the results on searches for new physics beyond the Standard Model (SM) in ttbar events at sqrt(s) = 7 TeV, using data collected by ATLAS during 2011 corresponding to an integrated luminosity of 4.7 fb-1 . The analysis is performed with three different final states: lepton+jets , dilepton and all-hadronic channels. We consider two types of models that can be tested with these analyses: vector-like quarks decaying into tWb or tbHq final states. In both cases we use simplified models where only one coupling parameter is varied while keeping others fixed to their SM values. No significant deviations are observed compared to the predictions based on the SM. Limits are set on the parameters describing the interactions between heavy particles and fermions. These limits improve upon previous measurements made at LEP2 and LHC Run I.  The ATLAS Collaboration", "paraphrased_abstract": "The results of this study are presented in the light of observations at the intersection of tbars of the universe with the astrophysics of 4.8 fb-1. Among these, we consider the case of a two-dimensional model in which the simplest parameter, the coupling of heavy particles and fermions, is varied, and the rest is fixed to the SM. We consider two different types of models: a vector containing a tittle particle that decays to tWb and tbHq in tWb or tbHq, in which, as with a tittle particle, the coupling of a heavy particle and a fermions, is incorporated in the model. In this way, we have obtained results on the search for new physics outside the SM in the events of ttBars, which lasted 4.6 fb-1. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2301, "title": "X-ray detection of the substellar twin 2MASS J11011926-7732383 AB", "abstract": "  2MASS J11011926-7732383 AB (hereafter 2M1101AB), located in the Cha I star forming region, is a rare wide-separation brown dwarf binary. XMM-Newton and Chandra observations of 2M1101AB have allowed us to examine the influence of physical parameters (mass, bolometric luminosity and effective temperature) on X-ray emission from a coeval pair of substellar objects. The spatial resolution of XMM-Newton is not sufficient to separate contributions from the two components in the binary. The X-ray source detected with XMM-Newton has a column density compatible with the infrared extinction of component A. On the other hand, the binary is resolved with Chandra, and the bulk of the X-ray emission is clearly associated with the photospherically cooler component B. These apparently contradictory results point at strong variability of 2M1101's X-ray emission. Combined with previous sensitive X-ray observations from low-mass members of ChaI, we find a decline of X-ray luminosity with decreasing (sub)stellar mass that is typical for star forming regions. 2M1101B is the coolest (spectral type M8.25) and least massive brown dwarf of ChaI detected in X-rays so far. It is also among the youngest (~1 Myr) substellar ChaI members, and therefore relatively luminous. Most bona fide brown dwarfs of Cha I have remained below the sensitivity limits of available X-ray observations, because of their low luminosity associated with higher age. ", "machine_abstract": "We report on the X-ray properties of the young, nearby (d = 11 pc), low-mass binary system 2MASS J1101-2677AB discovered by Burgasser et al. (2007) . The primary component is an M8 dwarf with T eff ~ 2600 K and log g ~ 5.0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log g ~ 4.2. We observed this target for about 50 ks using Chandra ACIS-S in order to search for evidence of coronal activity associated with either or both components. No significant emission was detected at the position of the source down to a 3-sigma upper limit of 1 x 10^28 erg s-1 cm-2 . This non-detection suggests that if there are active regions present they must be small and/or cool compared to those found on more massive stars. In addition we find no evidence of flaring behavior during our observation which constrains any possible magnetic field strength to less than 100 G.", "paraphrased_abstract": "At this point the source of a source of X-rays was observed at a distance of about 50 ks, observing the X-rays of a young, low-mass binary system, J1102-677, discovered by Burgasser et al. (2007). The source was a M8 dwarf with a log g of about 5,000 K and a log g of about 4.4. The allied component was a brown dwarf with a log g of 5.0, while the allied component was a brown dwarf with a log g of 4.2. This is an adsorption that suggests that if there are active regions, it must be small, or cool, compared with that found on larger stars. We will now present the X-ray properties of the young, low-mass binary system 2MASS J1101-2677AB, discovered by Burgasser et al. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.069, "title": "Dynamic Properties of Molecular Motors in Burnt-Bridge Models", "abstract": "  Dynamic properties of molecular motors that fuel their motion by actively interacting with underlying molecular tracks are studied theoretically via discrete-state stochastic ``burnt-bridge'' models. The transport of the particles is viewed as an effective diffusion along one-dimensional lattices with periodically distributed weak links. When an unbiased random walker passes the weak link it can be destroyed (``burned'') with probability p, providing a bias in the motion of the molecular motor. A new theoretical approach that allows one to calculate exactly all dynamic properties of motor proteins, such as velocity and dispersion, at general conditions is presented. It is found that dispersion is a decreasing function of the concentration of bridges, while the dependence of dispersion on the burning probability is more complex. Our calculations also show a gap in dispersion for very low concentrations of weak links which indicates a dynamic phase transition between unbiased and biased diffusion regimes. Theoretical findings are supported by Monte Carlo computer simulations. ", "machine_abstract": "We study the dynamic properties of molecular motors by using burnt-bridge models, which are simple kinetic Monte Carlo simulations for motor proteins such as kinesin and myosin V. We show that these models reproduce several experimental results on single-molecule experiments with high accuracy.  In particular, we find that the velocity distribution is well described by an exponential function at low load force but deviates from it when the load increases. The mean square displacement shows subdiffusive behavior under large loads. These behaviors can be explained by considering the effect of the elasticity of the cargoes carried by the motors. Our model also reproduces the dependence of stall forces on external viscous drag coefficients observed experimentally. Finally, our simulation results suggest that the number of steps taken per ATP hydrolysis cycle decreases exponentially with increasing load force. This result may explain why the step size fluctuation becomes larger than expected theoretically near stalling conditions.  I. INTRODUCTIO N Molecular motors play important roles in many biological processes including muscle contraction [1] , vesicle transport [2] , chromosome segregation [3] , and cell division [4] . They convert chemical energy into mechanical work through repeated cycles of binding to cytoskeletal filaments (e.g., microtubules) and releasing them [5] . The most extensively studied class of molecular motors is the kinesins [6] . Kinesins walk along microtubules toward their plus ends [7, 8] . Myosins move towards actin filaments' minus ends [9] . Both types of motors have been shown to take discrete steps [10 -12] . Recent studies have revealed that both kinesins [13] and myosins [14] exhibit stochastic stepping motions even without external loads [15 -19] . It has been suggested that this randomness arises mainly due to thermal fluctuations [20, 21] or internal noise [22] . However, there still remain open questions about how they respond to external loads [23] . In order to understand the mechanism underlying the operation of molecular motors, various theoretical approaches have been developed so far [24] . Among those methods, kinetic Monte Carlo (KMC) simulations [25] provide useful information on", "paraphrased_abstract": "The molecular motors play an important role in many biological processes, including muscle contraction, vesicle transportation, chromosome segregation, and cell division. They are carried by chemical energy, which is converted into mechanical work, by the repeated association of cytoskeletal filaments (e.g., microtubules), which in turn bind to cytoskeletal filaments and release them. Then, we demonstrate that the velocity of ATP hydrolysis is linearly symmetric with the low force of the motor, but in excess of the force of the motor, the mean square displacement is symmetrical. We show that our model is capable of predicting several experimental results on single molecules of the organism, with a high degree of precision, and a significant augmentation of the viscous drag coefficient.... This result is also consistent with the elasticity of the cargo carried by the motor, and therefore, in particular, it is based on the coefficient of elasticity of the kinetic cargo. In addition, it is shown that the number of steps per ATP hydrolysis cycle decreases exponentially with increasing force. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2735, "title": "Charge Polarization Effects and Hole Spectra Characteristics in Alxgai-xN/Gan Superlattices", "abstract": "  We study the effects of charge polarization on the extended physical properties of superlattices, such as transmission coefficients and valence band structure. We consider both linear and parabolic modulation of the band edge. Based on the theory of finite periodic systems (TFPS), analytic expressions and high precision calculations of the relevant physical quantities for n-cell systems are obtained. New and also well-known features of these systems are identified. Besides the well-known energy bandstructure, we also have the field bandstructure, with interesting characteristics.Wider field gaps at stronger internal electric fields and higher density of field bands for larger layer widths are some of these characteristics. Well denned level density asymmetries identify the minibands induced by charge polarization or the so-called Quantum Confining Stark Effect. We present the n-cell transmission amplitudes, transmission coefficients and miniband structures for different values of the relevant parameters. ", "machine_abstract": "The hole spectra characteristics are investigated for the superlattices (SLs) with different aluminum contents by using first-principles calculations based on density functional theory. The results show that the valence band maximum is located at \u0393 point, while conduction band minimum locates at M point. With increasing x value, the energy gap decreases gradually due to the increase of electronegativity difference between GaN and AlN layers. In addition, we find that there exists an obvious charge polarization effect in SLs which leads to the formation of electric field inside the SLs. This electric field causes the splitting of heavy-hole subband into two parts near Fermi level. Furthermore, it can be seen clearly that the split-off bands move towards lower energies as x increases. Finally, the calculated effective masses of holes decrease slightly when x changes from 0.25 to 1.0.     Keywords: AlGaInN/GaN superlattice; Charge polarization", "paraphrased_abstract": "The results indicate that the maximum valence is at the -point, while the conduction band is at the M-point. Adding to this, the charge polarization of SLs is visible, and this electric field forms in the SLs, which results in a splitting of the heavy p-aedone subband, and at the Fermi level, in the formation of a thin-film of metal atoms. The resulting density of the holes decreases gradually, with increasing x-values. As x-aeons decrease, the effective mass of the holes decreases. The spectra of the holes in the SLs of different compositions are examined by using density functional theory. The results show that the maximum valence is at, the minimum conduction is at M. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2396, "title": "Nonlinear generalized functions and the Heisenberg-Pauli foundations of Quantum Field Theory", "abstract": "  In 1929 Heisenberg and Pauli laid the foundations of QFT by quantizing the fields (method of canonical quantization). This general theory of quantized fields has remained undisputed up to now. We show how the unmodified Heisenberg-Pauli calculations make sense mathematically by using a theory of generalized functions adapted to nonlinear operations. By providing an appropriate mathematical setting, nonlinear generalized functions open doors for their understanding but there remains presumably very hard technical problems. (i) Domains of the interacting field operators: a priori the H-P calculations give time dependent dense domains, what is not very convenient; (ii) Calculations of the resulting matrix elements of the S operator: from the unitarity of the S operator as a whole there are no longer ``infinities,'' but a priori there is no other hope than heavy computer calculations; (iii) Connection with renormalization theory: it should provide an approximation when the coupling constant is small. The aim of this paper is to present, on the grounds of a standard mathematical model of QFT (a self interacting scalar boson field), a basis for improvement without significant prerequisites in mathematics and physics. It is an attempt to use nonlinear generalized functions in QFT, starting directly from the calculations made by physicists, in the same way as they have already been used in classical mechanics and general relativity. ", "machine_abstract": "The present work is devoted to the study of nonlinear generalized functions in connection with the foundations of quantum field theory (QFT). The main results are as follows:  1) We show that the concept of nonlinear generalized function can be used for constructing an alternative approach to QFT, which does not use any additional axioms beyond those of standard relativistic quantum mechanics.  2) In this approach we introduce new mathematical objects - nonlinear generalized fields, which play the role similar to that played by usual quantum fields in conventional QFT. 3) We prove that these new fields satisfy all basic properties required for them to serve as fundamental building blocks of QFT. 4) We also demonstrate how our formalism allows one to obtain the Feynman rules for calculating various physical quantities within the framework of QFT. 5) Finally, using the developed formalism, we derive some important relations between different Green's functions appearing in QFT.", "paraphrased_abstract": ") we conceive that the notion of nonlinear generalization can be adapted to construct a new method of astrophysics that does not involve any additional axioms beyond those of standard relativistic mechanics. We show that the method can satisfy the basic properties of all the fundamental building blocks of QFT. In addition, we show that our formalism can be used to calculate Feynman rules for the calculation of physical quantities in QFT. In this study we are presenting the following results: 1) we introduce new mathematical objects, nonlinear generalized fields, which serve in the same way as the classical quantum fields in QFT. 2) We demonstrate that the new fields fulfill all the fundamental properties required to make QFT fundamental. 3) We show that the new fields are quite compatible with the basic properties of QFT. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0314, "title": "Surface Structure in an Accretion Disk Annulus with Comparable Radiation and Gas Pressure", "abstract": "  We have employed a 3-d energy-conserving radiation MHD code to simulate the vertical structure and thermodynamics of a shearing box whose parameters were chosen so that the radiation and gas pressures would be comparable. The upper layers of this disk segment are magnetically-dominated, creating conditions appropriate for both photon bubble and Parker instabilities. We find little evidence for photon bubbles, even though the simulation has enough spatial resolution to see them and their predicted growth rates are high. On the other hand, there is strong evidence for Parker instabilities, and they appear to dominate the evolution of the magnetically supported surface layers. The disk photosphere is complex, with large density inhomogeneities at both the scattering and effective (thermalization) photospheres of the evolving horizontally-averaged structure. Both the dominant magnetic support and the inhomogeneities are likely to have strong effects on the spectrum and polarization of thermal photons emerging from the disk atmosphere. The inhomogeneities are also large enough to affect models of reflection spectra from the atmospheres of accretion disks. ", "machine_abstract": "We present the results of three-dimensional hydrodynamic simulations of accretion disks around black holes, which include both gas pressure and radiation pressure as well as self-gravity. We find that the surface density distribution is not smooth but shows spiral arms at radii where the disk becomes optically thick to its own emission. The spiral structure arises because of gravitational instability caused by the rapid increase of the Toomre Q parameter when the disk becomes optically thin. In addition we show that the radial velocity dispersion increases rapidly near the inner edge of the annulus due to shocks produced there. This may be responsible for producing broad line profiles observed in some AGNs.     Keywords: Black hole -accretion disk systems; Hydrodynamics; Self-gravitation; Shock waves; Gravitational instabilities; Opacity effects         1 Introduction     It has been suggested that many active galactic nuclei (AGN) are powered by supermassive black holes (SMBHs). A SMBH can grow through mass accretion onto it via an accretion disk surrounding the central object. Since the discovery of quasars more than 30 years ago, observations have shown that most AGNs exhibit double-humped broad-line profiles in their optical spectra (e.g., [1; 2]), indicating that they contain rotating accretion disks [3] . However, theoretical models predict that such disks should become unstable if they rotate too fast [4] , so how do these objects maintain stability? One possible explanation is that the disks are supported against gravity by magnetic fields [5] or relativistic jets [6] .   In this Letter, we study the properties of accretion disks using three-dimensional hydrodynamical simulations including both gas pressure and radiation pressures as well as self-gravity [7\u20139] . Our main goal here is to investigate whether the surface density distribution of the disk is smooth or exhibits spiral structures. If the latter case occurs, then what causes them? 2 Model Description  Model Setup The basic equations governing our model are given by:", "paraphrased_abstract": ". The results of this investigation are presented in the form of a three-dimensional hydrodynamical model of accretion disks, which include gas and radiation pressure, and self-gravity. We have studied the properties of accretion disks in terms of the physical properties of accretion disks by means of three-dimensional hydrodynamics, involving gas and radiation pressure, and self-gravity. We are interested in the dynamics of accretion disks under the influence of supermassive black holes. Our findings suggest that most of these AGNs have double-ribbed broadline profiles. This pattern may be attributed to the rotational instability of the accretion disks, which are encased by a disk, which is in turn accretion. We show that the surface density distribution of the disk is not smooth, but reveals a spiral structure at the very radii of the disk when it becomes optically thick. The spiral structure is due to the rapid increase of the Toomre Q parameter as the disk grows optically thin. But in some AGNs the radii are shifted. The effect of this is that the radial velocity is increased, which may lead to the formation of the characteristic lines. We therefore test the properties of the accretion disks by means of three-dimensional hydrodynamic models, including the ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3909, "title": "Bulk viscosity in a cold CFL superfluid", "abstract": "  We compute one of the bulk viscosity coefficients of cold CFL quark matter in the temperature regime where the contribution of mesons, quarks and gluons to transport phenomena is Boltzmann suppressed. In that regime dissipation occurs due to collisions of superfluid phonons, the Goldstone modes associated to the spontaneous breaking of baryon symmetry. We first review the hydrodynamics of relativistic superfluids, and remind that there are at least three bulk viscosity coefficients in these systems. We then compute the bulk viscosity coefficient associated to the normal fluid component of the superfluid. In our analysis we use Son's effective field theory for the superfluid phonon, amended to include scale breaking effects proportional to the square of the strange quark mass m_s. We compute the bulk viscosity at leading order in the scale breaking parameter, and find that it is dominated by collinear splitting and joining processes. The resulting transport coefficient is zeta=0.011 m_s^4/T, growing at low temperature T until the phonon fluid description stops making sense. Our results are relevant to study the rotational properties of a compact star formed by CFL quark matter. ", "machine_abstract": "We calculate the bulk viscosity for a strongly interacting fermion system at finite temperature and density using the relaxation time approximation (RTA). We find that the RTA is not applicable to the case where the chemical potential $\\mu$ approaches zero, because the relaxation rate vanishes as $1/\\mu^2$. In this regime we use an alternative approach based on the Kubo formula with the spectral function obtained by solving the Bethe-Salpeter equation numerically. The results show that the bulk viscosity increases rapidly near the critical point due to the enhancement of the relaxation rate caused by the increase of the effective mass. This behavior can be understood intuitively within the framework of Landau's theory of second-order phase transitions. Finally, we discuss how our result could be used to study the dynamics of neutron stars. Bulk viscosity plays an important role in understanding the damping mechanism of pulsar glitches.     In recent years there has been considerable interest in studying the properties of dense nuclear matter such as quark-gluon plasma or color-flavor-locked (CFL) state [1] . It was suggested [2] that the bulk viscosity may play an important role in explaining the observed rapid spin-down of young pulsars [3] , which cannot be explained by conventional mechanisms [4] .     In this Letter, we present numerical calculations of the bulk viscosity for a CFL superfluid at finite temperatures and densities. Our calculation is performed in the relaxation-time-approximation (RTA), which is valid when the relaxation rate is much larger than the typical frequency scale of the problem [5] . However, it turns out that the relaxation rate decreases very quickly as one moves towards lower temperatures [6] . Therefore, the applicability of the RTA becomes questionable if the chemical potential $\\mu \\ll T$ [7, 8] . To overcome this difficulty, we employ another method based on the Kubo formalism [9] .  The rest of this Letter is organized as follows. In Sec. II, we briefly review the basic idea behind the RTA. Then, in Secs. III and IV, we explain how we solve the", "paraphrased_abstract": "This letter is in this form: the first paragraph of this paper is a general introduction to the concept of the RTA. This method is useful in cases where the relaxation rate is much larger than the typical frequency. However, the relaxation rate increases rapidly as the temperature falls below zero, and the resolving time of the RTA is more uncertain. The second paragraph explains how we solve the equation of the density of a superfluid. This equation is based on the concept of the spectral function of the Bethe-Salpeter equation. The second paragraph presents an approach based on the Kubo formula, with the results of the Bethe-Salpeter equation numerically solved. This approach, which we employ, proves to be of a certain application to the dynamics of neutron stars. The bulk density of a superfluid is determined in the form of a resolving time, the resolving time of which is of the resolving time, which is quite different from the usual frequency range, and which is of the lowest temperature. However, the resolving time of the system is very small. The resolving time is therefore in question, and therefore we apply another method, based on the Kubo formalism, to a more precise degree, such as a reduction of the chemical potential to the resolving time", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3253, "title": "Non-Perturbative Corrections and Modularity in N=1 Type IIB Compactifications", "abstract": "  Non-perturbative corrections and modular properties of four-dimensional type IIB Calabi-Yau orientifolds are discussed. It is shown that certain non-perturbative alpha' corrections survive in the large volume limit of the orientifold and periodically correct the Kahler potential. These corrections depend on the NS-NS two form and have to be completed by D-instanton contributions to transform covariantely under symmetries of the type IIB orientifold background. It is shown that generically also the D-instanton superpotential depends on the two-form moduli as well as on the complex dilaton. These contributions can arise through theta-functions with the dilaton as modular parameter. An orientifold of the Enriques Calabi-Yau allows to illustrate these general considerations. It is shown that this compactification leads to a controlled four-dimensional N=1 effective theory due to the absence of various quantum corrections. Making contact to the underlying topological string theory the D-instanton superpotential is proposed to be related to a specific modular form counting D3, D1, D(-1) degeneracies on the Enriques Calabi-Yau. ", "machine_abstract": "We study the non-perturbative corrections to the low-energy effective action for type IIB string theory on Calabi-Yau threefolds with fluxes, using mirror symmetry techniques. We show that these corrections can be organized into an exact expression which is modular invariant under SL(2,Z). This result has important consequences both at weak coupling (where it implies the existence of new BPS states) as well as at strong coupling (where it provides evidence for the conjectured duality between M-theory compactified on G 2 manifolds and type IIA strings on K3 surfaces). The results presented here are based on joint work with A. Giveon, D. Kutasov, S. Raby, C. Vafa, E. Witten, J. Wulff, and B. Zwiebach.     Introduction     In this talk we will discuss some recent developments concerning non perturbative effects in supersymmetric theories. The main focus will be on the case of type IIB string theory compactified on Calabi Yau spaces. Our discussion will include several examples where we will try to explain how our understanding of such phenomena has been improved by exploiting various dualities among different string vacua.", "paraphrased_abstract": "During the discussion we will look at some recent developments concerning the non-contouristy of supersymmetric theories. In particular, we will focus on the case of the type IIB string theory of Calabiya space. In particular, we will consider the effect of the non-contouristy of type IIB on Calabiya space on Calabiya spaces of a kind which is based on the spherical asymmetry of the Yau space. This study is followed by the subsequent investigation by A. Giveon, D. Kutasov, S. Raby, C. Vafa, E. Witten, J. Wulff and B. Zwiebach. In this paper we present a novel method of measuring the non-contourance of type IIB on Calabiya space of threefolds, by means of symmetry. These corrections are given by a symmetry which is modular under SL(2 and 3). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3177, "title": "Computing modular polynomials in quasi-linear time", "abstract": "  We analyse and compare the complexity of several algorithms for computing modular polynomials. We show that an algorithm relying on floating point evaluation of modular functions and on interpolation, which has received little attention in the literature, has a complexity that is essentially (up to logarithmic factors) linear in the size of the computed polynomials. In particular, it obtains the classical modular polynomials $\\Phi_\\ell$ of prime level $\\ell$ in time O (\\ell^3 \\log^4 \\ell \\log \\log \\ell). Besides treating modular polynomials for $\\Gamma^0 (\\ell)$, which are an important ingredient in many algorithms dealing with isogenies of elliptic curves, the algorithm is easily adapted to more general situations. Composite levels are handled just as easily as prime levels, as well as polynomials between a modular function and its transform of prime level, such as the Schl\\\"afli polynomials and their generalisations. Our distributed implementation of the algorithm confirms the theoretical analysis by computing modular equations of record level around 10000 in less than two weeks on ten processors. ", "machine_abstract": "We present an algorithm for computing the modular polynomial $P(x,y)$ modulo prime $p$ with $O((\\log p)(\\log \\log p))$ bit operations and $O(1)$ space.  The algorithm is based on fast multiplication techniques over finite fields and uses only integer arithmetic. It can be used to compute discrete logarithms efficiently when the base field has characteristic 2 or 3 (e.g., elliptic curves). We also show how to use this algorithm as part of a subexponential-time factorization algorithm that runs in time $\\tilde{O}(2^{0.5}n^{2/3})$ where $n$ denotes the number of bits needed to represent the input integers.  The algorithm presented here improves upon previous work by Joux et al. (FOCS 2002), which required $O(p^{3/4})$ bit operations and $poly(p)$ space.", "paraphrased_abstract": "Moreover, we show how to use the method as part of a sub-exponential factorization which operates in time, at time tildeO3212/3, in which the number of bits is represented by a nth element, where n is the number of bits that represent the inputs. This method is based on fast multiplication over finite fields and uses only integer arithmetic. We have presented an algorithm for the modulo prime p(x,y) of O(log plog p) which uses O(log p) bits and O(1) space. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2762, "title": "Three Years of Mira Variable CCD Photometry: What Has Been Learned?", "abstract": "  The subject of micro-variability among Mira stars has received increased attention since DeLaverny et al. (1998) reported short-term brightness variations in 15 percent of the 250 Mira or Long Period Variable stars surveyed using the broadband 340 to 890 nm Hp filter on the HIPPARCOS satellite. The abrupt variations reported ranged 0.2 to 1.1 magnitudes, on time-scales between 2 to 100 hours, with a preponderance found nearer Mira minimum light phases. However, the HIPPARCOS sampling frequency was extremely sparse and required confirmation because of potentially important atmospheric dynamics and dust-formation physics that could be revealed. We report on Mira light curve sub-structure based on new CCD V and R band data, augmenting the known light curves of Hipparcos-selected long period variables [LPVs], and interpret same in terms of [1] interior structure, [2] atmospheric structure change, and/or [3] formation of circumstellar [CS] structure. We propose that the alleged micro-variability among Miras is largely undersampled, transient overtone pulsation structure in the light curves. ", "machine_abstract": "The OGLE project has been monitoring the Magellanic Clouds for three years with its 1.3m telescope at Las Campanas Observatory, Chile. The photometric data are being used to study variable stars in these galaxies and their properties. In this talk I will present some results on the variability of red giant branch (RGB) stars obtained by analyzing about one million light curves collected during the first two observing seasons. These include new periods found among RGB variables as well as period changes observed in many cases. We also discuss possible mechanisms responsible for such variations.     Keywords: Red Giant Branch Stars, Period Changes, Variability, Magellanic Clouds, OGLE Project     This work was supported by Polish KBN grant 2P03D00527. It is based on observations made with the Warsaw University Observatory operated by the Institute of Astronomy of the Polish Academy of Sciences under agreement with the National Science Center of Poland.", "paraphrased_abstract": "This work was funded by the Polish KBN grant number XP03D00527. The data derived from the study of the red giant branch stars, a spherical astronomy instituted at the Warsaw University, is the result of the cooperative cooperation of the Institute of Astronomy and the National Science Center of Poland. The project has been watching the Magellanic Clouds for three years, with its 1.3m astrophysics instrument, at Las Campanas Observatory, Chile. The study of the observable stars in the Magellanic Clouds is based on a number of measurements of the chroma-light curves of the first two seasons. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3336, "title": "Revisiting the effect of nearby supernova remnants on local cosmic rays", "abstract": "  In an earlier paper, the effect of the nearby known supernova remnants (SNRs) on the local cosmic-rays (CRs) was studied, considering different possible forms of the particle injection time. The present work is a continuation of the previous work, but assumes a more realistic model of CR propagation in the Galaxy. The previous work assumed an unbounded three-dimensional diffusion region, whereas the present one considers a flat cylindrical disc bounded in both the radial and vertical directions. The study has found that the effect of the vertical halo boundary $H$ on the local SNR contribution to the observed CR anisotropy is negligible as long as $H\\gtrsim 2kpc$. Considering the values of the halo height $H\\gtrsim 2kpc$ obtained by different authors, the present work suggests that the study of the effect of local sources on the CR anisotropy can be carried out without having much information on $H$ and hence, using the much simpler three-dimentional unbounded solution. Finally, the present work discusses about the possibility of explaining the observed anisotropy below the knee by a single dominant source with properly chosen source parameters, and claims that the source may be an \\textit{undetected} old SNR with a characteristic age of $\\sim 1.5\\times 10^5 yr$ located at a distance of $\\sim 0.57 kpc$ from the Sun. ", "machine_abstract": "We revisit the question whether or not there is an excess in cosmic ray flux near Supernova Remnant (SNR) shells, as reported by PAMELA and AMS-02 experiments. We find that this excess can be explained within uncertainties if one assumes that SNRs accelerate particles with a power law spectrum up to energies above 10^15 eV. The required spectral index for protons is 2.2 +/- 0.1 which agrees well with theoretical expectations based on diffusive shock acceleration theory. For electrons we require a harder spectrum with a slope of 3.0 +/- 0.3. This result implies that either the electron-to-proton ratio decreases rapidly at high energy or that most of the observed CREs are secondary products produced via interactions between accelerated hadrons and background gas. In addition, our results suggest that the total number density of CRs around SNRs should be higher than previously estimated.", "paraphrased_abstract": "The required spectral index for protons is 2.3 + 0.1, and resembles the theoretical expectations, which rely on the theory of diffusive shocks. We conclude that the ratio of electrons to protons is reducing rapidly at higher energies, or that most of the CREs observed are secondary products of the interactions between accelerated hadrons and background gases. Besides, our analysis indicates that the density of CRs around SNRs is higher than the estimates made by previous scientists. In addition, we find that the total number of CRs around SNRs should be higher than previously estimated. In addition, we observe that the ray flux of the cosmic rays near SNRs can be explained by the assumption that SNRs accelerate particles with a power law from 10-15 eV. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0025, "title": "Matching WMAP 3-yrs results with the Cosmological Slingshot Primordial Spectrum", "abstract": "  We consider a recently proposed scenario for the generation of primordial cosmological perturbations, the so called Cosmological Slingshot scenario. We firstly obtain a general expression for the Slingshot primordial power spectrum which extends previous results by including a blue pre-bounce residual contribution at large scales. Starting from this expression we numerically compute the CMB temperature and polarization power spectra arising from the Slingshot scenario and show that they excellently match the standard WMAP 3-years best-fit results. In particular, if the residual blue spectrum is far above the largest WMAP observed scale, the Slingshot primordial spectrum fits the data well by only fixing its amplitude and spectral index at the pivot scale k_p=10^{-3}h x Mpc^{-1}. We finally show that all possible distinctive Slingshot signatures in the CMB power spectra are confined to very low multipoles and thus very hard to detect due to large cosmic variance dominated error bars at these scales. ", "machine_abstract": "We have recently shown that the primordial spectrum of density fluctuations can be obtained by solving an initial value problem for a massless scalar field in deSitter space-time, which is known as the cosmological slingshot effect (CSE). In this work we show how to match the CSE predictions with those of the Wilkinson Microwave Anisotropy Probe 3-year data release (WMAP3) and compare them against other models. We find that our model fits well within 1-sigma error bars on all parameters except n_s, where it lies just outside 2-sigma limits. The best-fit values are given by:  H_0 = 72.6 +/- 0.9 km/s/Mpc,  Omega_m = 0.26 +/- 0.01,   Omega_Lambda = 0.74 +/- 0.02,    n_s = 0.96 +/- 0.06.   These results agree very well with recent measurements made using Type Ia supernovae.  Our analysis shows that the CSE provides a viable alternative explanation for the origin of cosmic structure formation.", "paraphrased_abstract": "This is very apt to be found in the latest experiments on supernovae. This work is based on a three-year study of the Wilkinson microwave experiment, which is presented by the three-year WMAP3 data. The model fits the most closely to the results of the WMAP3 data, except for the ns, which is within two-sigma limits. In this study we show that the primordial radiance of density is obtained by solving the first value of a massless scalar field in the space of destabilization, which is called the cosmological slingshot effect. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0907, "title": "Structure of the stationary state of the asymmetric target process", "abstract": "  We introduce a novel migration process, the target process. This process is dual to the zero-range process (ZRP) in the sense that, while for the ZRP the rate of transfer of a particle only depends on the occupation of the departure site, it only depends on the occupation of the arrival site for the target process. More precisely, duality associates to a given ZRP a unique target process, and vice-versa. If the dynamics is symmetric, i.e., in the absence of a bias, both processes have the same stationary-state product measure. In this work we focus our interest on the situation where the latter measure exhibits a continuous condensation transition at some finite critical density $\\rho_c$, irrespective of the dimensionality. The novelty comes from the case of asymmetric dynamics, where the target process has a nontrivial fluctuating stationary state, whose characteristics depend on the dimensionality. In one dimension, the system remains homogeneous at any finite density. An alternating scenario however prevails in the high-density regime: typical configurations consist of long alternating sequences of highly occupied and less occupied sites. The local density of the latter is equal to $\\rho_c$ and their occupation distribution is critical. In dimension two and above, the asymmetric target process exhibits a phase transition at a threshold density $\\rho_0$ much larger than $\\rho_c$. The system is homogeneous at any density below $\\rho_0$, whereas for higher densities it exhibits an extended condensate elongated along the direction of the mean current, on top of a critical background with density $\\rho_c$. ", "machine_abstract": "We study the structure of the stationary distribution for an asymmetric version of the Target Process (TP). The TP is a continuous-time Markov chain on the integers, which has been introduced by Aldous and Pitman in their seminal work [1] . It can be seen as a model for the evolution of a population with immigration and emigration at fixed rates. In this article we consider a generalization where individuals are allowed to emigrate only if they have reached some threshold value k \u2208 N 0 , i.e., the number of immigrants per unit time decreases linearly when the current size of the population increases beyond k. We show that under mild conditions there exists a unique stationary distribution \u03c0 = (\u03c0(n)) n\u2208N0 such that \u03c0(k) > 0 holds true. Moreover, we prove that \u03c0 satisfies a recursion formula similar to the one obtained for the original TP. Finally, we provide explicit expressions for \u03c0(1), ..., \u03c0(k+1). Keywords: Asymmetric target process, Stationary distribution", "paraphrased_abstract": "The asymmetrical target process is a continuous-time Markov chain on the integers, introduced by Aldous and Pitman in their seminal work... In their seminal work... we have developed an asymmetrical version of the target process... The asymmetrical target process is a continuous-time Markov chain on the integers, which Aldous and Pitman introduced in their seminal work. We will illustrate that a special stationary distribution exists in the absence of any -n, -n, -n-0, such that  -n = 0; we will prove that  satisfies a recursion formula similar to that obtained in the original TP. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2576, "title": "Adjointability of densely defined closed operators and the Magajna-Schweizer Theorem", "abstract": "  In this notes unbounded regular operators on Hilbert $C^*$-modules over arbitrary $C^*$-algebras are discussed. A densely defined operator $t$ possesses an adjoint operator if the graph of $t$ is an orthogonal summand. Moreover, for a densely defined operator $t$ the graph of $t$ is orthogonally complemented and the range of $P_FP_{G(t)^\\bot}$ is dense in its biorthogonal complement if and only if $t$ is regular. For a given $C^*$-algebra $\\mathcal A$ any densely defined $\\mathcal A$-linear closed operator $t$ between Hilbert $C^*$-modules is regular, if and only if any densely defined $\\mathcal A$-linear closed operator $t$ between Hilbert $C^*$-modules admits a densely defined adjoint operator, if and only if $\\mathcal A$ is a $C^*$-algebra of compact operators. Some further characterizations of closed and regular modular operators are obtained.   Changes 1: Improved results, corrected misprints, added references. Accepted by J. Operator Theory, August 2007 / Changes 2: Filled gap in the proof of Thm. 3.1, changes in the formulations of Cor. 3.2 and Thm. 3.4, updated references and address of the second author. ", "machine_abstract": "We prove that if an operator is adjointable, then it has dense range in its domain.  We also show how this result can be used to give another proof of the Magajna-Schwizer theorem on the existence of self-adjoint extensions for densely defined closed operators with equal deficiency indices. Let H be a Hilbert space over C. An operator T : D(T) \u2192 H is said to have dense range (or simply DR), where D(T) \u2286 H, if for every vector x \u2208 H there exists a sequence {xn}n\u2208N such that limn\u2192\u221e||xn\u2212Tx||=0. In other words, the image of T contains no proper subspace of H. The following theorem shows that any adjointable operator must necessarily have dense range.   THEOREM 1.1. If T:D(T)\u2286H\u2192H is adjointable, then R(T\u2217)=D(T).", "paraphrased_abstract": "Then, let C be a Hilbert space, and let T be a Hilbert space. Let T be a Hilbert space. If T:D(T) H is adjoined, then R(T) = T. The following formula proves that any adjoined operator must have dense range. Then, we prove that the operator T : D(T) H \u2013 DR, if for every vector x  H there exists a sequence limn||xnTx||0. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2507, "title": "The velocity structure of moving magnetic feature pairs around sunspots: support for the U-loop model", "abstract": "  Using data recorded by the Michelson Doppler Imager (MDI) instrument on the Solar and Heliospheric Observatory (SOHO), we have traced 123 pairs of opposite magnetic polarity moving magnetic features (MMFs) in three active regions NOAA ARs 8375, 0330 and 9575. At the time of observation, AR 8375 was young, AR 0330 mature, and AR 9575 decaying. The vertical velocity indicates that the elements of MMF pairs with polarity opposite to that of the sunspot support a downflow of around 50-100 m s$^{-1}$. The average Doppler shift difference between negative and positive elements of an MMF pair is about 150 m s$^{-1}$ in AR 8375, 100 m s$^{-1}$ in AR 0330, and 20 m s$^{-1}$ in AR 9575. These observational results are in agreement with the model that MMF pairs are part of a U-loop emanating from the sunspot's magnetic canopy. According to this model the downflow is caused by the Evershed flow returning below the solar surface. For AR 8375, the horizontal velocity of MMFs ranges from 0.1 km s$^{-1}$ to 0.7 km s$^{-1}$, and on average, the velocity of an MMF pair decreases significantly (from 0.6 km s$^{-1}$ to 0.35 km s$^{-1}$) with increasing distance from the MMF's birth place. This result suggests that the change in MMF flow speed does not reflect the radial structure of the moat flow, but rather is intrinsic to the evolution of the MMF pairs. This result is also in agreement with the U-loop model of MMF pairs. We also find that properties of MMF pairs, most strikingly the lifetime, depend on the evolution stages of the parent sunspot. The mean lifetimes of MMF pairs in ARs 9575 and 0330 are 0.7 hours and 1.6 hours, respectively, which is considerably shorter than the 4 hours lifetime previously found for AR 8375. ", "machine_abstract": "We analyze the evolution and kinematics of two opposite polarity magnetic features (MPFs) in an active region using high-resolution vector magnetograms obtained by Hinode/SOT-SP. The MPF pair is found to be associated with a coronal loop system that forms a \"U-shape\" when viewed along its axis, which we refer to as the \"U-loop\" model. We find that both MPFs are rooted at different depths below the solar surface; one MPF has a stronger field strength than the other but they have similar sizes. Both MPFs show outward motions away from their common center point during the time period studied here. In addition, there exists a counterclockwise rotation motion between these two MPFs. These results suggest that the MPF pair may evolve into a bipolar sunspot through flux cancellation processes. This work was supported by NASA grants NNX08AQ85G and NNG09FA40C.", "paraphrased_abstract": "And the kinetics of the two magnetic features are investigated in detail, and the results are presented. The two magnetic features are shown to be rooted at different depths below the solar surface. One of the ten ms is stronger than the other, but the two are the same size. During the period studied, the ms has moved from its common center. This result is a sign of a reconnection of the magnetic axis, which is known as the 'U-loop'. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.335, "title": "The Statistics of the Points Where Nodal Lines Intersect a Reference Curve", "abstract": "  We study the intersection points of a fixed planar curve $\\Gamma$ with the nodal set of a translationally invariant and isotropic Gaussian random field $\\Psi(\\bi{r})$ and the zeros of its normal derivative across the curve. The intersection points form a discrete random process which is the object of this study. The field probability distribution function is completely specified by the correlation $G(|\\bi{r}-\\bi{r}'|) = <\\Psi(\\bi{r}) \\Psi(\\bi{r}')>$.   Given an arbitrary $G(|\\bi{r}-\\bi{r}'|)$, we compute the two point correlation function of the point process on the line, and derive other statistical measures (repulsion, rigidity) which characterize the short and long range correlations of the intersection points. We use these statistical measures to quantitatively characterize the complex patterns displayed by various kinds of nodal networks. We apply these statistics in particular to nodal patterns of random waves and of eigenfunctions of chaotic billiards. Of special interest is the observation that for monochromatic random waves, the number variance of the intersections with long straight segments grows like $L \\ln L$, as opposed to the linear growth predicted by the percolation model, which was successfully used to predict other long range nodal properties of that field. ", "machine_abstract": "We consider the problem of finding the statistics of nodal lines in random wave fields, where we assume that the field is generated by an ensemble of monochromatic waves with randomly distributed phases and amplitudes. We show how to calculate these statistics using only the first two moments of the distribution function for the phase differences between pairs of waves. The results are applied to study the statistical properties of nodal lines on the surface of oceanic internal tides. In particular, it is shown that the probability density functions (PDFs) of the number of intersections per unit length of a reference curve depend strongly on whether or not this curve lies within one wavelength of the source region. If so, then the PDF has a power-law tail at large values of the intersection density; if not, then there is no such tail.     Keywords: Random wave fields, Oceanic internal tide, Nodal line, Phase difference", "paraphrased_abstract": "Moreover, it is observed that the density functions of the number of intersections per unit length of the reference curve are strongly affected by the presence of the source region; if the source region is located within a wavelength, then the density function has a power-law tail; if not, it is not.... Moreover, we show how to compute the statistics of nodal lines on the surface of the oceanic internal tides. In this paper we discuss the problem of finding the statistics of nodal lines in a wave field, where we assume that the field is created by a collection of monochromatic waves with randomly distributed phases and amplitudes. In the case of the nodal lines, the first two moments of the distribution function of the phase difference are considered to be the most important factors, and the first two moments are taken for the phase difference. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1609, "title": "GRB 061121: Broadband spectral evolution through the prompt and afterglow phases of a bright burst", "abstract": "  Swift triggered on a precursor to the main burst of GRB 061121 (z=1.314), allowing observations to be made from the optical to gamma-ray bands. Many other telescopes, including Konus-Wind, XMM-Newton, ROTSE and the Faulkes Telescope North, also observed the burst. The gamma-ray, X-ray and UV/optical emission all showed a peak ~75s after the trigger, although the optical and X-ray afterglow components also appear early on - before, or during, the main peak. Spectral evolution was seen throughout the burst, with the prompt emission showing a clear positive correlation between brightness and hardness. The Spectral Energy Distribution (SED) of the prompt emission, stretching from 1eV up to 1MeV, is very flat, with a peak in the flux density at ~1keV. The optical-to-X-ray spectra at this time are better fitted by a broken, rather than single, power-law, similar to previous results for X-ray flares. The SED shows spectral hardening as the afterglow evolves with time. This behaviour might be a symptom of self-Comptonisation, although circumstellar densities similar to those found in the cores of molecular clouds would be required. The afterglow also decays too slowly to be accounted for by the standard models. Although the precursor and main emission show different spectral lags, both are consistent with the lag-luminosity correlation for long bursts. GRB 061121 is the instantaneously brightest long burst yet detected by Swift. Using a combination of Swift and Konus-Wind data, we estimate an isotropic energy of 2.8x10^53 erg over 1keV - 10MeV in the GRB rest frame. A probable jet break is detected at ~2x10^5s, leading to an estimate of ~10^51 erg for the beaming-corrected gamma-ray energy. ", "machine_abstract": "We present broadband (radio to X-ray) observations of GRB 061121, one of the most energetic gamma-ray bursts ever detected by Swift/BAT with an isotropic equivalent energy release of 1.8 x 10^54 erg in the 15-350 keV band. The temporal behavior of this event was complex; it consisted of multiple pulses that were superimposed on top of each other during both the prompt emission phase as well as the early part of its afterglow. We find evidence for two distinct components in the optical light curve -one which decays rapidly at first but then flattens out later-on timescales ranging between 0.1-10 days post-burst. This flattening could be due either to continued activity of the central engine or to refreshed shocks. In addition we detect significant radio emission upto 100 days post-burst. Our results are consistent with those obtained using data from the VLA and ATCA telescopes.", "paraphrased_abstract": "The main axis of the optical curve, which decays at first, gradually flattens after, is of different nature; this flattening can be either the result of the central engine's activity or the result of a fresh shock. We present a new data set, which has been recorded by the VLA and ATCA, and which is compatible with the ones obtained by the VLA and ATCA. The events of this burst were complex, they were composed of several pulses that interposed in the initial phase of the eruption and in the early part of the afterglow. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0714, "title": "Maximum solutions of normalized Ricci flows on 4-manifolds", "abstract": "  We consider maximum solution $g(t)$, $t\\in [0, +\\infty)$, to the normalized Ricci flow. Among other things, we prove that, if $(M, \\omega) $ is a smooth compact symplectic 4-manifold such that $b_2^+(M)>1$ and let $g(t),t\\in[0,\\infty)$, be a solution to (1.3) on $M$ whose Ricci curvature satisfies that $|\\text{Ric}(g(t))|\\leq 3$ and additionally $\\chi(M)=3 \\tau (M)>0$, then there exists an $m\\in \\mathbb{N}$, and a sequence of points $\\{x_{j,k}\\in M\\}$, $j=1, ..., m$, satisfying that, by passing to a subsequence, $$(M, g(t_{k}+t), x_{1,k},..., x_{m,k}) \\stackrel{d_{GH}}\\longrightarrow (\\coprod_{j=1}^m N_j, g_{\\infty}, x_{1,\\infty}, ...,, x_{m,\\infty}),$$ $t\\in [0, \\infty)$, in the $m$-pointed Gromov-Hausdorff sense for any sequence $t_{k}\\longrightarrow \\infty$, where $(N_{j}, g_{\\infty})$, $j=1,..., m$, are complete complex hyperbolic orbifolds of complex dimension 2 with at most finitely many isolated orbifold points. Moreover, the convergence is $C^{\\infty}$ in the non-singular part of $\\coprod_1^m N_{j}$ and $\\text{Vol}_{g_{0}}(M)=\\sum_{j=1}^{m}\\text{Vol}_{g_{\\infty}}(N_{j})$, where $\\chi(M)$ (resp. $\\tau(M)$) is the Euler characteristic (resp. signature) of $M$. ", "machine_abstract": "In this article, we study the maximum solution to the normalized Ricci flow on closed 4-manifolds with positive scalar curvature and prove that it is unique up to scaling if its volume growth rate satisfies some conditions.  We also show that there are no nontrivial steady or expanding solitons for the normalized Ricci flow in dimension four. Finally, we give an example which shows that our results do not hold without assuming positivity of scalar curvature. Keywords: Maximum solution, normalized Ricci flow, uniqueness, volume growth rate, scalar curvature. 1 Introduction Let $(M,g)$ be a closed Riemannian manifold of dimension $n$. The Ricci flow is defined by $$\\displaystyle g_{t}=e^{-2t}g$$ where $t\\in[0,\\infty[$. It was introduced independently by Hamilton [H1] and by Perelman [P] . In particular, Hamilton proved that any compact 3-manifold admits a metric of constant sectional curvature under the Ricci flow (see [H1, H2] ). On the other hand, Perelman showed that the Ricci flow can be used as a tool to solve the Poincar\u00e9 conjecture and the Thurston geometrization conjecture (see [P] ) . Recently, Cao [Cao3] studied the behavior of the Ricci flow near singularities and proved that the limit space at infinity has finite topological type. Moreover, he gave examples showing that the limit spaces may have different topologies even when they share the same volume growth rates. For more information about the Ricci flow, see e.g., [WZ1, WZ2, ZH, CW1, CW2, CC, CLN, LY, YL, LL, LS, LT, MT, N, NW, OW, P, PS, S, SZ, T, V, X, Y, Z] . The normalized Ricci flow is given by $$\\displaystyle g_{*t}=e^{-t}g_t$$ where $t\\in[0, \\infty[$. This flow was first considered by Hamilton [H3] , who showed that the normalized Ricci flow preserves the total volume of the", "paraphrased_abstract": "The Ricci flow was first introduced by Hamilton, who showed that the Normisian Ricci flow preserves the total volume of the whole. Then he gave an example showing that our results are not quite true without a positivity of the Normisian Ricci flow. To this he added the expression \u201cnormal\u201d and \u201cnormal\u201d to be taken to mean: \u201cState.\u201d Then the equation was developed and used by Hamilton, who showed that the Normisian Ricci flow preserves the total volume of the whole. Then Perelman showed that this Normisian Ricci flow can be used as a tool for the Poincar\u00e9 equation and the Thurston geometries. This was also applied to the case of the Normisian Ricci flow, whose heights were given as the Normisian Ricci, where 000; 000; 20; 5050; 511552; 6510810101010, 534pcst, 765555255910710510", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2362, "title": "Geometric control theory I: mathematical foundations", "abstract": "  A geometric setup for control theory is presented. The argument is developed through the study of the extremals of action functionals defined on piecewise differentiable curves, in the presence of differentiable non-holonomic constraints. Special emphasis is put on the tensorial aspects of the theory. To start with, the kinematical foundations, culminating in the so called variational equation, are put on geometrical grounds, via the introduction of the concept of infinitesimal control . On the same basis, the usual classification of the extremals of a variational problem into normal and abnormal ones is also rationalized, showing the existence of a purely kinematical algorithm assigning to each admissible curve a corresponding abnormality index, defined in terms of a suitable linear map. The whole machinery is then applied to constrained variational calculus. The argument provides an interesting revisitation of Pontryagin maximum principle and of the Erdmann-Weierstrass corner conditions, as well as a proof of the classical Lagrange multipliers method and a local interpretation of Pontryagin's equations as dynamical equations for a free (singular) Hamiltonian system. As a final, highly non-trivial topic, a sufficient condition for the existence of finite deformations with fixed endpoints is explicitly stated and proved. ", "machine_abstract": "The book is divided into three parts, each part containing several chapters. The first two parts are devoted to the development and analysis of geometric control systems in finite-dimensional spaces (the state space) and infinite-dimensional Hilbert spaces (the phase space). In particular, the following topics are considered:  Part I - Geometric Control Theory in Finite-Dimensional Spaces.  Chapter 1 - Introduction to Geometric Control Theory.  Chapter 2 - Basic Concepts of Differential Geometry.  Chapter 3 - Lie Groups and Their Representations.  Chapter 4 - Invariant Manifolds for Group Actions on Vector Fields.  Chapter 5 - Stability Analysis of Nonlinear Systems with State Constraints.  Chapter 6 - Stabilization by Feedback of Linear Time-Invariant Systems.  Chapter 7 - Optimal Tracking Problems for Affine Systems.  Part II - Geometric Control Theory on Infinite-Dimensional Hilbert Spaces.   Chapter 8 - Generalized Euler-Lagrange Equations.  Chapter 9 - Hamilton-Jacobi Equations.  Chapter 10 - Pontryagin Maximum Principle.  Chapter 11 - Optimal Control Problem for Discrete-Time Systems.  Chapter 12 - Optimal Control Problem with Uncertain Dynamics.  Chapter 13 - Optimal Control Problem under Stochastic Disturbances.  Chapter 14 - Optimal Control Problem over Networks.  Part III - Applications of Geometric Control Theory.   Chapter 15 - Motion Planning for Mobile Robots.  Chapter 16 - Robot Manipulation Tasks via Task Space Formulation.  Chapter 17 - Visual Servoing Based on Image Feature Extraction.  Chapter 18 - Model Predictive Control for Industrial Processes.", "paraphrased_abstract": "The first three sections are the analysis and development of mathematical control systems in finite dimensions (states) and in infinite dimensions (phases). The second section is the analysis and analysis of geometric control systems in finite dimensions (states) and in infinite dimensions (phases). The third section is the analysis and analysis of the geometric control systems in finite dimensions (states) and in infinite dimensions (phases). The third section is the study of the optimum and optimal control of mobile robots. The fourth section is the enumeration of the optimum and ideal of the time - invariants of the vector fields. The fifth section is the enumeration of the invariants of the vector fields. The fifth section is the enumeration of the equilateral fields. The fifth section is the enumeration of the linear and the invariant systems. The sixth section is the analysis of the optimal control of the stationary systems. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3771, "title": "Primordial Evolution in the Finitary Process Soup", "abstract": "  A general and basic model of primordial evolution--a soup of reacting finitary and discrete processes--is employed to identify and analyze fundamental mechanisms that generate and maintain complex structures in prebiotic systems. The processes--$\\epsilon$-machines as defined in computational mechanics--and their interaction networks both provide well defined notions of structure. This enables us to quantitatively demonstrate hierarchical self-organization in the soup in terms of complexity. We found that replicating processes evolve the strategy of successively building higher levels of organization by autocatalysis. Moreover, this is facilitated by local components that have low structural complexity, but high generality. In effect, the finitary process soup spontaneously evolves a selection pressure that favors such components. In light of the finitary process soup's generality, these results suggest a fundamental law of hierarchical systems: global complexity requires local simplicity. ", "machine_abstract": "We present an approach to primordial evolution based on the finitary process soup (FPS) model, which is a generalization of the standard quantum field theory. The FPS consists of all possible finite sequences of elementary processes that can be constructed by applying a set of basic operations to a given initial sequence. We show how this formalism allows one to describe and analyze various aspects of primordial evolution such as entropy production, particle creation, time dilation etc., using only few parameters characterizing the initial state. In particular we demonstrate that the FPS provides a natural description for the inflationary scenario with no need to introduce additional fields or particles beyond those already existing within the Standard Model. Finally, we discuss some open problems related to our approach. PACS numbers: 04.60.Kz, 11.10.Wx, 12.20.Ds, 98.80.Cq .  I. INTRODUCTORY REMARkS  The idea behind the finitary process soup [1] , also known as the \"quantum soup\" [2] , is very simple - it represents any physical system as a collection of all its possible states. This concept has been used successfully in many areas of physics including statistical mechanics [3] , condensed matter [4] , nuclear [5] and atomic [6] physics, cosmology [7, 8] , quantum gravity [9] , string theory [10, 11] . In this work we apply the FPS formalism to study primordial evolution during the early stages of the universe's expansion. Our main goal will be to develop a general framework allowing us to describe different phenomena associated with the Big Bang without introducing new degrees of freedom not included into the Standard Model [12] . As we shall see below, the FPS naturally leads to a description of the inflationary scenario [13] where the inflaton field [14] emerges as a consequence of the underlying dynamics rather than being introduced ad hoc.  II. THE FINITARY PROCESS SOUP MODEL AND ITS APPLICATION TO PRIMORDIAL EVOLUTION A. General Description Let us start by briefly reviewing the key features of the FPS formalism", "paraphrased_abstract": "We present a general approach to the study of primordial evolution during the first stages of the universe\u2019s expansion. In particular, we present a description of the inflationary situation in which the inflationary field naturally emerges as a result of the underlying dynamics rather than being introduced ad hoc. In short, we present a general approach to primordial evolution based on the FPS, a derived form of the quantum field theory, and is based on the concept of the \u201cquantum soup\u201d, which is a generalization of the quantum field theory of physics. The FPS consists of all possible, finite sequences of elementary processes, which are constructed by means of a set of basic operations involving a given initial state, and, in particular, of the entropy produced by the particles, of the time lapse, and of the decay. We will use this formalism to study the primordial evolution of the universe in the first stages of its expansion. Our main goal is to develop a general framework in which we can describe different phenomena associated with the Big Bang without adding new levels of freedom which are not included in the standard model. This framework has been successfully applied in many different areas of physics, such as Statistical mechanics, Condensed Matter, atomic and nuclear physics, cosmology, quantum gravity, string theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4594, "title": "Two branches of neutron stars - reconciling a 2M_sun pulsar and SN1987A", "abstract": "  The analysis of SN1987A led Brown and Bethe (1995) to conclusion, that the maximum mass of cold neutron stars is low, M_max ~ 1.5M_sun. Such a low M_max, due to a kaon condensation in the stellar core, implies collapse of a too massive deleptonized protoneutron star into a black hole. This would naturally explain the lack of a neutron star in the SN1987A remnant. On the other hand, recent evaluation of mass of PSR J0751+1807 gives M_max > 2M_sun. This contradicts the original Bethe-Brown model, but can be reconciled within scenarios proposed in the present Letter. We consider two types of dense matter models with high-density softening, due to a transition from a non-strange N-phase of matter to a strangeness carrying phase S: kaon condensation and deconfinement of quarks. Two scenarios of neutron star formation in stellar core collapse are considered. In the first scenario, realized in sufficiently hot and dense supernova cores, nucleation of an S-phase is sufficiently rapid so as to form an S-phase core, and implying M_max = M^S_max =~ 1.5M_sun. In the second scenario, nucleation of the S-phase at neutron star birth is too slow to materialize, and the star becomes cold without forming an S-phase core. Then, stellar mass can increase via accretion, until central density \\rho_crit is reached, and the S phase forms. This N branch of neutron stars ends at M=M_crit. We select several models of N-phase satifying the necessary condition M^N_max > 2M_sun and combine them with models of kaon condensation and quark deconfinement. For kaon condensation, we get M_crit =~ M^S_max =~ 1.5M_sun, which is ruled out by PSR J0751+1807. On the contrary, for the EOSs with quark deconfinement we get M_crit =~ M^N_max > 2M_sun, which reconciles SN1987A and PSR J0751+1807. ", "machine_abstract": "We propose that the two branches in the mass distribution of neutron stars are due to different mechanisms for their formation, with one branch being formed by accretion-induced collapse (AIC) of white dwarfs into neutron stars, while another is formed through core-collapse supernovae (CCSNe). We show how this scenario can explain both the existence of massive pulsars as well as the absence of such objects in the observed sample of CCSNe remnants. The proposed model also explains why there have been no successful attempts so far at detecting gravitational waves emitted during AIC events. In addition we argue that our model provides an explanation for the apparent discrepancy between the masses inferred from observations of binary systems containing black holes or neutron stars on one hand, and those inferred from measurements of the radii of isolated neutron stars on the other. Finally, we discuss possible observational tests which could be used to verify our hypothesis.", "paraphrased_abstract": "A second hypothesis is formulated. It is formulated in the following way: he makes an assumption that the mass of neutron stars is, on the one hand, not in the other, the radii of atoms, black holes and neutron stars, on the other, not in the one. The third theory explains why there are no experiments yet, so far, to detect gravitational waves in the AIC world. The two branches in the distribution of neutron stars are the result of two different mechanisms: one is caused by accretion of white dwarfs into neutron stars, and the other by cis-ne, a core-collapse supernovae. This explanation can explain the existence of huge pulsars and the absence of such objects in the observed sample of cis-ne. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0637, "title": "Results on axion physics from the CAST Experiment at CERN", "abstract": "  Axions are expected to be produced in the sun via the Primakoff process. They may be detected through the inverse process in the laboratory, under the influence of a strong magnetic field, giving rise to X-rays of energies in the range of a few keV. Such an Axion detector is the CERN Axion Solar Telescope (CAST), collecting data since 2003. Results have been published, pushing the axion-photon coupling g$_{a\\gamma}$ below the 10$^{-10}$ GeV$^{-1}$ limit at 95% CL, for axion masses less than 0.02 eV. This limit is nearly an order of magnitude lower than previous experimental limits and surpassed for the first time limits set from astrophysical arguments based on the energy-loss concept. The experiment is currently exploring axion masses in the range of 0.02 eV $< m_a <$ 1.1 eV. In the next run, currently under preparation, the axion mass explored will be extended up to the limit of 1.1 eV, testing for the first time the region of theoretical axion models with the axion helioscope method. ", "machine_abstract": "The Axion Dark Matter Search (ADMX) experiment is designed to search for dark matter in the form of axions, which are hypothetical particles predicted by theories beyond the Standard Model.  The ADMX experiment consists of two main components: an antenna and a microwave cavity system that can be tuned over a wide range of frequencies.   In this thesis we present results obtained with the first phase of the experiment using data taken between September 2005 and March 2007.    We report limits on the coupling strength of axions to photons as well as limits on the mass of axions produced via Primakoff conversion inside a strong magnetic field.  These results improve upon previous experimental bounds by more than one order of magnitude. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The Axion Dark Matter Search experiment is designed to search directly for dark matter in the universe in the form of axionic particles. The experiment consists of two main parts: an antenna and a microwave resonator system that can be tunable across a large frequency range. In this dissertation I will discuss our recent results from the first stage of the experiment.", "paraphrased_abstract": "This study is sponsored by the National Institute of Energy, under contract No. 7/A-07-NA27344. The Axion Dark Matter Search is an experiment to find the dark matter in the universe in the form of axionic particles. The Axion Dark Matter Search apparatus consists of two parts, an antenna and a microwave cavity which can be tuned across a wide frequency range. The Axion Dark Matter Search apparatus consists of two parts: the antenna and the microwave cavity, which can be tuned to different frequencies. The first phase of the apparatus is a single antenna and the second is a microwave cavity, which is a device for tuning the wavelengths of the axionic particles. This apparatus is constructed by Lawrence Livermore National Laboratory under the contract TA-34-2734-03. This apparatus is made of a type of axion which is a particle whose density is far beyond the Standard Model. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0552, "title": "Succinct Indexable Dictionaries with Applications to Encoding $k$-ary Trees, Prefix Sums and Multisets", "abstract": "  We consider the {\\it indexable dictionary} problem, which consists of storing a set $S \\subseteq \\{0,...,m-1\\}$ for some integer $m$, while supporting the operations of $\\Rank(x)$, which returns the number of elements in $S$ that are less than $x$ if $x \\in S$, and -1 otherwise; and $\\Select(i)$ which returns the $i$-th smallest element in $S$. We give a data structure that supports both operations in O(1) time on the RAM model and requires ${\\cal B}(n,m) + o(n) + O(\\lg \\lg m)$ bits to store a set of size $n$, where ${\\cal B}(n,m) = \\ceil{\\lg {m \\choose n}}$ is the minimum number of bits required to store any $n$-element subset from a universe of size $m$. Previous dictionaries taking this space only supported (yes/no) membership queries in O(1) time. In the cell probe model we can remove the $O(\\lg \\lg m)$ additive term in the space bound, answering a question raised by Fich and Miltersen, and Pagh.   We present extensions and applications of our indexable dictionary data structure, including:   An information-theoretically optimal representation of a $k$-ary cardinal tree that supports standard operations in constant time,   A representation of a multiset of size $n$ from $\\{0,...,m-1\\}$ in ${\\cal B}(n,m+n) + o(n)$ bits that supports (appropriate generalizations of) $\\Rank$ and $\\Select$ operations in constant time, and   A representation of a sequence of $n$ non-negative integers summing up to $m$ in ${\\cal B}(n,m+n) + o(n)$ bits that supports prefix sum queries in constant time. ", "machine_abstract": "We present succinct indexable dictionaries for encoding k-ary trees, prefix sums and multisets in $O(n)$ bits of space using only $O(\\log n)$ time per operation on average (and worst-case). Our data structure is based on the concept of rank-select queries over bit vectors that are compressed by run-length coding or arithmetic coding. We also show how our results can be used to solve several problems related to range searching in static databases.  Index Terms: Data compression, Dictionary, Ranking, Searching, Run length coding, Arithmetic coding, Range searching, Database management systems, Computational geometry, Bit vector, Compressed sensing, Data structures, Information retrieval, Data mining, Computational complexity theory, Computational biology, Computational number theory, Computational statistics, Computational algebra, Computational geometry, Computational complexity theory. Data Structures. Algorithms", "paraphrased_abstract": "But we have the idea of a hierarchy of k-ary trees, prefix sums and multi-groups in O-N ns of space, and we have the advantage that we can do this in a static database. We have the advantage that we can solve many problems relating to the hierarchy of bits of space. In our paper we have taken up the subject of lexicon, a dictionary, a search, a lexicon, a lexicon, a lexicon, arithmetic, a hierarchy, a hierarchy, a hierarchy, a hierarchy, a hierarchy, a hierarchy, a hierarchy of the lexicon. The lexicon is based on the concept of rank-select queries, of the type of arithmetic or of the type of arithmetic. We present an algorithm for encoding trees, k-ary trees, tally and multisets, arithmetic, a hierarchy, and an algorithm for calculating averages, in order to be as efficient as possible. The algorithms are based on the concept of rank-selecting queries, of a kind similar to arithmetic or arithmetic queries. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4492, "title": "Jets, Blazars and the EBL in the GLAST-EXIST Era", "abstract": "  The synergy of GLAST and the proposed EXIST mission as the Black Hole Finder Probe in the Beyond Einstein Program is remarkable. With its full-sky per orbit hard X-ray imaging (3-600 keV) and \"nuFnu\" sensitivity comparable to GLAST, EXIST could measure variability and spectra of Blazars in the hard X-ray synchrotron component simultaneous with GLAST (~10-100GeV) measures of the inverse Compton component, thereby uniquely constraining intrinsic source spectra and allowing measured high energy spectral breaks to measure the cosmic diffuse extra-galactic background light (EBL) by determining the intervening diffuse IR photon field required to yield the observed break from photon-photon absorption. Such studies also constrain the physics of jets (and parameters and indeed the validity of SSC models) and the origin of the >100 MeV gamma-ray diffuse background likely arising from Blazars and jet-dominated sources. An overview of the EXIST mission, which could fly in the GLAST era, is given together with a synopsis of other key synergies of GLAST-EXIST science. ", "machine_abstract": "The Fermi Large Area Telescope (LAT) has revolutionized our understanding of blazar jets by providing unprecedented sensitivity to gamma rays above 100 MeV. The LAT is now routinely detecting flaring activity at GeV energies that are not seen with previous instruments such as EGRET on CGRO or AGILE/GRID. This new data allows us to probe deeper into the physics of these objects than ever before.     In this talk I will discuss how we can use the upcoming generation of space-based observatories - including GLAST and EXIST - to study blazars over an even broader energy range. These missions promise to provide detailed information about the physical processes occurring within relativistic jets through observations across many decades in photon energy. We will also explore some of the exciting science questions that could be addressed using these facilities. Finally, I will present preliminary results from my recent work studying the effects of intergalactic infrared background radiation on blazar spectra.", "paraphrased_abstract": "\u201cThis talk will cover the next generation of space-based telescopes, which include the new GOLD and EXIST, for the study of blazars of a broad spectrum of temperatures and intensities. This is an exciting opportunity for physics, and will also show some of the fascinating research questions that could be addressed with the help of these new telescopes. The Fermi LAT has changed our understanding of blazars in a new way by providing unprecedented sensitivity to gamma rays of a magnitude of 100 million. This new instrument has enabled us to probe the physics of these objects in much deeper depths. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1552, "title": "Fourier resolved spectroscopy of 4U 1728-34: New Insights into Spectral and Temporal Properties of Low-Mass X-ray Binaries", "abstract": "  Using archival RXTE data we derive the 2-16 keV Fourier-resolved spectra of the Atoll source 4U 1728-34 in a sequence of its timing states as its low QPO frequency spans the range between 6 and 94 Hz. The increase in the QPO frequency accompanies a spectral transition of the source from its island to its banana states. The banana-states' Fourier-resolved spectra are well fitted by a single blackbody component with $kT \\sim 2-3$ keV depending on the source position in the color -- color diagram and the Fourier frequency, thus indicating that this spectral component is responsible for the source variability on these timescales. This result is in approximate agreement with similar behavior exhibited by the Z sources, suggesting that, as in that case, the boundary layer -- the likely source of the thermal component -- is supported by radiation pressure. Furthermore, it is found that the iron line at $\\sim$6.6 keV, clearly present in the averaged spectra, not apparent within the limitations of our measurements in the frequency-resolved spectra irrespective of the frequency range. This would indicate that this spectral component exhibits little variability on time scales comprising the interval $10^{-2}-10^2$ seconds. In the island state the single blackbody model proved inadequate, particularly notable in our lowest frequency band ($0.008-0.8$ Hz). An absorbed powerlaw or an additive blackbody plus hard powerlaw model was required to obtain a satisfactory fit. Statistics do not allow unambiguous discrimination between these possible scenarios. ", "machine_abstract": "We present the first simultaneous broadband (0.5-10 keV) spectral analysis of the neutron star low-mass X-ray binary system 4U 1728-34 using data obtained with XMM-Newton, Chandra, Suzaku, Swift-XRT and RXTE. We find that the source spectrum is well described by an absorbed blackbody plus power-law model in all observations except for one observation where we detect emission lines at 6.7 and 7.1 keV which are consistent with being produced by highly ionized iron. The temperature of the blackbody component varies between 0.6-0.9 keV while its radius ranges between 3-7 km depending on whether or not the absorption column density was allowed to vary freely during fitting. In addition, we also found evidence for a soft excess below 1 keV in some of our spectra. Using these results as input parameters, we simulated light curves based on the continuum models used in this work. Our simulations show that the observed flux variations can be explained solely due to changes in the blackbody normalization factor without requiring any additional variability mechanism such as obscuration effects.", "paraphrased_abstract": "We found that the source spectrum was well interpreted by a blackbody and a power-law model, and all the observations were well described by the absorbed blackbody and the power-law model. All the observations were interpreted by a blackbody normalization factor of 0.6-0.9, while the radiance varied by 3-7 km, depending on the absorption column density. The light curves, based on these spectra, were modeled in the continuum model. We present the first, and only, simultaneous, X-ray-simulation of the neutron star binary system 1728-34. XMM-Newton, Chandra, Suzaku, Swift-XRT, and X-ray-Rain. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4104, "title": "On Newton's law in supersymmetric braneworld models", "abstract": "  We study the propagation of gravitons within 5-D supersymmetric braneworld models with a bulk scalar field. The setup considered here consists of a 5-D bulk spacetime bounded by two 4-D branes localized at the fixed points of an $S^1/Z_2$ orbifold. There is a scalar field $\\phi$ in the bulk which, provided a superpotential $W(\\phi)$, determines the warped geometry of the 5-D spacetime. This type of scenario is common in string theory, where the bulk scalar field $\\phi$ is related to the volume of small compact extra dimensions. We show that, after the moduli are stabilized by supersymmetry breaking terms localized on the branes, the only relevant degrees of freedom in the bulk consist of a 5-D massive spectrum of gravitons. Then we analyze the gravitational interaction between massive bodies localized at the positive tension brane mediated by these bulk gravitons. It is shown that the Newtonian potential describing this interaction picks up a non-trivial contribution at short distances that depends on the shape of the superpotential $W(\\phi)$. We compute this contribution for dilatonic braneworld scenarios $W(\\phi) = e^{\\alpha \\phi}$ (where $\\alpha$ is a constant) and discuss the particular case of 5-D Heterotic M-theory: It is argued that a specific footprint at micron scales could be observable in the near future. ", "machine_abstract": "We study the gravitational field equations for static spherically symmetric configurations on Randall-Sundrum type II brane-worlds with bulk cosmological constant and matter fields localized on the branes. We show that, under certain conditions, these solutions can be interpreted as black holes embedded into an anti-de Sitter space-time. In particular we find that there is no restriction to the mass parameter M0 appearing in the solution of the vacuum Einstein equation on the brane. The corresponding horizon radius r0 satisfies the relation r0 = (3M0/4\u03c0)1/3. This result implies that the Schwarzschild-de Sitter metric describes not only black hole but also naked singularity solutions. Finally, we discuss how this picture changes when one takes into account quantum corrections due to loop effects. PACS numbers: 04.20.-q; 11.10.Kk  Supersymmetry has been proposed as a possible extension of general relativity which could provide a consistent description of gravity at all scales [1] . It was shown recently [2] , however, that it does not lead to any new predictions if applied to standard four-dimensional theories. On the other hand, higher dimensional extensions of supergravity have attracted considerable attention during recent years [3] . In this letter we consider five-dimensional supergravities [4] where the extra dimension is compactified on a circle [5] or orbifold [6] . These are known as Randall-Sundrum type I [7] and type II [8] scenarios respectively. They allow for localization of Standard Model particles [9] and their excitations [10] on the so-called visible brane while gravitons propagate freely through the bulk [11] . As a consequence they may solve some problems associated with the hierarchy between the electroweak scale and the Planck scale [12] . Moreover, such models offer interesting possibilities for constructing regular black-hole-like objects [13] - [16] .", "paraphrased_abstract": "It was shown in the course of recent years that it did not yield any new results if the standard four-dimensional theories were applied. This is particularly true for the case of a vacuum Einstein-Einstein equation, which on the corresponding sphere of the world of the brane is not limited. However, this is an interesting application for the construction of regular black holes. This is because, according to these calculations, a vacuum Einstein equation can be solved for a sphere of the same name as a black hole. As a result, the Schwarzschild-De Sitter metric can be applied to any black hole, but it can also be applied to a pure singularity. Moreover, these models have the advantage of solving certain problems associated with the asymmetric hierarchy between the electroweak and the Planck scale. We have hereafter studied the gravity equations for static, symmetric, spherical and symmetrically symmetrical worlds with cosmological constants and material fields located in the brane. We have observed that this metric can be applied to black holes in an anti-de Sitter space. We have shown that the solution of the vacuum Einstein equation of the brane is free of any restrictions and satisfies the relation r0 = (3M0/)11. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1651, "title": "Route to Lambda in conformally coupled phantom cosmology", "abstract": "  In this letter we investigate acceleration in the flat cosmological model with a conformally coupled phantom field and we show that acceleration is its generic feature. We reduce the dynamics of the model to a 3-dimensional dynamical system and analyze it on a invariant 2-dimensional submanifold. Then the concordance FRW model with the cosmological constant $\\Lambda$ is a global attractor situated on a 2-dimensional invariant space. We also study the behaviour near this attractor, which can be approximated by the dynamics of the linearized part of the system. We demonstrate that trajectories of the conformally coupled phantom scalar field with a simple quadratic potential crosses the cosmological constant barrier infinitely many times in the phase space. The universal behaviour of the scalar field and its potential is also calculated. We conclude that the phantom scalar field conformally coupled to gravity gives a natural dynamical mechanism of concentration of the equation of state coefficient around the magical value $w_{\\text{eff}}=-1$. We demonstrate route to Lambda through the infinite times crossing the $w_{\\text{eff}}=-1$ phantom divide. ", "machine_abstract": "We study the evolution of an open universe with negative spatial curvature and a scalar field that is minimally coupled to gravity but has a positive kinetic energy density, known as a phantom field.  We find that this model can be mapped onto a closed universe filled with ordinary matter and radiation by means of a coordinate transformation which preserves the form of Einstein's equations.  This mapping allows us to use results obtained for standard Friedmann-Robertson-Walker models to calculate the time dependence of the scale factor and Hubble parameter during the entire history of our universe.   In particular we show how the present value of the dark energy density parameter $\\Omega_{\\Lambda}$ may be determined from measurements of the cosmic microwave background temperature anisotropy spectrum at large angular scales (low multipoles).   The resulting values are consistent with those inferred from observations of type Ia supernovae and large-scale structure formation. We also discuss some possible observational tests of these predictions.", "paraphrased_abstract": "It is possible to calculate the present value of the dark energy density parameter, of which the parameter \"Lambda\" is derived, in particular, from the measurements of the tidal waveform temperature induced by the cosmic microwave background, in the large angle at which the equilateral magnetic field is symmetric, and the resulting values are consistent with the ones obtained from a study of type Ia supernovae and their large scale formation. Then, for the first time, we study the evolution of an open universe with a negative spatial slant and a scalar field, which is insignificantly connected to gravity but which has a positive kinetic energy, which is known as the phantom field. We present the present value of the dark energy density of the phantom field, \"Omega-Lambda\", by means of measurements of the cosmic microwave background temperature anisotropy at high multipoles. We present the results of a test of these predictions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3872, "title": "In-plane structure and ordering at liquid sodium surfaces and interfaces from ab initio molecular dynamics", "abstract": "  Atoms at liquid metal surfaces are known to form layers parallel to the surface. We analyze the two-dimensional arrangement of atoms within such layers at the surface of liquid sodium, using ab initio molecular dynamics (MD) simulations based on density functional theory. Nearest neighbor distributions at the surface indicate mostly 5-fold coordination, though there are noticeable fractions of 4-fold and 6-fold coordinated atoms. Bond angle distributions suggest a movement toward the angles corresponding to a six-fold coordinated hexagonal arrangement of the atoms as the temperature is decreased towards the solidification point. We rationalize these results with a distorted hexagonal order at the surface, showing a mixture of regions of five and six-fold coordination. The liquid surface results are compared with classical MD simulations of the liquid surface, with similar effects appearing, and with ab initio MD simulations for a model solid-liquid interface, where a pronounced shift towards hexagonal ordering is observed as the temperature is lowered. ", "machine_abstract": "We have performed ab initio molecular dynamics simulations to study the in-plane structure, order parameters, and surface tension of liquid Na(l) in contact with vacuum or solid NaCl (001). We find that the density profile is strongly dependent on the presence of an underlying substrate; it exhibits a pronounced double peak for the case without substrate but becomes single-peaked when the substrate is present. The height fluctuations are found to be larger than those observed experimentally by STM measurements. This discrepancy may arise due to the fact that our simulation cell contains only one layer of liquid sodium atoms while experiments typically involve several layers. In addition, we observe that the average nearest neighbor distance decreases as the number of layers increases. Our results show that the in-plane structure of liquid sodium can be significantly influenced by its environment. Finally, we calculate the surface tensions using two different methods and compare them against each other.", "paraphrased_abstract": "Our results demonstrate that the density of liquid sodium can be significantly influenced by its surroundings. In this work we have simulated the in-plane structure, the order and the stress of liquid Na(l), in contact with liquid nitrogen or solid NaCl. Our simulations are conducted in an inert chamber, in which we have only a single layer of sodium atoms, and the experiments usually involve several layers. The density of the liquid is strongly dependent on the presence of a substrate; for instance, it is a double peak without a substrate, but becomes a single peak when a substrate is present. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0013, "title": "Theory of capillary-induced interactions beyond the superposition approximation", "abstract": "  Within a general theoretical framework we study the effective, deformation-induced interaction between two colloidal particles trapped at a fluid interface in the regime of small deformations. In many studies, this interaction has been computed with the ansatz that the actual interface configuration for the pair is given by the linear superposition of the interface deformations around the single particles. Here we assess the validity of this approach and compute the leading term of the effective interaction for large interparticle separation beyond this so-called superposition approximation. As an application, we consider the experimentally relevant case of interface deformations owing to the electrostatic field emanating from charged colloidal particles. In mechanical isolation, i.e., if the net force acting on the total system consisting of the particles plus the interface vanishes, the superposition approximation is actually invalid. The effective capillary interaction is governed by contributions beyond this approximation and turns out to be attractive. For sufficiently small surface charges on the colloids, such that linearization is strictly valid, and at asymptotically large separations, the effective interaction does not overcome the direct electrostatic repulsion between the colloidal particles. ", "machine_abstract": "We present an analytical theory for describing capillary forces between two spherical particles in contact with each other and immersed into a liquid, which is valid even when the separation distance between them becomes comparable to their size. The theory takes into account both the effect of surface tension on the shape of menisci formed around the particles as well as the effect of gravity. We show that these effects lead to new types of attractive and repulsive capillary forces acting between the particles at small separations. In particular, we find that the gravitational force can induce a net attraction between the particles even if they are completely wetted by the liquid phase (i.e., have no dry patches). This prediction agrees very well with our numerical results obtained using Surface Evolver software package. Our theoretical predictions are also confirmed by experiments performed with polystyrene microspheres suspended in water. Capillary forces play important role in many physical phenomena such as adhesion [1] , sedimentation [2] , flotation [3] , etc.. However, despite numerous experimental studies [4] - [8] there still remains significant uncertainty about how exactly these forces depend on various parameters characterizing the system under consideration [9] . One of the main reasons behind this situation is that existing theories [10] - [12] developed within the framework of classical continuum mechanics cannot be applied directly to describe capillary interactions occurring at distances smaller than the characteristic length scale associated with the curvature of interfaces separating different phases [13] . In order to overcome this difficulty one usually resorts to some approximate approaches based either on the concept of effective Hamaker constants [14] or on the so-called \"superposition approximation\" [15] . These methods allow one to obtain simple expressions for the total interaction energy but do not provide any information about its dependence on the detailed geometry of the problem [16] . Moreover, it has been shown recently [17] that the latter approach fails...", "paraphrased_abstract": "As a consequence, we have recently been able to show that this approach does not succeed. We have thus proposed an analytical theory of capillary forces between two spherical particles in contact with each other and immersed in a liquid, and we present a theory that can be applied to the case of capillary forces at distances of a lesser size than their size. In order to overcome this difficulty, we usually resort to approximate methods, either based on the concept of the effective Hamaker constants, or on what are called the \u2018superposition approximation\u2019. These methods are based on the concept of the Hamaker constants, or on what are called \u2018superposition approximation\u2019. The latter are only known to be very effective, but they have been shown to be unable to explain the nature of the effects. We propose an analytical theory for analyzing the force of capillary forces between two spherical particles, and immersed in a liquid, that are valid even if their separation is not so great as their size. Moreover, we have recently seen that the latter method does not work. The theoretical results agree very well with our numerical results, which we show with the application of Surface Evolutionver. We find that the gravitational force induced net attraction between the two particles, even though the particles were completely wetted with liquid (i.e.", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3329, "title": "Softening of the stiffness of bottlebrush polymers by mutual interaction", "abstract": "  We study bottlebrush macromolecules in a good solvent by small-angle neutron scattering (SANS), static light scattering (SLS), and dynamic light scattering (DLS). These polymers consist of a linear backbone to which long side chains are chemically grafted. The backbone contains about 1600 monomer units (weight average) and every second monomer unit carries side-chains with ca. 60 monomer units. The SLS- and SANS data extrapolated to infinite dilution lead to the form factor of the polymer that can be described in terms of a worm-like chain with a contour length of 380 nm and a persistence length of 17.5 nm. An analysis of the DLS data confirm these model parameters. The scattering intensities taken at finite concentration can be modeled using the polymer reference interaction site model. It reveals a softening of the bottlebrush polymers caused by their mutual interaction. We demonstrate that the persistence decreases from 17.5 nm down to 5 nm upon increasing the concentration from dilute solution to the highest concentration 40.59 g/l under consideration. The observed softening of the chains is comparable to the theoretically predicted decrease of the electrostatic persistence length of linear polyelectrolyte chains at finite concentrations. ", "machine_abstract": "The effect of intermolecular interactions on the elastic properties of bottlebrush polymer networks is investigated using molecular dynamics simulations and experimental measurements. The results show that the network becomes softer with increasing density, which can be explained in terms of an increase in the number of entanglements between neighboring chains. This softening behavior is also observed experimentally for bottlebrush polyurethane networks prepared at different densities. It is found that the simulated stress-strain curves are well described by the Gent model, where the effective spring constant decreases as the density increases. These findings suggest that the mechanical properties of bottlebrush polymer gels can be tuned by changing their density.     Soft materials such as rubbers have been widely used in many applications including tires, seals, adhesives, and biomaterials because they exhibit unique physical characteristics such as high extensibility and low modulus [1\u20133]. In particular, bottlebrush polymers consisting of densely grafted side chains have attracted considerable attention due to their outstanding mechanical properties [4\u20136]. For example, it has recently been reported that bottlebrush poly(ether urethane)s (PEUs), synthesized via ring-opening metathesis polymerization [7, 8], possess higher tensile strength than conventional PEU elastomers [9]. However, despite these advantages, there remain some challenges associated with the use of bottlebrush polymers in practical applications. One of them is how to control the mechanical properties of bottle brushes since the macroscopic properties depend strongly on the microstructure [10, 11]. Therefore, understanding the relationship between structure and property is important for designing new types of bottlebrush-based materials.  In this study, we investigate the influence of chain density on the mechanical properties of bottle-brush polyurethanes (BPUs). We find that BPUs become softer when the density increases. To understand this phenomenon, we perform molecular dynamics (MD) simulations based on coarse-grained models [12]. Our simulation results reveal that the network becomes sof...", "paraphrased_abstract": "But, despite these advantages, there are still some practical difficulties in the use of a lot of these types of polymer in the use of bottles. In general, bottlebrush polymer networks are popular in many industries, such as tires, seals, adhesives, and biomaterials, because they have properties that are very unique. However, it has recently been discovered that poly(ether urethane) is a new polyurethane synthesis and is harder than PEUs, so that the density of the chains increases, which is a consequence of the increased number of chains and thus a change in the strength of the network. The effect of intermolecular interactions on the elastic properties of the polyurethanes is investigated by means of the model of Gent, where the spring constant decreases with the increase in density. This result shows that the elasticity of the polyurethanes increases with the increase of density. The result of this study shows that the polyurethanes are softer with increasing density, and the elasticity increases with increasing density. The effect of density on the elastic properties of polyurethanes is investigated by using molecular dynamics and experimental measurement. The simulations show that the network becomes softer with increasing density, which is explained by the increase of the number of entanglements between chains. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0973, "title": "X-ray Timing Observations of PSR J1930+1852 in the Crab-like SNR G54.1+0.3", "abstract": "  We present new X-ray timing and spectral observations of PSR J1930+1852, the young energetic pulsar at the center of the non-thermal supernova remnant G54.1+0.3. Using data obtained with the Rossi X-ray Timing Explorer and Chandra X-ray observatories we have derived an updated timing ephemeris of the 136 ms pulsar spanning 6 years. During this interval, however, the period evolution shows significant variability from the best fit constant spin-down rate of $\\dot P = 7.5112(6) \\times 10^{-13}$ s s$^{-1}$, suggesting strong timing noise and/or glitch activity. The X-ray emission is highly pulsed ($71\\pm5%$ modulation) and is characterized by an asymmetric, broad profile ($\\sim 70%$ duty cycle) which is nearly twice the radio width. The spectrum of the pulsed emission is well fitted with an absorbed power law of photon index $\\Gamma = 1.2\\pm0.2$; this is marginally harder than that of the unpulsed component. The total 2-10 keV flux of the pulsar is $1.7 \\times 10^{-12}$ erg cm$^{-2}$ s$^{-1}$. These results confirm PSR J1930+1852 as a typical Crab-like pulsar. ", "machine_abstract": "We report on X-ray timing observations of the pulsar candidate PSR J1930+1855 located at the center of the supernova remnant (SNR) G54.1+0.     The source was discovered by Chandra and confirmed as a pulsar with XMM-Newton, but its spin period is not stable over time scales longer than one day. We performed two sets of pointed RXTE observations to study this behavior further. In both cases we found that the pulse frequency decreases smoothly during our observation runs. This trend can be described well using an exponential decay model for which we find characteristic timescales of 1.1 days and 0.7 days respectively. These values are consistent with those reported previously based on Chandra data alone. However, when comparing these results directly it should be noted that the uncertainties associated with the previous measurements were significantly larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE.", "paraphrased_abstract": "In the last few weeks we have had the opportunity to observe a pulsar in the center of the SNR, which was found by Chandra, and confirmed by Newton. But this pulsar is not stable over a day or two. Hence we present the results of a pair of pointed RXTE observations of the pulsar PSR J1930-1855, which lies at the centre of the SNR G54.1, and was confirmed by Chandra. But the magnitude of this pulsar is much greater than that of previous observations, as a result of the higher signal-to-noise ratio of Chandra compared to that of RXTE. We discuss here a recent study of the pulsar PSR J1930-1855, located in the center of the supernova remnant G54.1. This source has been discovered by Chandra, confirmed by Newton, but the duration of its spin is not constant. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2672, "title": "Collisions between equal sized ice grain agglomerates", "abstract": "  Following the recent insight in the material structure of comets, protoplanetesimals are assumed to have low densities and to be highly porous agglomerates. It is still unclear if planetesimals can be formed from these objects by collisional growth. Therefore, it is important to study numerically the collisional outcome from low velocity impacts of equal sized porous agglomerates which are too large to be examined in a laboratory experiment. We use the Lagrangian particle method Smooth Particle Hydrodynamics to solve the equations that describe the dynamics of elastic and plastic bodies. Additionally, to account for the influence of porosity, we follow a previous developed equation of state and certain relations between the material strength and the relative density. Collisional growth seems possible for rather low collision velocities and particular material strengths. The remnants of collisions with impact parameters that are larger than 50% of the radius of the colliding objects tend to rotate. For small impact parameters, the colliding objects are effectively slowed down without a prominent compaction of the porous structure, which probably increases the possibility for growth. The protoplanetesimals, however, do not stick together for the most part of the employed material strengths. An important issue in subsequent studies has to be the influence of rotation to collisional growth. Moreover, for realistic simulations of protoplanetesimals it is crucial to know the correct material parameters in more detail. ", "machine_abstract": "We present results on collisions between two identical spherical grains with radii ranging from 0.1 to 1 mm and velocities up to 10 m/s, which are relevant for the growth of planetesimals in protoplanetary disks. We find that the collision outcome depends strongly on the impact velocity. At low velocities (<5 m/s), the colliding particles stick together after the first contact without any fragmentation or erosion. For higher velocities we observe bouncing, sticking, partial fragmentation, and complete fragmentation depending on the impact angle. The transition between different outcomes is smooth but can be described by simple scaling laws. In particular, we show that the critical velocity at which the transition from sticking to bouncing occurs scales as vcrit \u221d R0.6 where R is the radius of one particle. This dependence agrees well with recent experiments performed under microgravity conditions. Finally, we discuss how these findings affect our understanding of dust coagulation in protoplanetary discs.", "paraphrased_abstract": "The transition from sticking to bouncing is smooth, but can be explained by simple scaling laws. Here we discuss dust coagulation in protoplanetary discs. We show that collisions with a few identical grains of spherical iron varying in radius from 0.1 to 1 mm have a range of velocities of between 0.1 to 10 m/s, which are useful for the growth of planetesimals in protoplanetary disks. At low velocities (5 m/s) the particles stick together after the first contact, without breaking or eroding, and at high velocities, bouncing, bouncing, fragmentation, and complete fragmentation, depending on the angle of the impact. We show that the transition from sticking to bouncing is smooth, and it can be summed up in terms of simple scaling laws. We show that the critical velocity at which the transition from bouncing to bouncing scales in the ratio of vcrit  R0.6, where R is the radius of one particle. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2162, "title": "The competition of hydrogen-like and isotropic interactions on polymer collapse", "abstract": "  We investigate a lattice model of polymers where the nearest-neighbour monomer-monomer interaction strengths differ according to whether the local configurations have so-called ``hydrogen-like'' formations or not. If the interaction strengths are all the same then the classical $\\theta$-point collapse transition occurs on lowering the temperature, and the polymer enters the isotropic liquid-drop phase known as the collapsed globule. On the other hand, strongly favouring the hydrogen-like interactions give rise to an anisotropic folded (solid-like) phase on lowering the temperature. We use Monte Carlo simulations up to a length of 256 to map out the phase diagram in the plane of parameters and determine the order of the associated phase transitions. We discuss the connections to semi-flexible polymers and other polymer models. Importantly, we demonstrate that for a range of energy parameters two phase transitions occur on lowering the temperature, the second being a transition from the globule state to the crystal state. We argue from our data that this globule-to-crystal transition is continuous in two dimensions in accord with field-theory arguments concerning Hamiltonian walks, but is first order in three dimensions. ", "machine_abstract": "We study the effect of competing short-range attractive and repulsive interactions in a model for collapsed polymers, which are described by an effective one-dimensional potential with two minima separated by a barrier. We show that this system exhibits a rich phase diagram as function of temperature T , interaction strength U0 and asymmetry parameter . The main results are: (i) For small values of we find three different phases: a low-temperature disordered phase, a high-temperature ordered phase and a critical point separating them. (ii) In the limit of large barriers between the wells, i.e., when becomes very large or T decreases to zero, the transition line approaches the value Uc = 2U0/3 predicted by mean-field theory.     Introduction     Collapsed polymers have been studied extensively over many years [1]-[6]. They can be found in biological systems such as proteins [7][8][9], but also occur in synthetic materials like micelles [10]-[12]. A common feature of these systems is their ability to form compact structures due to strong shortrange attractions combined with longer-ranged repulsions. This leads to a double-well type of potential energy landscape [13][14][15], where particles tend to aggregate into clusters [16]. These aggregates may undergo structural changes [17], resulting in transitions between different states [18]. Such phenomena are often observed experimentally [19][20][21] [22]. However, despite extensive research efforts there still remain open questions about the nature of the underlying mechanisms leading to these complex behaviors [23].", "paraphrased_abstract": "In general, collapsed polymers have been studied for many years, especially in biological systems, such as proteins, or in synthetic materials such as micelles. Such systems are often found in the cellular organisms such as proteins and micelles. They are formed by a combination of strong attraction and strong resistance, and by the asymmetry of their two minima, between which they have a wide symmetry. This symmetry is very large and complex in the sense of temperature, of the interaction strength, and of the symmetry of the well. In this system a broad symmetry of potentials is realized, and, as a result of this, the phase diagram is invariably arranged. This is because the temperature T is high, and the temperature is low, and the transition is between them. The result is: the symmetry of the phase diagram of the corresponding equilibrium at a given temperature. It is shown that the system shows a rich phase diagram as a function of temperature, of the interaction strength, of the asymmetry, and of the asymmetry. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3493, "title": "Simulation of ASTROD I test mass charging due to solar energetic particles", "abstract": "  As ASTROD I travels through space, its test mass will accrue charge due to galactic cosmic-rays and solar energetic particles incident on the spacecraft. This test mass charge will result in Coulomb forces between the test mass and the surrounding electrodes. In earlier work using the GEANT4 toolkit, we predicted a net charging rate of nearly 9.0 +e/s from cosmic-ray protons between 0.1 and 1000 GeV at solar maximum, and rising to 26.5 +e/s at solar minimum. Here we use GEANT4 to simulate the charging process due to solar energetic particle events and to estimate the magnitude of acceleration noise due to this charging. The predicted charging rates range from 2840 to 64300 +e/s, at peak intensity, for the 4 largest SEP events in September and October 1989. For the 2 larger events, the acceleration disturbances due to charging exceeds the ASTROD I acceleration noise budget requirement. Continuous discharge should be considered for suppressing this charging noise. The acceleration noise during the 2 small events is well below the design target, although during these events, the net charging rate will be dominated by these solar fluxes. ", "machine_abstract": "The Astrodynamics Space Test of Relativity using Optical Devices (ASTROD) mission is proposed as an experiment for testing the gravitational theory in space, and it will be launched into orbit around L2 point by 2020.  The ASTROD satellite system consists of two satellites with four free-floating test masses each. In this study we simulated the charging process of these test masses caused by solar energetic particles during their lifetime in space. We used the SPENVIS software package developed at the European Space Research and Technology Centre (ESTEC). Our results show that the maximum charge accumulated on one of the test masses can reach up to 1.5 nC after 10 years' exposure time. This value is much higher than the expected measurement uncertainty of 0.1 nC. Therefore, it should be taken into account when designing the electrostatic control systems of the ASTROD satellites. Keywords: Charging, Solar Energetic Particles, ASTROD", "paraphrased_abstract": "The ASTROD satellites consist of two satellites, each of which contains four free-floating, non-exploratory masses. We have used the SPENVIS physics program developed at the European Space Research and Education Centre, and it is used to simulate the charge of the charged masses of the ASTROD satellites. This system consists of two satellites with four free-floating, non-exploratory masses. The ASTROD mission is proposed as a test of the gravity theory in space, and will be launched by 2020 around L2 in orbit. The SPENVIS physics program is used to simulate the charge of the charged masses of the ASTROD satellites. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3885, "title": "Catastrophic eruption of magnetic flux rope in the corona and solar wind with and without magnetic reconnection", "abstract": "  It is generally believed that the magnetic free energy accumulated in the corona serves as a main energy source for solar explosions such as coronal mass ejections (CMEs). In the framework of the flux rope catastrophe model for CMEs, the energy may be abruptly released either by an ideal magnetohydrodynamic (MHD) catastrophe, which belongs to a global magnetic topological instability of the system, or by a fast magnetic reconnection across preexisting or rapidly-developing electric current sheets. Both ways of magnetic energy release are thought to be important to CME dynamics. To disentangle their contributions, we construct a flux rope catastrophe model in the corona and solar wind and compare different cases in which we either prohibit or allow magnetic reconnection to take place across rapidly-growing current sheets during the eruption. It is demonstrated that CMEs, even fast ones, can be produced taking the ideal MHD catastrophe as the only process of magnetic energy release. Nevertheless, the eruptive speed can be significantly enhanced after magnetic reconnection sets in. In addition, a smooth transition from slow to fast eruptions is observed when increasing the strength of the background magnetic field, simply because in a stronger field there is more free magnetic energy at the catastrophic point available to be released during an eruption. This suggests that fast and slow CMEs may have an identical driving mechanism. ", "machine_abstract": "We investigate the evolution of an eruptive coronal mass ejection (CME) by using three-dimensional magnetohydrodynamic simulations, which include both ideal MHD effects and resistivity. The initial condition is constructed as follows. First we solve the Grad-Shafranov equation for force-free equilibrium to obtain the distribution of poloidal current density on the surface of a torus-like structure. Then we construct a 3D model of the torus-like structure by assuming that it has uniform temperature T = 1 MK and pressure P = 0.1 dyn cm-2 inside its volume. We also assume that there are two types of plasma flows along the field lines; one is slow solar wind flowing out from the bottom boundary at r = 2 R_sun , where Rsun denotes the radius of the Sun, and another is fast solar wind flowing into the top boundary at r = 20 Rsun . Finally, we add random fluctuations to the velocity fields so that they have kinetic energy comparable to their thermal energies.     In our simulation runs, we find three different phases during the CME evolution. During phase I, the magnetic flux rope rises slowly due to the Lorentz force associated with the toroidal component of the magnetic field. At t ~ 50 min., when the height reaches about 5 RSUN , the rising motion suddenly accelerates because of the rapid expansion of the flux rope's cross section caused by magnetic reconnection between open field lines and closed loops around the flux rope. This acceleration continues until the flux rope reaches the upper boundary at t ~ 80 min. .     During phase II, the flux rope expands rapidly outward while maintaining its shape. After reaching the maximum size at t ~ 90 min. , however, the flux rope starts contracting inward again. As a result, the flux rope becomes thinner than before but still maintains its twisted structure. Meanwhile, the surrounding open field lines become more bent toward each other near the center of the flux rope. These results suggest that the flux rope undergoes a kink instability after expanding outward beyond its critical size.", "paraphrased_abstract": "The polar current on the surface of the polar current is uniformly temperature T = 1 MK and pressure P = 0.1 dyn cm-2. Next, we add a random increase in the velocity of the polar currents so that the kinetic energy is equal to that of the thermal power. This is how the polar currents are shaped and analyzed. In the first stage, we solve the Grad-Shafranov equation for the perfect equilibrium of the field. Then we assume that the surface of the polar current is uniformly temperature T = 1 MK and P = 0.1 dyn cm-2. The second stage, the polar current is a rapid increase in force caused by the Lorentz force of the magnetic field. At the moment of t 50, the polar currents increase and the polar currents are gradually twisted and twisted, and, in the end, the polar currents become more and more twisted. At t 90, the polar current shrinks back to the polar, and the polar currents revert to each other, but the polar channels and the polar channels are all bent towards one another. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1256, "title": "A Novel String Derived Z' With Stable Proton, Light-Neutrinos and R-parity violation", "abstract": "  The Standard Model indicates the realization of grand unified structures in nature, and can only be viewed as an effective theory below a higher energy cutoff. While the renormalizable Standard Model forbids proton decay mediating operators due to accidental global symmetries, many extensions of the Standard Model introduce such dimension four, five and six operators. Furthermore, quantum gravity effects are expected to induce proton instability, indicating that the higher energy cutoff scale must be above 10^{16}GeV. Quasi-realistic heterotic string models provide the arena to explore how perturbative quantum gravity affects the particle physics phenomenology. An appealing explanation for the proton longevity is provided by the existence of an Abelian gauge symmetry that suppresses the proton decay mediating operators. Additionally, such a low-scale U(1) symmetry should: allow the suppression of the left-handed neutrino masses by a seesaw mechanism; allow fermion Yukawa couplings to the electroweak Higgs doublets; be anomaly free; be family universal. These requirements render the existence of such U(1) symmetries in quasi-realistic heterotic string models highly non-trivial. We demonstrate the existence of a U(1) symmetry that satisfies all of the above requirements in a class of left-right symmetric heterotic string models in the free fermionic formulation. The existence of the extra Z' in the energy range accessible to future experiments is motivated by the requirement of adequate suppression of proton decay mediation. We further show that while the extra U(1) forbids dimension four baryon number violating operators it allows dimension four lepton number violating operators and R-parity violation. ", "machine_abstract": "We propose a new string derived model with stable proton in which the lightest supersymmetric particle (LSP) is not neutralino but gravitino. The LSP decays into photon or neutrino-antineutrino pair through gravitational interaction. In this scenario we can explain the observed dark matter abundance without conflicting with other experimental results such as relic density measurement by WMAP experiment. We also show that our model predicts interesting signatures at LHC experiments. Introduction:-The discovery of Higgs boson [1-3] has opened up an exciting possibility to explore physics beyond Standard Model(SM). Supersymmetry(SUSY), one of the most promising extensions of SM [4] , provides natural solution for hierarchy problem [5] . However, SUSY models are severely constrained by various experimental observations [6] . In order to solve these problems, several authors have proposed different mechanisms [7-9]. One of them is introducing additional gauge symmetries [10] . Another way is adding extra dimensions [11] . Recently, it was shown that there exists a class of string derived models where the lightest superpartner is gravitino [12] . Gravitino is weakly interacting massive particle so its decay rate is suppressed compared to neutralino case [13] . This feature makes gravitino a good candidate for cold dark matter [14] . Moreover, if gravitino mass m 3/2 < 1 GeV then its lifetime becomes longer than age of universe [15] . Therefore, gravitino may be regarded as a viable candidate for dark matter [16] . On the other hand, gravitino is unstable because it couples to gravity [17] . It decays into photon or lepton-neutrino pairs [18] . If gravitino is heavier than 100 MeV then its decay products will contribute to diffuse gamma ray background [19] . Thus, gravitino should satisfy following conditions [20] :", "paraphrased_abstract": "We have now found out about a new supersymmetry (Susy) which, after the discovery of Higgs boson, was one of the most promising extensions of SM. This supersymmetry, though it is a weakly interacting particle, produces a relatively lower decay rate than neutral particles, so that it can also be used as a candidate for cold dark matter. The string theory, however, has recently shown that it has a light-weight superpartner, gravitino. This superpartner is a heavy, unstable particle, and it decays in photon or lepton-neutrino pairs. When gravitino is heavier than 100 MeV, its decay product will give rise to a diffuse gamma-ray background. So gravitino has the following conditions: the mass of the particle is not limited to the mass of the universe, but rather to the mass of the universe. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2783, "title": "Revisiting the slow dynamics of a silica melt using Monte Carlo simulations", "abstract": "  We implement a standard Monte Carlo algorithm to study the slow, equilibrium dynamics of a silica melt in a wide temperature regime, from 6100 K down to 2750 K. We find that the average dynamical behaviour of the system is in quantitative agreement with results obtained from molecular dynamics simulations, at least in the long-time regime corresponding to the alpha-relaxation. By contrast, the strong thermal vibrations related to the Boson peak present at short times in molecular dynamics are efficiently suppressed by the Monte Carlo algorithm. This allows us to reconsider silica dynamics in the context of mode-coupling theory, because several shortcomings of the theory were previously attributed to thermal vibrations. A mode-coupling theory analysis of our data is qualitatively correct, but quantitative tests of the theory fail, raising doubts about the very existence of an avoided singularity in this system. We discuss the emergence of dynamic heterogeneity and report detailed measurements of a decoupling between translational diffusion and structural relaxation, and of a growing four-point dynamic susceptibility. Dynamic heterogeneity appears to be less pronounced than in more fragile glass-forming models, but not of a qualitatively different nature. ", "machine_abstract": "We revisit the slow relaxation in a model glass-forming system, namely a binary mixture of Lennard-Jones particles interacting via a Weeks-Chandler-Andersen potential with an additional repulsive shoulder at short distances. The main goal is to investigate whether this simple model can reproduce some of the features observed experimentally for silicate melts and glasses such as the stretched exponential decay of the intermediate scattering function or the presence of two distinct time scales in the structural relaxation process. To that end we perform molecular dynamics (MD) and Monte Carlo (MC) computer simulations on systems containing up to N = 1000 particles. In particular, by means of MC simulations we are able to study much larger systems than those accessible through MD runs. Our results show that both methods give rise to similar relaxation patterns but there exist subtle differences between them which may be relevant when comparing theory and experiment. Finally, we discuss how our findings compare with previous theoretical predictions based on mode-coupling theories.", "paraphrased_abstract": "In this paper we discuss the relaxation of a glass-forming system which is a binary mixture of Lennard-Jones particles, whose amorphous phases meet with a short-range ruffle, a repulsion that is particularly disabling at a short distance. The main goal of our research is to evaluate the properties of the simple model of silicate melts and glasses, namely the extended exponential decay of the intermediate scattering function, or the presence of two distinct time periods in the relaxation process. The results of our experiments show that they are in agreement with the theory and the experiment, but in some respects there are some subtle differences between them. Nevertheless, we consider the comparison with the predictions of theories based on mode-coupling. In particular, we investigate the slow relaxation in a model system, consisting of a bi-layer of Lennard-Jones particles interacting in a week-Chandler-Andersen potential, and the presence of an extra venomous shoulder at a short distance. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0672, "title": "Hamiltonian formalism in Friedmann cosmology and its quantization", "abstract": "  We propose a Hamiltonian formalism for a generalized Friedmann-Roberson-Walker cosmology model in the presence of both a variable equation of state (EOS) parameter $w(a)$ and a variable cosmological constant $\\Lambda(a)$, where $a$ is the scale factor. This Hamiltonian system containing 1 degree of freedom and without constraint, gives Friedmann equations as the equation of motion, which describes a mechanical system with a variable mass object moving in a potential field. After an appropriate transformation of the scale factor, this system can be further simplified to an object with constant mass moving in an effective potential field. In this framework, the $\\Lambda$ cold dark matter model as the current standard model of cosmology corresponds to a harmonic oscillator. We further generalize this formalism to take into account the bulk viscosity and other cases. The Hamiltonian can be quantized straightforwardly, but this is different from the approach of the Wheeler-DeWitt equation in quantum cosmology. ", "machine_abstract": "We present the Hamiltonian formulation for general relativity with matter fields on an arbitrary spacetime manifold, including both classical and quantum aspects. The basic idea is to use the ADM decomposition of the metric into space and time components as well as lapse and shift functions. We then introduce canonical momenta conjugate to these variables which are used to construct the primary constraints of the theory. These constraints generate gauge transformations under which all physical quantities must be invariant. In order to obtain the correct number of degrees of freedom we have to impose secondary constraints that eliminate unphysical modes. Finally, we perform the canonical quantization by promoting the phase-space variables to operators acting on wave-functions defined over superspace (the space of all possible metrics). This leads us to the Wheeler-DeWitt equation whose solutions can be interpreted as probability amplitudes between different states of the universe. We also discuss how this approach could be applied to inflationary models.", "paraphrased_abstract": "This is followed by the Wheeler-DeWitt equation whose solution can be interpreted as the probability of the difference between different states of the universe. In this respect we introduce canonical quantization by linking the phase space variables to the operators of wave functions defined over superspace (the space in which all possible metrics can be defined). Hence, in the encoding of the metric, we introduce a certain amount of the ADM in which we decompose the metric into a time dimension, time-period, and lapse-and-shift functions. We introduce the canonical moment in this way, and we use this to set up the main constraints of the theory, in which all physical quantities must be invariant. This requires a secondary constraint which eliminates the nonphysical modes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1401, "title": "Controlling for individual heterogeneity in longitudinal models, with applications to student achievement", "abstract": "  Longitudinal data tracking repeated measurements on individuals are highly valued for research because they offer controls for unmeasured individual heterogeneity that might otherwise bias results. Random effects or mixed models approaches, which treat individual heterogeneity as part of the model error term and use generalized least squares to estimate model parameters, are often criticized because correlation between unobserved individual effects and other model variables can lead to biased and inconsistent parameter estimates. Starting with an examination of the relationship between random effects and fixed effects estimators in the standard unobserved effects model, this article demonstrates through analysis and simulation that the mixed model approach has a ``bias compression'' property under a general model for individual heterogeneity that can mitigate bias due to uncontrolled differences among individuals. The general model is motivated by the complexities of longitudinal student achievement measures, but the results have broad applicability to longitudinal modeling. ", "machine_abstract": "We propose an extension of the linear mixed model that allows researchers to control for unobserved individual heterogeneity when estimating treatment effects using panel data. We show how this approach can be used to estimate the effect of school choice on test scores and college enrollment decisions while controlling for selection bias due to unobserved differences between students who choose different schools. Our results suggest that school choice has no significant impact on either test scores or college enrollment rates once we account for unobserved individual characteristics.     Keywords: Panel Data, Mixed Models, Treatment Effects, School Choice         1 Introduction     In recent years there have been numerous studies examining the relationship between school choice policies and student outcomes (see Hanushek et al., 2007 , for a review). Most of these studies use cross-sectional data to compare average performance across groups of students attending public versus private schools. However, it is well known that such comparisons are subject to endogeneity problems because they do not take into account the fact that students self-select into different types of schools based on their observed and unobserved characteristics (Hoxby & Weingarth, 2005) . This problem may lead to biased estimates if the factors driving students' choices also affect their academic performance.     2 The Model  In order to address this issue, several authors have proposed alternative estimation strategies aimed at correcting for sample selectivity by explicitly modeling the process through which individuals make educational choices (e.g., Heckman, 1979; Lee, 1997; Wooldridge, 2002 ) . These approaches typically rely on parametric assumptions about the distribution of the error terms associated with both the outcome equation and the selection equation. For example, one popular method consists of jointly estimating two equations -one describing the probability that each individual chooses a particular type of school,...", "paraphrased_abstract": "A good deal of literature has been written about the relation between the outcomes of schools and the school choice (Hanshek et al., 2007). However, many studies have tried to examine the effect of school choice policies on student performance. A lot of these studies have been based on a cross-sectional comparison of the average test scores of the two schools. It has been shown that such a comparison is subject to a bias due to the fact that students are recruited into different schools based on the observable and unobservable characteristics of the school. Our results show that school choice does not significantly affect the test scores or the college enrollment of students. Our results show that school choice has no significant effect on the test scores or the college enrollment of students. We consider an extension of the linear mixed model that allows the calculation of the effect of school choice on test scores and college admissions, and that the unobserved differences between students are not noticeable, but the results are quite similar. Several authors have proposed new approaches that correct this problem by explicitly relating to the process by which individuals select their schools (see: Heckman, 1979, Lee, 1997, Wooldridge, 2002). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1805, "title": "Asteroid Deflection: How, where and when?", "abstract": "  To deflect impact-trajectory of massive km^3 and spinning asteroid by a few terrestrial radius one need a large momentum exchange. The dragging of huge spinning bodies in space by external engine seems difficult or impossible. Our solution is based on the landing of multi screw-rockets, powered by mini-nuclear engines, on the body, that dig a small fraction of the soil surface, to use as an exhaust propeller, ejecting it vertically in phase among themselves. Such a mass ejection increases the momentum exchange, their number redundancy guarantees the stability of the system. The slow landing (below 40 cm s^-1) of each engine-unity at those lowest gravity field, may be achieved by save rolling and bouncing along the surface. The engine array tuned activity, overcomes the asteroid angular velocity. Coherent turning of the jet heads increases the deflection efficiency. A procession along its surface may compensate at best the asteroid spin. A small skin-mass (about 2 10^4 tons) may be ejected by mini nuclear engines. Such prototypes may also build first save galleries for humans on the Moon. Conclusive deflecting tests might be performed on remote asteroids. The incoming asteroid 99942 Apophis (just 2% of km^3) may be deflected safely a few Earth radius. How to tag its trajectory is described. Its encounter maybe not just a hazard but an opportunity, learning how to land, dig, build and also to nest save human station inside. Asteroids amplified deflections by gravity swing maybe driven into longest planetary journeys. Mars journey may benefict by Phobos natural asteroid parking and fueling role. ", "machine_abstract": "The possibility that an asteroid could hit the Earth is not new.  In fact, it has been discussed for many years in popular science fiction books such as H G Wells' The War of the Worlds (1898) or Arthur C Clarke's 2001 Space Odyssey (1968). However, there are now several groups around the world working on ways to deflect asteroids before they impact with our planet.   This talk will discuss how this might be achieved by using nuclear explosions, kinetic impacts, laser beams or even tractor beams. It will also look at what would happen if one were successful - would we survive?  Would life return to normal? This talk was given at the University of Southampton on 24th February 2015. For more information about Prof David Jewitt please visit his website: http://www.astro.su.se/~david/ .  Prof David Jewitt is Professor of Planetary Science at the University of Southampton. He leads the Asteroid Terrestrial-impact Last Alert System (ATLAS), which monitors near-Earth objects including asteroids and comets.", "paraphrased_abstract": "The author is a professor of Planetary Sciences and an author of the Planetary Atlas. It is the world\u2019s only book on asteroid impact, The War of the Worlds, The Space Odyssey, and the 2001 Space Odyssey. Professor Jewitt is the director of the Asteroid Terrestrial Last Alert System (ATLAS), which monitors nearby objects, including asteroids and comets. The asteroid that may hit the earth is not new, it has been discussed for a long time in popular science fiction books such as H. G. Wells\u2019 War of the Worlds, or Arthur Clarke\u2019s 2001 Space Odyssey. This talk was given at the University of Southampton on 24 February 2015. For more information about Prof Jewitt, please see: http://www.astro.su.se. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1466, "title": "The absolute Galois group acts faithfully on the connected components of the moduli space of surfaces of general type", "abstract": "  We show that the Galois group $Gal(\\bar{\\Q} /\\Q)$ operates faithfully on the set of connected components of the moduli spaces of surfaces of general type, and also that for each element $\\sigma \\in Gal(\\bar{\\Q} /\\Q)$ different from the identity and from complex conjugation, there is a surface of general type such that $X$ and the Galois conjugate variety $X^{\\sigma}$ have nonisomorphic fundamental groups. The result was announced by the second author at the Alghero Conference 'Topology of algebraic varieties' in september 2006. Before the present paper was actually written, we received a very interesting preprint by Robert Easton and Ravi Vakil (\\cite{e-v}), where it is proven, with a completely different type of examples, that the Galois group $Gal(\\bar{\\Q} /\\Q)$ operates faithfully on the set of irreducible components of the moduli spaces of surfaces of general type. We also give other simpler examples of surfaces with nonisomorphic fundamental groups which are Galois conjugate, hence have isomorphic algebraic fundamental groups. ", "machine_abstract": "We prove that for any surface S of general type, the action of its absolute Galois group G(S) on the set of connected components of the modulis space M_g(S) is faithful.  This result has been conjectured by Grothendieck and proved in many cases (e.g., when g = 0 or 1).  We use this to show that if S admits an automorphism of order p > 2 then it also admits one of order q prime to p; we give examples where both orders are arbitrarily large. The proof relies on results about the existence of certain families of curves on S which have been obtained recently using techniques from algebraic geometry and number theory. In particular, we make essential use of the fact that the canonical map of such a curve C onto P^1 is birational; this implies that the image of C under the Albanese map Alb_S : S -> Alb_S(S) is not contained in a fiber of Alb_S.", "paraphrased_abstract": "We rely on the result of the existence of certain families of curves on S, obtained recently by means of algebraic geometry and the theory of number. In particular, we make the most of the fact that the canonical map of a curve of the form C on P = 1 is birational. In other words, the image of C under the Albanese map Alb-S - Alb-S is not in the strand of Alb-S. This result was conjectured by Grothendieck and proven in many cases (e.g., g = 0). This result has been interpreted by Grothendieck and proved many times (e.g., when g = 0 or 1). We prove that on any surface S the action of the absolute Galois group G (S) is faithful on the corresponding parts of the moduli space M-g. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2169, "title": "Collective states of the odd-mass nuclei within the framework of the Interacting Vector Boson Model", "abstract": "  A supersymmetric extension of the dynamical symmetry group $Sp^{B}(12,R)$ of the Interacting Vector Boson Model (IVBM), to the orthosymplectic group $OSp(2\\Omega/12,R)$ is developed in order to incorporate fermion degrees of freedom into the nuclear dynamics and to encompass the treatment of odd mass nuclei. The bosonic sector of the supergroup is used to describe the complex collective spectra of the neighboring even-even nuclei and is considered as a core structure of the odd nucleus. The fermionic sector is represented by the fermion spin group $SO^{F}(2\\Omega)\\supset SU^{F}(2)$.   The so obtained, new exactly solvable limiting case is applied for the description of the nuclear collective spectra of odd mass nuclei. The theoretical predictions for different collective bands in three odd mass nuclei, namely $^{157}Gd$, $^{173}Yb$ and $^{163}Dy$ from rare earth region are compared with the experiment. The $B(E2)$ transition probabilities for the $^{157}Gd$ and $^{163}Dy$ between the states of the ground band are also studied. The important role of the symplectic structure of the model for the proper reproduction of the $B(E2)$ behavior is revealed. The obtained results reveal the applicability of the models extension. ", "machine_abstract": "The collective properties of odd-mass nuclei are investigated in terms of the interacting vector boson model (IVBM). The IVBM is based on an effective Lagrangian density that describes the coupling between nucleons and mesons, including the rho-meson field as well as the omega-meson fields with their respective neutral currents. In this work we have used the extended version of the IVBM which includes also the delta-resonance degrees of freedom. We have calculated the energy levels for some selected even-even nuclei along with those corresponding to the first excited state of neighboring odd-A nuclei using the same set of parameters. It has been found that the inclusion of the delta resonance leads to better agreement with experimental data than without it. This fact indicates that the role played by the delta resonance should not be neglected when studying nuclear structure phenomena such as pairing correlations or shape coexistence. Finally, we have studied the effect of the spin-orbit interaction on the ground-state band built upon the lowest 0+ state.", "paraphrased_abstract": "As a result of these experiments, we have measured the energy of selected even-even nuclei, as well as the energy of neighboring odd-A nuclei, with the same parameters. In addition, we have analyzed the effect of spin-orbit interactions on the ground-state band formed at the lowest 0+ level. The study of the interacting vector-brain model (IVBM) is based on an effective Lagrangian density, which includes a strong relation between nucleons and mesons, with the rho-meson field and the omega-meson field, with their neutral currents. We have used the extended version of IVBM, and we have added delta resonances. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2743, "title": "Simulation of Heme using DFT+U: a step toward accurate spin-state energetics", "abstract": "  We investigate the DFT+U approach as a viable solution to describe the low-lying states of ligated and unligated iron heme complexes. Besides their central role in organometallic chemistry, these compounds represent a paradigmatic case where LDA, GGA, and common hybrid functionals fail to reproduce the experimental magnetic splittings. In particular, the imidazole pentacoordinated heme is incorrectly described as a triplet by all usual DFT flavors. In this study we show that a U parameter close to 4 eV leads to spin transitions and molecular geometries in quantitative agreement with experiments, and that DFT+U represents an appealing tool in the description of iron porphyrin complexes, at a much reduced cost compared to correlated quantum-chemistry methods. The possibility of obtaining the U parameter from first-principles is explored through a self-consistent linear-response formulation. We find that this approach, which proved to be successful in other iron systems, produces in this case some overestimation with respect to the optimal values of U. ", "machine_abstract": "The electronic structure and magnetic properties of heme are investigated by density functional theory (DFT) with the Hubbard U correction for strong electron correlation effects, which is essential to reproduce the correct ground state spin multiplicity in this system. The calculated results show that the ferromagnetic coupling between iron ions leads to an antiferromagnetically ordered ground state when the axial ligands are included. In addition, we find that the inclusion of the solvent effect on the geometry optimization does not change the energy difference significantly but it can affect the relative stability among different spin states. Finally, our calculations suggest that the high-spin state may be more stable than previously thought based on experimental measurements. This work was supported by the National Natural Science Foundation of China under Grant No. 11404160. We thank Prof. Yi-Kai Wu at Peking University for helpful discussions.     Introduction     Heme is one of the most important cofactors found in many proteins involved in biological processes such as oxygen transport, storage, sensing, and catalysis1-5. It consists of a porphyrin ring coordinated to Fe(II), where four nitrogen atoms form two pyrrole rings connected through methine bridges6-8. Due to its importance in biology, there have been numerous theoretical studies investigating the structural9-12, optical13-15, vibrational16-19, and magnetic20-24 properties of heme. However, despite these efforts, some fundamental questions remain unanswered regarding the electronic structure and magnetic behavior of heme25-27. For example, although the ground state spin multiplicity has been determined experimentally28-30, the exact nature of the excited spin states remains unclear31-33. Moreover, the origin of the observed ferromagnetic coupling between iron centers34-36 still needs further investigation37-39.", "paraphrased_abstract": "As a result of its importance in a biosystem, hematology, and physics, have been studied by many theoretical and theoretical studies. In the study of the electronic and magnetic properties of heme, it was found to be a complex, resolving structure, which was not yet substantiated in the experimental study of the magnetosphere, which consisted of a porphyrin ring anchored to Fe (II), which was formed by four nitrogen atoms and two pyrrole rings connected by two methine bridges. Heme was discovered by density functional theory, using Hubbard U as the correction of the axial polarities, which was necessary to produce the correct spin multiplicity of heme in the system. Then, using a solvent-effect, we found that the high spin state was more stable than the experimentally determined state, and that the higher spin state could be more stable than the previous theory predicted. We thank Dr. Yi-Kai Wu for his assistance. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0921, "title": "Z^0 \\to 2\\gamma and the Twisted Coproduct of the Poincar\\'{e} Group", "abstract": "  Yang's theorem forbids the process $Z^0 \\to 2\\gamma$ in any Poincar\\'{e} invariant theory if photons are bosons and their two-particle states transform under the Poincar\\'{e} group in the standard way (under the standard coproduct of the Poincar\\'{e} group). This is an important result as it does not depend on the assumptions of quantum field theory. Recent work on noncommutative geometry requires deforming the above coproduct by the Drinfel'd twist. We prove that $Z^0 \\to 2\\gamma$ is forbidden for the twisted coproduct as well. This result is also independent of the assumptions of quantum field theory. As an illustration of the use of our general formulae, we further show that $Z^0 \\to \\nu + \\nu$ is forbidden for the standard or twisted coproduct of the Poincar\\'{e} group if the neutrino is massless, even if lepton number is not conserved. This is a special case of our general result that a massive particle of spin $j$ cannot decay into two identical massless particles of the same helicity if $j$ is odd, regardless of the coproduct used. ", "machine_abstract": "We study the decay Z_0\\to 2 \\gamma in terms of the twisted coproduct on the Poincare group, which is related to the Wess-Zumino-Witten model.  We show that this process can be described by an effective Lagrangian with two coupling constants, one for each helicity state of the photon pair. The corresponding Feynman rules are derived using the method developed in Ref. [1] . In particular we find that there exists no contribution from the axial vector current at tree level. This result agrees with the prediction based on chiral symmetry arguments [2] , but disagrees with previous calculations [3, 4] . The results presented here were obtained during my stay as a postdoctoral fellow at Harvard University supported by the Alexander von Humboldt Foundation. I would like to thank Prof. M. J. Duffy for his hospitality. Introduction -In recent years much attention has been paid to the investigation of processes involving photons in connection with their possible role in physics beyond the Standard Model (SM). One such process is the decay Z_0\\to2 \\gamma, where the neutral gauge boson Z_0 decays into a photon-antiphoton pair. It was first studied within the SM framework [3] , and later also considered in various extensions [4] . Recently it attracted renewed interest due to its potential relevance for Higgs searches [5] . However, since the experimental signature of this process is very similar to that of the background [6] , it will probably not be observable before LHC starts operating [7, 8] . Nevertheless, it still provides valuable information about the underlying theory [9] . For example, if the Higgs particle turns out to have spin zero or one [10] , then the decay Z_0\\to2\\gamma could provide important constraints on the couplings between the Higgs field and other particles [11] . Furthermore, the measurement of the branching ratio BR(Z_0\\to2\\gamma) allows us to determine the fine structure constant [12] . Finally, the decay Z_0\\to\\bar{\\nu} e e + e - followed by the subsequent decay of the electron-positron pair into a photon-antiphoton pair", "paraphrased_abstract": "\" The results of this experiment were obtained during my stay at Harvard University, under the auspices of the Alexander von Humboldt Foundation. The synthesis of the two electrons and the two protons, therefore, can be calculated by an effective Lagrangian, with two coupling constants, one for each helicity. This is also consistent with the analysis of the chiral symmetry, and thereby the aversion of the angular structure. Thereupon, in the synthesis of the chiral symmetry, we are able to find an effective Lagrangian, with two coupling constants, one for each helicity of the photon pair. In this respect, it is true that the decay Z0to 2gamma is the decay of the neutral gauge B, which is a scalar, and a scalar decay of the atom B, which is similar to the decay of the background, but which, however, has not yet been observable in the case of the scalar state, it is possible to determine the balance of the atoms with respect to the atoms in the background. In this respect, we can predict chiral symmetry based on chiral symmetry, but we reject this result. The arithmetic analysis of the decay of the atom B is", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2032, "title": "Effects of the galactic winds on the stellar metallicity distribution of dwarf spheroidal galaxies", "abstract": "  To study the effects of galactic winds on the stellar metallicity distributions and on the evolution of Draco and Ursa Minor dwarf spheroidal galaxies, we compared the predictions of several chemical evolution models, adopting different prescriptions for the galactic winds, with the photometrically-derived stellar metallicity distributions of both galaxies. The chemical evolution models for Draco and Ursa Minor, which are able to reproduce several observational features of these two galaxies, such as the several abundance ratios, take up-to-date nucleosynthesis into account for intermediate-mass stars and supernovae of both types, as well as the effect of these objects on the energetics of the systems. For both galaxies, the model that best fits the data contains an intense continuous galactic wind, occurring at a rate proportional to the star formation rate. Models with a wind rate assumed to be proportional only to the supernova rate also reproduce the observed SMD, but do not match the gas mass, whereas the models with no galactic winds fail to reproduce the observed SMDs. In the case of Ursa Minor, the same model as in previous works reproduces the observed distribution very well with no need to modify the main parameters of the model. The model for Draco, on the other hand, is slightly modified. The observed SMD requires a model with a lower supernova type Ia thermalization efficiency ($\\eta_{SNeIa}$ = 0.5 instead of $\\eta_{SNeIa}$ = 1.0) in order to delay the galactic wind, whereas all the other parameters are kept the same. The model results, compared to observations, strongly suggest that intense and continuous galactic winds play a very important role in the evolution of local dSphs. ", "machine_abstract": "We study how the effects of galactic winds can be used to explain the observed properties of the metal-poor tail in the stellar metallicity distributions (SMDs) of nearby dwarf spheroidal galaxies (dSph). We find that SMD is sensitive to both the mass loss rate and wind velocity, but not very sensitive to other parameters such as the initial mass function or star formation history. The best-fit model for each galaxy has been obtained by comparing its SMD with those predicted using different sets of free parameters. Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolution stages. These outflows are responsible for removing most metals produced by stars formed before z = 1.5-2.0. In addition, we also found that some of them may experience additional late-time outflow events which could remove more metals produced after this time period.", "paraphrased_abstract": "The results showed that all these dSph had experienced a strong outflow of supernovae during their early evolution, as a result of their supernova explosions, which had removed the majority of the metals that had been generated by stars of z=1.5-2.0. Besides, some of these stars may also experience some late-time outflows which may remove even more metals, after this time. We have therefore examined the relationship between the strength of the outflows and the composition of the spheres, and have therefore formulated the most accurate model of the SMD of the nearby spheroidal galaxies. Using the SMD as a parameter, we have determined the proportion of the spheres with the most favorable SMD, based on the mass of the spheres and the formation history of the stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2333, "title": "Gamma Rays frim the Galactic Centre", "abstract": "  Recent results from the HESS gamma ray telescope have shown the presence of both a diffuse, extended, flux of gamma rays above ~0.4 TeV and discrete sources in and near the Galactic Centre. Here, we put forward a possible explanation in terms of the diffusion of cosmic ray protons from a succession of supernova remnants (SNR) in the SgrA* region of the Galaxy plus a contribution from SNR in the rest of the Galactic Centre Region, to be called the Galactic Centre Ridge (GCR). Protons are favoured over electrons because the mG magnetic fields in the Region will attenuate energetic electrons severely. Prominent features are the need for 'anomalous diffusion' of the protons in the whole region and the adoption of low efficiency for SNR acceleration in the high density regions. The latter is related by us to the well-known low 'cosmic ray gradient' in the Galaxy. A corroborating feature is the close correlation of inferred cosmic ray intensity with the smoothed intensity of 5 GHZ radio radiation. We attribute this to the presence of the SNR in the GCR. ", "machine_abstract": "The gamma ray emission in the energy range 100 MeV to 10 GeV is studied using data taken by EGRET on board CGRO during its first four years of operation (1991) (1992) (1993) (1994) . The analysis has been performed for two different regions, one centered at l = 0\u00b0 and b = - 5\u00b0 , which includes the galactic centre region, and another centered at l = 180\u00b0 and b = + 5\u00b0 . In both cases we have used an iterative maximum likelihood method to determine the fluxes of individual sources as well as their spectral parameters.  We find that there are three distinct components contributing to the observed gamma-ray flux above 1 GeV :  A diffuse component with a power law spectrum extending upto ~10 GeV . A point source located near Sgr A* with a power law spectrum . An extended source towards the galactic center with a broken power law spectrum .  We also present results obtained when the same analysis was repeated after excluding the contribution due to the central part of the Galaxy .", "paraphrased_abstract": "I have analyzed the gamma-ray flux at a rate of ten gb per arcsecond, from which the gamma-ray flux of one symphonic scale is measured. We report the results from the first study in which EGRET took the data taken on board the CGRO during its first four years of operation (1991, 1992, 1993, 1994). In both cases, we have used a maximum likelihood method to calculate the fluxes of each source. There are three components that contribute to the flux of gamma-rays in the region of ten gb per arcsecond above one symphonic scale: A diffuse component, with a power-law that extends to ten gb per arc, a point source, near the star A, with a power-law that extends to ten gb per arc, and a further source near the center of the galaxy, which has a broken power-law. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0871, "title": "Unitarity analysis of a non-Abelian gauge invariant action with a mass", "abstract": "  In previous work done by us and coworkers, we have been able to construct a local, non-Abelian gauge invariant action with a mass parameter, based on the nonlocal gauge invariant mass dimension two operator F1/D^2F. The renormalizability of the resulting action was proven to all orders of perturbation theory, in the class of linear covariant gauges. We also discussed the perturbative equivalence of the model with ordinary massless Yang-Mills gauge theories when the mass is identically zero. Furthermore, we pointed out the existence of a BRST symmetry with corresponding nilpotent charge. In this paper, we study the issue of unitarity of this massive gauge model. Firstly, we provide a short review how to discuss the unitarity making use of the BRST charge. Afterwards we make a detailed study of the most general version of our action, and we come to the conclusion that the model is not unitary, as we are unable to remove all the negative norm states from the physical spectrum in a consistent way. ", "machine_abstract": "We study the unitarity properties of an effective field theory describing the interactions between gluons and quarks in QCD at high energies, where perturbation theory is not applicable anymore. The model we consider consists of a gauge-invariant action containing both massive and massless fields. We show that this model can be written as a sum over Feynman diagrams which are all unitary individually if certain conditions on the parameters appearing in the Lagrangian density are satisfied. In particular, it turns out that the masses of the particles involved must satisfy some relations to ensure unitarity. Finally, we discuss how these results could be used for phenomenological applications. PACS numbers: 11.10.Wx, 12.38.Qk, 13 .60.Hb  I. INTRODUCTORY REMAR K S The Standard Model (SM) describes successfully most experimental data available today [1] , but its validity has been tested only up to energies of about 1 TeV [2] . At higher energies new phenomena may appear beyond those predicted by the SM [3] . In order to describe such effects one usually considers extensions of the SM [4] or models based on effective theories [5] . Effective theories provide a systematic way to include corrections due to physics at scales above the energy scale considered [6] . They allow us to calculate observables using perturbative techniques even when the underlying dynamics cannot be described within the framework of standard quantum mechanics [7, 8] . This approach is particularly useful in cases where there exists no fundamental description of the physical system under consideration [9] . One example of an effective theory is Quantum Chromodynamics (QCD), the theory of strong interactions [10] . It predicts the existence of hadrons made of quarks and gluons [11] . However, since the typical momentum transfer inside a hadron is much smaller than the characteristic scale of QCD processes [12] , the latter can be studied separately from the former [13] . For instance, the production of jets [14] and heavy flavors [15] in high-energy collisions can be calculated using perturbative methods [16] . On the other hand, the interaction among partons [17]", "paraphrased_abstract": "In this regard, we consider the results for the analysis of phenomenological phenomena. The Standard Model (SM) is the most successful of the experimental results of recent times, but its validity has not been tested to the level of 1 TeV. In a higher energy, some phenomena may emerge which the Standard Model is not capable of predicting. In this regard, the most logical extension of the Standard Model (S), in the form of a Gaussian action, is called the Gaussian action, and we study how this action can be constructed on the basis of a Lagrangian reaction. The simplest form of the action, i.e., the existence of a gluon and a gluon gluon, is called the \u201ceffective field\u201d and is used to simulate the interaction of gluons and gluons with a QCD-strengthy reaction. However, as the average particle radial velocity is much smaller than the characteristic scale of the QCD, the QCD gluons can be studied separately. In fact, it is possible to use this method to calculate the observables of a physical system, even though it is not described in the context of the standard quantum mechanics. This method is very useful, especially when the physics of a system is not described in a classical sense. This method is based on", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0259, "title": "Formation and Collisional Evolution of Kuiper Belt Objects", "abstract": "  This chapter summarizes analytic theory and numerical calculations for the formation and collisional evolution of KBOs at 20--150 AU. We describe the main predictions of a baseline self-stirring model and show how dynamical perturbations from a stellar flyby or stirring by a giant planet modify the evolution. Although robust comparisons between observations and theory require better KBO statistics and more comprehensive calculations, the data are broadly consistent with KBO formation in a massive disk followed by substantial collisional grinding and dynamical ejection. However, there are important problems reconciling the results of coagulation and dynamical calculations. Contrasting our current understanding of the evolution of KBOs and asteroids suggests that additional observational constraints, such as the identification of more dynamical families of KBOs (like the 2003 EL61 family), would provide additional information on the relative roles of collisional grinding and dynamical ejection in the Kuiper Belt. The uncertainties also motivate calculations that combine collisional and dynamical evolution, a `unified' calculation that should give us a better picture of KBO formation and evolution. ", "machine_abstract": "The Kuiper belt is the source region for most short period comets, but its formation remains poorly understood. We present results from N-body simulations that show how collisions among planetesimals in Neptune's feeding zone can produce objects with orbits similar to those observed today. The initial conditions are based on models of planet migration during which Neptune migrated outward by about 30 AU before being halted at its current location. Our calculations suggest that the Kuiper belt formed as a result of collisional grinding between bodies whose sizes were comparable to Pluto (r ~ 1000 km). This process produced a population of small bodies with orbital eccentricities ranging up to 0.3. Subsequent encounters with Neptune caused some of these bodies to be scattered into highly eccentric orbits. These results provide an explanation for why there appears to be no correlation between the size distribution of KBOs and their orbital eccentricity.", "paraphrased_abstract": "The results are as follows: we consider that the Kuiper belt formed in consequence of collisions between bodies that were not small in size (r 1000 km), producing a population of small bodies with orbits of about 0.3, and subsequently smashed them into very small orbits. These results explain why the distribution of KBOs is not explained by the orbital eccentricity of KBOs. We propose that Kuiper belts form as a result of collisions between bodies of a size comparable to Pluto. This process created a population of small bodies with orbital eccentricities of 0.3  0.3. The next time they encounter Neptune, they fall into a highly eccentric orbit. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0961, "title": "The InterHourly-Variability (IHV) Index of Geomagnetic Activity and its Use in Deriving the Long-term Variation of Solar Wind Speed", "abstract": "  We describe the derivation of the InterHourly Variability (IHV) index of geomagnetic activity. The IHV-index for a geomagnetic element is mechanically derived from hourly values as the sum of the unsigned differences between adjacent hours over a seven-hour interval centered on local midnight. The index is derived separately for stations in both hemispheres within six longitude sectors using only local night hours. It is intended as a long-term index. Available data allows derivation of the index back well into the 19th century. On a time scale of a 27-day Bartels rotation, IHV averages for stations with corrected geomagnetic latitude less than 55 degrees are strongly correlated with midlatitude range indices. Assuming a constant calibration of the aa-index we find that observed yearly values of aa before the year 1957 are 2.9 nT too small compared to values calculated from IHV using the regression constants based on 1980-2004. We interpret this discrepancy as an indication that the calibration of the aa index is in error before 1957. There is no such problem with the ap index. Rotation averages of IHV are also strongly correlated with solar wind parameters (BV^2). On a time scale of a year combining the IHV-index and the recently-developed Inter-Diurnal Variability (IDV) index (giving B) allows determination of solar wind speed, V, from 1890-present. Over the ~120-year series, the yearly mean solar wind speed varied from a low of 303 km/s in 1902 to a high value of 545 km/s in 2003. The calculated yearly values of the product BV using B and V separately derived from IDV and IHV agree quantitatively with (completely independent) BV derived from the amplitude of the diurnal variation of the H component in the polar caps since 1926 and sporadically beyond. ", "machine_abstract": "The inter-hourly-variability index is proposed to measure geomagnetic activity on hourly basis, which can be used as an indicator for solar wind speed variation over time scales longer than one day.  The IHV index was calculated using hourly values of Kp index during the period 1957-2009. It shows that there are two peaks at about 1965-1970 and 1990-1995 . A linear regression analysis between the IHV index and solar wind speed data obtained by spacecraft observations reveals that they have good correlation with each other. This suggests that the IHV index may provide useful information for long-term prediction of solar wind speed. Keywords: Geomagnetism; Solar wind; Variability index; Prediction modeling; Data mining. 1 Introduction Space weather has been attracting more attention recently because it affects many aspects of human life such as satellite communications, aviation safety, power grids etc. (e.g., [1] ). In particular, solar wind plays important roles in space weather since it carries away magnetic fluxes generated by the Sun's dynamo action [2] , and thus controls the Earth's magnetosphere [3] . Solar wind is driven out from the Sun by coronal mass ejections [4] . Coronal mass ejections occur when huge amounts of plasma suddenly escape from the Sun into space [5] . They usually last several hours [6] . Therefore, solar wind speed varies significantly within 24 h [7, 8] . However, most previous studies only focus on the average solar wind speed or daily averaged solar wind speed [9] . As a result, little work has been done on investigating how solar wind speed changes on shorter timescales [10] .", "paraphrased_abstract": ".. In addition, the Sun\u2019s heat plays a crucial role in space weather, which carries out magnetic influxes from the Sun\u2019s dynamo action, thereby controlling the magnetic field in the earth. In addition, the sun\u2019s heat also has a vital role in the formation of space weather. In particular, the solar wind is important in space weather, as it transports away the magnetic fluxes produced by the Sun\u2019s dynamo action and controls the magnetosphere of Earth. Moreover, most of the previous studies have been conducted on average daily average daily average wind speed, and on average daily average wind speed, but there has been little interest in studying the changes of this speed over shorter time periods. The inter-hourly variable index was calculated by using the Kp index from 1957 to 2009, and found that the two peakages were about 1965-1970 and 1990-1995. The inter-hourly variable index correlated with the average daily averaged, and the inter-hourly variable index gave a good correlation with the observed solar wind speed. The inter-hourly variable index was calculated from the hourly Kp index, during the period 1957-2009. It was found that the peaks were located at the peaks at about 1965-70 and at about 1990-95. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4365, "title": "Integral Field Spectroscopy of the Extended Emission-Line Region of 4C 37.43", "abstract": "  We present Gemini integral field spectroscopy and Keck II longslit spectroscopy of the extended emission-line region (EELR) around the quasar 4C 37.43. The velocity structure of the ionized gas is complex and cannot be explained globally by a simple dynamical model. The spectra from the clouds are inconsistent with shock or ``shock + precursor'' ionization models, but they are consistent with photoionization by the quasar nucleus. The best-fit photoionization model requires a low-metallicity (12+log(O/H) < 8.7) two-phase medium, consisting of a matter-bounded diffuse component with a unity filling-factor (N ~ 1 \\cc, T ~ 15000 K), in which are embedded small, dense clouds (N ~ 400 \\cc, T ~ 10^4 K). The high-density clouds are transient and can be re-generated through compressing the diffuse medium by low-speed shocks (V_S \\lesssim 100 \\kms). Our photoionization model gives a total mass for the ionized gas of about 3x10^{10} M_sun, and the total kinetic energy implied by this mass and the observed velocity field is ~2x10^{58} ergs. The fact that luminous EELRs are confined to steep-spectrum radio-loud quasars, yet show no morphological correspondence to the radio jets, suggests that the driving force producing the 4C 37.43 EELR was a roughly spherical blast wave initiated by the production of the jet. That such a mechanism seems capable of ejecting a mass comparable to that of the total interstellar medium of the Milky Way suggests that ``quasar-mode'' feedback may indeed be an efficient means of regulating star formation in the early universe. ", "machine_abstract": "We present integral field spectroscopy (IFS) observations of the extended emission-line region (EELR)         We use these data to study the kinematics and excitation conditions in this EELR, which is found         The IFS data reveal that the gas has an average velocity dispersion of ~200 km s-1 across most of the         We find evidence for two distinct components within the EELR: one component with low-excitation         properties similar to those observed in other radio galaxies; and another component with higher-         excitation properties than are typically seen in radio galaxies.    The high-excitation component appears to be associated with shocks driven by the radio jets into the surrounding medium. In addition, we detect several compact knots of [O III] emission along the northern edge of the EELR, which may represent sites where new stars have formed as a result of jet-induced star formation or galaxy mergers.", "paraphrased_abstract": "But there are several compact knots of III in the north of the EELR, and this may represent the sites where the new stars have formed by the formation of a jet or by the merger of a star. The IFS shows that the gas is a mixture of two different types: the first is low in the range of motion of other galaxies, and the second is higher, as is the case in the typical galaxies. This is what the IFS is like, and the data show that the gas is about 100 km s-1 in the region of the EELR. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1928, "title": "Highly turbulent solutions of LANS-alpha and their LES potential", "abstract": "  We compute solutions of the Lagrangian-Averaged Navier-Stokes alpha-model (LANS) for significantly higher Reynolds numbers (up to Re 8300) than have previously been accomplished. This allows sufficient separation of scales to observe a Navier-Stokes (NS) inertial range followed by a 2nd LANS inertial range. The analysis of the third-order structure function scaling supports the predicted l^3 scaling; it corresponds to a k^(-1) scaling of the energy spectrum. The energy spectrum itself shows a different scaling which goes as k^1. This latter spectrum is consistent with the absence of stretching in the sub-filter scales due to the Taylor frozen-in hypothesis employed as a closure in the derivation of LANS. These two scalings are conjectured to coexist in different spatial portions of the flow. The l^3 (E(k) k^(-1)) scaling is subdominant to k^1 in the energy spectrum, but the l^3 scaling is responsible for the direct energy cascade, as no cascade can result from motions with no internal degrees of freedom. We verify the prediction for the size of the LANS attractor resulting from this scaling. From this, we give a methodology either for arriving at grid-independent solutions for LANS, or for obtaining a formulation of a LES optimal in the context of the alpha models. The fully converged grid-independent LANS may not be the best approximation to a direct numerical simulation of the NS equations since the minimum error is a balance between truncation errors and the approximation error due to using LANS instead of the primitive equations. Furthermore, the small-scale behavior of LANS contributes to a reduction of flux at constant energy, leading to a shallower energy spectrum for large alpha. These small-scale features, do not preclude LANS to reproduce correctly the intermittency properties of high Re flow. ", "machine_abstract": "We present highly resolved numerical simulations of the incompressible Navier-Stokes equations with the LANS-alpha model, which is known to produce good results for wall-bounded flows at low Reynolds numbers. We show that this method can also be used in high-Reynolds number situations where it produces accurate results even though its underlying assumptions are not valid anymore. The main advantage over standard LES methods lies in the fact that no explicit subgrid-scale models have to be introduced. This makes the approach very attractive since there is no need to tune any parameters or coefficients as required by other LES approaches. In addition we demonstrate how the LANS-alpha method can be combined with an implicit LES scheme based on the variational multiscale formulation (VMS-LES) to obtain more efficient computations. Finally, we discuss some open issues related to the use of these schemes in practical applications. Turbulence plays a crucial role in many physical phenomena ranging from weather prediction to oceanic circulation and combustion processes. However, despite decades of research turbulence still remains one of the most challenging problems in computational fluid dynamics. One reason for this difficulty is due to the wide range of length scales involved in turbulent flows. While large eddies contain most of the kinetic energy they only occupy a small fraction of the total volume. On the other hand smaller eddies fill up almost all space but contribute little to the overall kinetic energy. Therefore, if one wants to resolve all relevant flow structures accurately enough then extremely fine grids would be needed leading to prohibitively expensive calculations. To overcome this problem so-called Large Eddy Simulations (LESs) were developed during the last two decades [1, 2] . These techniques aim at resolving only those large-scale motions responsible for the bulk of the kinetic energy while modeling the effect of unresolved small-scale fluctuations using suitable closure relations. Although LES has been successfully applied to various engineering problems [3\u20135] , it suffers from several drawbacks such as the lack of universality of the employed sub-grid scale models [6] . In recent years new classes of LES-like methods have emerged [7\u201310] . They are based", "paraphrased_abstract": "In recent years, however, new methods for calculating turbulent flows have appeared, and they are based on a system of large-eddy simulations. These simulations are intended to resolve only the major motions which are the main source of the kinetic energy, and to calculate the effect of smaller-eddy motions by means of appropriate closure relations. In order to overcome this problem, LES-like simulations have been introduced to solve the following problem: the main problem is that the subgrid-scale model is not used, and the LES-like model does not require any special adjustment in the resulting parameters or coefficients. This advantage is due to the fact that there is no need to add a subgrid-scale model to the LES-like approach, which is also attractive since there is no need to change any parameters or coefficients as with the LES-like approach. In order to solve the kinetic structure of the stream, we use the Navier-Stokes equation, which is known to be very accurate at low Reynolds numbers. Moreover, we show how this equation can be combined with an implicit LES-like LES, based on the VMS-LES, in order to obtain a better calculation. The problem of turbulence, though studied for many years, is still the most difficult to solve in computational fluid dynamics. One reason for this difficulty is that the range of", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1939, "title": "3-He in the Milky Way Interstellar Medium: Ionization Structure", "abstract": "  The cosmic abundance of the 3-He isotope has important implications for many fields of astrophysics. We are using the 8.665 GHz hyperfine transition of 3-He+ to determine the 3-He/H abundance in Milky Way HII regions and planetary nebulae. This is one in a series of papers in which we discuss issues involved in deriving accurate 3-He/H abundance ratios from the available measurements. Here we describe the ionization correction we use to convert the 3-He+/H+ abundance, y3+, to the 3-He/H abundance, y3. In principle the nebular ionization structure can significantly influence the y3 derived for individual sources. We find that in general there is insufficient information available to make a detailed ionization correction. Here we make a simple correction and assess its validity. The correction is based on radio recombination line measurements of H+ and 4-He+, together with simple core-halo source models. We use these models to establish criteria that allow us to identify sources that can be accurately corrected for ionization and those that cannot. We argue that this effect cannot be very large for most of the sources in our observational sample. For a wide range of models of nebular ionization structure we find that the ionization correction factor varies from 1 to 1.8. Although large corrections are possible, there would have to be a conspiracy between the density and ionization structure for us to underestimate the ionization correction by a substantial amount. ", "machine_abstract": "We present new measurements of 3 He+ and 3 He++ column densities toward eight nearby stars, using data obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE). The results are compared to previous observations made by Copernicus and IUE satellites as well as FUSE. We find that our values for N(3 He+)/N(H+), which range between 0.0015-0.0125, agree within uncertainties with those measured previously at high latitudes but disagree significantly with lower latitude measurements. Our results suggest that there is an additional source of ionization near the Galactic plane not accounted for by cosmic rays or X-rays. This could be due to shocks driven into the interstellar medium by supernovae remnants and/or winds associated with massive OB associations.     Keywords: Helium abundance, Interstellar medium, Shocks, Supernova remnant, Winds, Cosmic ray", "paraphrased_abstract": "We describe here a new study of the density of helium in the vicinity of eight stars, and we compare it with that of Copernicus and IUE satellites, and also the results of FUSE. We show that there is another source of ionization in the Galactic plane, which is not accounted for by cosmic rays or X-rays, and that this is due to shocks from supernovae and winds, which are caused by enormous OBs. Moreover, we propose that a further source of ionization is not accounted for by cosmic rays or X-rays, and that it may be due to shocks from supernovae and wind from supernovae. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1272, "title": "Grain Retention and Formation of Planetesimals near the Snow Line in MRI-driven Turbulent Protoplanetary Disks", "abstract": "  The first challenge in the formation of both terrestrial planets and the cores of gas giants is the retention of grains in protoplanetary disks. In most regions of these disks, gas attains sub-Keplerian speeds as a consequence of a negative pressure gradient. Hydrodynamic drag leads to orbital decay and depletion of the solid material in the disk, with characteristic timescales as short as only a few hundred years for meter-sized objects at 1 AU. In this paper, we suggest a particle retention mechanism which promotes the accumulation of grains and the formation of planetesimals near the water sublimation front or ``snow line.'' This model is based on the assumption that, in the regions most interesting for planet formation, the viscous evolution of the disk is due to turbulence driven by the magneto-rotational instability (MRI) in the surface layers of the disk. The depth to which MRI effectively generates turbulence is a strong function of the grain size and abundance. A sharp increase in the grain-to-gas density ratio across the snow line reduces the column depth of the active layer. As the disk evolves towards a quasi-steady-state, this change in the active layer creates a local maximum in radial distribution of the gas surface density and pressure, causing the gas to rotate at super-Keplerian speed and halting the inward migration of grains. This senario presents a robust process for grain retention which may aid in the formation of proto-gas-giant cores preferentially near the snow line. ", "machine_abstract": "We perform global three-dimensional magnetohydrodynamic simulations to study grain growth, dust settling, and planetesimal formation around protoplanets embedded within turbulent disks with dead zone boundaries. We find that grains grow by coagulation up to millimeter sizes at the snow line ($R_s = 2.7 \\ast d_{s} AU$), where $R_s$ is the distance between the central star and the snowline, $ds$ is the disk scale height, and $AU$ denotes astronomical unit. Grains are trapped inside the dead zone due to magnetic pressure gradient force and settle toward midplane. The maximum size of grains increases as they approach the planet's location because of higher gas density there. Dust particles can be captured into growing planetesimals when their Stokes number becomes smaller than unity. Our results show that planetesimals form efficiently only if the initial dust-to-gas mass ratio is larger than $10^{-3}$.", "paraphrased_abstract": "We study the development of grain and dust in the vicinity of equilateral disks and the formation of planetesimals, and we have shown that in the case of equilateral disks the dust-to-gas ratio is smaller than the order of 10:5:6. We study the growth of grains and the settling of planetesimals at the height of the snow line, R-s = 2.7 ast d-s AU-acquire, where R-s is the distance between the central star and the snowline, d-s = the disk scale, AU-acquire the unit of the astronomical unit. The grain is stuffed in the dead zone by the magnetic force of the eddy, and it settles at the middle. Grains become entangled in the dead zone, because they are engulfed by the magnetic gradient, and settle at the midplane. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1388, "title": "Some properties of the resonant state in quantum mechanics and its computation", "abstract": "  The resonant state of the open quantum system is studied from the viewpoint of the outgoing momentum flux. We show that the number of particles is conserved for a resonant state, if we use an expanding volume of integration in order to take account of the outgoing momentum flux; the number of particles would decay exponentially in a fixed volume of integration. Moreover, we introduce new numerical methods of treating the resonant state with the use of the effective potential. We first give a numerical method of finding a resonance pole in the complex energy plane. The method seeks an energy eigenvalue iteratively. We found that our method leads to a super-convergence, the convergence exponential with respect to the iteration step. The present method is completely independent of commonly used complex scaling. We also give a numerical trick for computing the time evolution of the resonant state in a limited spatial area. Since the wave function of the resonant state is diverging away from the scattering potential, it has been previously difficult to follow its time evolution numerically in a finite area. ", "machine_abstract": "The problem of finding the energy levels of an electron bound to a nucleus is one of the most important problems in physics, which has been studied for many years. In this article we consider the case where there are two electrons with opposite spins that can occupy different orbitals around the same nucleus. We show how to compute these states using only polynomial time computations on classical computers. This result was obtained by applying some techniques developed recently in computational complexity theory such as the PCP theorem (the probabilistic checkable proof) and the local testability of CSPs (constraint satisfaction problems). The results presented here have applications not only in theoretical physics but also in computer science. For example they provide new insights into the structure of NP-complete problems. Quantum mechanical systems play an essential role in modern physics. One of their main features is that particles may be found in superposition of several states at once. A famous example is Schr\u00f6dinger's cat experiment [1] . Another feature is entanglement [2] , i.e., correlations between particles that cannot be explained classically [3] . In this work we study the following problem: given a system consisting of N spin-1/2 particles, what is the ground state? That means, if all particles were measured simultaneously, what would be the probability distribution over the possible outcomes? We will focus our attention on the simplest non-trivial case: two spin-\u00bd particles occupying different orbitals around the nucleus [4] . It turns out that it is sufficient to solve this problem in order to find the ground state of any number of particles [5] .", "paraphrased_abstract": "I'm going to examine the following problem: If a system is composed of N spin-1/2 particles, what is the base state of it? In general, it is a very difficult problem, but it is a very important one, and it has been studied for many years. It is a very simple problem, for instance, of two spin-1/2 particles, which have different orbitals around the nucleus, and in this way we will find out how the basis state is distributed over all the particles. The result of this experiment is that there are two spin-1/2 particles, and in that case we can measure the base state of all the particles. In the following, we will take the simple, the most difficult one: two spin-1/2 particles with different orbitals around the nucleus, that is, to measure the ground state of all the particles, we will study the following: the ground state of two spin-1/2 particles, the first, and the second, and the third, in the second, and the third, in the third, in the fourth. In this work, we study the following problem: given a system of N spin-1/2 particles, what is the ground state? We will take the most elementary example: two spin-1/2 particles around the same nucleus... ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2745, "title": "Pioneer Anomaly and Accelerating Universe as Effects of the Minkowski Space Conformal Symmetry", "abstract": "  On the basis of the nonisometric transformations subgroup of the SO(4.2) group, the nonlinear time inhomogeneity one-parameter conformal transformations are constructed. The connection between the group parameter and the Hubble constant H0 is established. It is shown that the existence of an anomalous blue-shifted frequency drift is a pure kinematic manifestation of the time inhomogeneity induced by the Universe expansion. This conclusion is confirmed via a generalization of the standard Special Relativity clock synchronization procedure to the space expanding case. The obtained formulae are in accordance with the observable Pioneer Anomaly effect. The anomalous blue-shifted drift is universal, does not depend on the presence of graviting centers and can be, in principle, observed on any frequencies under suitable experimental conditions. The explicit analytic expression for the speed of recession--intergalactic distance ratio is obtained in the form of a function of the red shift z valid in the whole range of its variation. In the small z limit this expression exactly reproduces the Hubble law. The maximum value of this function at z=0.475 quantitatively corresponds to the experimentally found value z(exp) = 0.46 +/- 0.13 of the transition from the decelerated to the accelerated expansion of the Universe. ", "machine_abstract": "We show that the Pioneer anomaly, which is an anomalous acceleration observed in radio tracking data for two spacecrafts launched by NASA (Pioneer 10/11), can be explained if we assume that there exists a conformal symmetry breaking effect at large distances from our solar system. We also argue that this effect may lead to accelerating expansion of the universe. The idea behind these results is based on the fact that the space-time metric around massive bodies has been shown to have a certain degree of anisotropy due to quantum gravity effects. This anisotropic nature of space-time leads to violation of Lorentz invariance, which in turn causes violations of energy-momentum conservation laws. In order to explain such violations within the framework of general relativity one needs to introduce new fields into the theory. These are called \"dark fields\" because they do not interact with ordinary matter but only affect gravitational interactions between particles.", "paraphrased_abstract": "Then a new field has to be introduced, and it is called the dark field. It is necessary to introduce such fields in general relativity, because they are not the ordinary ones, but only the gravitational ones. The pioneer anomaly, which is the acceleration observed in the data of two spacecrafts of NASA\u2019s fleet, is the result of a conformal symmetry breaking effect at a distance of ten thousand kilometers. It is predicted that this symmetry breaking effect could lead to a massive expansion of the universe. The idea behind this hypothesis is that the metric of space around huge bodies has an anisotropic nature, due to the quantum gravity of the universe. This anisotropic nature leads to a violation of Lorentz invariance, and therefore violates the energy-momentum conservation laws. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1434, "title": "Nonlinear Dynamics of Infectious Diseases Transfer with Possible Applications for Tubercular Infection", "abstract": "  In this paper, we model a nonlinear dynamics of infectious diseases transfer. Particularly, we study possible applications to tubercular infection in models with different profiles (peak values) of the population density dependence on spatial coordinates. Our approach is based on the well known method of instantons which has been used by the authors to describe kinetics of adiabatic chemical reactions as a function of the heat-bath temperature and other system parameters. In our approach, we use \"social temperature\" T as one of the controlling parameters. Increase of T leads to acceleration of the infectious diseases transfer. The \"blockage\" effect for the infectious diseases transfer has been demonstrated in the case when peak values (in the population density) are equal to one and under condition that the \"social temperature\" is low. Existence of such effect essentially depends from environment \"activity\" (social and prophylactic). Results of our modeling qualitatively meet the tuberculosis dynamic spread data in Penza region of Russia. ", "machine_abstract": "We study the nonlinear dynamics of infectious diseases transfer in a population, where individuals are divided into three classes: susceptible (S), infected (I) and recovered/removed (R). We consider two different models: SIR model and SEIR model. In both cases we assume that there is no birth or death in the population. The main goal of this work is to investigate how the disease spreads through the population depending on its parameters. For example, if the infection rate is too high then it may lead to an epidemic outbreak. On the other hand, if the recovery rate is very large compared to the infection rate then the number of infectives will decrease rapidly. Finally, we show some numerical simulations which illustrate our results.     Keywords: Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. 1 Introduction     Many mathematical models have been developed over time to describe the spread of infectious diseases within populations [1\u20133] . These models can be used as tools to understand the transmission mechanisms of these diseases and help public health authorities make decisions about prevention strategies [4] .   In particular, many researchers have studied the effects of vaccination programs [5\u20137] , quarantine [8, 9] and isolation [10, 11] on the evolution of epidemics. Other studies focus on the impact of environmental factors such as temperature [12, 13] , humidity [14, 15] and rainfall [16] on the propagation of pathogens.  The majority of existing works use deterministic models based on ordinary differential equations [17] . However, stochastic models [18, 19] and agent-based models [20, 21] also exist. Agent-based models allow us to take into account individual behaviors [22] while stochastic models provide more realistic descriptions of random events [23] .    In this article, we propose new mathematical models describing the spread of infectious diseases in a closed population. Our aim is to analyze the influence of various parameters on the behavior of the system. More specifically, we want to determine whether the disease will die out naturally or cause an epidemic outbreak. To do so, we first introduce the basic reproduction number R0 [24] , which represents the average number", "paraphrased_abstract": "For this purpose, we propose a new model to be used to analyze the spread of infectious diseases in a closed population. We will first present a basic model for the spread of infectious diseases in the closed population. The model is based on the deterministic deterministic equations, but there are also deterministic models, based on agents, based on the observable characteristics of an organism. We are primarily concerned with the epidemiology of epidemics and the epidemiology of infection, as well as with the influence of environmental factors, such as the temperature, humidity, and rainfall, on the spread of infectious diseases. Several studies have already been conducted on the effects of vaccinations, quarantine, isolation, and diseases. Several studies have studied the influence of abiotic factors on the spread of epidemics. We have studied the effects of the epidemics by measuring the proportion of individuals who are ill or being cured. We will then use two methods, the SIR model and the SEIR model. We assume that there is no birth or death in the population. In addition, we have proposed a numerical model of our experiments, based on which we have been able to obtain the average of all the population characteristics. Then we will present a numerical model of the transmission of epidemics in a closed population. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0357, "title": "Ongoing Galactic Accretion: Simulations and Observations of Condensed Gas in Hot Halos", "abstract": "  Ongoing accretion onto galactic disks has been recently theorized to progress via the unstable cooling of the baryonic halo into condensed clouds. These clouds have been identified as analogous to the High-Velocity Clouds (HVCs) observed in HI in our Galaxy. Here we compare the distribution of HVCs observed around our own Galaxy and extra-planar gas around the Andromeda galaxy to these possible HVC analogs in a simulation of galaxy formation that naturally generates these condensed clouds. We find a very good correspondence between these observations and the simulation, in terms of number, angular size, velocity distribution, overall flux and flux distribution of the clouds. We show that condensed cloud accretion only accounts for ~ 0.2 M_solar / year of the current overall Galactic accretion in the simulations. We also find that the simulated halo clouds accelerate and become more massive as they fall toward the disk. The parameter space of the simulated clouds is consistent with all of the observed HVC complexes that have distance constraints, except the Magellanic Stream which is known to have a different origin. We also find that nearly half of these simulated halo clouds would be indistinguishable from lower-velocity gas and that this effect is strongest further from the disk of the galaxy, thus indicating a possible missing population of HVCs. These results indicate that the majority of HVCs are consistent with being infalling, condensed clouds that are a remnant of Galaxy formation. ", "machine_abstract": "We present new simulations that show how condensed gas is formed by ongoing accretion onto galactic halos, which are heated to temperatures above 10 million degrees Kelvin (MK). The condensed gas forms as the hot halo cools radiatively on timescales shorter than its dynamical time. We compare these results with observations of cool clouds in nearby galaxy clusters. Our models predict that condensed gas should be found at distances between 100 kpc and 1 Mpc from galaxies within massive clusters. This prediction agrees well with recent X-ray observations of cool clouds in Abell 1795 and Abell 2163. These clouds have masses ranging from $10^{9}$ to $10^{11}$ solar masses, sizes less than 50 kiloparsecs, and densities greater than 1000 atoms per cubic centimeter. They also appear to be moving outward relative to their host galaxies at velocities up to several hundred kilometers per second. In addition, we find that our model predicts cloud lifetimes longer than one billion years for most observed clouds.", "paraphrased_abstract": "Our model predicts that condensed gas may be found in a range of distances from a radius of 100 kpc to a radius of one-hundred and a half from galaxies, and is consistent with observations of cool clouds in Abell 1795 and Abell 2163. This is confirmed by recent X-ray observations of cool clouds in Abell 1795 and Abell 2163. These clouds have solar masses of less than 50 kiloparsecs and density of up to 1000 atoms per cubic centimeter. These clouds are observable in their radiances of several hundred kilometers per second. Our models estimate that condensed gas is formed when the hot halo cools in a time shorter than its dynamic time. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4195, "title": "VSOP: The Variable Star One-shot Project I. Project presentation and first data release", "abstract": "  The Variable Star One-shot Project (VSOP) is aimed at (1) providing the variability type and spectral type of all unstudied variable stars, (2) process, publish, and make the data available as automatically as possible, and (3) generate serendipitous discoveries. This first paper describes the project itself, the acquisition of the data, the dataflow, the spectroscopic analysis and the on-line availability of the fully calibrated and reduced data. We also present the results on the 221 stars observed during the first semester of the project. We used the high-resolution echelle spectrographs HARPS and FEROS in the ESO La Silla Observatory (Chile) to survey known variable stars. Once reduced by the dedicated pipelines, the radial velocities are determined from cross correlation with synthetic template spectra, and the spectral types are determined by an automatic minimum distance matching to synthetic spectra, with traditional manual spectral typing cross-checks. The variability types are determined by manually evaluating the available light curves and the spectroscopy. In the future, a new automatic classifier, currently being developed by members of the VSOP team, based on these spectroscopic data and on the photometric classifier developed for the COROT and Gaia space missions, will be used. We confirm or revise spectral types of 221 variable stars from the GCVS. We identify 26 previously unknown multiple systems, among them several visual binaries with spectroscopic binary individual components. We present new individual results for the multiple systems V349 Vel and BC Gru, for the composite spectrum star V4385 Sgr, for the T-Tauri star V1045 Sco, and for DM Boo which we re-classify as a BY Draconis variable. The complete data release can be accessed via the VSOP web site. ", "machine_abstract": "The VSOP (Variable Star One-shot Project) is an ambitious project to obtain high-precision radial velocities for thousands of stars with the goal of detecting extrasolar planets by means of gravitational microlensing, transit photometry or direct imaging techniques.  This article presents the main characteristics of this new survey as well as its first results obtained on two open clusters. We have observed more than 1000 targets in the open cluster M67 during three nights using the HARPS spectrograph at La Silla Observatory. Our observations show that we can reach precisions better than 1 m/s over periods of one hour. In addition, we present our preliminary analysis of these data which shows that we are able to detect binaries down to separations smaller than 0.1 AU. Finally, we report on the detection of several new spectroscopic binary systems among our sample. Keywords: Open Cluster, Radial Velocity Survey", "paraphrased_abstract": "The VSOP (Small Star Observation Project) is a project of the X-ray Society for the study of the radial velocity of thousands of stars. Its goal is to measure the radial velocity at a metrical scale of several hundred million stars. The project is based on a novel method of detecting the extrasolar planets through gravitational microlensing, transit photometry or direct imaging. This article presents the main characteristics of this new research, and the first results of the first test of two open clusters. We have observed more than 1000 targets in M-67 in the open cluster M-67 during three nights of observation with the HARPS telescope. We show that the spectral velocity is as large as a m/s, and we also show that the spectral velocity is as large as a tenth of an AU. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2491, "title": "Sparse and Dense Encoding in Layered Associative Network of Spiking Neurons", "abstract": "  A synfire chain is a simple neural network model which can propagate stable synchronous spikes called a pulse packet and widely researched. However how synfire chains coexist in one network remains to be elucidated. We have studied the activity of a layered associative network of Leaky Integrate-and-Fire neurons in which connection we embed memory patterns by the Hebbian Learning. We analyzed their activity by the Fokker-Planck method. In our previous report, when a half of neurons belongs to each memory pattern (memory pattern rate $F=0.5$), the temporal profiles of the network activity is split into temporally clustered groups called sublattices under certain input conditions. In this study, we show that when the network is sparsely connected ($F<0.5$), synchronous firings of the memory pattern are promoted. On the contrary, the densely connected network ($F>0.5$) inhibit synchronous firings. The sparseness and denseness also effect the basin of attraction and the storage capacity of the embedded memory patterns. We show that the sparsely(densely) connected networks enlarge(shrink) the basion of attraction and increase(decrease) the storage capacity. ", "machine_abstract": "We propose an encoding scheme for layered associative network (LAN) of spiking neurons, which is based on sparse coding and dense coding. The proposed method can be used to learn the association between input patterns and output patterns by using unsupervised learning algorithms such as Hebbian learning rule or spike-timing-dependent plasticity (STDP). We show that our model has better performance than conventional LANs with respect to both storage capacity and retrieval accuracy. In addition, we demonstrate that our model can perform pattern completion task without any additional training data.  Index Terms-Layered associaton network, Sparse coding, Dense coding, Pattern completion, STDP, Unsupervised learning algorithm. 1 Introduction Recently, there have been many studies about artificial neural networks [1] . Among them, layered associative network (L AN ) [2] , which consists of multiple layers of neurons connected through synapses, has attracted much attention because it shows high storage capacity and good retrieval accuracy [3] . In L AN s, each neuron receives inputs from all neurons in previous layer via synaptic connections. Then, the activity level of each neuron is determined by its firing rate according to the following equation:  where x i denotes the activity level of ith neuron at time t, w ij represents connection weight from jth neuron in previous layer to ith neuron in current layer, f(\u00b7) stands for activation function, and b i indicates bias term [4] . Since the number of possible combinations of activities among neurons increases exponentially when the number of neurons becomes large, storing information in L AN requires huge amount of memory space [5] . To overcome this problem, several approaches have been suggested [6] - [8] . For example, sparseness constraint was introduced into L AN so that only small fraction of neurons are active simultaneously [9] . However, these methods require supervised learning algorithms to train parameters of L AN , which makes their applications limited [10] .", "paraphrased_abstract": "For example, we have introduced a sparse constraint in order to have only a small number of neurons active simultaneously; but we have proposed a method of forming LANS, whereby only a small number of neurons can be active simultaneously; we have proposed a method of forming LANS, which has been developed mainly in the sense of storing data and retrieving it, and we have proved that our method is as good as other systems in terms of storage and retrieval. Our study consists of the development of a neural network consisting of a number of layers of neurons, connecting them through synaptic connections; and the formation of LANS is by means of the sparseness constraint of the neurons, so that only the smallest number of neurons can act simultaneously. The problem of storing data in LANS is a very important one. The layered associative network (LAN) is a system consisting of multiple layers of neurons connected by a single synapse; the information is received by each of the neurons in the previous layer through synaptic connections; the activity of each neuron is determined by the rate at which it is firing, and xi is the activity of each neuron, f is the activation coefficient, and bi is the bias term. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1952, "title": "Dynamic Effects Increasing Network Vulnerability to Cascading Failures", "abstract": "  We study cascading failures in networks using a dynamical flow model based on simple conservation and distribution laws to investigate the impact of transient dynamics caused by the rebalancing of loads after an initial network failure (triggering event). It is found that considering the flow dynamics may imply reduced network robustness compared to previous static overload failure models. This is due to the transient oscillations or overshooting in the loads, when the flow dynamics adjusts to the new (remaining) network structure. We obtain {\\em upper} and {\\em lower} limits to network robustness, and it is shown that {\\it two} time scales $\\tau$ and $\\tau_0$, defined by the network dynamics, are important to consider prior to accurately addressing network robustness or vulnerability. The robustness of networks showing cascading failures is generally determined by a complex interplay between the network topology and flow dynamics, where the ratio $\\chi=\\tau/\\tau_0$ determines the relative role of the two of them. ", "machine_abstract": "We study the vulnerability of interdependent networks under cascading failures, where nodes are subject to random attacks and their neighbors can fail as well due to loss of connectivity. We show that dynamic effects increase network vulnerability by enhancing the spreading of failures in time. In particular, we find that increasing the number of attackers or decreasing the recovery rate increases the probability for global cascade failure. Our results suggest that it is important to consider both static and dynamic aspects when studying the robustness of real-world systems against cascading failures. Interdependence between different components of complex systems has been shown to be crucial for understanding many phenomena such as epidemic outbreaks [1] , traffic jams [2] , financial crashes [3] , and blackouts [4] . The recent 2008 power grid crisis caused by an unprecedented series of cascading failures [5] highlighted the importance of considering interdependence among system elements [6] . In this work, we focus on interdependent networks [7, 8] , which consist of two types of nodes: source (S) and target (T). Source nodes provide services to other nodes while target nodes depend on these services. For example, in the case of the power grid, generators supply electricity to substations; if one generator fails then its neighboring substations will also lose power [9] . Similarly, in social networks people may rely on each other's opinions [10] ; if someone becomes ill [11] or loses her job [12] she might affect others' health status [13] or income [14] respectively. Recent studies have shown that interdependency plays an important role in determining the resilience of interconnected systems [15, 16] . However, most previous works focused only on static properties [17] , i.e., they assumed that all links remain stable over time [18] . This assumption does not hold true in practice since links often break down [19] and new ones form [20] . Therefore, it is necessary to take into account the dynamics of interactions [21] .", "paraphrased_abstract": "The importance of interdependence between the various components of a complex system has been seen to be important, especially in the case of calamities such as epidemics, traffic jams, financial crises, blackouts and blackouts. This work was made in order to investigate the vulnerability of interdependent networks under cascading failures, in which the nodes are subject to random attacks, and the neighbors are subject to calamities. This approach does not work in practice; instead it takes into account the dynamics of the connections. Moreover, social networks are dependent on each other's opinions; if someone is ill, or loses her job, it may affect the health of others and the income of the others. In the past, many studies have been conducted that interdependence plays a crucial role in the resilience of complex systems. The recent crisis of the power grid, caused by an unprecedented series of disasters, emphasized the importance of interdependence among all its elements. In this work, we study the vulnerability of interdependent networks against random attacks, where nodes are subject to random attacks and where the neighbors are also liable to failure. We show that by increasing the number of attackers or decreasing the recovery rate, the network is less vulnerable. In the case of interdependence, for example, the generators provide electricity to the substations; if one of the substations fails,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2304, "title": "Inter-network regions of the Sun at millimetre wavelengths", "abstract": "  The continuum intensity at wavelengths around 1 mm provides an excellent way to probe the solar chromosphere. Future high-resolution millimetre arrays, such as the Atacama Large Millimeter Array (ALMA), will thus produce valuable input for the ongoing controversy on the thermal structure and the dynamics of this layer. Synthetic brightness temperature maps are calculated on basis of three-dimensional radiation (magneto-)hydrodynamic (MHD) simulations. While the millimetre continuum at 0.3mm originates mainly from the upper photosphere, the longer wavelengths considered here map the low and middle chromosphere. The effective formation height increases generally with wavelength and also from disk-centre towards the solar limb. The average intensity contribution functions are usually rather broad and in some cases they are even double-peaked as there are contributions from hot shock waves and cool post-shock regions in the model chromosphere. Taking into account the deviations from ionisation equilibrium for hydrogen gives a less strong variation of the electron density and with it of the optical depth. The result is a narrower formation height range. The average brightness temperature increases with wavelength and towards the limb. The relative contrast depends on wavelength in the same way as the average intensity but decreases towards the limb. The dependence of the brightness temperature distribution on wavelength and disk-position can be explained with the differences in formation height and the variation of temperature fluctuations with height in the model atmospheres. ", "machine_abstract": "We present new observations made with the Atacama Large Millimeter/submillimeter Array (ALMA) of two inter-network sunspots in active region NOAA AR 12192 on 2013 May 24 and 25, respectively. The first sunspot was observed for about 3 hours during which time it rotated by more than 90 degrees. We find that this sunspot is composed of several magnetic flux tubes with different orientations. In addition to these features we also observe an extended bright feature located between the main sunspot umbrae. This feature has been previously reported as a penumbral filament but our data show no evidence of such structure. Instead, we interpret this feature as a coronal rain blob. The second sunspot was observed for only 1 hour before being occulted by Earths atmosphere. During this observation period the sunspot rotated by less than 30 degrees. Our analysis shows that both sunspots are surrounded by a dark lane which may be associated with the moat surrounding large sunspots.", "paraphrased_abstract": "The first sunspot was observed for about three hours, which was about 90 degrees, then it rescinded. This sunspot had a magnetic smear, but the data we have show it to be a smear. The second sunspot was observed for only a moment, after which it was occupied by the atmosphere. This smear was observed by about half a millimeter, which is not much different from the previous one. In the first sunspot we have found several magnetic fluxes, which have different orientations. We see that the sunspots are surrounded by a dark lane that is probably the result of a large moat surrounded by a large sunspot. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.168, "title": "Carbon Nanotube Thin Film Field Emitting Diode: Understanding the System Response Based on Multiphysics Modeling", "abstract": "  In this paper, we model the evolution and self-assembly of randomly oriented carbon nanotubes (CNTs), grown on a metallic substrate in the form of a thin film for field emission under diode configuration. Despite high output, the current in such a thin film device often decays drastically. The present paper is focused on understanding this problem. A systematic, multiphysics based modelling approach is proposed. First, a nucleation coupled model for degradation of the CNT thin film is derived, where the CNTs are assumed to decay by fragmentation and formation of clusters. The random orientation of the CNTs and the electromechanical interaction are then modeled to explain the self-assembly. The degraded state of the CNTs and the electromechanical force are employed to update the orientation of the CNTs. Field emission current at the device scale is finally obtained by using the Fowler-Nordheim equation and integration over the computational cell surfaces on the anode side. The simulated results are in close agreement with the experimental results. Based on the developed model, numerical simulations aimed at understanding the effects of various geometric parameters and their statistical features on the device current history are reported. ", "machine_abstract": "Carbon nanotubes (CNTs) are promising materials for field emission devices due to their unique physical and chemical properties, such as high aspect ratio, low work function, and excellent mechanical strength.  In this study, we present an integrated multiphysics model that can be used to simulate the system response of CNT-based field emitting diodes (FEDs). The proposed model consists of three sub-models: 1) electron transport in CNT; 2) electrostatic potential distribution; 3) current density distribution. We have developed these models using COMSOL Multiphysics software package with built-in physics modules. To verify our simulation results, we fabricated a CNT-FED device by growing vertically aligned CNTs onto silicon substrate via plasma-enhanced chemical vapor deposition method followed by sputtering gold film over them. Our experimental data show good agreement with simulated results obtained from the proposed model.", "paraphrased_abstract": "The proposed model is composed of three submodels: 1) the electron transport in the CNT; 2) the electrostatic potential; 3) the current density; the simulations are based on the COMSOL Multiphysics, which has its own module of physics. In this paper, we present an integrated model of the reaction of the CNTs with FEDs. The present model consists of three submodels: 1) the electron transport in the CNT; 2) the electrostatic potential; 3) the current density. The carbon nanotubes are promising for FEDs because of their special physical properties, high density, high strength, and high strength. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1155, "title": "Evolution of interstellar dust and stardust in the solar neighbourhood", "abstract": "  The abundance evolution of interstellar dust species originating from stellar sources and from condensation in molecular clouds in the local interstellar medium of the Milky Way is studied and the input of dust material to the Solar System is determined. A one-zone chemical evolution model of the Milky Way for the elemental composition of the disk combined with an evolution model for its interstellar dust component similar to that of Dwek (1998) is developed. The dust model considers dust-mass return from AGB stars as calculated from synthetic AGB models combined with models for dust condensation in stellar outflows. Supernova dust formation is included in a simple parameterized form which is gauged by observed abundances of presolar dust grains with supernova origin. For dust growth in the ISM a simple method is developed for coupling this with disk and dust evolution models. The time evolution of the abundance of the following dust species is followed in the model: silicate, carbon, silicon carbide, and iron dust from AGB stars and from SNe as well as silicate, carbon, and iron dust grown in molecular clouds. It is shown that the interstellar dust population is dominated by dust accreted in molecular clouds; most of the dust material entering the Solar System at its formation does not show isotopic abundance anomalies of the refractory elements, i.e., inconspicuous isotopic abundances do not point to a Solar System origin of dust grains. The observed abundance ratios of presolar dust grains formed in SN ejecta and in AGB star outflows requires that for the ejecta from SNe the fraction of refractory elements condensed into dust is 0.15 for carbon dust and is quite small ($\\sim10^{-4}$) for other dust species. ", "machine_abstract": "We present an analysis of the evolution of interstellar dust grains, based on their size distribution inferred by infrared observations with ISO (Infrared Space Observatory). We find that the grain growth is dominated by coagulation at all times since the formation of the Sun. The total mass density of dust increases by about one order of magnitude during this time span. This increase can be explained by accretion of gas-phase metals onto pre-existing grains or condensation of new material out of the gas phase. In addition to these processes we also consider fragmentation as well as shattering due to collisions between particles. Fragmentation dominates over coagulation for small grains but becomes less important when the grains grow larger than 0.1 micrometres. For large grains shattering leads to a decrease in number density which counteracts the effect of coagulation. Our results are consistent with previous studies using different methods.     Keywords: Interstellar medium", "paraphrased_abstract": "This is consistent with other previous studies on the same subject. We present a study of the evolution of interstellar dust, based on the size distribution of the infrared radiation with ISO. We find that the grain growth is dominated by coagulation since the formation of the Sun. The increase in the mass of dust is caused by the accretion of metals into the preexisting particles, or by the condensation of new materials out of the gas phase. Fragmentation is most important for small grains, but less important for larger ones. In the case of large ones, shattering leads to a decrease in density, countering the effect of coagulation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2668, "title": "Super-Poissonian Shot Noise as a Measure of Dephasing in Closed Quantum Dots", "abstract": "  Electron-electron interactions play a major role in determining the low-temperature rate of phase loss of electrons in mesoscopic systems. The study of the dephasing rate is expected to contribute to the understanding of the many-body nature of such systems. Closed quantum dots are of special interest in this respect, due to theoretical predictions suggesting a possible transition temperature below which the dephasing rate vanishes. This prediction has attracted much attention, since closed quantum dots are prime candidates for storage units in quantum computers, and thus their phase coherence properties are of great importance. However, an effective method for measuring the dephasing rate within a closed quantum dot is still lacking. Here we study two-level systems and show that the Fano factor has a sharp peak as a function of the chemical potential, the location of which can be simply related to the dephasing rate. We thus suggest to use the properties of the Fano factor peak in the super-Poissonian regime as a probe for the dephasing rate. ", "machine_abstract": "We report on the observation of super-Poissonian shot noise in closed quantum dots (QDs). We show that this effect is due to dephasing and can be used for its characterization. The QD emission linewidth was measured by scanning Fabry-Perot interferometry, while the photon statistics were studied using Hanbury Brown-Twiss experiments with two avalanche photodiodes operating at 1 GHz bandwidth. In order to study the influence of temperature we performed these measurements between 4 K and 300 K. At low temperatures, below 50 K, the QD emission linewidth decreases exponentially with decreasing temperature following an activation energy of about 0.3 meV. Above 100 K it follows a power law dependence with T-1/2. Super-Poissonian shot-noise appears above 60 K and increases rapidly up to room temperature where it reaches values more than twice those expected for Poissonian light.", "paraphrased_abstract": "The measurement of the amplitude of the QDs was made by scanning Fabry-Perot interferometry, and the photon statistics were analyzed with the brown-twiss experiment with two avalanche photodiodes at a bandwidth of one GHz. The super-Poissonian phase noise appeared at the lowest temperature of 60 K, and gradually rose to the upper temperature of 100 K, at which point it grew to a value of more than twice that expected by Poissonian light. We also present an analysis of the super-Poissonian phase noise in closed quantum dots (QDs). We show that this phase noise is due to dephasing and that it can be utilised in the determination of its properties. We present the observation of super-Poissonian phase noise in closed quantum dots, with the aim of demonstrating that this phenomena is based on the dephasing of the physics, and that this effect is useful for determining its properties. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1892, "title": "Unambiguous coherent state identification: Searching a quantum database", "abstract": "  We consider an unambiguous identification of an unknown coherent state with one of two unknown coherent reference states. Specifically, we consider two modes of an electromagnetic field prepared in unknown coherent states alpha_1 and alpha_2, respectively. The third mode is prepared either in the state alpha_1 or in the state alpha_2. The task is to identify (unambiguously) which of the two modes are in the same state. We present a scheme consisting of three beamsplitters capable to perform this task. Although we don't prove the optimality, we show that the performance of the proposed setup is better than the generalization of the optimal measurement known for a finite-dimensional case. We show that a single beamsplitter is capable to perform an unambiguous quantum state comparison for coherent states optimally. Finally we propose an experimental setup consisting of 2N-1 beamsplitters for unambiguous identification among N unknown coherent states. This setup can be considered as a search in a quantum database. The elements of the database are unknown coherent states encoded in different modes of an electromagnetic field. The task is to specify the two modes that are excited in the same, though unknown, coherent state. ", "machine_abstract": "We present an algorithm for searching in databases that are stored as qubits, which is the basic unit of information in quantum computers. The search problem can be formulated by using Grover's algorithm and its variants to find one or more solutions among many possibilities. We show how this approach can be used to solve problems such as finding a particular molecule within a large chemical compound library. Our results demonstrate that it may be possible to use quantum algorithms to accelerate searches on future quantum computer hardware. Quantum computing has been proposed as a new paradigm for solving computational problems with applications ranging from chemistry to optimization theory [1-3]. In contrast to classical computers, where data is represented by bits (0s or 1s), quantum computers store information in qubits, which can take any superposition of 0s and 1s [4] . This feature allows quantum computers to perform certain computations exponentially faster than their classical counterparts [5] . In order to make practical use of these advantages, however, we need efficient ways to implement quantum algorithms [6] , including those based on Grover's algorithm [7-9]. Here, we propose a method for identifying unambiguously a single solution out of multiple possibilities [10] . As an example application, our technique could be used to identify a specific molecule within a larger chemical compound library [11] .", "paraphrased_abstract": "The quantum computer is a new system for the analysis of problems, from chemistry to optimization. In contrast to the classical computers, which are based on bits and syllables, quantum computers contain information in syllables, whose contents can be superposed of syllables and 0s. This enables quantum computers to perform a wide variety of computations faster than classical computers. Our result shows how the method can be used to solve problems on future quantum computers. We present an algorithm for the analysis of databases in which information is stored in syllables. We show how it can be applied to solving the search for a particular molecule in a large chemical library. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0454, "title": "An Extrasolar Planet Census with a Space-based Microlensing Survey", "abstract": "  A space-based gravitational microlensing exoplanet survey will provide a statistical census of exoplanets with masses down to 0.1 Earth-masses and orbital separations ranging from 0.5AU to infinity. This includes analogs to all the Solar System's planets except for Mercury, as well as most types of planets predicted by planet formation theories. Such a survey will provide results on the frequency of planets around all types of stars except those with short lifetimes. Close-in planets with separations < 0.5 AU are invisible to a space-based microlensing survey, but these can be found by Kepler. Other methods, including ground-based microlensing, cannot approach the comprehensive statistics on the mass and semi-major axis distribution of extrasolar planets that a space-based microlensing survey will provide. The terrestrial planet sensitivity of a ground-based microlensing survey is limited to the vicinity of the Einstein radius at 2-3 AU, and space-based imaging is needed to identify and determine the mass of the planetary host stars for the vast majority of planets discovered by microlensing. Thus, a space-based microlensing survey is likely to be the only way to gain a comprehensive understanding of the nature of planetary systems, which is needed to understand planet formation and habitability. The proposed Microlensing Planet Finder (MPF) mission is an example of a space-based microlensing survey that can accomplish these objectives with proven technology and a cost that fits comfortably under the NASA Discovery Program cost cap. ", "machine_abstract": "We present the results of an analysis of microlensing events detected by the Optical Gravitational Lensing Experiment (OGLE) and its follow-up network, including the Microlensing Observations in Astrophysics collaboration (MOA). We have identified 16 new planets among these events using high-precision photometry obtained at Subaru Observatory. The masses of all but one planet are determined to be less than 1 M\u2295 . Fourteen of them are found to orbit stars more massive than 0.5 M\u2299 , while two others are around low-mass dwarfs. These planets are located between 0.1 AU and 4 AU away from their host stars. This is the first time that such a large number of extrasolar planets has been discovered through space-based microlensing surveys. Our sample includes several planets whose orbits lie close to or beyond Neptune's semi-major axis. In addition, we find evidence for planetary companions to three binary systems.", "paraphrased_abstract": "It is the first time that such a large number of extrasolar planets has been discovered in space by microlensing. This is the first time that the microlensing probes have been detected in space., it is a new group of extrasolar planets whose orbits lie between Neptune and Neptune, and the planetary companions in these three binary systems are also found. There are sixteen new planets discovered from the observations of the optical gravitational lensing experiment of the OGLLE, the supplementary network of the MOA, and four more planets orbiting stars of a size smaller than 0.5 M, four more are dwarfs, and two more are surrounded by a dwarf. The mass of the remaining six planets is estimated to be less than one megatons, and the spherical mass of the planets is found to be less than one megatons, and two are confined to the very low mass of their host stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0654, "title": "An integral field spectroscopic survey for high redshift damped Lyman-alpha galaxies", "abstract": "  We search for galaxy counterparts to damped Lyman-alpha absorbers (DLAs) at z>2 towards nine quasars, which have 14 DLAs and 8 sub-DLAs in their spectra. We use integral field spectroscopy to search for Ly-alpha emission line objects at the redshifts of the absorption systems. Besides recovering two previously confirmed objects, we find six statistically significant candidate Ly-alpha emission line objects. The candidates are identified as having wavelengths close to the DLA line where the background quasar emission is absorbed. In comparison with the six currently known Ly-alpha emitting DLA galaxies the candidates have similar line fluxes and line widths, while velocity offsets between the emission lines and systemic DLA redshifts are larger. The impact parameters are larger than 10 kpc, and lower column density systems are found at larger impact parameters. Assuming that a single gas cloud extends from the QSO line of sight to the location of the candidate emission line, we find that the average candidate DLA galaxy is surrounded by neutral gas with an exponential scale length of ~5 kpc. ", "machine_abstract": "We present the results of an optical integral-field spectroscopy (IFS) survey to find high-redshift Damped Lyman Alpha Galaxies (DLAs). The sample consists of 16 DLAs at redshifts between 2 and 4, selected by their strong Mg II absorption lines in quasar spectra observed with the Keck telescope. We use the Gemini MultiObject Spectrograph on the 8-meter Gemini North Telescope to obtain spatially resolved observations over a 1 arcsec2 area centered around each absorber. Our IFS data are used to study the kinematics of these absorbers as well as their physical properties such as metallicity and dust content. In addition we compare our new measurements with previous studies based on single-aperture spectrophotometry. This work is part of the Gemini Observatory program GN-2007B-Q-206. Keywords: High-redshift universe; Galaxy evolution; Absorber-galaxy relationship", "paraphrased_abstract": "This research is part of the project of the Gemini Observatory. The program is GN-2007-Q-206. The spectroscopic observations we make are from the eight-meter Gemini North Telescope, and the measurements we make are spatially resolved over a single arcsec of space, around which the sulcus is observed. We have measured a number of sulcus-shaped DLAs from 2 to 4 redshifts, based on their high Mg II absorption lines. Our results provide an in-depth account of the kinematics of these sulcus-like substances, their metallicity and their content in dust. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.07, "title": "Detailed kinetic study of the ring opening of cycloalkanes by CBS-QB3 calculations", "abstract": "  This work reports a theoretical study of the gas phase unimolecular decomposition of cyclobutane, cyclopentane and cyclohexane by means of quantum chemical calculations. A biradical mechanism has been envisaged for each cycloalkane, and the main routes for the decomposition of the biradicals formed have been investigated at the CBS-QB3 level of theory. Thermochemical data (\\delta H^0_f, S^0, C^0_p) for all the involved species have been obtained by means of isodesmic reactions. The contribution of hindered rotors has also been included. Activation barriers of each reaction have been analyzed to assess the 1 energetically most favorable pathways for the decomposition of biradicals. Rate constants have been derived for all elementary reactions using transition state theory at 1 atm and temperatures ranging from 600 to 2000 K. Global rate constant for the decomposition of the cyclic alkanes in molecular products have been calculated. Comparison between calculated and experimental results allowed to validate the theoretical approach. An important result is that the rotational barriers between the conformers, which are usually neglected, are of importance in decomposition rate of the largest biradicals. Ring strain energies (RSE) in transition states for ring opening have been estimated and show that the main part of RSE contained in the cyclic reactants is removed upon the activation process. ", "machine_abstract": "The reaction mechanism for the ring-opening reactions of cyclic alkanes with water has been studied using density functional theory (DFT) and ab initio molecular orbital methods at the B3LYP/6-311++G(d,p), MP2/6-31+G*, QCISD/6-31+G* levels in conjunction with the CBS-QB3 method to calculate enthalpies of formation.  The results show that the activation energies are lower than those obtained previously by DFT or semiempirical methods. In addition, it is found that the transition states have one imaginary frequency along the reaction coordinate corresponding to the breaking of C-H bonds on both sides of the carbonyl group. The calculated rate constants agree well with experimental data over a wide temperature range. Finally, we propose an explanation for the observed differences between theoretical predictions and experiments based on the fact that the solvent effect was not taken into account in previous studies.", "paraphrased_abstract": "Consequently, the coefficients of the redox and the redox coefficients of the redox coefficients were calculated to the limits of a very wide temperature range. Moreover, the transition states had an imaginary frequency along the direction of the reaction that corresponded to the breaking of C-H bonds on both sides of the carbonyl group. We then formulated an explanation for the differences between predictions and experimental results, by claiming that the solvent effect was not taken into account in previous studies. The redox coefficients were found to be lower than those of the DFT or semiempirical method. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4424, "title": "Magnetic fluctuations in n-type high-$T_c$ superconductors reveal breakdown of fermiology", "abstract": "  By combining experimental measurements of the quasiparticle and dynamical magnetic properties of optimally electron-doped Pr$_{0.88}$LaCe$_{0.12}$CuO$_4$ with theoretical calculations we demonstrate that the conventional fermiology approach cannot possibly account for the magnetic fluctuations in these materials. In particular, we perform tunneling experiments on the very same sample for which a dynamical magnetic resonance has been reported recently and use photoemission data by others on a similar sample to characterize the fermionic quasiparticle excitations in great detail. We subsequently use this information to calculate the magnetic response within the conventional fermiology framework as applied in a large body of work for the hole-doped superconductors to find a profound disagreement between the theoretical expectations and the measurements: this approach predicts a step-like feature rather than a sharp resonance peak, it underestimates the intensity of the resonance by an order of magnitude, it suggests an unreasonable temperature dependence of the resonance, and most severely, it predicts that most of the spectral weight resides in incommensurate wings which are a key feature of the hole-doped cuprates but have never been observed in the electron-doped counterparts. Our findings strongly suggest that the magnetic fluctuations reflect the quantum-mechanical competition between antiferromagnetic and superconducting orders. ", "machine_abstract": "We report the observation of magnetic fluctuations at low temperatures and high fields in single crystals of YBa2Cu3O6+x (YBCO) with x=0.4, 0.45, and 0.5 using muon spin relaxation measurements. The data show that these materials are characterized by an unusual temperature dependence of the fluctuation rate which is not consistent with predictions based on Fermi liquid theory or any other conventional model for fermionic quasiparticles. We argue that this behavior can be understood within a phenomenological description of the electronic excitations as bosonic collective modes. These results provide strong evidence against the existence of well-defined fermionic quasiparticles in the normal state of these compounds. They also suggest that the pseudogap phase may have some features in common with the superfluid state.     High-temperature cuprate superconductors exhibit many remarkable properties including a rich variety of competing ground states. In particular, it has been suggested that they undergo a quantum phase transition into a novel ordered state known as the \"pseudogap\" phase [1] . This phase appears to exist between the underdoped regime where there is no static order but only short-range correlations [2] , and the overdoped regime where antiferromagnetism disappears [3] . It is believed that the pseudogap state plays an important role in understanding the mechanism responsible for high-Tc superconductivity [4] . In recent years much attention has focused on the possibility that the pseudogap is associated with preformed pairs of charge carriers [5] . However, despite considerable experimental effort [6] , direct evidence for such pairing remains elusive [7, 8] . One possible explanation for this lack of success is that the pseudogap does not arise directly from pair formation [9] . Instead, it could result from the condensation of another type of collective mode [10] . For example, if the pseudogap were related to the onset of density wave ordering [11] then one would expect to see signatures of its presence in the form of low-energy magnetic fluctuations [12] . Indeed, several experiments have reported the detection of such fluctuations [13] - [16] .", "paraphrased_abstract": "\u201cThis is the first time that we have observed a pronounced ro-ta-ta at a low temperature, and we will not be able to make any generalization of the nature of the ro-ta-ta-ta; we will explain why. The reason is that the ro-ta-ta-ta is not a fusion of a pair of charge carriers, but the formation of a new symmetral state. It is said that this symmetral state is associated with a pair of charged carriers, and in the early days of the density of the order there were signs of a symmetral order, a p-disordered symmetrality, a weak and magnetic gyro, a low energy, low magnetic flux. Moreover, the symmetral gyro, which is known by the name of the p-disordered gyro, is known by the name of the p-disordered gyro. The symbol of these crystals is a peculiarity of temperature and magnetic flux that is not consistent with the predictions based on Fermi liquid theory or the orthodox theories for fermionic quasi", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.11, "title": "A New Tool for Seismology--the Cumulative Spectral Power", "abstract": "  The power spectral density (PSD) function is commonly used to specify seismometer performance. It is derived from the FFT of acceleration and correction is made for the transfer function of the instrument that generated the data. As with any such spectrum of density (`per Hz') type, the noise inherent to a PSD is large. This article illustrates the value of a function that is derived from the PSD and for which the influence of noise is significantly reduced. Called the cumulative spectral power (CSP), it is obtained from the PSD through the noise-reducing process of integration. The maximum of the CSP (corresponding to the longest graphed value of the period) provides a means for estimating the total vibrational power of the earth. The present author has significantly simplified the process of PSD generation. Thus routine graphing is straightforwared-of first the FFT, followed by the generation of both a PSD and its associated CSP. The unique properties of the CSP make it valuable for the study of a variety of earth dynamics. For example, the strking simplicity of a CSP graph generated from a record containing a strong teleseismic earthquake is undoubtedly important to the development and refinement of any viable theory of earthquake dynamics. ", "machine_abstract": "The cumulative spectral power (CSP) is introduced as an alternative to the traditional method in analyzing earthquake data, which has been widely used by seismologists and geophysicists since it was first proposed by Aki(1957). The new tool can be applied to both earthquakes with known locations and those without any information on their epicenters. It also provides more detailed information about the source mechanism than that obtained using the traditional method. In this study we apply the new technique to analyze two large earthquakes occurred in China during recent years. We find that the results are consistent with previous studies based on other methods. This suggests that the new tool may provide useful information for studying seismic activities. Keywords: Earthquake; Source mechanism; Cumulative spectral power. 1 Introduction. Seismological research plays an important role in understanding the physical processes involved in earthquakes. Since its introduction into seismology by Aki(1957), the traditional method of calculating the cumulative energy released by earthquakes has become one of the most popular techniques among seismologists and geophysics researchers. However, there have been some problems associated with this method such as: i)it requires accurate location of the epicenter before analysis; ii)the result depends strongly upon the choice of time window length; iii)it cannot give enough information about the source mechanisms of earthquakes. To overcome these difficulties, many attempts have been made recently to develop new tools for analyzing earthquake data. For example, Sato et al.(1989) , Kao & Chen(1990) , Wu & Chiao(1991a , 1991b )and Wu(1993 developed different approaches to calculate the cumulative energy release rate. These authors found that the results were not always consistent with each other due to differences in the assumptions they had adopted.", "paraphrased_abstract": "He did not have the same result as the previous one, but had some difficulties. He used a fixed period of time, which was not too short. He regarded this as a very incomplete calculation. He used the averaging method for the calculation of the cumulative strength of earthquakes, which he had previously invented. But there were many difficulties in the calculation, as follows: first, the method required the accurate position of the epicenter before it was applied to the data, second, the calculation was influenced by the duration of the window of the time, third, and it was not enough to give an accurate account of the sources. The study of the two largest earthquakes in China was undertaken, in this way, by Sato et al. (1991), Kao Chen and Wu, (1990) and Wu et al., (1991a) and (1993b) and Wu (1993) were unable to obtain the corresponding data. These problems were subsequently overcome by the application of new methods. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2113, "title": "Scattering amplitudes in strongly coupled N=4 SYM from semiclassical strings in AdS", "abstract": "  Very recently in arXiv:0705.0303 Alday and Maldacena gave a string theory prescription for computing (all) planar amplitudes in N=4 supersymmetric gauge theory at strong coupling using the AdS/CFT correspondence. These amplitudes are determined by a classical string solution and contain a universal exponential factor involving the action of the classical string. On the gauge theory side, expressions for perturbative amplitudes at strong coupling were previously proposed only for specific helicities of external particles -- the maximally helicity violating or MHV amplitudes. These follow from the exponential ansatz of Bern, Dixon and Smirnov for MHV amplitudes in N=4 SYM. In this paper we examine the amplitudes dependence on helicities and particle-types of external states. We consider the prefactor of string amplitudes and give arguments suggesting that the prefactor at strong coupling should be the same as the Yang-Mills tree-level amplitude for the same process. This implies that scattering amplitudes in N=4 SYM simplify dramatically in the strong coupling limit. It follows from our proposal that in this limit all (MHV and non-MHV) n-point amplitudes are given by the (known) tree-level Yang-Mills result times the helicity-independent (and particle-type-independent) universal exponential. ", "machine_abstract": "We study the scattering amplitudes for gluons and gravitons at strong coupling using classical string solutions in Anti-de Sitter space (AdS). We show that these amplitudes can be obtained by evaluating certain correlation functions on the worldsheet boundary, which are related to tree-level gauge theory amplitudes via holography. The results agree with those found previously using integrability techniques. In addition we find new contributions to the graviton-graviton amplitude involving an infinite tower of massive states. These arise because our solution is not invariant under global Poincare transformations; they correspond to corrections to the supergravity action induced by higher derivative terms in the bulk effective field theory.  Introduction  The AdS/CFT correspondence [1] relates type IIB superstrings propagating in ten-dimensional anti-de Sitter space-time (AdS) to conformal field theories living on its four-dimensional boundary. This duality has been used extensively over recent years as a tool to explore non-perturbative phenomena in quantum gravity [2] . It also provides a novel approach to studying strongly-coupled gauge theories such as QCD [3] . In this talk we will consider the simplest example of the AdS/CFT correspondence -the maximally supersymmetric Yang-Mills (N=4 SYM) theory [4] , whose dual description involves type IIA strings moving in AdS 5 \u00d7 S 5 [5] . At weak 't Hooft coupling \u03bb = g 2 Y M N \u226a 1, where g Y M denotes the Yang-Mills coupling constant, perturbative calculations have shown that the two descriptions match exactly [6] . However, it remains unclear how to calculate quantities like scattering amplitudes directly within the gauge theory at large values of \u03bb [7, 8] . On the other hand, one may use the AdS/CFT dictionary [9] to translate between observables calculated in either side of the duality. For instance, the expectation value of Wilson loops in the gauge theory corresponds to the area of minimal surfaces embedded into AdS [10] ; while n-point correlators of local operators in the gauge theory are given by functional integrals over n-punctured Riemann surfaces [11] .", "paraphrased_abstract": "The AdS/CFT correspondence is an important and widely used tool in the study of non-perturbative phenomena in quantum gravity. It is a novel approach to studying the elasticity of gauge theories, such as QCD. Moreover, we will present new contributions to the elasticity of gravitons and gravitons, the infinite tower of massive states, and its entanglement in a soaring axis. We will present the simplest example of the AdS/CFT correspondence: the maximal supersymmetric Yang-Mills theory based on the theory of type IIA strings in AdS5. The assumption of the Wilson loop in the gauge theory corresponds to the area of minimal surfaces embedded in AdS, while the function of local operators in the gauge theory is expressed by functional integrals of the Riemann surface,. In the gauge theory, the expectation of Wilson loops corresponds to the area of minimal surfaces embedded in AdS, while n points of the functional integrals of local operators in the gauge theory are given by functional integrals of the Riemann surface. This connection is based on the fact that our solution does not change under the global Poincare transformations, and consequently corrects the supergravity induced by higher derivative terms in the core effective field theory. We also consider the graviton-graviton amplitude by means of an infinite", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2689, "title": "Mass loss from Luminous Blue Variables and Quasi-Periodic Modulations of Radio Supernovae", "abstract": "  Massive stars, supernovae (SNe), and long-duration gamma-ray bursts (GRBs) have a huge impact on their environment. Despite their importance, a comprehensive knowledge of which massive stars produce which SN/GRB is hitherto lacking. We present a brief overview about our knowledge of mass loss in the Hertzsprung-Russell Diagram (HRD) covering evolutionary phases of the OB main sequence, the unstable Luminous Blue Variable (LBV) stage, and the Wolf-Rayet (WR) phase. Despite the fact that metals produced by ``self-enrichment'' in WR atmospheres exceed the initial -- host galaxy -- metallicity, by orders of magnitude, a particularly strong dependence of the mass-loss rate on the initial metallicity is found for WR stars at sub-solar metallicities (1/10 -- 1/100 solar). This provides a significant boost to the collapsar model for GRBs, as it may present a viable mechanism to prevent the loss of angular momentum by stellar winds at low metallicity, whilst strong Galactic WR winds may inhibit GRBs occurring at solar metallicities. Furthermore, we discuss recently reported quasi-sinusoidal modulations in the radio lightcurves of SNe 2001ig and 2003bg. We show that both the sinusoidal behaviour and the recurrence timescale of these modulations are consistent with the predicted mass-loss behaviour of LBVs, and we suggest LBVs may be the progenitors of some core-collapse SNe. ", "machine_abstract": "We present the results of our study on mass-loss rates in luminous blue variables (LBVs) based on radio observations at 1.4 GHz with the VLA, as well as optical spectroscopy obtained by us or taken from the literature.  We find that LBV stars have typical mass-loss rates between 10^-6 M_sun/yr to 10^-4 M_sun/yr. The mass-loss rate is found to be correlated with luminosity but not with stellar radius. In addition we report quasi-periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic changes in their circumstellar environments. These variations may also explain why these two objects were observed to undergo large amplitude outbursts during their late stages. This research was supported by NASA grant NAG5-7262. Keywords: Mass loss, Stellar evolution", "paraphrased_abstract": "I will present our findings on the mass-loss rate in luminous blue variables (LBVs) based on observations from a 1.3 GHz radiance from the VLA, and optical spectroscopy that we have taken from the literature. We show the occurrence of periodic oscillations of radio supernovae in the constellations SN 1987A and SN 1993J, which are likely to be a result of a periodic change in their circumstellar environment. These variations may explain the large-scale events observed in these two objects during their late stages. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3671, "title": "Quantum Hall ferromagnetism in graphene: a SU(4) bosonization approach", "abstract": "  We study the quantum Hall effect in graphene at filling factors \\nu = 0 and \\nu = \\pm, concentrating on the quantum Hall ferromagnetic regime, within a non-perturbative bosonization formalism. We start by developing a bosonization scheme for electrons with two discrete degrees of freedom (spin-1/2 and pseudospin-1/2) restricted to the lowest Landau level. Three distinct phases are considered, namely the so-called spin-pseudospin, spin, and pseudospin phases. The first corresponds to a quarter-filled (\\nu =-1) while the others to a half-filled (\\nu = 0) lowest Landau level. In each case, we show that the elementary neutral excitations can be treated approximately as a set of n-independent kinds of boson excitations. The boson representation of the projected electron density, the spin, pseudospin, and mixed spin-pseudospin density operators are derived. We then apply the developed formalism to the effective continuous model, which includes SU(4) symmetry breaking terms, recently proposed by Alicea and Fisher. For each quantum Hall state, an effective interacting boson model is derived and the dispersion relations of the elementary excitations are analytically calculated. We propose that the charged excitations (quantum Hall skyrmions) can be described as a coherent state of bosons. We calculate the semiclassical limit of the boson model derived from the SU(4) invariant part of the original fermionic Hamiltonian and show that it agrees with the results of Arovas and co-workers for SU(N) quantum Hall skyrmions. We briefly discuss the influence of the SU(4) symmetry breaking terms in the skyrmion energy. ", "machine_abstract": "We study the quantum Hall effect (QHE) and its interplay with magnetism in monolayer graphene by using an effective low-energy model that takes into account both electron-electron interactions and disorder effects. We show how to derive this model starting from first principles, and we discuss some of its main properties. In particular, we find that at half-filling it exhibits two different phases depending on the strength of the Coulomb interaction between electrons. For weak coupling these are separated by a phase transition driven by spontaneous breaking of time-reversal symmetry; for strong coupling they correspond respectively to a conventional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states. The latter is shown to be equivalent to a spin-1/2 Heisenberg antiferromagnet on the honeycomb lattice, which can thus be described within the framework of the so-called SU(4) bosonic representation.", "paraphrased_abstract": "The resulting model is based on a very low-energy, low-energy, recursive, and chaotic model. We describe this model from the first principles and show some of its properties. We show that, as the aquiline is bonded at half-filling, it has two phases according to the strength of the Coulomb interaction between the electrons. The weak one is separated by a phase transition, which is a natural phenomenon, while the strong one is the property of a novel fractionalized topological insulator based on chiral edges. This is a counterpart to the chiral edge of the honeycomb lattice, which is described in the framework of the so-called SU(4) representation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3154, "title": "Solar heavy element abundance: constraints from frequency separation ratios of low-degree p modes", "abstract": "  We use very precise frequencies of low-degree solar-oscillation modes measured from 4752 days of data collected by the Birmingham Solar-Oscillations Network (BiSON) to derive seismic information on the solar core. We compare these observations to results from a large Monte Carlo simulation of standard solar models, and use the results to constrain the mean molecular weight of the solar core, and the metallicity of the solar convection zone. We find that only a high value of solar metallicity is consistent with the seismic observations. We can determine the mean molecular weight of the solar core to a very high precision, and, dependent on the sequence of Monte Carlo models used, find that the average mean molecular weight in the inner 20% by radius of the Sun ranges from 0.7209 to 0.7231, with uncertainties of less than 0.5% on each value. Our lowest seismic estimate of solar metallicity is Z=0.0187 and our highest is Z=0.0239, with uncertainties in the range of 12--19%. Our results indicate that the discrepancies between solar models constructed with low metallicity and the helioseismic observations extend to the solar core and thus cannot be attributed to deficiencies in the modeling of the solar convection zone. ", "machine_abstract": "We present new solar models that are consistent with the helioseismic data and reproduce the observed frequencies of acoustic modes in the Sun to better than 0.1% accuracy for degrees l = 0, 1, 2, 3 (n = 0 - 4). We use these models to calculate theoretical frequency separations between adjacent radial orders n for different degree l values as functions of the mean density $\\rho_0$ at the base of the convection zone. The results show that the ratio of the frequency separations for l = 1 and l = 0 is sensitive to the heavy-element abundances near the surface of the Sun. This sensitivity can be used to determine the relative amount of heavy elements in the outer layers of the Sun by comparing the calculated frequency separations with those inferred from observations.     In this work we apply our method to infer the heavy-element abundance profile in the Sun using the recently published high-precision measurements of the mode frequencies obtained by GOLF on board SOHO spacecraft. Our analysis shows that the best-fit model has an iron abundance of $A(Fe) \\sim 7$ times solar value at the photosphere.", "paraphrased_abstract": "This work applies our method to infer the heavy abundance in the Sun by means of the new high-precision measurements of GOLF from the SOHO spacecraft. Our findings show that the ideal model is based on the highest intensity of iron, whose total amount is one hundred and eighty tenths of the solar mass. Our calculation, taking into account the solar abundance, produces the corresponding fractions of the two classes of acoustic elements, and is therefore able to recompense the relative abundance of the heavy elements to the density of the lower layer of the Sun. We find that the ratio of the radial fractions for l = 1 and l = 0 is influenced by the abundance of heavy elements, near the surface of the Sun, as a function of the density at the base of the convection zone. We present a new solar model, in accordance with the helioseismic data, and the corresponding acoustic frequencies of the Sun are reproduced to more than 0.1% accuracy in degree l. We also calculate the ratio of the theoretical frequency separation between radial orders n in the degree l. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1761, "title": "GLSM's for partial flag manifolds", "abstract": "  In this paper we outline some aspects of nonabelian gauged linear sigma models. First, we review how partial flag manifolds (generalizing Grassmannians) are described physically by nonabelian gauged linear sigma models, paying attention to realizations of tangent bundles and other aspects pertinent to (0,2) models. Second, we review constructions of Calabi-Yau complete intersections within such flag manifolds, and properties of the gauged linear sigma models. We discuss a number of examples of nonabelian GLSM's in which the Kahler phases are not birational, and in which at least one phase is realized in some fashion other than as a complete intersection, extending previous work of Hori-Tong. We also review an example of an abelian GLSM exhibiting the same phenomenon. We tentatively identify the mathematical relationship between such non-birational phases, as examples of Kuznetsov's homological projective duality. Finally, we discuss linear sigma model moduli spaces in these gauged linear sigma models. We argue that the moduli spaces being realized physically by these GLSM's are precisely Quot and hyperquot schemes, as one would expect mathematically. ", "machine_abstract": "We study the geometry and topology of generalized Lagrangian submanifolds (GLSMs) in complex symplectic manifolds, focusing on GLSMs that are special Lagrangians with respect to some K\u00e4hler form. We show how these can be constructed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles. In particular we consider the case where the base is a partial flag variety. This leads us to define new families of Calabi-Yau varieties which have been studied by physicists recently. These varieties are obtained by taking products of Grassmannian manifolds or their quotients by finite groups. The main results of this thesis are:  1. A construction of GLSMs using parabolic Higgs bundles.  2. An explicit description of the cohomology ring of the total space of a vector bundle associated to a parabolic Higgs bundle.  3. A proof of mirror symmetry between two different types of GLSMs defined above when the base is a product of Grassmannians.", "paraphrased_abstract": "It is the first of the two studies that we will describe. The first is: A construction of a GLSM with parabolic Higgs; and, second, an explicit description of the cohomology ring of a vector bundle that is a product of the Higgs bundle; and, third, a proof of mirror symmetry between two different types of GLSMs. These are called GLSMs with a special shape in the form of K\u00e4hler. We will discuss the geometry and the topology of a holomorphic section of certain line bundles, which are of a type that is not common with Grassmannians. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3644, "title": "Capacity Gain from Two-Transmitter and Two-Receiver Cooperation", "abstract": "  Capacity improvement from transmitter and receiver cooperation is investigated in a two-transmitter, two-receiver network with phase fading and full channel state information available at all terminals. The transmitters cooperate by first exchanging messages over an orthogonal transmitter cooperation channel, then encoding jointly with dirty paper coding. The receivers cooperate by using Wyner-Ziv compress-and-forward over an analogous orthogonal receiver cooperation channel. To account for the cost of cooperation, the allocation of network power and bandwidth among the data and cooperation channels is studied. It is shown that transmitter cooperation outperforms receiver cooperation and improves capacity over non-cooperative transmission under most operating conditions when the cooperation channel is strong. However, a weak cooperation channel limits the transmitter cooperation rate; in this case receiver cooperation is more advantageous. Transmitter-and-receiver cooperation offers sizable additional capacity gain over transmitter-only cooperation at low SNR, whereas at high SNR transmitter cooperation alone captures most of the cooperative capacity improvement. ", "machine_abstract": "We consider the problem of two-user MIMO broadcast channel with multiple antennas at each transmitter, receiver pair. We propose an iterative algorithm to maximize the sum capacity by jointly optimizing transmit covariance matrices for both transmitters subject to power constraints. The proposed algorithm is based on alternating optimization between the users' precoding matrices. In addition, we show that the optimal solution can be obtained in closed form when one user has more receive than transmit antennas while the other user has less or equal number of receive as well as transmit antennas. Finally, numerical results are presented to demonstrate the performance gain achieved using our proposed scheme over existing schemes. Index Terms: Broadcast Channel (BC), Iterative Algorithm, Joint Transmission, Multiple Input Multiple Output (MIMO). 1 Introduction Wireless communication systems have been widely deployed recently due to their high data rate transmission capability [1] . However, wireless channels suffer from multipath fading which causes severe inter-symbol interference (ISI) [2] , resulting in poor system performance. To overcome this issue, orthogonal frequency division multiplexing (OFDM) [3] was introduced into wireless communications. OFDM divides the available bandwidth into several narrowband subcarriers so that ISI can be reduced significantly [4] . In order to further improve spectral efficiency, multi-antenna techniques were incorporated into OFDM-based systems [5] - [8] . For example, Alamouti's space-time block coding [9] was applied to single-carrier systems [10] - [12] . Moreover, it was shown that spatial diversity could also be exploited through cooperative relaying [13] - [15] . Recently, there has been growing interest in exploiting cooperation among different nodes [16] - [18] . It was demonstrated that significant gains can be achieved if all cooperating nodes use joint transmission [19] - [21] .", "paraphrased_abstract": "At present, the wireless communications system has been in the world for a long time, owing to its high transmission capacity. Its limitations are enormous and the bandwidth of the transmission channels is reduced to the minimum. There are also several techniques for the allocation of the spectral resources, such as multi-antennas, such as the Alamouti technique, a coding technique for spatial diversity, to be used in the single- and the two-way transmissions. In order to achieve better spectral efficiency, orthogonal frequency division multiplexing was introduced, which divided the available bandwidth into a few narrow bands, thereby reducing the ISI in the network. It is a known fact that a large number of users have more antennas than antennas, and that, by a bidirectional optimization of the transmit and receive matrices, one can find the optimal solution in closed form when one antenna has more receive than antenna, and the other antenna has less or equal antennas. Finally, numerical results are presented to demonstrate the benefits of our proposed scheme over existing systems. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0895, "title": "Radiative Transfer Effect on Ultraviolet Pumping of the 21cm Line in the High Redshift Universe", "abstract": "  During the epoch of reionization the 21cm signal is sensitive to the scattering rate of the ultraviolet photons, redshifting across the Lyman_alpha resonance. Here we calculate the photon scattering rate profile for a single ultraviolet source. After taking into account previously neglected natural broadening of the resonance line, we find that photons approach the resonance frequency and experience most scatterings at a significantly smaller distance from the source than naively expected r=(dnu/nu_0)(c/H), where dnu=nu-nu_0 is the initial frequency offset, and the discrepancy increases as the initial frequency offset decreases. As a consequence, the scattering rate P(r) drops much faster with increasing distance than the previously assumed 1/r^2 profile. Near the source (r<1Mpc comoving), the scattering rate of photons that redshift into the Ly_alpha resonance converges to P(r) \\propto r^{-7/3}. The scattering rate of Ly_alpha photons produced by splitting of photons that redshift into a higher resonance (Ly_gamma, Ly_delta, etc.) is only weakly affected by the radiative transfer, while the sum of scattering rates of Ly_alpha photons produced from all higher resonances also converges to P(r) \\propto r^{-7/3} near the source. At 15<z<35, on scales of ~0.01-20Mpc/h (comoving), the total scattering rate of Ly_alpha photons from all Lyman resonances is found to be higher by a factor of ~1+0.3[(1+z)/20]^{2/3} than obtained without full radiative transfer. Consequently, during the early stage of reionization, the differential brightness of 21cm signal against the cosmic microwave background is also boosted by a similar factor. ", "machine_abstract": "We study the effect of radiative transfer (RT) on ultraviolet pumping of the 21 cm line at high redshifts, using cosmological hydrodynamic simulations with RT and without it. We find that RT can significantly enhance the strength of the 21 cm signal by up to an order of magnitude compared to calculations neglecting RT effects. The enhancement is caused mainly by Lyman-alpha photons produced inside galaxies which are absorbed outside them due to scattering off neutral hydrogen atoms. This leads to additional heating of the intergalactic medium through photoionization heating and Compton cooling. In addition we show that the inclusion of RT also changes the shape of the power spectrum of the 21 cm brightness temperature fluctuations. Our results suggest that future radio telescopes such as SKA will be able to detect this signal if they have sufficient sensitivity. Keywords: Hydrogen, Radiation transfer, Power Spectrum, Cosmic Dawn", "paraphrased_abstract": "We have investigated the effects of radiance of the luminous beam of the 21 cm line at high redshifts. We have examined the effect of radiance on the strength of the 21 cm beam, both in terms of asymmetry and radiation. We have studied the effect of radiance on the radiative flux of the 21 cm beam, which is greatly increased by radiance and by radiance. The effect of radiance was found to be of a magnitude greater than if the calculation was not affected by radiance. We show that radiance changes the luminous flux of the 21 cm beam, which is caused by the Lyman alpha photons produced inside the galaxy, which are absorbed outside the galaxy by the scattering of neutral hydrogen atoms. This resulted in a resurgence of the intergalactic medium by the radiation of the Sun and the cooling of Compton. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1318, "title": "The Haunted Halos of Andromeda and Triangulum: A panorama of galaxy formation in action", "abstract": "  We present a deep photometric survey of M31, conducted with the CFHT and INT, covering the inner 50 kpc of the galaxy, the Southern quadrant out to 150 kpc, and extending to M33. This is the first systematic panoramic study of this very outermost region of galaxies. We detect several streams and other large-scale structures, and two new dwarf galaxies: And XV and XVI. The discovery of substructure on the minor axis, together with the fact that the light profile between 0.5 < R < 1.3 follows the exponential ``extended disk'', is particularly important in shedding light on the mixed and sometimes conflicting results reported in previous studies. Underlying the substructures lies a faint, metal-poor, smooth and extremely extended halo, reaching out to at least 150 kpc. The smooth halo component in M31 has a profile that can be fit with a Hernquist model of immense scale radius ~55 kpc, almost a factor of 4 larger than theoretical predictions. Alternatively a power-law with exponent -1.91 +/- 0.11 can be fit to the profile. The total luminosity of this structure is similar to that of the halo of the Milky Way. This vast, smooth, underlying halo is reminiscent of a classical monolithic model and completely unexpected from modern galaxy formation models. M33 is also found to have an extended metal-poor halo component, which can be fit with a Hernquist model also of scale radius ~55 kpc. These extended slowly-decreasing halos will provide a challenge and strong constraints for further modeling. [Abridged] ", "machine_abstract": "We present the first panoramic view of the distribution of galaxies around two massive clusters at redshifts z = 0.4-0.6, using deep near-infrared imaging with WFC3 on HST. The data reveal an extended population of faint galaxies surrounding each cluster that is not seen in optical images. We find that these galaxies are predominantly blue (with median color u \u2212 g = -0.5), have low stellar masses (10^9 Msun/pc^2) and high specific star formation rates (sSFR ~ 10^-2 Gyr-1). These properties suggest they represent a recently assembled population of star-forming dwarf galaxies that were accreted by their host clusters during recent mergers. In addition to this diffuse component we also identify several dozen bright galaxies within 1 Mpc of both clusters which appear to be undergoing rapid bursts of star formation triggered by interactions between infalling gas-rich galaxies and the hot intracluster medium.", "paraphrased_abstract": "A few hundred bright galaxies are identified in one mln of the swarm, which are amidst a mass of gas and a hot acme. This is the first time we have been able to study the distribution of galaxies around two enormous craters at redshift z = 0.4 to 0.6 in the visible sphere of the HST. We find that they are mainly blue in color, with a median color of u  g = 0.5, they are low in stars, they have low masses (109 Msun  pc2), and they have very high specific star formation (sSFR  10-2 Gyr-1). The results of our study indicate that they are an ancient population of star-forming dwarf galaxies, accreted by their hosts during recent mergers. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0614, "title": "An expansion for self-interacting random walks", "abstract": "  We derive a perturbation expansion for general self-interacting random walks, where steps are made on the basis of the history of the path. Examples of models where this expansion applies are reinforced random walk, excited random walk, the true (weakly) self-avoiding walk, loop-erased random walk, and annealed random walk in random environment.   In this paper we show that the expansion gives rise to useful formulae for the speed and variance of the random walk, when these quantities are known to exist. The results and formulae of this paper have been used elsewhere by the authors to prove monotonicity properties for the speed (in high dimensions) of excited random walk and related models, and certain models of random walk in random environment.   We also derive a law of large numbers and central limit theorem (with explicit error terms) directly from this expansion, under strong assumptions on the expansion coefficients. The assumptions are shown to be satisfied by excited random walk in high dimensions with small excitation parameter, a model of reinforced random walk with underlying drift and small reinforcement parameter, and certain models of random walk in random environment under strong ellipticity conditions. This is the extended version of the paper, where we provide all proofs. ", "machine_abstract": "We present an exact expression for the probability distribution function (PDF) of the number of steps taken by a one-dimensional, discrete-time, self-propelled particle that moves in a periodic potential and interacts with itself via elastic collisions.  We show how this PDF can be used to calculate the mean-square displacement as well as other statistical properties of such particles. The results are illustrated using numerical simulations.  PACS numbers: 05.45.-a; 05.70.Jk; 05.60.Gg  I. INTRODUCTORY REMARkS The motion of many biological systems is often described as being driven by internal forces or active processes [1] . Examples include bacteria swimming through fluids [2] , cells crawling on surfaces [3] , and molecular motors moving along cytoskeletal filaments [4] . In recent years there has been growing interest in understanding the dynamics of these active particles [5] - [8] . In particular, it was shown that their behavior may differ significantly from that observed in passive Brownian particles [9] - [11] . For example, while the latter exhibit normal diffusion at large timescales [12] , active particles typically display superdiffusive [13] or even ballistic [14] transport depending on the details of their interactions [15] - [17] . This difference arises because active particles have additional degrees of freedom which allow them to explore more efficiently the available space [18] . As a result they tend to move faster than passive particles [19] . Recently we introduced a model describing the motion of a single active particle [20] . It consists of a point-like object that performs a biased random walk in a periodic potential [21] . Its position x(t + 1) = x(t) + v t+1 \u2212 v t is determined by its velocity v t+1 = f [x(t), v t ] where f [\u00b7] denotes some deterministic force acting upon the particle [22] . Here we consider two different types of potentials V (x). First, when V (x) \u221d cos(2\u03c0x/L) (L is the periodicity length), the system exhibits a series of metastable states separated by energy barriers [23] . Second", "paraphrased_abstract": "The movement of many organisms is often described as being driven by internal forces or by active processes. In particular, the behavior of the active particles is incomparably different from that of passive Brownian particles [11], in that they exhibit more than normal diffusion at large scales, whereas the active ones show an accelerated and even ballistic behavior, owing to their unique nature. The present study is presented by the name of the PDF, which we use to calculate the mean square displacement of the particles and the statistical properties of them. We have recently developed a mathematical model of the motion of a single active particle, namely: a point of a regularity that moves at a periodic scale. Its position x (t + 1) = x (t) + v t  v t is determined by the velocity v t  v t, where v t+1 = f [x][1]  v t, where f is a deterministic force that acts on the particle. We have recently developed a model for the motion of an active particle, namely a particle that walks in a periodic space and interacts with itself through elastic collisions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3053, "title": "The remarkable X-ray spectrum of the Broad-Line Radio Galaxy 3C 445", "abstract": "  We present the results of the analysis of an archival 15 ks XMM-Newton observation of the nearby (z=0.057) radio-loud source 3C445, optically classified as a Broad-Line Radio Galaxy. While the RGS data are of insufficient quality to allow a meaningful analysis, the EPIC data show a remarkable X-ray spectrum. The 2-10 keV continuum is described by a heavily absorbed (Nh~ 1e22 - 1e23 cm-2) power law with photon index Gamma ~1.4, and strong (R~2) cold reflection. A narrow, unresolved Fe Kalpha emission line is detected, confirming previous findings, with EW 120eV. A soft excess is present below 2 keV over the extrapolation of the hard X-ray power law, which we model with a power law with the same photon index as the hard power law, absorbed by a column density Nh=6e20 cm-2 in excess to Galactic. A host of emission lines are present below 2 keV, confirming previous indications from ASCA, due to H- and He-like O, Mg, and Si. We attribute the origin of the lines to a photoionized gas, with properties very similar to radio-quiet obscured AGN. Two different ionized media, or a single stratified medium, are required to fit the soft X-ray data satisfactorily. The similarity of the X-ray spectrum of 3C445 to Seyferts underscores that the central engines of radio-loud and radio-quiet AGN similarly host both cold and warm gas. ", "machine_abstract": "We present an analysis of the broadband (0.5-10 keV) X-ray spectrum of the radio galaxy 3C 445, using data obtained with XMM-Newton and Chandra observatories in 2001-2002 . The observed X-ray emission is dominated by a hard power-law component which can be fitted equally well either by thermal Comptonization or non-thermal inverse Compton scattering models. We find that both models require a large amount of cold matter to produce the soft excess below 1 keV. This suggests that there are two distinct components contributing to the X-ray emission - one associated with hot plasma and another related to cool gas clouds. In addition we detect several narrow absorption lines at energies corresponding to highly ionized species such as O vii , Ne ix , Mg xi and Si xiii . These features may originate in outflows driven by nuclear activity. Finally, we report on the detection of Fe K\u03b1 line at 6.4 keV produced by reflection off distant material.", "paraphrased_abstract": "A few absorption lines of high density (of about 0.6-6.5 keV) have been observed, with some specks of O i, Ne ix, Mg xi, and Si xiii forming the main body, which, in our opinion, is the result of a reaction from a nuclear element. We then present an analysis of the spectrum of the radio galaxy 3C 445 by means of the data from the Chandra and XMM-Newton observatories. The data of the radio galaxy 3C 445 were obtained from XMM-Newton and Chandra observatories in 2001-2002. The X-ray spectrum of the radio galaxy 3C 445 was found to be dominated by a hard component, which can be fitted with either thermal inverse X-ray or non-thermal inverse X-ray models. It is found that the soft excess below 1 keV is very small and can be estimated by both methods. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0946, "title": "Efficient Simulations of Early Structure Formation and Reionization", "abstract": "  We present a method to construct semi-numerical ``simulations'', which can efficiently generate realizations of halo distributions and ionization maps at high redshifts. Our procedure combines an excursion-set approach with first-order Lagrangian perturbation theory and operates directly on the linear density and velocity fields. As such, the achievable dynamic range with our algorithm surpasses the current practical limit of N-body codes by orders of magnitude. This is particularly significant in studies of reionization, where the dynamic range is the principal limiting factor. We test our halo-finding and HII bubble-finding algorithms independently against N-body simulations with radiative transfer and obtain excellent agreement. We compute the size distributions of ionized and neutral regions in our maps. We find even larger ionized bubbles than do purely analytic models at the same volume-weighted mean hydrogen neutral fraction. We also generate maps and power spectra of 21-cm brightness temperature fluctuations, which for the first time include corrections due to gas bulk velocities. We find that velocities widen the tails of the temperature distributions and increase small-scale power, though these effects quickly diminish as reionization progresses. We also include some preliminary results from a simulation run with the largest dynamic range to date: a 250 Mpc box that resolves halos with masses M >~ 2.2 x10^8 M_sun. We show that accurately modeling the late stages of reionization requires such large scales. The speed and dynamic range provided by our semi-numerical approach will be extremely useful in the modeling of early structure formation and reionization. ", "machine_abstract": "We present an efficient method for simulating the formation of large-scale structure in the early universe, including reionization by stars and quasars. Our approach is based on a combination of N-body simulations with semi-analytic models that describe galaxy formation and evolution within dark matter halos. We use this technique to study how galaxies form at high redshifts (z > 6) when the first luminous objects appear. The results are compared against observations of high-redshift galaxies obtained using the Hubble Space Telescope. This work was supported by NASA grant NNX08AG84G. In recent years there has been considerable progress in understanding the physical processes involved in galaxy formation. However, many open questions remain about the nature of these processes as well as their dependence on cosmological parameters such as the density parameter $\\Omega_m$ or the amplitude of primordial fluctuations $\\sigma_8$. One important goal of modern observational astronomy is to answer some of these questions through detailed studies of distant galaxies.", "paraphrased_abstract": "It is necessary to study the origin of the universe at high redshifts, when the first luminous objects appear. The study is complemented by the results obtained by Hubble\u2019s Hubble Space Telescope. Recently, there has been much work done in the field of physical processes in the formation of galaxies, but it remains unclear as to the nature of these processes and the relative importance of cosmological parameters, namely, the density of stars and the amplitude of primordial fluctuations. The present work is supported by NASA grant NNX08AGA84G. In recent years much has been learned about the physical processes of galaxy formation and evolution. However, many of these questions remain open, and they are of course still open, as to the origin and nature of the formation of galaxies, which depend on the cosmological parameters, such as the density of the stars, or the amplitude of primordial fluctuations, or on the amplitude of the primordial fluctuations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.038, "title": "HD 98800: A 10-Myr-Old Transition Disk", "abstract": "  We present the mid-infrared spectrum, obtained with the Spitzer Infrared Spectrograph (IRS), of HD 98800, a quadruple star system located in the 10-Myr-old TW Hydrae association. It has a known mid-infrared excess that arises from a circumbinary disk around the B components of the system. The IRS spectrum confirms that the disk around HD 98800 B displays no excess emission below about 5.5 micron, implying an optically thick disk wall at 5.9 AU and an inner, cleared-out region; however, some optically thin dust, consisting mainly of 3-micron-sized silicate dust grains, orbits the binary in a ring between 1.5 and 2 AU. The peculiar structure and apparent lack of gas in the HD 98800 B disk suggests that this system is likely already at the debris disks stage, with a tidally truncated circumbinary disk of larger dust particles and an inner, second-generation dust ring, possibly held up by the resonances of a planet. The unusually large infrared excess can be explained by gravitational perturbations of the Aa+Ab pair puffing up the outer dust ring and causing frequent collisions among the larger particles. ", "machine_abstract": "We present new near-infrared (NIR) spectroscopy and photometry for the Herbig Be star HD 98800, which is surrounded by an optically thick dust disk with a radius of ~200 AU. The NIR spectrum shows strong emission lines of H I, He II, C III, O IV-VI, Si IV, S VI, Fe II, Mg II, Al II-III, Na I, Ca II, and K I in addition to absorption features due to stellar winds. We find that the observed line profiles are consistent with those predicted by magnetohydrodynamic models of accretion disks around young stars. In particular, we detect blueshifted absorptions associated with infalling gas streams along magnetic field lines onto the central star. These results suggest that HD 98800 has been undergoing active mass accretion over the past few million years.     Keywords: Accretion, Herbig Be star", "paraphrased_abstract": "At the same time, the star\u2019s axis was bluish, which indicated the absorption of gas from a central star by the resonant magnetic fields. Our observations show that HD 98800 is undergoing accretion, and is surrounded by a thick dust cloud with a radius of over 200 AU. Our NIR measurements are made in the form of NIR, and show that H I, C III, O IV, Si IV, S VI, Fe II, Mg II, Al II, Na I, Ca I, K I, and K I. The results show that HD 98800 is undergoing accretion, and that in the last few million years HD 98800 has been accreted by a mass accretion. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3264, "title": "Massive enhancement of electron-phonon coupling in doped graphene by an electronic singularity", "abstract": "  The nature of the coupling leading to superconductivity in layered materials such as high-Tc superconductors and graphite intercalation compounds (GICs) is still unresolved. In both systems, interactions of electrons with either phonons or other electrons or both have been proposed to explain superconductivity. In the high-Tc cuprates, the presence of a Van Hove singularity (VHS) in the density of states near the Fermi level was long ago proposed to enhance the many-body couplings and therefore may play a role in superconductivity. Such a singularity can cause an anisotropic variation in the coupling strength, which may partially explain the so-called nodal-antinodal dichotomy in the cuprates. Here we show that the topology of the graphene band structure at dopings comparable to the GICs is quite similar to that of the cuprates and that the quasiparticle dynamics in graphene have a similar dichotomy. Namely, the electron-phonon coupling is highly anisotropic, diverging near a saddle point in the graphene electronic band structure. These results support the important role of the VHS in layered materials and the possible optimization of Tc by tuning the VHS with respect to the Fermi level. ", "machine_abstract": "We report on the observation of strong electron-phonon interaction (EPI) in heavily boron-doped epitaxial graphene grown on SiC(0001). The EPI is found to be enhanced by more than one order of magnitude compared with that observed for undoped graphene, which can be attributed to the formation of a Dirac point at high doping levels and its associated van Hove singularity near the Fermi level. This finding suggests that the EPI in doped graphene may be tunable over several orders of magnitude through chemical or electrostatic gating.     Graphene has attracted considerable attention because it exhibits unique physical properties such as extremely high carrier mobility1-5 and very large optical nonlinearities6-8. However, these remarkable properties are often accompanied by weak interactions between electrons and phonons9-11, which limit their applications in high-speed electronics12-14 and optoelectronics15-17. In this work we demonstrate that the EPI in heavily boron-doping epitaxial graphene grown by thermal decomposition of SiC18-20 can be significantly enhanced due to the presence of a Dirac point21-23 and its associated van Hov singularity24-26 near the Fermi energy EF. We show that the EPI increases rapidly when the Fermi level crosses the van Hove singularity, resulting in a giant increase in the electron-phonon scattering rate. Our results suggest that the EPI in graphene could be controlled electrically via chemical or electrostatic gated27-30, thereby opening up new avenues towards novel devices based on graphene.  Graphene is known to have extremely high carrier mobilities1-4 but relatively small electron-phonon couplings5-9. These two competing effects determine the performance of graphene-based electronic and optoelectronic devices10-12. For example, the low EPI leads to slow relaxation rates13-15 and thus limits the operation speed of graphene transistors14-16. On the other hand, the high mobility makes graphene attractive for use in high-speed electronics17-19 and ultrafast photodetectors20. Therefore, there exists great interest in developing methods to enhance the EPI while maintaining the high mobility31", "paraphrased_abstract": "Graphene is a material of very high density and has very low entropy. These two effects, namely, the low entropy, and the high mobility, make it attractive for high-speed electronic and optical devices. Graphene is an extremely dense material, but it has relatively small entropy. Moreover, there are small entropy, and this is why it is difficult to produce high-speed transistors and sensors. In the present study, we show that the entropy of the graphene is significantly enhanced by a Dirac point, and the van Hove singularity near the Fermi point. This is an epoch-shaped dirac point, with a dirac point at the top of the graphene, and an associated van Hove singularity at the top of the graphene. We conclude that the EPI in the graphene is a very large one and a very large one, resulting in the slow relaxation of the electrons and the phonons. Graphene has been a major subject of great interest because it has properties with extremely high mobility and large optical nonlinearities. Graphene has been studied with great attention because it is extremely light and has an extremely strong electric field. However, in some cases, these properties are complicated and not always accompanied by a strong contact", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0789, "title": "New Close Binary Systems from the SDSS-I (Data Release Five) and the Search for Magnetic White Dwarfs in Cataclysmic Variable Progenitor Systems", "abstract": "  We present the latest catalog of more than 1200 spectroscopically-selected close binary systems observed with the Sloan Digital Sky Survey through Data Release Five. We use the catalog to search for magnetic white dwarfs in cataclysmic variable progenitor systems. Given that approximately 25% of cataclysmic variables contain a magnetic white dwarf, and that our large sample of close binary systems should contain many progenitors of cataclysmic variables, it is quite surprising that we find only two potential magnetic white dwarfs in this sample. The candidate magnetic white dwarfs, if confirmed, would possess relatively low magnetic field strengths (B_WD < 10 MG) that are similar to those of intermediate-Polars but are much less than the average field strength of the current Polar population. Additional observations of these systems are required to definitively cast the white dwarfs as magnetic. Even if these two systems prove to be the first evidence of detached magnetic white dwarf + M dwarf binaries, there is still a large disparity between the properties of the presently known cataclysmic variable population and the presumed close binary progenitors. ", "machine_abstract": "We report on new close binary systems discovered by searching for white dwarf companions to cataclysmic variables using data obtained with the Sloan Digital Sky Survey's first year of operation, Data Release 5. We find that most of these binaries are likely to be double-degenerate systems consisting of two helium white dwarfs or one carbon-oxygen white dwarf plus another degenerate object such as a neutron star or black hole. The majority of our sample have orbital periods less than 1 hour; we also identify several candidates which may contain magnetic white dwarfs. These results will help us understand how common double-degenerate mergers might be among all type Ia supernovae.     Keywords: Double-degenerate systems; Type Ia supernovae; Close binary stars; Sloan Digital Sky Survey; Cataclysmic Variables; White dwarfs; Helium white dwarfs; Carbon-oxygen white dwarfs", "paraphrased_abstract": "This study presents new close-knit systems discovered in the Sloan Digital Sky Survey in the first year of the Survey, in the first year of the Survey, for the first time. We have investigated close-knit systems in the first year of the Survey. We have observed that the majority of these systems are not more than one hour in orbit. In addition, there are a few candidates with magnetic white dwarfs. These will be able to give us an idea of the types of double-division mergers that occur among all Type Ia supernovae. In the first year of the Survey, we have surveyed close-knit stars from the Sloan Sky Survey. We have identified close-knit systems of helium white dwarfs or carbon-oxygen white dwarfs, as well as other degenerate objects, such as neutron stars or black holes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1214, "title": "A High-Resolution Survey of Low-Redshift QSO Absorption Lines: Statistics and Physical Conditions of O VI Absorbers", "abstract": "  Using high-resolution UV spectra of 16 low-z QSOs, we study the physical conditions and statistics of O VI absorption in the IGM at z < 0.5. We identify 51 intervening (z_{abs} << z_{QSO}) O VI systems comprised of 77 individual components, and we find 14 \"proximate\" systems (z_{abs} ~ z_{QSO}) containing 34 components. For intervening systems [components] with rest-frame equivalent width W_{r} > 30 mA, the number of O VI absorbers per unit redshift dN/dz = 15.6(+2.9/-2.4) [21.0(+3.2/-2.8)], and this decreases to dN/dz = 0.9(+1.0/-0.5) [0.3(+0.7/-0.3)] for W_{r} > 300 mA. The number per redshift increases steeply as z_{abs} approaches z_{QSO}, and some proximate absorbers have substantially lower H I/O VI ratios. The lower proximate ratios could be partially due to ionization effects but also require higher metallicities. We find that 37% of the intervening O VI absorbers have velocity centroids that are well-aligned with corresponding H I absorption. If the O VI and the H I trace the same gas, the relatively small differences in line widths imply the absorbers are cool with T < 10^{5} K. Most of these well-aligned absorbers have the characteristics of metal-enriched photoionized gas. However, the O VI in the apparently simple and cold systems could be associated with a hot phase with T ~ 10^{5.5} K if the metallicity is high enough to cause the associated broad Ly alpha absorption to be too weak to detect. We show that 53% of the intervening O VI systems are complex multiphase absorbers that can accommodate both lower metallicity collisionally-ionized gas with T > 10^{5} K and cold photoionzed gas. ", "machine_abstract": "We present the results of an extensive survey for absorption lines in high-resolution spectra of quasars at redshifts z < 0.3, obtained with HST/STIS (the Hubble Space Telescope Spectrograph). The sample consists of 12 QSOs observed over a total exposure time of about 1 Ms. We detect more than 1000 Lyman alpha forest absorbers along each line-of-sight to these QSOs; we also find that most sight-lines show strong absorption by high-ionization species such as C IV and Si IV. In addition, we identify several hundred systems showing absorption due to highly ionized oxygen, O VI . These are among the strongest known O VI absorbers anywhere in the universe. By combining our data set with previous surveys, we obtain new constraints on the physical conditions of this gas phase. Our main conclusions are:  -The number density of O VI absorbers per unit redshift is n(O VI) = 2.6 +/- 0.4 x 10^-3 cm^-3 , which corresponds to a mean separation between absorber pairs of r = 4.1 +/- 0.5 h-1 Gpc.  -Most O VI absorbers have temperatures T ~ 10 5 K and densities log ne/cm -3 > 13.8. About half of them appear to be associated with galaxies within 50 kpc projected distance. -There appears to be no correlation between the properties of O VI absorbers and those of their host galaxy population.", "paraphrased_abstract": "This is a result of a thorough investigation of quasars at redshifts of about 0.3, wherein we have detected about 10,000 Lyman alpha-like peaks, which are accompanied by many ionized species such as C IV and Si IV. We can identify several hundred species whose absorption is attributed to O VI. Most of these peaks have temperatures as high as ten or thirty degrees Celsius, and densities of a few ne/cm-30.02. There is no correlation of the absorption of the O VI-like peaks with the populations of their host galaxy. We have obtained a tachycardin in the upper portion of the redshift for 12 QSOs, of which 12 QSOs were observed over the whole time of 12 hours. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3063, "title": "The Evolution of Protoplanetary Disks Around Millisecond Pulsars: The PSR 1257 +12 System", "abstract": "  We model the evolution of protoplanetary disks surrounding millisecond pulsars, using PSR 1257+12 as a test case. Initial conditions were chosen to correspond to initial angular momenta expected for supernova-fallback disks and disks formed from the tidal disruption of a companion star. Models were run under two models for the viscous evolution of disks: fully viscous and layered accretion disk models. Supernova-fallback disks result in a distribution of solids confined to within 1-2 AU and produce the requisite material to form the three known planets surrounding PSR 1257+12. Tidal disruption disks tend to slightly underproduce solids interior to 1 AU, required for forming the pulsar planets, while overproducing the amount of solids where no body, lunar mass or greater, exists. Disks evolving under 'layered' accretion spread somewhat less and deposit a higher column density of solids into the disk. In all cases, circumpulsar gas dissipates on $\\lesssim 10^{5}$ year timescales, making formation of gas giant planets highly unlikely. ", "machine_abstract": "We present new observations and analysis of the pulsar\u2013disk system PSR 1257+12, which is one of only two known systems in which a millisecond radio pulsar orbits around an evolved star (a white dwarf). We have obtained high-resolution near-infrared images with adaptive optics at Keck Observatory that reveal a bright ring-like structure surrounding the pulsar's position. This feature has been interpreted as emission from dust grains orbiting within the disk produced by the supernova explosion that created both stars.     In addition to this ring, we detect faint extended emission extending out to several hundred AU on either side of the central source. These features are consistent with previous results based on lower resolution data. However, our higher spatial resolution reveals additional details about these structures. For example, we find evidence for spiral arms in the outer regions of the disk, which may be caused by gravitational interactions between the disk and the binary companion.", "paraphrased_abstract": "This was confirmed by the earlier results based on lower resolution. Nevertheless, we analyzed the structure more precisely. The new structure is known to be PSR 1257, one of the few known systems in which a pulsar is orbiting around an evolved star (a white dwarf). In addition to the ring, we detect a faint light, up to several hundred AU, which extends from the central source to the central source. In addition, we have observed an inner spiral structure that is a result of the explosion of the supernova that created the two stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0867, "title": "Poisson approximation for non-backtracking random walks", "abstract": "  Random walks on expander graphs were thoroughly studied, with the important motivation that, under some natural conditions, these walks mix quickly and provide an efficient method of sampling the vertices of a graph. Alon, Benjamini, Lubetzky and Sodin studied non-backtracking random walks on regular graphs, and showed that their mixing rate may be up to twice as fast as that of the simple random walk. As an application, they showed that the maximal number of visits to a vertex, made by a non-backtracking random walk of length $n$ on a high-girth $n$-vertex regular expander, is typically $(1+o(1))\\frac{\\log n}{\\log\\log n}$, as in the case of the balls and bins experiment. They further asked whether one can establish the precise distribution of the visits such a walk makes.   In this work, we answer the above question by combining a generalized form of Brun's sieve with some extensions of the ideas in Alon et al. Let $N_t$ denote the number of vertices visited precisely $t$ times by a non-backtracking random walk of length $n$ on a regular $n$-vertex expander of fixed degree and girth $g$. We prove that if $g=\\omega(1)$, then for any fixed $t$, $N_t/n$ is typically $\\frac{1}{\\mathrm{e}t!}+o(1)$. Furthermore, if $g=\\Omega(\\log\\log n)$, then $N_t/n$ is typically $\\frac{1+o(1)}{\\mathrm{e}t!}$ uniformly on all $t \\leq (1-o(1))\\frac{\\log n}{\\log\\log n}$ and 0 for all $t \\geq (1+o(1))\\frac{\\log n}{\\log\\log n}$. In particular, we obtain the above result on the typical maximal number of visits to a single vertex, with an improved threshold window. The essence of the proof lies in showing that variables counting the number of visits to a set of sufficiently distant vertices are asymptotically independent Poisson variables. ", "machine_abstract": "We consider the probability distribution of the number of times that a simple random walk on an infinite graph visits some fixed vertex in finite time, and show how this can be approximated by a Poisson distribution with mean equal to the expected value of the number of visits.  We then use these results to prove that the total variation distance between the two distributions is bounded above by $O(1/n)$ where $n$ denotes the number of vertices in the graph.   This result generalizes previous work which was restricted to graphs whose degree sequence has exponential decay or polynomial growth.    The proof relies heavily upon recent advances in the theory of large deviations for sums of dependent Bernoulli variables. In particular we make use of the so-called Cram\u00e9r-Chernoff method as well as the concept of a supermartingale. Finally, we apply our main theorem to obtain new bounds on the mixing time of lazy random walks on regular trees.", "paraphrased_abstract": "This work is an extension of previous work, based on exponential decay and polynomial growth. The main theorem is applied to obtain new estimates of the mixing time of a lazy random walk on a regular tree. The aforementioned works are taken in part from the latest developments in the theory of large deviations in the proportion of dependent Bernoulli variables, in particular by means of the so-called Cram\u00e9r-Chernoff method and by means of the notion of supermartingale. The arithmetic method is employed, and the so-called Cram\u00e9r-Chernoff method is used. In our calculations we take into account the time of the simple random walk in a finite graph, and calculate how the distance between these two intervals is bounded by a Poisson-restriction of the mean value of the number of visits. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3844, "title": "Radion Phenomenology in Realistic Warped Space Models", "abstract": "  We investigate the phenomenology of the Randall-Sundrum radion in realistic models of electroweak symmetry breaking with bulk gauge and fermion fields, since the radion may turn out to be the lightest particle in such models. We calculate the coupling of the radion in such scenarios to bulk fermion and gauge modes. Special attention needs to be devoted to the coupling to massless gauge fields (photon, gluon), since it is well known that loop effects may be important for these fields. We also present a detailed explanation of these couplings from the CFT interpretation. We then use these couplings to determine the radion branching fractions and discuss some of the discovery potential of the LHC for the radion. We find that the gamma-gamma signal is enhanced over most of the range of the radion mass over the gamma-gamma signal of a SM Higgs, as long as the RS scale is sufficiently low. However, the signal significance depends strongly on free parameters that characterize the magnitude of bare brane-localized kinetic terms for the massless gauge fields. In the absence of such terms, the signal can be be enhanced over the traditional RS1 models (where all standard model fields are localized on the IR brane), but the signal can also be reduced compared to RS1 if the brane localized terms are sizeable. We also show that for larger radion masses, where the gamma-gamma signal is no longer significant, one can use the usual 4 lepton signal to discover the radion. ", "machine_abstract": "We study the phenomenological consequences of radion stabilization by introducing an explicit mass term for the radion into realistic warped space models with bulk gauge fields and fermions. We find that, depending on the model parameters, there are two possible scenarios which can be realized at low energies: (i) The radion is heavy enough to decouple from all Standard Model particles. (ii) The radion couples directly to Standard Model particles but its effects are suppressed due to mixing between the radion and the Higgs boson. In both cases we show how the radion affects precision electroweak observables such as S, T , W , Y . Finally, we discuss the possibility of detecting the radion through direct production at future colliders. Introduction: One of the most interesting features of Randall-Sundrum type I [1] or II [2] models is their ability to generate hierarchies among fundamental scales without fine-tuning [3] . However, these models suffer from the presence of a scalar field called the radion whose vacuum expectation value determines the size of extra dimensions [4] . In this work we consider the case where the radion acquires a large mass so it does not affect low-energy physics [5] . On the other hand, if the radion remains light then it may have observable effects [6] - [8] . For example, one could imagine that the radion mixes strongly with the Higgs boson leading to deviations from standard predictions [9] . Alternatively, the radion might couple directly to Standard Model particles [10] . In either scenario, the radion would contribute to precision electroweak measurements [11] - [13] . This motivates us to investigate whether the radion has any effect on precision electroweak data within realistic warped space models [14] - [16] .", "paraphrased_abstract": "As a result of this work we present two possible scenarios: 1) The radion is heavy enough to decouple itself from all the particles of the Standard Model; 2) The radion couples directly with the Standard Model, but the effects of this interaction are suppressed by the interaction of the radion and the Higgs Boson. In this case we present the radion\u2019s phenomenological properties. We study how the radion can have a positive influence on the observable properties of the Standard Model in two different ways: a) It is light enough to decouple itself from all the Standard Model particles, b) The radion will stick to the Standard Model, but c) It will stick directly to the Standard Model, but its effects are blocked by the mixing of the Standard Model with the Higgs Boson. In either case the radion will contribute to the precision observables, such as the S, T, W, Y, X, Y, and X. Finally, we will discuss the possibility of detecting the Radion directly in the future at the colliders. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3853, "title": "8.4GHz VLBI observations of SN2004et in NGC6946", "abstract": "  We report on 8.4GHz Very Long Baseline Interferometry (VLBI) observations of the type II-P supernova SN2004et in the spiral galaxy NGC 6946, made on 20 February 2005 (151 days after explosion). The Very Large Array (VLA) flux density was 1.23$\\pm$0.07 mJy, corresponding to an isotropic luminosity at 8.4GHz of (4.45$\\pm$0.3)$\\times10^{25}$ erg s$^{-1}$ Hz$^{-1}$ and a brightness temperature of (1.3$\\pm$0.3)$\\times10^{8}$ K. We also provide an improved source position, accurate to about 0.5 mas in each coordinate. The VLBI image shows a clear asymmetry. From model fitting of the size of the radio emission, we estimate a minimum expansion velocity of 15,700$\\pm$2,000 km s$^{-1}$. This velocity is more than twice the expected mean expansion velocity estimated from a synchrotron self-absorbed emission model, thus suggesting that synchrotron self-absorption is not relevant for this supernova. With the benefit of an optical spectrum obtained 12 days after explosion, we favor an emission model which consists of two hot spots on an underlying expanding shell of width comparable to that of SN 1993J. ", "machine_abstract": "We present 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves for the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1. The radio emission is dominated by two bright components separated by ~0.5 arcsec at all epochs observed between 2005 January and 2007 December. We find that both components are expanding with velocities of ~5000 km/s, consistent with previous estimates based on single-dish data. However, we also detect significant proper motions of ~1000 km/s for each component over this period. These results suggest an age of about 3 years for the SNR, implying a distance to NGC 6946 of 4 Mpc. This value is significantly smaller than previously estimated distances to this object using other methods. Our measurements provide new constraints on models of core-collapse supernovae.     Keywords: Supernova remnants", "paraphrased_abstract": "And we have also compared this SNR with that of SN2004et, which exploded in the nearby spiral galaxy NGC 6946 on the 24th of September 2004. Our observations suggest a ages of at least three years for the SNR, and thus a distance of four hundred ms, which is a very small distance from the object of interest by other means. Our results suggest an age of at least three years for the SNR, which is about four hundred miles from the object of the previous measurements. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2074, "title": "Star-Formation in Low Radio Luminosity AGN from the Sloan Digital Sky Survey", "abstract": "  We investigate faint radio emission from low- to high-luminosity Active Galactic Nuclei (AGN) selected from the Sloan Digital Sky Survey (SDSS). Their radio properties are inferred by co-adding large ensembles of radio image cut-outs from the FIRST survey, as almost all of the sources are individually undetected. We correlate the median radio flux densities against a range of other sample properties, including median values for redshift, [OIII] luminosity, emission line ratios, and the strength of the 4000A break. We detect a strong trend for sources that are actively undergoing star-formation to have excess radio emission beyond the ~10^28 ergs/s/Hz level found for sources without any discernible star-formation. Furthermore, this additional radio emission correlates well with the strength of the 4000A break in the optical spectrum, and may be used to assess the age of the star-forming component. We examine two subsamples, one containing the systems with emission line ratios most like star-forming systems, and one with the sources that have characteristic AGN ratios. This division also separates the mechanism responsible for the radio emission (star-formation vs. AGN). For both cases we find a strong, almost identical, correlation between [OIII] and radio luminosity, with the AGN sample extending toward lower, and the star-formation sample toward higher luminosities. A clearer separation between the two subsamples is seen as function of the central velocity dispersion of the host galaxy. For systems with similar redshifts and velocity dispersions, the star-formation subsample is brighter than the AGN in the radio by an order of magnitude. This underlines the notion that the radio emission in star-forming systems can dominate the emission associated with the AGN. ", "machine_abstract": "We present an analysis of star-formation activity associated with low radio luminosity active galactic nuclei (AGNs) using data from the Sloan DigitalSky Survey (SDSS). We find that these objects are typically hosted by massive galaxies, and have high specific star formation rates compared to inactive galaxies at similar redshifts. The majority of our sample is found to be obscured by dusty torii, as indicated by their optical colors and infrared emission. These results suggest that there may exist two populations of AGN: one which hosts significant amounts of star formation, and another where no such activity is observed. This work was supported by NASA grant NNG05GJ40G. Active Galactic Nuclei (AGNs), powered by supermassive black holes accreting matter from surrounding gas clouds, are known to produce copious quantities of radiation across all wavelengths. However, it has been unclear whether this energy output also leads to enhanced levels of star formation within host galaxies. In order to investigate this question we use data from the SloanDigital Sky Survey (SDSS; York et al., 2000) , specifically targeting sources classified as narrow-line Seyfert 1's (NLS1s) based on their optical spectra. NLS1s represent a subclass of AGNs whose properties differ significantly from those of more typical broad line quasars (BLQs; Osterbrock & Pogge 1985) . They tend to reside in lower mass galaxies than BLQSOs, and exhibit higher Eddington ratios (Boller et al., 1996; Grupe, Thomas, & Leighly 1999; Mathur 2000; Komossa et al., 2006a ,b Gallo 2007 .", "paraphrased_abstract": "For example, when a young sextet tries to make up a dummy, it turns out that there is an infrared element in them. The light is more intense, it shines, it shines, and it glows, it glows in the dark, it shines brightly, and it emits a yellowish tidal ray. The rays produced by AGNs are well known, they emit a dazzling beam of light, with a high Eddington ratio, and, according to Bocker et al., they tend to live in the lowest mass galaxies, in the form of BLQSOs, and the Eddington ratio is high. We find that this AGN may be two, one is forming a very large star, and another, untouched, has no activity. This research was supported by NASA grant NNG03J40G. The study was funded by NASA grant NNG03-04-060G. The results of this study are presented in accordance with the spectral spectral spectral spectral spectral spectra of the low-resolution active galactic nuclei (AGNs), which are swollen by supermassive black holes accreting matter from the surrounding clouds. However, it is unclear whether this energy also", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4053, "title": "ARCRAIDER I: Detailed optical and X-ray analysis of the cooling flow cluster Z3146", "abstract": "  We present a detailed analysis of the medium redshift (z=0.2906) galaxy cluster Z3146 which is part of the ongoing ARCRAIDER project, a systematic search for gravitational arcs in massive clusters of galaxies. The analysis of Z3146 is based on deep optical wide field observations in the B, V and R bands obtained with the WFI@ESO2.2m, and shallow archival WFPC2@HST taken with the F606W filter, which are used for strong as well as weak lensing analyses. Additionally we have used publicly available XMM/Newton observations for a detailed X-ray analysis of Z3146. Both methods, lensing and X-ray, were used to determine the dynamical state and to estimate the total mass. We also identified four gravitational arc candidates. We find this cluster to be in a relaxed state, which is confirmed by a large cooling flow with nominal ~1600M_\\odot per year, regular galaxy density and light distributions and a regular shape of the weak lensing mass reconstruction. The mass content derived with the different methods agrees well within 25% at r_{200}=1661 h_{70}^{-1}kpc indicating a velocity dispersion of \\sigma_v=869^{+124}_{-153}km/s. ", "machine_abstract": "We present an in-depth study of the galaxy cluster Z3146, which is one of the most luminous clusters in X-rays known to date (Lx = 1.5 x 1045 erg/s). The main goal of this work was to investigate whether or not there are any signatures of AGN activity associated with the central cD galaxy. We have obtained deep optical spectroscopy for more than 100 galaxies within the virial radius of the cluster using the Keck telescope. In addition we have carried out Chandra observations that allow us to resolve individual point sources down to luminosities as low as Lx ~ 3 x 1041 erg/s. Our results show that:  -The brightest cluster galaxy has a very complex spectrum dominated by emission lines produced by gas ionized by hot stars.  -There is no evidence for significant nuclear activity in the form of broad line regions and/or high excitation narrow-line regions.  -The total mass profile derived from our spectroscopic data agrees well with previous estimates based on gravitational lensing measurements.", "paraphrased_abstract": "\" We have also studied the central cD galaxy, Z3146. It is one of the most luminous clusters in X-rays known to the eye. It is a very complex spectrograph, which is dominated by emission lines of gas ionized by the stars. The result is that the central cD galaxy is not in the central cD galaxy. This work is an important step towards detecting a few nuclear signatures in the central cD galaxy. The results show that the brightest cluster galaxy is a complex, complex spectrum, which is dominated by emission lines produced by hot stars, and no signs of nuclear activity in the form of broad spectral lines or in the form of high-voltage narrow-circuits. The mass of the universe derived from our study is in agreement with other calculations based on gravitational lensing. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3652, "title": "Chi2 and chi3 harmonic generation at a critical power in inhomogeneous doubly resonant cavities", "abstract": "  We derive general conditions for 100 percent frequency conversion in any doubly resonant nonlinear cavity, for both second- and third-harmonic generation via chi2 and chi3 nonlinearities. We find that conversion efficiency is optimized for a certain critical power depending on the cavity parameters, and assuming reasonable parameters we predict 100 percent conversion using milliwatts of power or less. These results follow from a semi-analytical coupled-mode theory framework which is generalized from previous work to include both chi2 and chi3 media as well as inhomogeneous (fully vectorial) cavities, analyzed in the high-efficiency limit where down-conversion processes lead to a maximum efficiency at the critical power, and which is verified by direct finite-difference time-domain (FDTD) simulations of the nonlinear Maxwell equations. Explicit formulas for the nonlinear coupling coefficients are derived in terms of the linear cavity eigenmodes, which can be used to design and evaluate cavities in arbitrary geometries. ", "machine_abstract": "We study the second-harmonic (SH) and third-harmonic (TH) generations in an inhomogeneously broadened cavity with two different resonance frequencies for fundamental wave (FW). We show that, when the intracavity FW intensity reaches its maximum value, both SHG and THG can be enhanced simultaneously by increasing the pumping rate or decreasing the detuning between the two modes. The enhancement is due to the fact that the nonlinear susceptibility becomes larger than zero at this point. This effect has been observed experimentally recently.     In addition, we find that there exists another regime where only one type of harmonics can be generated efficiently while suppressing other types of harmonics. For example, if the intracavity FW field is tuned close to the lower mode frequency, then it will generate mainly SH light but very little TH light; on the contrary, if the intracavty FW field is tuned near the upper mode frequency, then it generates mainly TH light but almost no SH light.", "paraphrased_abstract": "Besides, we have found that the second harmonic (SH) and third harmonic (TH) harmonics may be made more energetic with each other when the second harmonic becomes sufficiently high. We show that when the second harmonic becomes sufficiently high the second harmonics may be increased in the sense that they are increased by increasing the pumping speed or decreasing the detuning between the two modes. Moreover, it was observed that, when the third harmonic was at its highest point, the SHG and THG intensified simultaneously by increasing the amplitude of the pump or by reducing the detuning between the two modes, the SHG and THG grew stronger. This result was observed in the field, and was observed in the present case. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.218, "title": "Observation of Electron-Hole Puddles in Graphene Using a Scanning Single Electron Transistor", "abstract": "  The electronic density of states of graphene is equivalent to that of relativistic electrons. In the absence of disorder or external doping the Fermi energy lies at the Dirac point where the density of states vanishes. Although transport measurements at high carrier densities indicate rather high mobilities, many questions pertaining to disorder remain unanswered. In particular, it has been argued theoretically, that when the average carrier density is zero, the inescapable presence of disorder will lead to electron and hole puddles with equal probability. In this work, we use a scanning single electron transistor to image the carrier density landscape of graphene in the vicinity of the neutrality point. Our results clearly show the electron-hole puddles expected theoretically. In addition, our measurement technique enables to determine locally the density of states in graphene. In contrast to previously studied massive two dimensional electron systems, the kinetic contribution to the density of states accounts quantitatively for the measured signal. Our results suggests that exchange and correlation effects are either weak or have canceling contributions. ", "machine_abstract": "We report the observation of electron-hole puddles in graphene using scanning single-electron transistors (SETs). The SET is fabricated on top of an exfoliated monolayer graphene flake and operated at cryogenic temperatures down to 4 K. We observe that the conductance through the SET depends strongly on its position with respect to the underlying graphene sheet, which we attribute to local variations in charge carrier density induced by charged impurities trapped between the substrate and the graphene layer. This effect can be suppressed by applying a gate voltage Vg = -40 V across the graphene sample. Our results demonstrate that the use of SETs as probes for studying electronic properties of two-dimensional materials such as graphene has great potential. In recent years there have been significant advances in the fabrication of devices based on carbon nanotubes [1] , silicon nanowires [2] or semiconductor quantum dots [3] . These nanostructures are used as active elements in various types of sensors [4] , optoelectronic [5] and photovoltaic [6] applications. However, these structures suffer from several drawbacks including poor reproducibility due to their small size and low yield during growth processes [7, 8] . In contrast, graphene [9] offers many advantages over other two dimensional materials [10] : it is mechanically flexible [11] , chemically stable [12] , biocompatible [13] and electrically conductive [14] . Moreover, it can be produced in large quantities via chemical vapor deposition [15] or mechanical exfoliation [16] techniques [17] . Recently, graphene-based field-effect transistors [18] were demonstrated [19, 20] opening up new avenues towards high-performance electronics [21] . Despite all these attractive features, however, one major challenge remains in achieving high-quality electrical contacts to graphene [22] .", "paraphrased_abstract": "At the moment of our writing, we have a very large number of carbon nanotubes, silicon nanowires, and quantum dots. These nanostructures are used in various sensors, optoelectronics, and photovoltaics, but they have some disadvantages: they are small and hardly reproducible, and they require a lot of time for growth. We have been developing graphene-based sensors in recent years, and these devices are promising to bring new advances to electronics and electronics. The most recent breakthroughs have been made in the field of chemical vapor deposition and the chemical vaporization of swollen graphene. These devices have the advantages of elasticity, flexibility, chemical stability, biocompatible, and electrically conductive. But there are some disadvantages to these electrodes, such as poor electrical permeability and low yield during the growth process. In contrast, graphene-based electrodes have a great advantage over other two-dimensional materials, in that it is flexible, chemically stable, biocompatible, and electrically conductive, it is able to be produced in large quantities by chemical vapor deposition or mechanical exfoliation, which, however, are not very suitable. The polarity of the electrodes, which is very strong, depends on the position of the graphene layer, which, according to our measurements, is caused by the local", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0541, "title": "Fundamental Parameters of He-Weak and He-Strong Stars", "abstract": "  We carried out low resolution spectroscopic observations in the wavelength range 3400-4700 A of 20 He-weak and 8 He-strong stars to determine their fundamental parameters by means of the Divan-Chalonge-Barbier (BCD) spectrophotometric system. For a few He-weak stars we also estimate the effective temperatures and the angular diameters by integrating absolute fluxes observed over a wide spectral range. Non-LTE model calculations are carried out to study the influence of the He/H abundance ratio on the emergent radiation of He-strong stars and on their Teff determination. We find that the effective temperatures, surface gravities and bolometric absolute magnitudes of He-weak stars estimated with the BCD system and the integrated flux method are in good agreement between each other, and they also agree with previous determinations based on several different methods. The mean discrepancy between the visual absolute magnitudes derived using the Hipparcos parallaxes and the BCD values is on average 0.3 mag for He-weak stars, while it is 0.5 mag for He-strong stars. For He-strong stars, we note that the BCD calibration, based on stars in the solar environment, leads to overestimated values of Teff. By means of model atmosphere calculations with enhanced He/H abundance ratios we show that larger He/H ratios produce smaller BD which naturally explains the Teff overestimation. We take advantage of these calculations to introduce a method to estimate the He/H abundance ratio in He-strong stars. The BD of HD 37479 suggests that the Teff of this star remains fairly constant as the star spectrum undergoes changes in the intensity of H and He absorption lines. Data for the He-strong star HD 66765 are reported for the first time. ", "machine_abstract": "We present the results of our study on fundamental parameters of helium-weak (He-W) stars, which are evolved red giants with helium abundance Y=0.25-0.40 in their envelopes.  We have calculated evolutionary tracks for these stars using the Yale Rotating Evolution Code including diffusion processes as well as nuclear reactions relevant to stellar evolution up to carbon burning stage. The mass loss rates were taken into account by adopting Reimers formula. In order to investigate the effect of rotation on the structure and evolution of He-W stars we performed calculations for different initial rotational velocities ranging from 0 to 300 km/sec.  The comparison between theoretical predictions and observations shows that there is no agreement between theory and observation if one adopts standard values of mixing length parameter $\\alpha$ = 1.9-2.1. However, it was found that the best fit can be obtained when the value of $\\alpha$ is reduced down to $1.5 \\pm 0.1$.", "paraphrased_abstract": "When a mixture is modeled on a two-dimensional, double-layered model of a star, it is calculated to the extent of two hundred and twenty degrees of separation. The comparison between the theory and the observation results in a disagreement that can not be resolved except for the -alpha, which is 1.9-2.1, and which, on the other hand, is lowered to a value of 0.1 pm, which is quite different from the -alpha. We present the results of our research on the basic properties of He-W stars, which are red giants of helium abundance, Y=0.2-0.3 in their envelopes. The mass-loss rate is calculated by the Reimers formula. We present the fundamental parameters of He-W stars, which are evolved red giants, with a helium abundance of Y=0.25-0.27 in their envelopes, which are in the envelopes of red giants. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3758, "title": "The habitability of super-Earths in Gliese 581", "abstract": "  Aims: The planetary system around the M star Gliese 581 consists of a hot Neptune (Gl 581b) and two super-Earths (Gl 581c and Gl 581d). The habitability of this system with respect to the super-Earths is investigated following a concept that studies the long-term possibility of photosynthetic biomass production on a dynamically active planet. Methods: A thermal evolution model for a super-Earth is used to calculate the sources and sinks of atmospheric carbon dioxide. The habitable zone is determined by the limits of biological productivity on the planetary surface. Models with different ratios of land / ocean coverage are investigated. Results: The super-Earth Gl 581c is clearly outside the habitable zone, since it is too close to the star. In contrast, Gl 581d is a tidally locked habitable super-Earth near the outer edge of the habitable zone. Despite the adverse conditions on this planet, at least some primitive forms of life may be able to exist on its surface.Therefore, Gl 581d is an interesting target for the planned TPF/Darwin missions to search for biomarkers in planetary atmospheres. ", "machine_abstract": "We present the results of our study on the possible existence and stability of terrestrial planets around the star Gliese 581, which is located at about 20 light-years away from Earth. We have performed numerical simulations for different orbital configurations of three hypothetical terrestrial planets with masses ranging between 1 to 10 times that of Earth's mass (1-10 M\u2295). Our calculations show that all these systems are dynamically stable over time scales longer than 100 Myr. The most massive planet has an eccentric orbit with e=0.2 and its periastron distance ranges between 0.05 AU and 0.15 AU depending on the initial conditions used. This planet can be considered as a hot Jupiter-like planet because it orbits very close to its host star. However, we find that there exists another region where two or more terrestrial planets may exist stably. In this region, one of them could be a super-Earth-type planet with a mass larger than 5M\u2295 but smaller than 8M\u2295.", "paraphrased_abstract": "There are two or three planets that are very stable, and one is a super Earth with a mass of at least five million, but smaller than eight million. This planet is the hot Jupiter, because it is near its host. We present the results of our study, which are based on the numerical analysis of three hypothetical planets with masses between one and five million, which are different from that of the earth, and with masses of at least five thousand, or about ten million. This planet is a hot Jupiter, since it orbits very close to the host star. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2886, "title": "Control of mechanical systems on Lie groups and ideal hydrodynamics", "abstract": "  In contrast to the Euler-Poincar{\\'e} reduction of geodesic flows of left- or right-invariant metrics on Lie groups to the corresponding Lie algebra (or its dual), one can consider the reduction of the geodesic flows to the group itself. The reduced vector field has a remarkable hydrodynamic interpretation: it is a velocity field for a stationary flow of an ideal fluid. Right- or left-invariant symmetry fields of the reduced field define vortex manifolds for such flows.   Consider now a mechanical system, whose configuration space is a Lie group and whose Lagrangian is invariant to left translations on that group, and assume that the mass geometry of the system may change under the action of internal control forces. Such system can also be reduced to the Lie group. With no controls, this mechanical system describes a geodesic flow of the left-invariant metric, given by the Lagrangian, and thus its reduced flow is a stationary ideal fluid flow on the Lie group. The standard control problem for such system is to find the conditions, under which the system can be brought from any initial position in the configuration space to another preassigned position by changing its mass geometry. We show that under these conditions, by changing the mass geometry, one can also bring one vortex manifold to any other preassigned vortex manifold. ", "machine_abstract": "We consider the controllability properties for linearized models of mechanical systems evolving on Lie groups, with particular emphasis on the case where the configuration space is the group of rigid motions in three dimensions. We show that these systems are not locally controllable around any equilibrium point unless they have an additional symmetry which we call ``isotropy''. This result has important consequences for the study of ideal fluid flows governed by Euler's equations. In fact, it implies that such flows cannot be controlled at all if their initial conditions belong to certain invariant sets (the so-called vortex cores). The results presented here were obtained during my PhD thesis under the supervision of Prof. Miroslav Krstic at ETH Zurich. I would like to thank him as well as Dr. David J. Simoes for many useful discussions about this work. Controllability problems arise naturally when one considers optimal control or feedback stabilization questions for dynamical systems. For example, given some desired state trajectory $x_d(t)$, one may ask whether there exists a control input $u(t)$ steering the system from its current state $x$ towards the target trajectory $x_d(t), \\forall t\\in [0,T]$ within finite time $T$.     In general, the answer to this question depends strongly on the choice of the cost functional used to measure the distance between trajectories. However, if one restricts attention to quadratic costs, then the problem becomes much more tractable since it can be formulated as a convex optimization program over the set of admissible controls. Moreover, if the dynamics evolve on a vector space, then the solution to this problem always exists; see e.g., [1] . Unfortunately, things become considerably more complicated if the underlying state space is a manifold instead; see [2] , [3, Chapter 4]     [4]-[6] . Indeed, even though the problem remains convex whenever the cost function is quadratic, the existence of solutions no longer follows automatically [7]; see also [8] - [10] .     In recent years, several authors have studied the controllability properties of various classes of nonlinear systems evolving on manifolds using", "paraphrased_abstract": "Moreover, if the dynamics of a fluid are developed on a vector, the solution to this problem is always the same: e.g., e.g., e.g., e.g., e.g., [1], e.g., [2] [3] [4] [6]. In this case, the controllability of the dynamical systems is naturally the most important topic for physics: they can\u2019t be controlled at all if they are not invariant to certain invariant sets (the so-called vortex cores). In this sense, the problem is convex, but when the cost function is quadratic, the problem is much easier to solve, since one can optimize one\u2019s program with the set of controls, and we\u2019ve obtained a solution to this problem, as shown in Figure 1, \u201cthe solution to this problem depends on the choice of cost function, the distance between two points, and therefore, in general, the solution is dependent on the choice of cost function, which is, in general, a convex optimization of the system. This result, however, has important consequences for the study of the ideal fluid flows governed by Euler\u2019s equations, in that it follows that it can\u2019t be controlled at all if the initial conditions are governed by some", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0296, "title": "Generalized Twistor Transform And Dualities, With A New Description of Particles With Spin, Beyond Free and Massless", "abstract": "  A generalized twistor transform for spinning particles in 3+1 dimensions is constructed that beautifully unifies many types of spinning systems by mapping them to the same twistor, thus predicting an infinite set of duality relations among spinning systems with different Hamiltonians. Usual 1T-physics is not equipped to explain the duality relationships and unification between these systems. We use 2T-physics in 4+2 dimensions to uncover new properties of twistors, and expect that our approach will prove to be useful for practical applications as well as for a deeper understanding of fundamental physics. Unexpected structures for a new description of spinning particles emerge. A unifying symmetry SU(2,3) that includes conformal symmetry SU(2,2)=SO(4,2) in the massless case, turns out to be a fundamental property underlying the dualities of a large set of spinning systems, including those that occur in high spin theories. This may lead to new forms of string theory backgrounds as well as to new methods for studying various corners of M theory. In this paper we present the main concepts, and in a companion paper we give other details. ", "machine_abstract": "The generalized twistor transform is an extension of the ordinary twistor transform to include particles with spin in arbitrary dimensions. It has been used recently for describing scattering amplitudes involving massive particles as well as massless ones. In this talk I will present some new results on the subject including a description of how one can use it to describe particles beyond free or massless theories. The talk will also cover recent developments related to dualities between different quantum field theories. This includes duality relations that have been found using the generalized twistor transform such as those relating N=4 super Yang-Mills theory to self-dual gravity and its supersymmetric extensions. Finally we will discuss how these ideas are connected to string theory. Generalized twistors were introduced by Witten more than twenty years ago [1] . They provide a powerful tool for studying gauge theories in general relativity [2] , and they play important roles in understanding various aspects of string/M-theory [3] . In particular, the so-called Penrose limit [4] provides a way to relate scattering amplitudes in gauge/gravity theories at weak coupling to correlation functions in conformal field theories (CFTs) at strong coupling [5] . Recently there has been renewed interest in the study of twistors [6] - [11] due to their applications in computing scattering amplitudes [12] - [16] . For example, the tree-level S-matrix elements of gluons [17] and gravitons [18] in four-dimensional N = 4 Super-Yang-Mills theory (SYM), which was conjectured to be dual to type-IIB superstrings [19] , were computed via the generalized twistor transform [20] .", "paraphrased_abstract": "He then formulated a generalized spinor which is extended to include particles of arbitrary dimension. This is particularly useful in the case of gluons and gravitons in four-dimensional N = 4 SYM. Those are supposed to be two different superstrings of type IIb. In this regard, we will describe the recent developments in the field of the generalized spinor. These are referred to as Penrose transforms. They can be used to relate amplitudes of gauge and gravity in weak coupling to the correlation functions of conformal field theories, or in general. In this way, the generalized spinor can be used to describe particles with spin. In particular, the generalized spinor is useful for the study of the duality between different fields of quantum mechanics. In particular, it has been shown to be possible to find amplitudes of the N=4 super-Yang-Mills theory. This is, in particular, an extension of the ordinary spinor, which allows the expansion of particles to a large scale, i.e., they are characterized by the so-called Penrose limit, which gives the ability to equate the scattering amplitudes of gauge and gravity theories in a weak way with the correlation coefficients of conformal field theories in a strong way. Then, after presenting some recent results, we will explain", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0106, "title": "Multiple Parton Scattering in Nuclei: Quark-quark Scattering", "abstract": "  Modifications to quark and antiquark fragmentation functions due to quark-quark (antiquark) double scattering in nuclear medium are studied systematically up to order \\cal{O}(\\alpha_{s}^2)$ in deeply inelastic scattering (DIS) off nuclear targets. At the order $\\cal{O}(\\alpha_s^2)$, twist-four contributions from quark-quark (antiquark) rescattering also exhibit the Landau-Pomeranchuck-Midgal (LPM) interference feature similar to gluon bremsstrahlung induced by multiple parton scattering. Compared to quark-gluon scattering, the modification, which is dominated by $t$-channel quark-quark (antiquark) scattering, is only smaller by a factor of $C_F/C_A=4/9$ times the ratio of quark and gluon distributions in the medium. Such a modification is not negligible for realistic kinematics and finite medium size. The modifications to quark (antiquark) fragmentation functions from quark-antiquark annihilation processes are shown to be determined by the antiquark (quark) distribution density in the medium. The asymmetry in quark and antiquark distributions in nuclei will lead to different modifications of quark and antiquark fragmentation functions inside a nucleus, which qualitatively explains the experimentally observed flavor dependence of the leading hadron suppression in semi-inclusive DIS off nuclear targets. The quark-antiquark annihilation processes also mix quark and gluon fragmentation functions in the large fractional momentum region, leading to a flavor dependence of jet quenching in heavy-ion collisions. ", "machine_abstract": "We present the first calculation of quark-quark scattering at next-to-leading order (NLO) accuracy within the framework of multiple parton scattering (MPI). We show that this process is dominated by gluons and quarks with small transverse momenta, which are produced via initial state radiation off one or both incoming hadrons. The cross section for quark-quark scattering can be written as an integral over the transverse momentum distribution of these radiated particles times their squared matrix element. This allows us to calculate the total cross section using standard Monte Carlo techniques. In addition we provide analytic expressions for the differential distributions in rapidity and azimuthal angle between the outgoing quarks. Our results agree well with those obtained numerically. Finally, we discuss how our formalism could be used to study MPI effects on jet production in proton-nucleus collisions. PACS numbers: 12.38.Qk, 13 .60.Le", "paraphrased_abstract": "I will present an example of the first numerical application of MPI to proton-nucleus collisions. We present an analysis of the whole cross-section of the quark-quark collision, which is at least as accurate as that for the other two particles, and which we consider to be of the highest accuracy in the theory of multiple parton scattering. We show that this process is dominated by gluons and quarks with small transverse moments that are emitted from the hadrons by the initial re-emission of one or the other. We prove that the process is dominated by gluons and quarks with small transverse moments, which are produced by the re-emission of one or the other hadrons. We conclude by demonstrating the accuracy of our calculations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1521, "title": "Cosmological Shock Waves in the Large Scale Structure of the Universe: Non-gravitational Effects", "abstract": "  Cosmological shock waves result from supersonic flow motions induced by hierarchical clustering of nonlinear structures in the universe. These shocks govern the nature of cosmic plasma through thermalization of gas and acceleration of nonthermal, cosmic-ray (CR) particles. We study the statistics and energetics of shocks formed in cosmological simulations of a concordance $\\Lambda$CDM universe, with a special emphasis on the effects of non-gravitational processes such as radiative cooling, photoionization/heating, and galactic superwind feedbacks. Adopting an improved model for gas thermalization and CR acceleration efficiencies based on nonlinear diffusive shock acceleration calculations, we then estimate the gas thermal energy and the CR energy dissipated at shocks through the history of the universe. Since shocks can serve as sites for generation of vorticity, we also examine the vorticity that should have been generated mostly at curved shocks in cosmological simulations. We find that the dynamics and energetics of shocks are governed primarily by the gravity of matter, so other non-gravitational processes do not affect significantly the global energy dissipation and vorticity generation at cosmological shocks. Our results reinforce scenarios in which the intracluster medium and warm-hot intergalactic medium contain energetically significant populations of nonthermal particles and turbulent flow motions. ", "machine_abstract": "We present an analysis of cosmological shock waves in the large scale structure (LSS) of the universe, which are generated by gravitational collapse and merging processes between dark matter haloes. We show that these shocks can be detected as discontinuities in the density field traced by galaxy clusters using weak lensing observations. In addition to their gravitational effects on the LSS, we find that they also produce non-gravitational signatures such as: i) anisotropic pressure gradients; ii) bulk motions; iii) temperature fluctuations; iv) magnetic fields. These effects may have important consequences for our understanding of the formation and evolution of large-scale structures. The detection of these phenomena will provide new insights into the physics behind cosmic acceleration.     Keywords: Cosmology, Cosmic Acceleration, Dark Matter Halos, Weak Gravitational Lensing, Galaxy Clusters         1 Introduction     Recent observational data suggest that the expansion history of the Universe is accelerating at late times [1\u20133] . This phenomenon has been explained within the framework of general relativity with the introduction of a mysterious component known as \"dark energy\" [4] , whose nature remains unknown today. However, it seems clear that this exotic form of energy must play some role in explaining the observed accelerated expansion of the universe [5\u20137] .   In order to understand how dark energy affects the dynamics of the universe, one needs to study its interaction with other components of the cosmos. One possible way to do so is through the use of numerical simulations [8\u201310] . Another possibility consists of studying the effect of dark energy on the growth rate of perturbations [11\u201313] . Finally, another approach involves analyzing the statistical properties of the distribution of galaxies [14\u201318] or galaxy clusters [19\u201321] .", "paraphrased_abstract": "But, it is said, the mysterious nature of this energy remains unknown. In the past, it has been assumed, in general, that the expansion of the universe was accelerated in recent times. The study of the universe and its interaction with the other elements in the universe was studied in numerical simulations, but one approach could be used to investigate the effect of dark energy on the growth of perturbations. One method is to examine the influence of dark energy on the growth of perturbations. We also analyze the effects of dark energy on the density of galaxies, in which the galaxy clusters are located. We examine the effect of dark energy on the growth of perturbations and the density of the galaxy clusters. In order to study the effect of dark energy on the structure of the universe, we have to study the interaction of it with other elements. We will present here the effects of the cosmological shock waves in the large-scale structure of the universe, resulting from the collapse of the dark matter haloes, induced by gravity, forming in the apex of the galaxy clusters, by a weak glare. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2532, "title": "Signatures of Heavy Z-prime in the Extra U(1) Superstring Inspired Model: RGEs Analysis", "abstract": "  In the extra U(1) superstring inspired model, we examine the electroweak and U(1)-prime symmetry breaking with the singlet and exotic quark D, D+{\\c}along with the study of heavy Z-prime boson in accordance with the top quark mass region. For this, we have done the analysis of complete renormalization group equations (RGEs)pertaining to the anomaly free E-{\\6}-Eta model of rank 5. The Z-prime is found to the order of TeV or above with allowed small Z-Zprime mixing angle, for which the large singlet VEV is required. This is done by considering the only non-universality of Yukawa couplings at GUT scale because these do not obey the E-{\\6}relationship and also satisfies the unitarity constraints both at GUT and weak scale, where rest of the parameters, i.e., gaugino masses, tri-linear couplings, and soft supersymmetric breaking masses are kept universal at GUT scale with the gauge couplings unification. The large value of Yukawa couplings (order of 1) triggered the symmetry breaking radiatively and induces the effective-Mu parameter at the electroweak scale and lead to a viable low energy spectrum at weak scale. ", "machine_abstract": "In this work, we study the renormalization group equations (RGEs) for extra U(1) supersymmetric string inspired model with heavy Z' prime and its effects on gauge coupling unification at one-loop level. We find that the inclusion of new particles such as vector-like quarks and leptons can significantly affect the running behavior of gauge couplings. In particular, it is found that the presence of these new particles leads to an enhancement effect on the evolution speed of gauge couplings which may be helpful to solve the gauge hierarchy problem. Furthermore, by using the experimental data of low energy physics, we obtain some constraints on the mass spectrum of extra particles involved in our model. Finally, we also discuss briefly about the possible signatures of heavy Z'-prime boson at future colliders. The results are summarized below.  I. INTRODUCTORY REMARK The Standard Model (SM), based on SU(3) C \u00d7SU(2) L \u00d7U(1) Y gauge symmetry, has been very successful in describing all known phenomena upto TeV scale energies [1] . However, there exist several open questions related to SM like fermion masses and mixing angles [2] , neutrino oscillations [3] etc., which cannot be explained within the framework of SM. To address these issues, many extensions beyond SM have been proposed [4] - [8] . Among them, Grand Unified Theory (GUTs) [9] provides a natural solution to the above mentioned problems [10] . It predicts the existence of superheavy gauge bosons called GUT-scale gauge bosons [11] whose masses lie around 10 16 GeV [12] . These GUT-scale gauge boson interactions lead to non-renormalizable operators [13] which break the SM gauge symmetries [14] . Therefore, they should not appear in any physical process [15] . This implies that their contributions must vanish when summed over all states [16] . Thus, the appearance of these nonrenormalizable operators will spoil the successes of SM [17] .", "paraphrased_abstract": "But a great deal of what is known about SM, the quark masses and the mixing angle, the neutrino oscillations, are not sufficiently explained in SM, and this necessitates further investigation of the underlying physical processes. In this work we examine the renormalization group equations of the extra-U(1) supersymmetric string model with the heavy Z\u2019-prime, and examine the effect of this on the stability of the couplings, and we consider that the addition of these new particles, such as the ray-shaped quarks and the leptons, would thereby improve the stability of the couplings and may be useful to solve the gauge hierarchy. In this study we consider the effect of the addition of new particles, such as the quarks and the rays, of the atoms, and the effects of the extra atoms. This experiment is based on the experiment of low energy physics, and the results of our work are summarized below. The SM has been very successful, based on the C-C-C-L-U-Y gauge symmetry, which has been a success for all known phenomena, up to the energy of TeV. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.407, "title": "SNO+: predictions from standard solar models and spin flavour precession", "abstract": "  Time variability of the solar neutrino flux especially in the low and intermediate energy sector remains an open question and, if it exists, it is likely to be originated from the magnetic moment transition from active to light sterile neutrinos at times of intense solar activity and magnetic field. We examine the prospects for the SNO+ experiment to address this important issue and to distinguish between the two classes of solar models which are currently identified as corresponding to a high (SSM I) and a low (SSM II) heavy element abundance. We also evaluate the predictions from these two models for the Chlorine experiment event rate in the standard LMA and LMA+Spin Flavour Precession (SFP) scenarios. It is found that after three years of SNO+ data taking, the pep flux measurement will be able to discriminate between the standard LMA and LMA+SFP scenarios, independently of which is the correct solar model. If the LMA rate is measured, SFP with $B_0 \\sim 280kG$ for the resonant $\\Delta m^2_{01}$ can be excluded at more than $4\\sigma$. A low rate would signal new physics, excluding all the 90% allowed range of the standard LMA solution at 3$\\sigma$, and a time variability would be a strong signature of the SFP model. The CNO fluxes are the ones for which the two SSM predictions exhibit the largest differences, so their measurement at SNO+ will be important to favour one or the other. The distinction will be clearer after LMA or SFP are confirmed with pep, but still, a CNO measurement at the level of SSM I/LMA will disfavour SSM II at about $3 \\sigma$. We conclude that consistency between future pep and CNO flux measurements at SNO+ and Chlorine would either favour an LMA+SFP scenario or favour SSM II over SSM I. ", "machine_abstract": "The SNO+ experiment is designed to measure the neutrino fluxes in the energy range between 1 MeV and 20 MeV, with an expected sensitivity at low energies comparable to that achieved by Super-Kamiokande (SK). The measurement will be performed using two detection techniques: charged-current interactions on deuterium nuclei via elastic scattering off electrons; neutral current reactions on carbon nuclei through coherent elastic scattering off neutrons.     In this work we present results for the predicted rates of these processes as well as their uncertainties based on state-of-the-art Standard Solar Models (SSMs) and nuclear cross sections. We also discuss how the experimental data can help to constrain SSM parameters such as the helium abundance YHe or the heavy element abundances Z/X. Finally, we show how the combination of different experiments may allow us to test whether the observed neutrinos are produced mainly in pp-chain fusion reactions or in CNO cycles.", "paraphrased_abstract": ", spherical fusions or CNOs. We have discussed the general rates of these processes, their uncertainties, and the uncertainty of the SSM, such as the helium YHe and the heavy element X., and we have discussed how to recast the SSM with certain parameters, such as the abundance of helium and the density of heavy elements, respectively. This work is dedicated to the analysis of the concentration of neutrinos at a temperature of 1 MeV to 20 MeV. It has a sensitivity of ten times higher than the S.K., for example. The SNO+ experiment is planned to measure the concentration of neutrinos at a temperature of one and a half MeV, and its accuracy is expected to be as high as the S.K. experiment. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0703, "title": "Production of a sterile species: quantum kinetics", "abstract": "  Production of a sterile species is studied within an effective model of active-sterile neutrino mixing in a medium in thermal equilibrium. The quantum kinetic equations for the distribution functions and coherences are obtained from two independent methods: the effective action and the quantum master equation. The decoherence time scale for active-sterile oscillations is $\\tau_{dec} = 2/\\Gamma_{aa}$, but the evolution of the distribution functions is determined by the two different time scales associated with the damping rates of the quasiparticle modes in the medium: $\\Gamma_1=\\Gamma_{aa}\\cos^2\\tm ; \\Gamma_2=\\Gamma_{aa}\\sin^2\\tm$ where $\\Gamma_{aa}$ is the interaction rate of the active species in absence of mixing and $\\tm$ the mixing angle in the medium. These two time scales are widely different away from MSW resonances and preclude the kinetic description of active-sterile production in terms of a simple rate equation. We give the complete set of quantum kinetic equations for the active and sterile populations and coherences and discuss in detail the various approximations. A generalization of the active-sterile transition probability \\emph{in a medium} is provided via the quantum master equation. We derive explicitly the usual quantum kinetic equations in terms of the ``polarization vector'' and show their equivalence to those obtained from the quantum master equation and effective action. ", "machine_abstract": "We study the production of a sterile species in an open system with two stable particles and one unstable particle, where the decay products are not observed.  We show that if the initial state is pure then there exists no final mixed state which can be reached by unitary evolution. This result implies that the production of a sterility cannot occur under any circumstances for such systems. If we allow the possibility to prepare arbitrary states as input, however, it turns out that the production of a certain kind of sterility may still take place. In this case, the output state will always contain some amount of entanglement between the subsystems corresponding to the different types of particles involved. The results presented here have been obtained within the framework of Quantum Kinetic Theory (QKT). QKT provides a description of non-equilibrium phenomena at mesoscopic scales based on the concept of entropy production rate. It has recently attracted considerable attention due to its potential applications in many areas ranging from physics to biology.  I. INTRODUCTORY REMARK The phenomenon of spontaneous emission plays a crucial role in modern physics. For example, it is responsible for the cooling process in laser-cooling experiments [1] . On the other hand, spontaneous emission also leads to decoherence effects [2] , which limit the performance of quantum information processing devices [3] . In recent years, several authors [4] - [8] studied the problem of producing a particular type of \"sterility\" in open quantum systems. A state is called \"sterile\" when it does not interact with itself or another given set of states [9] . More specifically, let us consider a bipartite Hilbert space H = H 1 \u2297H 2 , where dim(H i ) = N i . Then, a density matrix \u03c1 \u2208 B(H) is said to be \"sterile\" wrt. a subset S \u2286 H iff Tr[\u03c1\u03c3] = 0 for all \u03c3 \u2208 S. Here, Tr denotes the trace operation over either H 1 or H 2 depending on whether \u03c3 belongs to H 1 or H 2 respectively. Note that the notion of \"ster", "paraphrased_abstract": "It was recently studied in the field of quantum kinetics by several authors, namely: - The phenomenon of spontaneous evaporation plays an important role in modern physics, namely, for example, it is the cooling process for lasers, but it also leads to decoherence, which can limit the speed of quantum information processing. Several authors have recently been studying the question of a certain sterility in open systems. We will assume a bipartite Hilbert space H = H 1 - H 2 - where   -  - a density matrix S - H is called sterile if it is not connected with itself or with another spectra, and it is thus called sterile if it is not related to itself or to another spectra. In our example, it is a pure state, and there is no intermediate state, a mixed one, which cannot be created in such a system. We study the formation of a sterile species in an open system with two stable particles and one unstable one, where the decay of the sterile species is not observed. However, if the state is unresolved, it is not impossible to produce a sterile state. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1645, "title": "Dimer Models, Free Fermions and Super Quantum Mechanics", "abstract": "  This note relates topics in statistical mechanics, graph theory and combinatorics, lattice quantum field theory, super quantum mechanics and string theory. We give a precise relation between the dimer model on a graph embedded on a torus and the massless free Majorana fermion living on the same lattice. A loop expansion of the fermion determinant is performed, where the loops turn out to be compositions of two perfect matchings. These loop states are sorted into co-chain groups using categorification techniques similar to the ones used for categorifying knot polynomials. The Euler characteristic of the resulting co-chain complex recovers the Newton polynomial of the dimer model. We re-interpret this system as supersymmetric quantum mechanics, where configurations with vanishing net winding number form the ground states. Finally, we make use of the quiver gauge theory - dimer model correspondence to obtain an interpretation of the loops in terms of the physics of D-branes probing a toric Calabi-Yau singularity. ", "machine_abstract": "We present the results on the super quantum mechanics (SQM) in dimer models with free fermion boundary conditions. We show that SQM is equivalent to the usual quantum mechanics for periodic boundary condition case. In addition we find that there are two different types of supersymmetric ground states depending on whether or not the number of particles N is even. For odd N , the ground state has zero energy but it does not have any degeneracy. On the other hand, when N is even, the ground state becomes doubly degenerate. The degeneracy can be understood by using the concept of parity symmetry breaking. Finally we discuss how our result fits into the general framework of supersymmetry. PACS: 11.10.Wx, 05.45.Mt, 11.15.Tk  I. INTRODUCTORY REMARK In this work we study the super quantum mechanics ( S Q M ) i n d im e r m odels w ith f ree fermi b oundary c o ndition s . T hese models were first introduced by Rokhsar et al [1] as an exactly solvable model which describes spin-1/2 Heisenberg antiferromagnet on a square lattice. They showed that these models exhibit many interesting properties such as spontaneous dimerization at low temperatures [2] . The main purpose of this work is to investigate the effect of boundary conditions on the supersymmetric structure of the system. It turns out that the supersymmetric structure depends crucially on the boundary conditions imposed on the system. As will become clear later, the supersymmetric structure changes drastically if one switches between periodic and free-fermion boundary conditions.", "paraphrased_abstract": "As you will observe later, this supersymmetric structure depends on the boundary conditions of the system. It turns out that the supersymmetric structure is shaped in an important way by the boundary conditions of the system. The main object of this research is to investigate the effect of boundary conditions on the supersymmetric structure of the system. We will investigate the supersymmetric structure of the system in dimly stacked models, in the form of Heisenberg antiferromagnets, in the form of a square sphere. The models were introduced by Rokhsar and others as an exactly-solvable model of a spin-1/2 Heisenberg antiferromagnet on a square sphere. The models were shown to have very interesting properties, and to be completely inverted at low temperatures. We will also explain the properties of the supersymmetric model in general, and then we will explain in detail how it fits into the general framework of supersymmetry. Then we will present the results of the supersymmetrical model of the ground, in a slender form, with free fermions. The supersymmetrical model was introduced by Rokhsar et al as a perfectly solvable, and very accurate one, namely, the slender antiferromagnet with a double energy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2841, "title": "A High-Throughput Cross-Layer Scheme for Distributed Wireless Ad Hoc Networks", "abstract": "  In wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. In recent work, Ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. However, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. In this paper, a technique that significantly reduces the collaboration overhead is proposed. It consists of two phases. In the first phase, nodes transmit locally in a random access fashion. Collisions, when they occur, are viewed as linear mixtures of the collided packets. In the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. This step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. The scheme requires that each node has global knowledge of the network coordinates. The proposed scheme can achieve high throughput, which in certain cases exceeds one. ", "machine_abstract": "In this work, we propose a novel cross-layer scheme to improve the performance of distributed wireless ad hoc networks (DWAHNs). The proposed scheme is based on an adaptive routing protocol and a dynamic channel allocation algorithm. In particular, our approach uses a new metric called \"expected transmission count\" in order to select routes with minimum expected number of transmissions per packet delivery. Furthermore, it employs a modified version of the well-known proportional fairness criterion as well as a utility function that takes into account both the current network conditions and user preferences. Finally, the proposed scheme also incorporates a mechanism which allows nodes to dynamically change their operating channels according to the traffic load at each node. Extensive simulation experiments are conducted using NS-2 simulator to evaluate the effectiveness of the proposed scheme under different scenarios. Results show that the proposed scheme outperforms existing approaches by achieving higher throughput while maintaining low end-to-end delay and packet loss rate.", "paraphrased_abstract": "It is also an improved approach to the transmission of ad-hoc packets. The results show that the proposed approach is more effective than existing methods, in the high throughput and the low packet loss rate. In addition, it incorporates a new approach, a new metric called the expected transmission count, to select routes which are least likely to transmit packets at all. The proposed method employs a modified version of the well-known proportional fairness, and an utility function that takes into account the current conditions and user preferences. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2384, "title": "Galois theory of iterated endomorphisms", "abstract": "  Given an abelian algebraic group $A$ over a global field $F$, $\\alpha \\in A(F)$, and a prime $\\ell$, the set of all preimages of $\\alpha$ under some iterate of $[\\ell]$ generates an extension of $F$ that contains all $\\ell$-power torsion points as well as a Kummer-type extension. We analyze the Galois group of this extension, and for several classes of $A$ we give a simple characterization of when the Galois group is as large as possible up to constraints imposed by the endomorphism ring or the Weil pairing. This Galois group encodes information about the density of primes $\\p$ in the ring of integers of $F$ such that the order of $(\\alpha \\bmod{\\p})$ is prime to $\\ell$. We compute this density in the general case for several classes of $A$, including elliptic curves and one-dimensional tori. For example, if $F$ is a number field, $A/F$ is an elliptic curve with surjective 2-adic representation and $\\alpha \\in A(F)$ with $\\alpha \\not\\in 2A(F(A[4]))$, then the density of $\\mathfrak{p}$ with ($\\alpha \\bmod{\\p}$) having odd order is 11/21. ", "machine_abstract": "The Galois group is the fundamental object in classical Galois theory, which studies the solvability of polynomial equations over finite fields.  In this talk we will introduce the notion of ``Galois groups'' for infinite families of polynomials and study their properties using tools from algebraic geometry.   We will also discuss some applications to number theory such as the proof of the abc conjecture by Vojta (joint work with J. Pila). The main results are joint works with A. Chambert-Loir, D. Ghioca, M. Harris, C. Lairez, S. Popescu-Tarauca, B. Stoll, E. Szpiro, T. Tucker-Drob, and V. Vojta. This talk was given at the conference \"Algebraic Geometry and Arithmetic Dynamics\" held on June 24-28, 2013 at MSRI Berkeley. It has been recorded by Adam Hartung.", "paraphrased_abstract": "This lecture was given at the conference of the Algebraic Geometry and Arithmetic Dynamics at Berkeley on June 24, 2013. It was recorded by Adam Hartung. This talk was given at the conference of Algebraic Geometry and Arithmetic Dynamics, held on June 24 and 27, 2013 at MSRI Berkeley. We will introduce the concept of the Galois group for infinite families of polynomials, and we will study their properties by using algebraic geometry. In addition, we will give some general results from our own work, such as the proof of the abc conjecture by Vojta. The most recent results are joint with those of J. Pila. The Galois group is the primary objective of the classical Galois theory, which studies the solvability of polynomial equations over finite fields. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1038, "title": "The Design of Parallel Kinematic Machine Tools Using Kinetostatic Performance Criteria", "abstract": "  Most industrial machine tools have a serial kinematic architecture, which means that each axis has to carry the following one, including its actuators and joints. High Speed Machining highlights some drawbacks of such architectures: heavy moving parts require from the machine structure high stiffness to limit bending problems that lower the machine accuracy, and limit the dynamic performances of the feed axes. That is why PKMs attract more and more researchers and companies, because they are claimed to offer several advantages over their serial counterparts, like high structural rigidity and high dynamic capacities. Indeed, the parallel kinematic arrangement of the links provides higher stiffness and lower moving masses that reduce inertia effects. Thus, PKMs have better dynamic performances. However, the design of a parallel kinematic machine tool (PKMT) is a hard task that requires further research studies before wide industrial use can be expected. Many criteria need to be taken into account in the design of a PKMT. We pay special attention to the description of kinetostatic criteria that rely on the conditioning of the Jacobian matrix of the mechanism. The organisation of this paper is as follows: next section introduces general remarks about PKMs, then is explained why PKMs can be interesting alternative machine tool designs. Then are presented existing PKMTs. An application to the design of a small-scale machine tool prototype developed at IRCCyN is presented at the end of this paper. ", "machine_abstract": "The design and kinematics analysis of parallel kinematic machine tools (PKMTs) is presented in this work using kinetic-static performance criteria. The proposed approach considers the dynamic behavior of PKMTs during their operation, which has been neglected by previous works on PKMTs. In addition to the static stiffness matrix, the inertia properties are also considered for the evaluation of the overall dynamic response of PKMTs. A new method based on the concept of virtual joints is developed to calculate the mass distribution along each leg of the PKMT under consideration. This information can be used as an input data set for further dynamic analyses such as modal or harmonic vibration analysis. Finally, two different PKMTs with three degrees-of-freedom per joint are designed and analyzed using the proposed methodology. It was found that the first PKMT exhibits better dynamic characteristics than its counterpart due to its lower natural frequencies and higher damping ratios.", "paraphrased_abstract": "The kinetic-static performance of PKMTs has been compared with that of other PKMTs. The first one, in fact, exhibits a better kinetic performance than the second, by virtue of the lower natural frequencies and the higher damping ratio. A new method for calculating the mass distribution of each leg of the PKMT is proposed, which can be used to obtain a further dynamical analysis, such as the modal and harmonic vibration analysis. The analysis is based on the principle of virtual joints, and the results of this analysis can be used to obtain the dynamic behavior of the PKMT. The kinetics of the PKMT are used for the determination of the static stiffness of the PKMT, and the inertia properties are also considered. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1668, "title": "A new search for planet transits in NGC 6791", "abstract": "  Context. Searching for planets in open clusters allows us to study the effects of dynamical environment on planet formation and evolution.   Aims. Considering the strong dependence of planet frequency on stellar metallicity, we studied the metal rich old open cluster NGC 6791 and searched for close-in planets using the transit technique.   Methods. A ten-night observational campaign was performed using the Canada-France-Hawaii Telescope (3.6m), the San Pedro M\\'artir telescope (2.1m), and the Loiano telescope (1.5m). To increase the transit detection probability we also made use of the Bruntt et al. (2003) eight-nights observational campaign. Adequate photometric precision for the detection of planetary transits was achieved.   Results. Should the frequency and properties of close-in planets in NGC 6791 be similar to those orbiting field stars of similar metallicity, then detailed simulations foresee the presence of 2-3 transiting planets. Instead, we do not confirm the transit candidates proposed by Bruntt et al. (2003). The probability that the null detection is simply due to chance coincidence is estimated to be 3%-10%, depending on the metallicity assumed for the cluster.   Conclusions. Possible explanations of the null-detection of transits include: (i) a lower frequency of close-in planets in star clusters; (ii) a smaller planetary radius for planets orbiting super metal rich stars; or (iii) limitations in the basic assumptions. More extensive photometry with 3-4m class telescopes is required to allow conclusive inferences about the frequency of planets in NGC 6791. ", "machine_abstract": "We report on the detection and characterization of two transit-like events observed with Spitzer Space Telescope (Werner et al., 2004) at 3.6 microns, which we interpret as being due to planets orbiting an M-dwarf star located near the center of the open cluster NGC 6791.  The first event was detected during our initial survey of this field using the IRAC camera on board Spitzer between 2005 May 24 and 2006 January 1.   We subsequently obtained follow-up observations over three epochs spanning more than one year that allowed us to confirm its planetary nature by detecting it again in all four bands simultaneously.  This is only the second time such a confirmation has been made after the original discovery observation.  The second event was discovered serendipitously while searching for additional transiting objects around the same target star following up on the first event.  Both events are characterized by their short duration (\u223c3 hours), shallow depth (0.1%-0.2%), and relatively high impact parameter (b=0.7-1).  These properties suggest they may be Neptune-sized or smaller planets.  Follow-up radial velocity measurements will be required to determine whether these candidates are indeed bona fide exoplanets.", "paraphrased_abstract": "The first was detected in our first observation in the field of the Spitzer between the 24th of May and the 24th of January. We followed up the observation with other observations over a period of more than one year, so that we could verify the planetary nature of these objects by detecting it once more in all four radii. These two events, a short duration, a short depth, a relatively high impact force, and a low temperature of b, indicate that they are probably not planets, but smaller planets. The second event, a serendipitous finding of another target star, was discovered by chance after the first event. We report the detection and characterization of two transits with the Spitzer space telescope at 3.6 microns, which are interpreted to be caused by planets orbiting an M-dwarf star, at the center of the open cluster of NGC 6791. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0512, "title": "Stable oscillations of a predator-prey probabilistic cellular automaton: a mean-field approach", "abstract": "  We analyze a probabilistic cellular automaton describing the dynamics of coexistence of a predator-prey system. The individuals of each species are localized over the sites of a lattice and the local stochastic updating rules are inspired on the processes of the Lotka-Volterra model. Two levels of mean-field approximations are set up. The simple approximation is equivalent to an extended patch model, a simple metapopulation model with patches colonized by prey, patches colonized by predators and empty patches. This approximation is capable of describing the limited available space for species occupancy. The pair approximation is moreover able to describe two types of coexistence of prey and predators: one where population densities are constant in time and another displaying self-sustained time-oscillations of the population densities. The oscillations are associated with limit cycles and arise through a Hopf bifurcation. They are stable against changes in the initial conditions and, in this sense, they differ from the Lotka-Volterra cycles which depend on initial conditions. In this respect, the present model is biologically more realistic than the Lotka-Volterra model. ", "machine_abstract": "We study the dynamics of a probabilistic cellular automaton (PCA) with two species, prey and predators, which interact in an oscillatory way. The PCA is defined on a square lattice where each site can be occupied by at most one particle of either type. We show that for certain values of the parameters there are stable periodic solutions to this system. These results are obtained using a mean field approximation method. In particular we find that the periodicity depends only on the number of particles per unit area. This dependence agrees well with numerical simulations performed on finite lattices. Finally, we discuss how our model could be used as a simple description of population cycles observed in nature. Probabilistic cellular automata have been widely studied during recent years due to their potential applications in many fields such as physics [1] , biology [2] or computer science [3] . They consist of a set of cells arranged in some regular structure like a grid [4] whose state evolves according to local rules depending on its own state and those of its neighbors [5] . In this work we consider a two-dimensional probabilistic cellular automaton [6] consisting of N sites located on a square lattice L = Z 2 . Each cell i \u2208 L has four possible states denoted by 0, 1, 2 and 3 corresponding respectively to empty space, prey, predator and dead. At time t = 0 all sites are initialized randomly with probability p 0 = 1/4 of being vacant, p 1 = 1/2 of having a prey and p 2 = 1/4 of containing a predator. Then, the evolution rule consists of applying simultaneously the following transition probabilities between consecutive times t and t + 1:", "paraphrased_abstract": "Then we have taken an instance of the cellular automaton of two dimensions, i.e., an array of cells, each arranged in a regular structure, like a grid, and whose state is governed by local rules, according to the state of its neighbours and of the neighboring cells. These systems are composed of cells in a continuous structure, such as a grid, and their condition is determined by local rules according to its own state and those of its neighbours. These systems are known to be stable in certain conditions of the cellular automaton, and we will give an example of a system that is to be regarded as a simple description of the population cycle observed in nature. We consider a two-dimensional system consisting of N locations, each containing two cells, i.e., empty space, prey, predator, dead. Each cell iii has four potential states, namely : p0 = half vacant, p1 = half prey, p2 = half predator, p 2 = half dead. We present the results in the form of a mean-field approximation, and we show that the periodicity depends only on the number of particles per square metre. We show that this system is suitable for general population cycles in nature. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4139, "title": "Our Peculiar Motion Away from the Local Void", "abstract": "  The peculiar velocity of the Local Group of galaxies manifested in the Cosmic Microwave Background dipole is found to decompose into three dominant components. The three components are clearly separated because they arise on distinct spatial scales and are fortuitously almost orthogonal in their influences. The nearest, which is distinguished by a velocity discontinuity at ~7 Mpc, arises from the evacuation of the Local Void. We lie in the Local Sheet that bounds the void. Random motions within the Local Sheet are small. Our Galaxy participates in the bulk motion of the Local Sheet away from the Local Void. The component of our motion on an intermediate scale is attributed to the Virgo Cluster and its surroundings, 17 Mpc away. The third and largest component is an attraction on scales larger than 3000 km/s and centered near the direction of the Centaurus Cluster. The amplitudes of the three components are 259, 185, and 455 km/s, respectively, adding collectively to 631 km/s in the reference frame of the Local Sheet. Taking the nearby influences into account causes the residual attributed to large scales to align with observed concentrations of distant galaxies and reduces somewhat the amplitude of motion attributed to their pull. On small scales, in addition to the motion of our Local Sheet away from the Local Void, the nearest adjacent filament, the Leo Spur, is seen to be moving in a direction that will lead to convergence with our filament. Finally, a good distance to an isolated galaxy within the Local Void reveals that this dwarf system has a motion of at least 230 km/s away from the void center. Given the velocities expected from gravitational instability theory in the standard cosmological paradigm, the distance to the center of the Local Void must be at least 23 Mpc from our position. The Local Void is large! ", "machine_abstract": "We present an analysis of peculiar velocities in the local universe, based on data obtained by the Sloan Digital Sky Survey (SDSS). We use a sample of galaxies with measured redshifts and distances to construct a map of the velocity field around us. The resulting flow is dominated by coherent motions that are consistent with our location within a void centered at about 50 Mpc distance. This result confirms earlier findings using different methods.     In addition we find evidence for a dipole component in this flow which points away from the center of the void towards Virgo. Our results suggest that the bulk motion of matter in the nearby universe may be influenced by large-scale structure. These results have implications for cosmological models as well as for studies of galaxy formation and evolution. They also provide new constraints on theories of dark energy. A full version of this article can be found at: http://arxiv.org/abs/astro-ph/0403320", "paraphrased_abstract": "In this paper, the author gives an analysis of the velocities of the local universe, based on the data from the Survey of the Southern Sky. The data are gathered from the galaxy redshifts and distances measured by the SDSS. The analysis confirms the earlier findings, but there is evidence for a dipole at the top of the void, towards the constellation of Virgo. The results are based on the hypothesis that the bulk of matter in the local universe is influenced by a large structure. We are the first to report that we have the best data available, and are in a position to study the formation of galaxy formation and evolution, and that we have a new basis for the theories of dark energy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3639, "title": "Fundamental Limits to Position Determination by Concentration Gradients", "abstract": "  Position determination in biological systems is often achieved through protein concentration gradients. Measuring the local concentration of such a protein with a spatially-varying distribution allows the measurement of position within the system. In order for these systems to work effectively, position determination must be robust to noise. Here, we calculate fundamental limits to the precision of position determination by concentration gradients due to unavoidable biochemical noise perturbing the gradients. We focus on gradient proteins with first order reaction kinetics. Systems of this type have been experimentally characterised in both developmental and cell biology settings. For a single gradient we show that, through time-averaging, great precision can potentially be achieved even with very low protein copy numbers. As a second example, we investigate the ability of a system with oppositely directed gradients to find its centre. With this mechanism, positional precision close to the centre improves more slowly with increasing averaging time, and so longer averaging times or higher copy numbers are required for high precision. For both single and double gradients, we demonstrate the existence of optimal length scales for the gradients, where precision is maximized, as well as analyzing how precision depends on the size of the concentration measuring apparatus. Our results provide fundamental constraints on the positional precision supplied by concentration gradients in various contexts, including both in developmental biology and also within a single cell. ", "machine_abstract": "We present an analysis of the fundamental limits on position determination using concentration gradients in biological systems, such as bacteria and neurons. We show that these limits are determined by the number of molecules available for measurement at each location along the gradient. The optimal strategy is to measure all locations with equal precision; this leads to a limit on the accuracy of position determination given by $1/\\sqrt{N}$ where $N$ is the total number of molecules measured across the entire system. This result holds even if we allow arbitrary prior information about the distribution of positions within the system. Our results provide insight into how cells can optimally use concentration gradients to determine their positions relative to other cells or external cues. In addition, our results suggest new ways to improve existing techniques used to track cell migration over time.     Bacteria and neurons both sense chemical concentrations through receptors located throughout their surfaces. These receptors convert local chemical concentrations into electrical signals which are then transmitted back to the cell body via specialized signaling pathways. As a consequence, the amount of signal received depends not only upon the local concentration but also upon the distance between the receptor and the source of the signal (see Figure 1). For example, when a bacterium senses a nutrient molecule it will produce more signal if the nutrient is closer than if it were farther away. Similarly, when a neuron receives input from another neuron, its response will depend upon whether the synapse connecting them lies close to the center of the cell's dendritic tree or near one of its tips.", "paraphrased_abstract": "We have shown that the position of cells can be determined optimally by the use of concentration gradients, namely by determining their relative positions relative to other cells and external stimuli. We have likewise shown that the value of position depends not only on the concentration of the cell but also on the distance from the source of the signal. The proportion of signal varies not only according to the local concentration, but also the distance between the receptor and the source of the signal. This result is valid even if there are unknown earlier information about the distribution of positions in the system. This is a result which we describe in detail and which is consistent with the principle of optimal concentration gradients of biological systems, such as bacteria and neurons. Our study has shown that it is possible to calculate the relative positions according to different levels of concentration and to determine their relative positions, in a balanced way, according to the density of the receptor. The bacteria and neurons sense the chemical properties of substances through receptors that are placed in their surfaces. These receptors convert local chemical signals into electrical signals, which are then passed on to the cell body through the specialized pathways. The results of this study show that the most precise method is to measure all the concentrations with equal precision, and that the relative position is based on the ratio of the number of molecules in the total mass. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0316, "title": "The Millennium Galaxy Catalogue: The local supermassive black hole mass function in early- and late-type galaxies", "abstract": "  We provide a new estimate of the local supermassive black hole mass function using (i) the empirical relation between supermassive black hole mass and the Sersic index of the host spheroidal stellar system and (ii) the measured (spheroid) Sersic indices drawn from 10k galaxies in the Millennium Galaxy Catalogue. The observational simplicity of our approach, and the direct measurements of the black hole predictor quantity, i.e. the Sersic index, for both elliptical galaxies and the bulges of disc galaxies makes it straightforward to estimate accurate black hole masses in early- and late-type galaxies alike. We have parameterised the supermassive black hole mass function with a Schechter function and find, at the low-mass end, a logarithmic slope (1+alpha) of ~0.7 for the full galaxy sample and ~1.0 for the early-type galaxy sample. Considering spheroidal stellar systems brighter than M_B = -18 mag, and integrating down to black hole masses of 10^6 M_sun, we find that the local mass density of supermassive black holes in early-type galaxies rho_{bh, early-type} = (3.5+/-1.2) x 10^5 h^3_{70} M_sun Mpc^{-3}, and in late-type galaxies rho_{bh, late-type} = (1.0+/-0.5) x 10^5 h^3_{70} M_sun Mpc^{-3}. The uncertainties are derived from Monte Carlo simulations which include uncertainties in the M_bh-n relation, the catalogue of Sersic indices, the galaxy weights and Malmquist bias. The combined, cosmological, supermassive black hole mass density is thus Omega_{bh, total} = (3.2+/-1.2) x 10^{-6} h_70. That is, using a new and independent method, we conclude that (0.007+/-0.003) h^3_{70} per cent of the universe's baryons are presently locked up in supermassive black holes at the centres of galaxies. ", "machine_abstract": "We present the first measurement of the supermassive black hole (SMBH) mass function for both early-(elliptical/S0, Sa-Sb) and latetype (Scd-Sm) galaxies using data from the Millennium Galaxy Catalogue (MGC). We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations. Our results show that there is no significant difference between the SMBH mass functions of these galaxy types at z < 0.1. However we find evidence for an evolution with redshift such that the number density of massive SMBHs decreases more rapidly than less-massive ones. This suggests that the most massive SMBHs are likely to have grown by accretion over cosmic time rather than merging events. These findings will be important constraints on models of SMBH growth and AGN feedback.", "paraphrased_abstract": "The first measurements we make are of the mass of the supermassive black hole (SMBH) at z  0.1. Moreover, we have evidence for an evolution with redshift which means that the number of large SMBHs is decreasing more rapidly than those of small ones. This may be the case, because the most massive SMBHs were probably formed by accretion over cosmic time rather than by a collision. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1145, "title": "Lyman Break Galaxies at z~5: Rest-frame UV Spectra II", "abstract": "  We present the results of spectroscopy of Lyman Break Galaxies (LBGs) at z~5 in the J0053+1234 field with the Faint Object Camera and Spectrograph on the Subaru telescope. Among 5 bright candidates with z' < 25.0 mag, 2 objects are confirmed to be at z~5 from their Ly alpha emission and the continuum depression shortward of Ly alpha. The EWs of Ly alpha emission of the 2 LBGs are not so strong to be detected as Ly alpha emitters, and one of them shows strong low-ionized interstellar (LIS) metal absorption lines. Two faint objects with z' \\geq 25.0 mag are also confirmed to be at z~5, and their spectra show strong Ly alpha emission in contrast to the bright ones. These results suggest a deficiency of strong Ly alpha emission in bright LBGs at z~5, which has been discussed in our previous paper. Combined with our previous spectra of LBGs at z~5 obtained around the Hubble Deep Field-North (HDF-N), we made a composite spectrum of UV luminous (M_1400 \\leq -21.5 mag) LBGs at z~5. The resultant spectrum shows a weak Ly alpha emission and strong LIS absorptions which suggests that the bright LBGs at z~5 have chemically evolved at least to ~0.1 solar metallicity. For a part of our sample in the HDF-N region, we obtained near-to-mid infrared data, which constraint stellar masses of these objects. With the stellar mass and the metallicity estimated from LIS absorptions, the metallicities of the LBGs at z~5 tend to be lower than those of the galaxies with the same stellar mass at z \\lesssim 2, although the uncertainty is very large. ", "machine_abstract": "We present new rest-UV spectra for four Lyman break galaxies (LBGs) with redshifts between 5 and 6, obtained using the Ultraviolet Echelle Spectrograph on board the Hubble Space Telescope.  The data are used to measure the interstellar medium properties in these objects by fitting models to their observed absorption line profiles. We find that all four LBGs have high metallicities ranging from 0.2 solar to 1 solar, which is consistent with previous results based on optical spectroscopy. In addition we detect strong outflows in three of our targets, as evidenced by blueshifted interstellar absorption lines. These observations suggest that massive stars may be responsible for driving galactic-scale winds even before reionization has completed. This work was supported by NASA grant HST-GO-10775.01-A awarded through the Space Telescope Science Institute, which is operated by AURA under NASA contract NAS5-26555.", "paraphrased_abstract": "The result of this study was the result of the NASA grant GO-10775.01 - a project of the Astronomy Institute, a project of the AURA under contract No. NAS-266555. The data of the rest-UV spectra of four Lyman-break galaxies (LBGs) are presented, which are redshifts from 5 to 6 are obtained by the Ultraviolet Echelle Spectrograph of Hubble's space telescope. In addition, we observe that the outermost eddy has blued to blue, as reflected by blued eddy. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2629, "title": "Dual Computations of Non-abelian Yang-Mills on the Lattice", "abstract": "  In the past several decades there have been a number of proposals for computing with dual forms of non-abelian Yang-Mills theories on the lattice. Motivated by the gauge-invariant, geometric picture offered by dual models and successful applications of duality in the U(1) case, we revisit the question of whether it is practical to perform numerical computation using non-abelian dual models. Specifically, we consider three-dimensional SU(2) pure Yang-Mills as an accessible yet non-trivial case in which the gauge group is non-abelian. Using methods developed recently in the context of spin foam quantum gravity, we derive an algorithm for efficiently computing the dual amplitude and describe Metropolis moves for sampling the dual ensemble. We relate our algorithms to prior work in non-abelian dual computations of Hari Dass and his collaborators, addressing several problems that have been left open. We report results of spin expectation value computations over a range of lattice sizes and couplings that are in agreement with our conventional lattice computations. We conclude with an outlook on further development of dual methods and their application to problems of current interest. ", "machine_abstract": "We present an algorithm for computing non-abelian gauge theories in two different ways, which we call \"directly\" and \"indirectly\". The directly computed quantities are related to those obtained by standard Monte Carlo techniques via Feynman rules. In contrast, indirectly computed quantities can be evaluated without any reference to Feynman diagrams or perturbation theory at all. We demonstrate that both computations agree with each other within statistical errors over several orders of magnitude in lattice spacing and volume. This provides strong evidence that our approach is correct. Finally, we show how this method allows us to compute observables such as glueball masses and decay constants accurately even when they cannot be extracted using conventional methods. Introduction: Gauge theories play a central role in particle physics. However, it has been notoriously difficult to study them numerically because of their nonperturbative nature. Here we propose a new numerical technique based on dual formulations of quantum field theories (QFTs) [1] . Duality transformations relate QFTs formulated in terms of fields living on spacetime points to equivalent ones defined in terms of fields living only on links between these points [2] . In particular, we consider SU(Nc) Yang-Mills theories coupled to fermions transforming according to some representation R of SU(Nc). These theories have a number of interesting physical applications ranging from nuclear physics [3] , where Nc= 3, to high-energy phenomenology [4] , where Nc> 4. For example, one may use this formalism to calculate hadronic matrix elements relevant for precision measurements of Standard Model parameters [5] .", "paraphrased_abstract": "Besides, these two calculations have no connection with the usual 1215-152312451103720 of the Feynman 244219166; the two calculations have a positive relationship, in that they are in accordance with each other in the statistical accuracy of several orders of magnitude. In this way, we are able to compute the gauge of non-abelian particle physics in two ways, which are 236/238/32237 26n2/4 and 22/nl2. 237/  2-h -, which is, for example, a model for detecting a certain ps1423, 23721 205/2  2, 2, 2,,,,,, ,,,,,,,,,,,,,,,, ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0798, "title": "Stringy E-functions of hypersurfaces and of Brieskorn singularities", "abstract": "  We show that for a hypersurface Batyrev's stringy E-function can be seen as a residue of the Hodge zeta function, a specialization of the motivic zeta function of Denef and Loeser. This is a nice application of inversion of adjunction. If an affine hypersurface is given by a polynomial that is non-degenerate with respect to its Newton polyhedron, then the motivic zeta function and thus the stringy E-function can be computed from this Newton polyhedron (by work of Artal, Cassou-Nogues, Luengo and Melle based on an algorithm of Denef and Hoornaert). We use this procedure to obtain an easy way to compute the contribution of a Brieskorn singularity to the stringy E-function. As a corollary, we prove that stringy Hodge numbers of varieties with a certain class of strictly canonical Brieskorn singularities are nonnegative. We conclude by computing an interesting 6-dimensional example. It shows that a result, implying nonnegativity of stringy Hodge numbers in lower dimensional cases, obtained in our previous paper, is not true in higher dimension. ", "machine_abstract": "We study the stringy e-functions of smooth hypersurfaces in weighted projective spaces, which are defined by the generating functions for Gromov-Witten invariants with insertions of arbitrary genus zero descendant classes. We show that these functions can be written as certain infinite products over all prime ideals in the homogeneous coordinate ring of the ambient space. As an application we give explicit formulas for the stringy e-function of any smooth hypersurface in $\\mathbb{P}^n(\\mathcal{O}_{\\mathbb{C}})$ or in $\\mathbb{P}^n(t_1^{a_1}, ..., t_r^{a_0})$ (with $a_i \\geq 1$). In particular this gives new proofs of the results of Batyrev-Borisov on the stringy Hodge numbers of Calabi-Yau varieties.  The second part is devoted to the stringy e-functons of Brieskorn-Pham singularities. These are given by the generating series for Gromov-Wittet invariants with insertions only of one-pointed genus-zero descendent classes. We prove that they also have an expression as infinite products over all prime ideal in their homogeneous coordinate rings. This allows us to compute explicitly the stringy e-functons of many examples including the quintic threefold and its generalizations.", "paraphrased_abstract": "The second part, which deals with the stringy e-functions of Brieskorn-Pham singularities, is an example of the stringy e-functions of Brieskorn-Pham singularities, which are the results of the Gromov-Wittet invariants, with a single-pointed genus-zero classes, and which are expressed as infinite products in the homogeneous coordinates of the surrounding space. These e-functions can be computed in the form of a stringy e-function of any smooth surface in Matrix-Bb-Pn(mathcal-Omath-matrix-B) or in Matrix-Bn(t1a1a, tra0) (with $aigeq1$). This proves that they also have an expression for the sums of all prime ideals in the homogeneous coordinate system. We show that we can define these sums as the sums of infinite products over all prime ideals in the homogeneous coordinate system. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0941, "title": "Recovering galaxy star formation and metallicity histories from spectra using VESPA", "abstract": "  We introduce VErsatile SPectral Analysis (VESPA): a new method which aims to recover robust star formation and metallicity histories from galactic spectra. VESPA uses the full spectral range to construct a galaxy history from synthetic models. We investigate the use of an adaptative parametrization grid to recover reliable star formation histories on a galaxy-by-galaxy basis. Our goal is robustness as opposed to high resolution histories, and the method is designed to return high time resolution only where the data demand it. In this paper we detail the method and we present our findings when we apply VESPA to synthetic and real Sloan Digital Sky Survey (SDSS) spectroscopic data. We show that the number of parameters that can be recovered from a spectrum depends strongly on the signal-to-noise, wavelength coverage and presence or absence of a young population. For a typical SDSS sample of galaxies, we can normally recover between 2 to 5 stellar populations. We find very good agreement between VESPA and our previous analysis of the SDSS sample with MOPED. ", "machine_abstract": "We present the results obtained by applying the semi-analytic code VESPA to model the evolution of galaxies in the Millennium Simulation, including chemical enrichment as well as dust extinction effects on their observed properties.  We show that our models reproduce many observational trends for different types of galaxies at z=0 (e.g., luminosity functions), but also predict some new ones which can be tested with future surveys such as Euclid or LSST. In particular we find that:  - The predicted number density of massive quiescent galaxies is too high compared to observations; this problem could be alleviated if AGN feedback were more efficient than assumed here. - Our predictions are consistent with current estimates of the cosmic SFRD out to redshifts of about 4.5, although they tend to overpredict it slightly beyond that redshift. - At low masses (Mstar < 10^10 Msun) there appears to be an excess of blue galaxies relative to red galaxies in both the real Universe and in our simulations. This may indicate that either our treatment of supernova feedback and/or reionization physics needs improvement, or else that these processes have been affected by baryonic effects not included in our simulation.", "paraphrased_abstract": "The results of our model are in all dimensions, including the number density of the largest quiescent galaxies, as well as the numerical results of a few additional physics that are not well understood in our simulations. We show that our model reproduces many observations, such as the luminosity, the soaring of stars, the chemical enrichment of stars and the dust extinction effects. Our predictions correspond to the current models of the cosmic swarms to redshifts of about 4.5, although they overestimate it slightly beyond that. At the lower levels (Mstar  1010 Msun), the density of blue galaxies is far too great for the spectra of stars, which are not the ones we are modeling. This may indicate that our modeling of supernova feedback and reionization is inadequate or that we are underestimating the effects of the baryonic effects which are not included in our model. We show that our models reproduce many of the observed trends at z0 (e.g., luminosity), and even suggest new ones, which can be tested with the future surveys, for example, by Euclid or by LSST. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2195, "title": "Absolute Calibration and Characterization of the Multiband Imaging Photometer for Spitzer. I. The Stellar Calibrator Sample and the 24 micron Calibration", "abstract": "  We present the stellar calibrator sample and the conversion from instrumental to physical units for the 24 micron channel of the Multiband Imaging Photometer for Spitzer (MIPS). The primary calibrators are A stars, and the calibration factor based on those stars is 4.54*10^{-2} MJy sr^{-1} (DN/s)^{-1}, with a nominal uncertainty of 2%. We discuss the data-reduction procedures required to attain this accuracy; without these procdures, the calibration factor obtained using the automated pipeline at the Spitzer Science Center is 1.6% +/- 0.6% lower. We extend this work to predict 24 micron flux densities for a sample of 238 stars which covers a larger range of flux densities and spectral types. We present a total of 348 measurements of 141 stars at 24 micron. This sample covers a factor of ~460 in 24 micron flux density, from 8.6 mJy up to 4.0 Jy. We show that the calibration is linear over that range with respect to target flux and background level. The calibration is based on observations made using 3-second exposures; a preliminary analysis shows that the calibration factor may be 1% and 2% lower for 10- and 30-second exposures, respectively. We also demonstrate that the calibration is very stable: over the course of the mission, repeated measurements of our routine calibrator, HD 159330, show a root-mean-square scatter of only 0.4%. Finally, we show that the point spread function (PSF) is well measured and allows us to calibrate extended sources accurately; Infrared Astronomy Satellite (IRAS) and MIPS measurements of a sample of nearby galaxies are identical within the uncertainties. ", "machine_abstract": "We present an absolute calibration of MIPS photometry at 24, 70, and 160 microns using stellar calibrators observed by the Infrared Array Camera (IRAC) onboard the Spitzer Space Telescope. We use these observations to derive corrections that account for differences in aperture size between IRAC and MIPS as well as color-dependent effects due to differing filter profiles. These corrections are applied to all sources detected with signal-to-noise ratios greater than 5 in each band. For fainter sources we apply additional corrections based upon the measured fluxes of bright stars within the same field-of-view. This method is used to calibrate over 1 million objects across the sky. We find excellent agreement between our results and those obtained independently by other groups. Our final uncertainties include contributions from both statistical errors and systematics associated with the choice of stellar calibrators. We also provide estimates of the uncertainty introduced into the derived colors when applying this technique.", "paraphrased_abstract": "A total of 1 million objects are calibrated in the sky by this method. We compare our results with those of other groups, e.g., e.g., which are not independently validated by others. We estimate the uncertainty in the color obtained by the method. The MIPS corrections were calculated from observations of the Infrared Camera (IRAC) onboard the Spitzer Space Telescope. This method was employed to calibrate over one million objects in the sky. The first uncertainties included the errors of the statistics and systematics in choosing the calibrator. The second uncertainties included the error in the measurement of the stars, which is caused by the different filters. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3708, "title": "Network statistics on early English Syntax: Structural criteria", "abstract": "  This paper includes a reflection on the role of networks in the study of English language acquisition, as well as a collection of practical criteria to annotate free-speech corpora from children utterances. At the theoretical level, the main claim of this paper is that syntactic networks should be interpreted as the outcome of the use of the syntactic machinery. Thus, the intrinsic features of such machinery are not accessible directly from (known) network properties. Rather, what one can see are the global patterns of its use and, thus, a global view of the power and organization of the underlying grammar. Taking a look into more practical issues, the paper examines how to build a net from the projection of syntactic relations. Recall that, as opposed to adult grammars, early-child language has not a well-defined concept of structure. To overcome such difficulty, we develop a set of systematic criteria assuming constituency hierarchy and a grammar based on lexico-thematic relations. At the end, what we obtain is a well defined corpora annotation that enables us i) to perform statistics on the size of structures and ii) to build a network from syntactic relations over which we can perform the standard measures of complexity. We also provide a detailed example. ", "machine_abstract": "The present study investigates the structural properties of Early Modern English syntax by applying network analysis to data obtained through corpus linguistics techniques. The results show that, in general terms, syntactic networks are characterized by high clustering coefficients and low average path lengths. In addition, it is shown how these two parameters can be used as indicators for identifying different types of syntactic structures. Finally, some possible applications of this approach are discussed. Keywords: Network Analysis; Corpus Linguistics; Syntactic Structures; Clustering Coefficients; Average Path Lengths. 1 Introduction A growing number of studies have been carried out recently using network theory (Watts & Strogatz 1998 , Newman 2003a ) to investigate various aspects of language structure (Ferrer-i-Cancho 2004 , Ferrer-i-Cancho et al 2005 . These investigations have mainly focused on phonological systems (e.g., Ferrer-i-Cancho 2002 , Ferrer-i-Cancho & Sol\u00e9 2007 or lexical-semantic relations (e.g., Steyvers & Tenenbaum 2005 ) . However, there has also been interest in exploring other linguistic levels such as morphosyntax (Ferrer-i-Canchos 2006 ) , prosody (Ferrer-i-Canch\u00f3s 2008) , pragmatics (Ferrer-i-Canchis 2009) or even discourse (Ferrer-i-Canchi 2010) . This article focuses on one particular aspect of syntax -namely, word order-using network analysis to explore its structural characteristics. More specifically, we will use network theory to analyze data collected with corpus-linguistic methods. We believe that this type of investigation could provide new insights into the way in which syntactic structures emerge during language acquisition.", "paraphrased_abstract": "It has been a great deal of interest to research morphosyntax (Ferrer-i-Canchos) and prosody (Ferrer-i-Canchos & Sol\u00e9 2007) and even speech (Ferrer-i-Canchos et al. 2004). In particular, we will analyze the structure of the phonological network, namely, phonology (Ferrer-i-Canchos et al. 2005), phonological-semantic relations (e.g. Steyvers et al. 2005). In the last decade, many studies have been carried out, primarily, based on network analysis, mainly focusing on phonological systems (Ferrer-i-Canchos et al. 2006), prosody (Ferrer-i-Canchos et al. 2007) and discourse (Ferrer-i-Canchis 2010). Besides, a number of other studies have been done in linguistics, particularly on morphosyntax, morphosyntax, prosody, pragmatics, Ferrer-i-Canchis 2009, and even on discourse (Ferrer-i-Canchis 2010). This study focuses on a particular aspect of the", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2075, "title": "The Araucaria Project. The Distance to the Local Group Galaxy WLM from Cepheid Variables discovered in a Wide-Field Imaging Survey", "abstract": "  We have conducted an extensive wide-field imaging survey for Cepheid variables in the Local Group irregular galaxy WLM. From data obtained on 101 nights, we have discovered 60 Cepheids which include 14 of the 15 Cepheid variables previously detected by Sandage and Carlson. Down to a period of 3 days, our Cepheid survey in WLM should be practically complete. Importantly, we have found for the first time a long-period Cepheid (P=54.2 days) in this galaxy, alleviating the puzzle that WLM with its many blue, massive stars does not contain Cepheids with periods longer than about 10 days. Our data define tight period-luminosity relations in V, I and the reddening-free Wesenheit magnitude ${\\rm W}_{\\rm I}$ which are all extremely well fit by the corresponding slopes of the LMC Cepheid PL relation, suggesting no change of the PL relation slope down to a Cepheid metal abundance of about -1.0 dex, in agreement with other recent studies. We derive a true distance modulus to WLM of 25.144 $\\pm$0.03 (r) $\\pm$0.07 (s) mag from our data, in good agreement with the earlier 24.92 $\\pm$ 0.21 mag determination of Lee, Freedman and Madore (1993a) from Cepheid variables. The quoted value of the systematic uncertainty does not include the contribution from the LMC distance which we have assumed to be 18.50 mag, as in the previous papers in our project. ", "machine_abstract": "We present results on distance determinations for two galaxies, NGC 3109 and WLM, based on observations made with the Hubble Space Telescope (HST). We have used HST/WFPC2 images taken through filters F555W and F814W to search for Cepheids among young open clusters located within these galaxies. Our survey has resulted in the discovery of four new Cepheids in NGC 3109 and one in WLM. These five Cepheids are all short-period classical pulsators with periods ranging between 4.5 days and 8.6 days. Using the period-luminosity relation derived by Madore & Freedman we find distances to NGC 3109 and W LM that agree well with previous estimates obtained using other methods.     Keywords: Cepheid variables; open cluster; galaxy distance scale; Hubble Space Telescope; Araucaria Project. 1. Introduction     Open clusters provide an important tool for determining extragalactic distances because they contain many stars at nearly identical ages and chemical compositions. In addition, open clusters can be found over a wide range of galactocentric radii, allowing us to probe different environments. However, open clusters are relatively rare objects compared to field stars or globular clusters. Therefore, it is necessary to conduct surveys covering large areas of sky in order to obtain statistically significant samples of open clusters suitable for use as calibrators of the cosmic distance ladder.     The Araucaria Project was initiated in 1998 with the goal of obtaining accurate distances to nearby galaxies via measurements of Cepheid variable stars associated with open clusters. This project uses data collected primarily with the Hubble Space Telescope's WFPC2 camera. A total of eight fields were observed during Cycle 9-10 of the HST program. Each field covers about 0.25 square degrees centered around a target galaxy. For each field, deep exposures were obtained in both the F555W and F850LP bands. Details regarding this project may be found in Pietrzy\u0144ski et al. (2002) and references therein.     2. Searching for Cepheids Among Young Open", "paraphrased_abstract": "This is an attempt to obtain accurate distances to the nearby galaxies, using the Cepheid Variables detector. These detectors are equipped with a scanning radar, and are located in all directions. We have investigated four newly discovered Cepheids in NGC 3109 and one in WLM. Our results are based on the reconstructed interval-luminosity relationship derived from Madore and Freedman. The results show that the distances of NGC 3109 and WLM are quite in keeping with the estimates obtained by other methods. The Araucaria Project was launched in 1998 with the purpose of measuring the distances of nearby galaxies by means of measurements of Cepheid Variable Stars, grouped in open Clusters. We obtained, for each of the eight Fields in the Araucaria Project, from the data of the WFPC2 Camera. For each Field, the deepest pixel was obtained in the F555W and F850LP bands. We searched for Cepheids in the Young Open Clusters, in the Four Open Clusters, and in the Five Open Clusters, we found four new Cepheids, all short-period pulsating Stars with a period of 4.5 days and 8.6 days, based on the Period-Linosity relation of", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2258, "title": "Dark Matter in Gauge Mediation from Emergent Supersymmetry", "abstract": "  We investigated the viability of neutralino dark matter in the gauge mediation from emergent supersymmetry proposal. In this proposal, supersymmetry is broken at Planck scale and consequently, the gravitino is superheavy and completely decouples from the low energy theory. Squarks and sleptons obtain their soft masses dominantly through gauge mediation with other mechanisms highly suppressed. The lightest supersymmetric partner, in contrast to traditional gauge mediation, is a neutralino which is also a dark matter candidate. By explicit calculation of the low energy spectra, the parameter space was constrained using the WMAP observed relic density of dark matter, LEP2 Higgs mass bounds, collider bounds on supersymmetric partners and exotic B-meson decays. We found that the model has intriguing hybrid features such as a nearly gauge-mediated spectrum (the exception being the superheavy gravitino) but with a dominant mSUGRA-like bino-stau coannihilation channel and at large $\\tan \\beta$, A-resonance-like annihilation. ", "machine_abstract": "We propose that the dark matter (DM) and supersymmetric particles are generated by an emergent gauge symmetry at high energy scales, which is broken down to Standard Model symmetries below TeV scale. The DM candidate can be identified as a pseudo-Nambu-Goldstone boson associated with spontaneous breaking of global U(1) symmetry. We show how this scenario can explain various experimental results on DM searches including recent LHC data. In addition we discuss possible collider signatures for future experiments such as ILC or CLIC. Introduction: Dark matter (DM), whose existence has been inferred through its gravitational effects over many decades [1] , remains one of the most mysterious phenomena in particle physics today [2] . Although there have been numerous proposals for explaining the origin of DM [3] , none of them has yet provided compelling evidence for their viability [4] . In this work, motivated by the idea of \"emergent\" theories [5] - [8] , we consider a new possibility where DM emerges from a spontaneously-broken global symmetry [9] . This approach provides a simple explanation for why DM should exist without introducing any additional fields beyond those already present within the Standard Model [10] . Furthermore, it allows us to identify the DM candidate as a pseudo-NambuGoldstone boson [11] , thereby providing a natural solution to the so-called \"WIMP miracle\" [12] problem [13] . Finally, our model also predicts the presence of light scalar superpartners [14] , which may provide interesting signals at upcoming high-energy accelerator facilities [15] . The rest of this article is organised as follows. In Sec. 2, we introduce our theoretical framework based upon emergent gauge mediation [16] . Then, in Secs. 3-7, we demonstrate how this framework can simultaneously address all current experimental constraints [17] - [20] while predicting novel phenomenological features [21] . Finally, in Sec. 8, we conclude with some remarks about further directions of research.", "paraphrased_abstract": "Consequently, this is a general article. In the first section, we introduce the framework of entropy, and then in the second section we give an overview of the latest results, including the recent results of the LHC, Xuzhou, and Xichang. Afterwards, we discuss the possible collider signatures for the future experiments, such as ILC or CLIC. The rest of this article is presented in the following order: in the first place, we introduce the theoretical framework of entropy, and then in the second part, in the third and fourth sections, we show how this model can be used to solve all the current experimental constraints and to imply novel features. Then, in the fourth part, we show how DM and supersymmetric particles are created from a global symmetry of the high energy, breaking down to the standard model symmetries below the TeV level. Moreover, we have obtained the candidate of DM as a pseudo-NambuGoldstone boson, which can be considered a natural solution to the so-called \u201cWIMP miracle\u201d. In the fifth part, we propose that DM arises from a spontaneously broken U-symmetry, which, therefore, is a natural solution to the so-called \u201cWIMP miracle\u201d. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0694, "title": "Culminating paths", "abstract": "  Let a and b be two positive integers. A culminating path is a path of Z^2 that starts from (0,0), consists of steps (1,a) and (1,-b), stays above the x-axis and ends at the highest ordinate it ever reaches. These paths were first encountered in bioinformatics, in the analysis of similarity search algorithms. They are also related to certain models of Lorentzian gravity in theoretical physics. We first show that the language on a two letter alphabet that naturally encodes culminating paths is not context-free. Then, we focus on the enumeration of culminating paths. A step by step approach, combined with the kernel method, provides a closed form expression for the generating fucntion of culminating paths ending at a (generic) height k. In the case a=b, we derive from this expression the asymptotic behaviour of the number of culminating paths of length n. When a>b, we obtain the asymptotic behaviour by a simpler argument. When a<b, we only determine the exponential growth of the number of culminating paths. Finally, we study the uniform random generation of culminating paths via various methods. The rejection approach, coupled with a symmetry argument, gives an algorithm that is linear when a>= b, with no precomputation stage nor non-linear storage required. The choice of the best algorithm is not as clear when a<b. An elementary recursive approach yields a linear algorithm after a precomputation stage involving O(n^3) arithmetic operations, but we also present some alternatives that may be more efficient in practise. ", "machine_abstract": "We consider the problem of finding a shortest path between two nodes in an undirected graph with non-negative edge weights, subject to the constraint that no node can be visited more than once along this path.  We show how to solve this problem by reducing it to the minimum cost flow problem on a directed acyclic network and then applying standard techniques for solving such problems.   The running time is O(n3 log n), where n denotes the number of vertices in the input graph. This result improves upon previous algorithms which have been known only for special cases (e.g., when all edges are unit-weight).    Keywords: Shortest paths, Minimum-cost flows, Directed graphs, Undirected graphs, Culminating paths, Constraints, Flow networks, Computational complexity theory.    1 Introduction  In many applications one needs to find a shortest path between two given nodes s and t in an undirected weighted graph G = (V , E) without allowing any vertex to appear twice or more times in the path.   For example, if we want to compute the distance between two cities using road maps, we may not allow a city to be used as a starting point again after visiting another city.   In other words, we do not want to use the same road segment twice even though there might exist several different ways to reach some destination.   Another application arises in computational biology, where one wants to find a sequence of genes that minimizes the total length while avoiding certain regions of DNA.   A third application occurs in computer vision, where one seeks to minimize the total length of a path through a set of images while ensuring that each image appears at most once in the path.   Finally, our results also apply to the problem of computing the diameter of a connected undirected graph.  The first algorithm for computing the shortest path satisfying these constraints was presented by Hassin and Rubinov [HR91] who showed how to reduce the problem to the minimum cost flow", "paraphrased_abstract": "This result is much better than other previous ones, which have been known only for special cases (e.g., when all edges are units). We have the problem of finding a shortest path between two nodes in an undirected graph, with non-negative indices of the line, and we have the constraint that no node can be visited more than once in the path. In computing distances between two cities, we may not allow the same path to be used to arrive at another city. We therefore need not repeat the same path in the course of a long journey. We present an efficient solution: we find a shortest path between two nodes in an undirected graph, if there is no corresponding edge on it. This solution improves the present approach of previous ones, which were only used for special cases, and when the edges were unit weighted. We present an efficient method to solve this problem by reducing it to the minimum cost flow of a graph in the direction of a directed acyclic network, and applying the usual techniques to the problem. We present a method for finding the shortest path between two nodes in an undirected graph, wherein, without excluding any vertex from the path, we do not allow the same path to be followed twice. We also show that the solution to this problem is to reduce the problem to the minimum cost flow of a graph in", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.309, "title": "Modules over operads and functors", "abstract": "  In the theory of operads we consider functors of generalized symmetric powers defined by sums of coinvariant modules under actions of symmetric groups. One observes classically that the construction of symmetric functors provides an isomorphism from the category of symmetric modules to a subcategory of the category of functors on the base category. The purpose of this book is to obtain a similar relationship for functors on a category of algebras over an operad.   We observe that right modules over operads, symmetric modules equipped with a right operad action, give rise to functors on categories of algebras and we prove that this construction yields an embedding of categories. Then we check that right modules over operads form a model category. In addition we prove that weak-equivalences of right modules correspond to pointwise weak-equivalences at the functor level. As a conclusion, we obtain that right modules over operads supply good models for the homotopy of associated functors on algebras over operads. ", "machine_abstract": "In this article, we introduce the notion of modules over an operad in any symmetric monoidal category $\\mathcal{C}$ with unit object $I$.  We define the free module on a set $X$ to be the tensor product $\\bigotimes_{X\\times X} I$ (where $*$ denotes the cartesian product); it is shown that this construction satisfies all axioms for a free module except possibly associativity.   The missing axiom is established by showing that the free module on two sets $X,Y$ can be constructed as the coequalizer of certain morphisms between free modules on three sets $Z_1,Z_2, Z_3$ where $Z_1$ has one element less than $X$, $Z_2$ has one element more than $Y$, and $Z_3$ has one element less than $Y\\cup X$.    This result generalizes the classical theorem stating that the free group on a set $X$ is the quotient of the free group on the disjoint union of $X$ and its complement modulo the relation that identifies each element $x$ of $X$ with its inverse $x^{-1}$.  The main results are applied to give new proofs of some well-known theorems about groups, rings, algebras, Lie algebras, and Hopf algebras.", "paraphrased_abstract": "I introduce the notion of modules over an operad in any symmetric monoidal category of mathematics - C. The tensor product bigotimes X - I' is formulated, and the result is that it fulfills all the axioms for a free module, except that it is not associative. The main result is that the free group on a set of X is the quotient of the free group on a disjoint union of X and its complement, namely the relation that identifies each element of X with its inverse, x--1. The main result is that the free group on a set of X is the quotient of the free group on the disjoint union of X, and whose complement consists in the relation that connects each element of X with its inverse x--1. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0406, "title": "Evolutionary dynamics of the most populated genotype on rugged fitness landscapes", "abstract": "  We consider an asexual population evolving on rugged fitness landscapes which are defined on the multi-dimensional genotypic space and have many local optima. We track the most populated genotype as it changes when the population jumps from a fitness peak to a better one during the process of adaptation. This is done using the dynamics of the shell model which is a simplified version of the quasispecies model for infinite populations and standard Wright-Fisher dynamics for large finite populations. We show that the population fraction of a genotype obtained within the quasispecies model and the shell model match for fit genotypes and at short times, but the dynamics of the two models are identical for questions related to the most populated genotype. We calculate exactly several properties of the jumps in infinite populations some of which were obtained numerically in previous works. We also present our preliminary simulation results for finite populations. In particular, we measure the jump distribution in time and find that it decays as $t^{-2}$ as in the quasispecies problem. ", "machine_abstract": "We study evolutionary dynamics in an ensemble of genotypes, each with its own fitness value and connected to other genotypes by mutations. We show that for large ensembles there is always one genotype which has more connections than any other genotype. This genotype dominates all others at equilibrium. The dominance of this genotype depends only on the distribution of mutational distances between pairs of genotypes. For example, if we consider two different distributions of mutational distances (the exponential and power law), then the same genotype will dominate both cases. However, the time it takes to reach equilibrium can be very different depending on how many paths lead away from the dominant genotype. In particular, when the number of paths leading away from the dominant genotype increases exponentially as a function of distance, the system reaches equilibrium much faster compared to the case where the number of paths grows slower or even remains constant.     We also find that the average population size of the dominant genotype decreases slowly over time until reaching equilibrium.", "paraphrased_abstract": ", we can see that the average population size of the dominant genotype falls slowly as it comes to equilibrium. But we find that the time taken to reach equilibrium is very different from one to another, as a result of how many paths there are to the dominant genotype. This is especially true when the number of paths to the dominant genotype increases exponentially with distance. This means that the prevailing genotype dominates all others at equilibrium. This dominance is determined only by the distribution of the distances between two genotypes. We find that the distance between two genotypes increases exponentially with distance, so that the population reaches equilibrium much faster than the one with the least distance. If, for example, two different distributions of distances are considered, the exponential and the power law, the same genotype dominates both. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2955, "title": "How a \"Hit\" is Born: The Emergence of Popularity from the Dynamics of Collective Choice", "abstract": "  In recent times there has been a surge of interest in seeking out patterns in the aggregate behavior of socio-economic systems. One such domain is the emergence of statistical regularities in the evolution of collective choice from individual behavior. This is manifested in the sudden emergence of popularity or \"success\" of certain ideas or products, compared to their numerous, often very similar, competitors. In this paper, we present an empirical study of a wide range of popularity distributions, spanning from scientific paper citations to movie gross income. Our results show that in the majority of cases, the distribution follows a log-normal form, suggesting that multiplicative stochastic processes are the basis for emergence of popular entities. This suggests the existence of some general principles of complex organization leading to the emergence of popularity. We discuss the theoretical principles needed to explain this socio-economic phenomenon, and present a model for collective behavior that exhibits bimodality, which has been observed in certain empirical popularity distributions. ", "machine_abstract": "We study how popularity emerges in collective choice dynamics, where individuals sequentially choose between two options and are influenced by their peers. We show that when agents have boundedly rational expectations about others' choices they may end up choosing an option which has no majority support but nevertheless becomes popular over time. This phenomenon can be explained as a consequence of herding behavior among agents who do not fully understand the underlying social network structure. Our results suggest that such herding behavior could play an important role for understanding the emergence of popularity in real-world settings like online communities or political elections. In many situations people make decisions based on what other people think. For example, consumers often base their purchase decision on reviews written by other customers [1] , while voters decide to vote for one candidate rather than another because of information received through word-of-mouth communication [2] . Such phenomena are known under different names including bandwagon effects [3] , herd behavior [4] , informational cascades [5] , peer pressure [6] , conformity [7] , imitation [8] , and influence [9] . In this work we focus on the case where individuals' decisions are made collectively via voting [10] . More specifically, consider a group of N agents who must select one out of M possible alternatives (e.g., candidates) at each round t = 1, ..., T . At every round t \u2208 [T ], agent i chooses alternative xt(i), receives feedback f (xt\u22121)(\u2212i) (e.g., votes casted against his/her choice), and updates his/her belief about the state of nature \u03b8 according to Bayes' rule [11] :", "paraphrased_abstract": "In many situations people make decisions based on what other people think. For instance, people make purchases based on reviews from other people, and they are more likely to vote for one candidate than another, because they have been informed through word of mouth. These phenomena are known as \u201ccoercion\u201d in the minds of unacquainted agents, who do not fully understand the underlying social network. We present a syllable that we call \u201ccommon causation\u201d and refer to the phenomenon of mutual emulation where the decision is made by a group of N agents, who, at each round, selects one of the M possible candidates (e.g. candidates) and receives feedback f(xt) (i) (that is, a feedback whose votes are cast against him), and reassesses the belief he has in the state of nature in accordance with the rule of Bayes. This phenomenon is known as \u201csociety-strength,\u201d \u201csociety-strength,\u201d \u201cinformational cascades\u201d, \u201cinformational cascades\u201d, \u201cinvolvement\u201d, \u201ccompetition\u201d, \u201cethics,\u201d \u201cinfluence,\u201d and influence. This phenomenon is known as \u201cgroup-strength\u201d, \u201ccompetitiveness,\u201d \u201csocial dispersion,\u201d \u201cinformation-based,\u201d \u201cc", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3402, "title": "A ferrofluid based neural network: design of an analogue associative memory", "abstract": "  We analyse an associative memory based on a ferrofluid, consisting of a system of magnetic nano-particles suspended in a carrier fluid of variable viscosity subject to patterns of magnetic fields from an array of input and output magnetic pads. The association relies on forming patterns in the ferrofluid during a trainingdphase, in which the magnetic dipoles are free to move and rotate to minimize the total energy of the system. Once equilibrated in energy for a given input-output magnetic field pattern-pair the particles are fully or partially immobilized by cooling the carrier liquid. Thus produced particle distributions control the memory states, which are read out magnetically using spin-valve sensors incorporated in the output pads. The actual memory consists of spin distributions that is dynamic in nature, realized only in response to the input patterns that the system has been trained for. Two training algorithms for storing multiple patterns are investigated. Using Monte Carlo simulations of the physical system we demonstrate that the device is capable of storing and recalling two sets of images, each with an accuracy approaching 100%. ", "machine_abstract": "We present the design and fabrication of a novel analog electronic device, which is capable to store information in its magnetic state. The proposed system consists of two layers of ferromagnetic wires with perpendicular anisotropy (PMA) that are connected by Josephson junctions. We show how this structure can be used as an analogue associative memory. In particular we demonstrate that it allows for storing multiple patterns simultaneously and retrieving them using only one input signal. This feature makes our proposal suitable for applications where high storage density or low power consumption are required.     Keywords: Analogue memory, Ferrofluids, Magnetic wire networks, PMA, Josephson junction, Pattern recognition, Associative memory. Introduction     Memory devices play a crucial role in many modern technologies such as computers, mobile phones, digital cameras etc., but their development has been mainly focused on digital memories due to their higher performance compared to conventional analog ones [1] . However, there exist several important applications where analog memories could provide significant advantages over digital counterparts [2] , e.g. pattern recognition [3] , data compression [4] , image processing [5] , bio-inspired computing [6] .   In recent years, considerable efforts have been made towards developing new types of analog memories [7, 8] . Among these proposals, spintronic systems [9] offer great potential thanks to their unique properties [10] including non-volatility [11] , fast switching [12] , large integration densities [13] , compatibility with CMOS technology [14] and energy efficiency [15] . These features make spintronic devices promising candidates for future generations of nonvolatile memories [16] .     Herein, we propose a novel concept of an analogue memory based on a magnetic wire network [17] . Our approach relies on the use of a ferrofluid [18] , i.e. colloidal suspension of superparamagnetic nanoparticles [19] , confined between two layers of ferromagnetic nanowires [20] . Due to the presence of the ferrofluid layer, each nanowire acquires a net magnetization [21] . As shown schematically in Fig.   1(a) , when applying a bias current I b through the nanowires, they become superconducting [22]", "paraphrased_abstract": "When you push an electric current through the ferrofluids, the nanowires become superconductible, as shown in Figure 1a. By a single pulse, the nanowires become superconductible. Besides, they are also more suitable for the generation of future generations of the nonvolatile memories. In recent years, there have been great efforts to develop new types of analog memories, especially spintronic ones. These are known as'spintronic', they are able to transmit data, and in addition to that, they are very fast, and the integration is very large. This enables them to store multiple patterns simultaneously, and they can be retrieved with one single input. The invention of this type of analogue memory, with two layers of ferromagnetic wires, shows how an analogue memory can be made with a magnetic structure. In this work, we present a novel method of storing information in a magnetic state, in which each layer has a ferromagnetic component. The ferroelectric structure is composed of two layers of ferromagnetic wires, with a magnetic grating. It is a non-volatile memory, a memory compatible with CMOS, a memory with low energy consumption. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2803, "title": "Simulation of Single Reed Instruments Oscillations Based on Modal Decomposition of Bore and Reed Dynamics", "abstract": "  This paper investigates the sound production in a system made of a bore coupled with a reed valve. Extending previous work (Debut, 2004), the input impedance of the bore is projected on the modes of the air column. The acoustic pressure is therefore calculated as the sum of modal components. The airr&#114;&#64258;ow blown into the bore is modulated by reed motion, assuming the reed to be a single degree of freedom oscillator. Calculation of self-sustained oscillations controlled by time-varying mouth pressure and player's embouchure parameter is performed using ODE solvers. Results emphasize the par ticipation of the whole set of components in the mode locking process. Another impor tant feature is the mutual inn&#110;&#64258;uence of reed and bore resonance during growing blowing pressure transients, oscillation threshold being altered by the reed natural frequency and the reed damping. Steady-state oscillations are also investigated and compared with results given by harmonic balance method and by digital sound synthesis. ", "machine_abstract": "The reed is the key component in single-reed musical instruments, such as clarinets or saxophones. The dynamics of its motion are strongly influenced by the bore geometry and boundary conditions at both ends. In this work we present an approach to simulate the oscillations of a single reed instrument based on modal decomposition of the bore and reed dynamics. We show that it is possible to accurately reproduce the sound produced by a real clarinet using only one degree-of-freedom for each mode. This method can be used to study the influence of different parameters (such as mouthpiece diameter) on the acoustic response of the instrument without requiring expensive experimental measurements. It also allows us to investigate how changes in the design affect the performance of new prototypes.     Keywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method", "paraphrased_abstract": "... The sound produced by the instrument is of the most important importance in a musical instrument, the clarinet or the saxophone. The dynamical resonant power of the instrument is strongly influenced by the bore geometry and the boundaries of the two ends. The resonant power is a key element in the resonant power of the instrument, a fundamental component of the clarinet or a saxophone. The resonant power is the most important component of the instrument, such as the clarinet, or the saxophone. Its resonant power is directly influenced by the resonant power of the instrument and by the characteristics of its structure. This method is useful for studies of the influence of various parameters, such as the width of the mouthpiece, on the acoustic response of the instrument. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0514, "title": "Complexity Considerations, cSAT Lower Bound", "abstract": "  This article discusses completeness of Boolean Algebra as First Order Theory in Goedel's meaning. If Theory is complete then any possible transformation is equivalent to some transformation using axioms, predicates etc. defined for this theory. If formula is to be proved (or disproved) then it has to be reduced to axioms. If every transformation is deducible then also optimal transformation is deducible. If every transformation is exponential then optimal one is too, what allows to define lower bound for discussed problem to be exponential (outside P). Then we show algorithm for NDTM solving the same problem in O(n^c) (so problem is in NP), what proves that P \\neq NP.   Article proves also that result of relativisation of P=NP question and oracle shown by Baker-Gill-Solovay distinguish between deterministic and non-deterministic calculation models. If there exists oracle A for which P^A=NP^A then A consists of infinite number of algorithms, DTMs, axioms and predicates, or like NDTM infinite number of simultaneous states. ", "machine_abstract": "We consider the complexity class #P and its subclass #P^{NP}. We show that for any polynomial time computable function f : {0,1}^n \\to {0,1} there is an algorithm A such that for every input x to A it computes f(x) in time O(n^k), where k = log2|f|. This implies that if we can solve SAT in time n^k then we can compute f in time O(n^l). In particular this shows that if P != NP then #P^{NP} cannot be solved by deterministic algorithms running in sub-exponential time unless co-NP=PSPACE. The proof uses ideas similar to those used in proving lower bounds on the number of queries needed to solve problems like 3-SUM or Vertex Cover using dynamic programming techniques. It also relies heavily on the fact that the problem being considered has no efficient parallel solution (i.e., it does not belong to NC). Theorem 1 Let f : {0,1}^m \\to {0,1} be a polynomial-time computable function with m > 0. Then there exists an algorithm A which solves f in time O(m^k), where k = \\lg |f| . Proof:  Let us assume without loss of generality that m is even. For each i between 1 and m/2 let g_i : {0,1}^{2i} \\to {0,1} be defined as follows:", "paraphrased_abstract": "For example, let g i : 0,12i to 0,1 be defined as follows: Let m i be a polynomial time computable function of m > 0, and arithmetic of m 0,12i to 0,1. The proofs are similar to those based on the results of the numerical study of the complexity of the three-symbol of a series of volumes or a vertex-covered swarm. We assume that m is even, and we owe this to the fact that the problem is in fact unrelated to nilometrics. Let n=1 and n=1 the following: We can solve f 0,1nto 0,1, and the subclass PNP is the corresponding subclass of NPNP. This is the proof of the existence of P and its subclass PNP. In particular, this proves that if P=NP, the subclass PNP cannot be solved in subexponential time by deterministic algorithms, if P=NP. Then, if we can solve SAT in time nk, then we can solve f in time nl.", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1625, "title": "A Systematic Scan for 7-colourings of the Grid", "abstract": "  We study the mixing time of a systematic scan Markov chain for sampling from the uniform distribution on proper 7-colourings of a finite rectangular sub-grid of the infinite square lattice, the grid. A systematic scan Markov chain cycles through finite-size subsets of vertices in a deterministic order and updates the colours assigned to the vertices of each subset. The systematic scan Markov chain that we present cycles through subsets consisting of 2x2 sub-grids and updates the colours assigned to the vertices using a procedure known as heat-bath. We give a computer-assisted proof that this systematic scan Markov chain mixes in O(log n) scans, where n is the size of the rectangular sub-grid. We make use of a heuristic to compute required couplings of colourings of 2x2 sub-grids. This is the first time the mixing time of a systematic scan Markov chain on the grid has been shown to mix for less than 8 colours. We also give partial results that underline the challenges of proving rapid mixing of a systematic scan Markov chain for sampling 6-colourings of the grid by considering 2x3 and 3x3 sub-grids. ", "machine_abstract": "We present an algorithm that systematically scans all possible 7-colourings of the grid, and report on its performance in terms of running time and memory consumption. The algorithm is based on a simple backtracking scheme combined with some heuristics to prune parts of the search space. We have implemented this algorithm using Java 1.6 and tested it on several instances ranging from small grids up to large ones containing more than one million nodes. For each instance we provide detailed information about how much time was spent by our program during colouring as well as how many colours were used. In addition, we also show how these results compare against those obtained by other algorithms proposed recently in the literature.     Keywords: Coloring problems, Computational complexity theory, Graphs, Backtrack search, Heuristic methods, Grid graphs, Integer programming, Optimization problems, Search trees, Time-complexity analysis         INTRODUCTION     A graph G = (V, E) consists of two sets V and E where V denotes the set of vertices or nodes and E denotes the set of edges between pairs of nodes. An edge e=(u,v) connects node u \u2208 V to v \u2208 V . If there exists no such connection then e is not included in E. A path P is defined as a sequence of distinct nodes v1 , v2 , \u2026 , vn such that vi\u22121vi \u2208 E for i = 2 , 3 , \u2026 , n . A cycle C is defined as a path whose first and last nodes are identical. A connected component is a subgraph H of G which has the property that any pair of nodes in H can be joined by a path within H but cannot be joined by paths outside H. A clique K is a complete subgraph of G; that is, every pair of nodes in K is adjacent to each other. A k-clique is a clique consisting of exactly k nodes. A vertex cover S is a subset of V such that every edge in G has at least one endpoint in S. A dominating set D is a subset of V", "paraphrased_abstract": "X -    - is a 'complete' subgraph of G, that is, all the nodes in K are adjacent. A clique is a complete subgraph of G, that is, every node in K is adjacent to each other. X -  - is a clique consisting of exactly k nodes. We are testing the algorithm in Java 1.6 and we compare it with other algorithms. We have examined the program's performance in the field of coloring, and we show that the results are in comparison with other algorithms. We present a program for colorizing seven colours of the grid and report on its performance. We have a graph (V, E) of two lines: V is the number of vertices or nodes, and E is the number of edges between two nodes. Hence, if there is no such link, e is not included in E. A path P is a sequence of separate nodes v1 and v2 in a sequence, so that vi = u  V, if there is no such connection, e is not included in E. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0204, "title": "Using Images to create a Hierarchical Grid Spatial Index", "abstract": "  This paper presents a hybrid approach to spatial indexing of two dimensional data. It sheds new light on the age old problem by thinking of the traditional algorithms as working with images. Inspiration is drawn from an analogous situation that is found in machine and human vision. Image processing techniques are used to assist in the spatial indexing of the data. A fixed grid approach is used and bins with too many records are sub-divided hierarchically. Search queries are pre-computed for bins that do not contain any data records. This has the effect of dividing the search space up into non rectangular regions which are based on the spatial properties of the data. The bucketing quad tree can be considered as an image with a resolution of two by two for each layer. The results show that this method performs better than the quad tree if there are more divisions per layer. This confirms our suspicions that the algorithm works better if it gets to look at the data with higher resolution images. An elegant class structure is developed where the implementation of concrete spatial indexes for a particular data type merely relies on rendering the data onto an image. ", "machine_abstract": "We present an algorithm for creating a hierarchical grid spatial index using images as the basis for its construction.  The algorithm is based on the observation that many real-world datasets are naturally represented by images, and can be used in conjunction with existing techniques such as R-tree or Quadtree to improve performance.   We show how our technique performs against these other methods through experiments conducted over synthetic data sets generated according to different distributions (uniform, normal, exponential) and sizes ranging between 1K and 100M points.   Our results demonstrate significant improvements in query response times when compared to traditional approaches. In this work we propose a new approach for building a spatial index which uses image processing algorithms to extract information about the dataset being indexed. This information is then used to build a hierarchy of grids whose leaf nodes contain pointers to individual objects within the dataset. These grids provide efficient access to all objects contained therein while also allowing fast queries across multiple grids at once.", "paraphrased_abstract": "In this work, we propose a new approach for building a spatial database, which is based on the analysis of the data, based on images, which are taken as an input to a graphical representation of the dataset, and can be used in conjunction with other algorithms, such as R-tree and Quadtree, to increase the accuracy of the graphical representation. We show how our method works against other methods, in experiments of a dataset of different sizes, uniform, normal, exponential, and large, and show that it is significantly faster than these other methods. We develop a new method of spatial mapping that uses the knowledge extracted from the dataset by image processing and then assembles a hierarchical grid, each of which contains a pointer to the individual objects contained within it, while allowing fast searches across several grids. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1783, "title": "Unicast and Multicast Qos Routing with Soft Constraint Logic Programming", "abstract": "  We present a formal model to represent and solve the unicast/multicast routing problem in networks with Quality of Service (QoS) requirements. To attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different QoS metric value (e.g. bandwidth, cost, delay, packet loss). The second step consists in writing this graph as a program in Soft Constraint Logic Programming (SCLP): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to QoS routing problems. Moreover, c-semiring structures are a convenient tool to model QoS metrics. At last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved. ", "machine_abstract": "We present an algorithm for unicast and multicast quality-of-service (QoS) routing in the Internet using soft constraint logic programming (SCLP). The proposed approach is based on the concept that each node maintains its own view about the network topology, which may be different than other nodes' views due to link failures or congestion. We use SCLP as our underlying framework because it can naturally represent such inconsistent information among nodes. In addition, we show how to incorporate bandwidth constraints into the SCLP model by introducing new variables representing available bandwidths between two adjacent links. Finally, we propose several algorithms to solve the problem efficiently. Our experimental results demonstrate that the proposed method outperforms existing approaches significantly under various conditions. Keywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast", "paraphrased_abstract": "In short, the problem can be solved, and our results show that our method outperforms the others under various conditions. We introduce a method for the unicast and the multicast quality of service of the Internet in soft constraints. The proposed method considers the fact that each node has a particular view of the network, and it is of course different from that of other nodes, because of link problems or congestion. The proposed method is based on the notion that each node is encoding its own view of the network, and this view may be different from the views of other nodes due to link failures or congestion. In this paper, we present a method for the simultaneous and simultaneous quality of service of the Internet, and we also present a method for distributing bandwidth between two adjacent links. We propose an algorithm for the efficient allocation of bandwidth between two adjacent links, which uses the concept of bandwidth as a starting point. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3187, "title": "The building up of the disk galaxy M33 and the evolution of the metallicity gradient", "abstract": "  The evolution of radial gradients of metallicity in disk galaxies and its relation with the disk formation are not well understood. Theoretical models of galactic chemical evolution make contrasting predictions about the time evolution of metallicity gradients. To test chemical evolution models and trace the star formation and accretion history of low luminosity disk galaxies we focus on the Local Group galaxy M33. We analyze O/H and S/H abundances in planetary nebulae, H{\\sc ii} regions, and young stars, together with known [Fe/H] abundances in the old stellar population of M33. With a theoretical model, we follow the time evolution of gas (diffuse and condensed in clouds), stars, and chemical abundances in the disk of M33, assuming that the galaxy is accreting gas from an external reservoir. Our model is able to reproduce the available observational constraints on the distribution of gas and stars in M33 and to predict the time evolution of several chemical abundances. In particular, we find that a model characterized by a continuous infall of gas on the disk, at a rate of $\\dot M_{\\rm inf}\\approx 1$ $M_\\odot$ yr$^{-1}$, almost constant with time, can also account for the relatively high rate of star formation and for the shallow chemical gradients. Supported by a large sample of high resolution observations for this nearby galaxy, we conclude that the metallicity in the disk of M33 has increased with time at all radii, with a continuous flattening of the gradient over the last $\\sim 8$ Gyr. ", "machine_abstract": "We present new spectroscopic observations in the near-infrared (NIR) for a sample of about 100 HII regions located along the major axis of the nearby spiral galaxy M33, at distances between 1 kpc and 8 kpc from its center. The main goal is to study how the chemical composition changes with galactocentric distance within this galaxy. We have used the NIR spectrograph ISAAC on board the European Southern Observatory's Very Large Telescope UT1 telescope to obtain high signal-to-noise ratio spectra covering the wavelength range 0.9-2.5 microns. From these data we derive oxygen abundances using two different methods based on strong emission lines. In addition, we use our results together with published photometric data to construct radial profiles of several physical parameters such as electron density, temperature or ionized gas mass fraction. Our analysis shows that there are no significant differences between both abundance determination techniques. However, when comparing our results with previous studies carried out by other authors it becomes clear that some discrepancies exist among them which can be explained mainly due to the fact that they were obtained under very different observational conditions.", "paraphrased_abstract": "We have been observing a new spectrograph ISAAC in the Very Large Telescope of the European Southern Observatory, UT1 at a resolution of a few hundred tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of microns in the range of a few hundred tens of tens of tens of tens of tens of tens of tens of tens of thousands of tens of thousands of tens of tens of thousands of tens of thousands of tens of thousands of tens of thousands of tens of thousands of miles from the centre of the spiral galaxy, at a distance of tens of tens of tens of thousands of tens of thousands of miles from its centre. This is the main goal of our research. We present here a study of the chemical composition of the spiral galaxy M33, about 100 tens of miles away from the center of the spiral galaxy, which is about one-eight kpc from the center of the galaxy, at", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.076, "title": "Neutral Networks of Sequence to Shape Maps", "abstract": "  In this paper we present a novel framework for sequence to shape maps. These combinatorial maps realize exponentially many shapes, and have preimages which contain extended connected subgraphs of diameter n (neutral networks). We prove that all basic properties of RNA folding maps also hold for combinatorial maps. Our construction is as follows: suppose we are given a graph $H$ over the $\\{1 >...,n\\}$ and an alphabet of nucleotides together with a symmetric relation $\\mathcal{R}$, implied by base pairing rules. Then the shape of a sequence of length n is the maximal H subgraph in which all pairs of nucleotides incident to H-edges satisfy $\\mathcal{R}$. Our main result is to prove the existence of at least $\\sqrt{2}^{n-1}$ shapes with extended neutral networks, i.e. shapes that have a preimage with diameter $n$ and a connected component of size at least $(\\frac{1+\\sqrt{5}}{2})^n+(\\frac{1-\\sqrt{5}}{2})^n$. Furthermore, we show that there exists a certain subset of shapes which carries a natural graph structure. In this graph any two shapes are connected by a path of shapes with respective neutral networks of distance one. We finally discuss our results and provide a comparison with RNA folding maps. ", "machine_abstract": "We introduce the concept of neutral networks, which are graphs that represent relationships between sequences and shapes in an unsupervised manner. We show how these can be used for shape recognition by training them on synthetic data generated using computer graphics techniques. The resulting network is able to recognize shapes with high accuracy even when they have been transformed or distorted. Finally we demonstrate our method on real world images showing its ability to generalize well beyond the training set. In this work we present a novel approach to learning about shapes through their relationship to sequences. Our main contribution is introducing the concept of neutral networks as a way to learn such relationships without supervision. Neutral networks are graphs whose nodes correspond to sequences (e.g., strings) and edges connect similar sequences. They provide a natural representation for capturing relationships between different types of information. For example, one could use a neutral network to capture relationships between sentences and documents, or between music notes and melodies. Here we focus on applying neutral networks to the problem of recognizing shapes based solely on their sequence of points sampled along their boundary. This task has many applications including medical image analysis where it may not always be possible to obtain ground truth labels due to privacy concerns.", "paraphrased_abstract": "In this work, we are presenting a new approach to learning about shapes by their relation to sequences. We present a neutral network, which consists of nodes that correspond to sequences, and with edges that connect similar sequences. This is a natural representation of the relationship between different kinds of information. For example, we can use it to find the relationships between sentences and documents, or to discover the relationship between music notes and melodies. This approach is a novel one, and is of great interest to many of us, in medical imaging, where the public's right to know the ground truth may not be always possible. We introduce the concept of neutral networks, which are graphs that represent relations between sequences and shapes without supervision. We introduce the concept of neutral networks, which are graphs which represent relationships between sequences and shapes in an unsupervised way. We illustrate the application of the system to a number of cases, and show that our method can be applied to many other cases as well. The system can be used to identify shapes by detecting them through their sequences, even when they are transformed or distorted. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3852, "title": "Fluidization of a vertically oscillated shallow granular layer", "abstract": "  Molecular dynamics simulations are used to study fluidization of a vertically vibrated, three-dimensional shallow granular layer. As the container acceleration is increased above g, the granular temperature and root mean square particle displacement increase, gradually fluidizing the layer. For nearly elastic particles, or low shaking frequencies, or small layer depths, the end of the fluidization process is marked by an abrupt increase in the granular temperature and rms particle displacement. The layer is then fully fluidized since macroscopic, fluid-like phenomena such as convection rolls and surface waves are observed. Increasing the total dissipation (by either decreasing the restitution coefficient or increasing the total number of particles) decreases the increase in granular temperature and rms particle displacement at fluidization, and shifts the increase to higher accelerations. Increasing the frequency also decreases the magnitude of the jump, and shifts the change to lower accelerations. ", "machine_abstract": "We report on the fluidization of a thin, horizontally vibrating granular bed by vertical oscillations at low frequency and amplitude. The system is studied experimentally using high-speed video imaging in combination with particle tracking velocimetry (PTV). We find that for sufficiently large vibration amplitudes, particles are ejected from the surface into the air as they reach their maximum height during an upward motion. This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container. In addition we observe convection rolls which transport grains across the entire depth of the bed. These results show striking similarities to those observed in vibrated beds of sand or glass beads but differ significantly from previous studies performed on systems where only horizontal vibrations were applied. Vibrations can induce transitions between different states of matter such as solids, liquids, and gases [1] . For example, it has been shown that a solid state may become unstable when subjected to periodic forcing [2] , resulting in the spontaneous generation of traveling waves [3] . A particularly interesting case occurs if both horizontal and vertical components of the driving force act simultaneously [4] . In this work we study the response of a thin granular layer to simultaneous application of horizontal and vertical vibrations. Our experiments reveal new phenomena not seen before in other types of driven granular media.", "paraphrased_abstract": ", a liquid, a liquid, and a gas have a transition between the three, namely, that solids, water, and gases can be separated from one another by constant, repeated pressure. Then, the bending of the granular layer, by means of vertical and horizontal movements, causes the liquid to move upwards, and then the gas evaporates and condenses below the dense packed surface. These results, in contrast with those in vibrating sand or glass beads, are strikingly similar to those of vibrating sand and glass beads, but differing greatly from those previously studied on the basis of horizontal and vertical vibrations. Moreover, we see the rolling of the sieves, which transport the grains across the entire depth of the bed. In this study, we study the response of a thin granular layer to simultaneous vertical and horizontal vibrations. In the grating, for example, a solid state becomes unstable under the influence of a constant force, and a corresponding oscillation is created. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0646, "title": "Almost the supersymmetric Standard Model from intersecting D6-branes on the Z_6' orientifold", "abstract": "  Intersecting stacks of supersymmetric fractional branes on the Z_6' orientifold may be used to construct the supersymmetric Standard Model. If a,b are the stacks that generate the SU(3)_{colour} and SU(2)_L gauge particles, then, in order to obtain {\\em just} the chiral spectrum of the (supersymmetric) Standard Model (with non-zero Yukawa couplings to the Higgs mutiplets), it is necessary that the number of intersections a \\cap b of the stacks a and b, and the number of intersections a \\cap b' of a with the orientifold image b' of b satisfy (a \\cap b,a \\cap b')=(2,1) or (1,2). It is also necessary that there is no matter in symmetric representations of the gauge group, and not too much matter in antisymmetric representations, on either stack. Fractional branes having all of these properties may be constructed on the Z_6' orientifold. We construct a (four-stack) model with two further stacks, each with just a single brane, which has precisely the matter spectrum of the supersymmetric Standard Model, including a single pair of Higgs doublets. However, the gauge group is SU(3)_{\\rm colour} x SU(2)_L x U(1)_Y x U(1)_H. Only the Higgs doublets are charged with respect to U(1)_H. ", "machine_abstract": "We propose an explicit construction for a string model with gauge group SU(3)xSU(2)xU(1), three generations of quarks and leptons, right-handed neutrinos, and no exotic particles beyond those in the minimal supersymmetric standard model (MSSM). The model is based on type IIA string theory compactified to four dimensions on a Calabi-Yau manifold X which has h^{1,1} = 1 and h^{1,2} = 0. We consider a stack of N=2 D6-branes wrapping a divisor S inside X that supports a non-abelian gauge symmetry U(N_c) where N_c = 3. In addition we introduce two sets of fractional branes at singularities of order 6 along divisors T_1 and T_2 such that the total number of D6-branes is 24. By using mirror symmetry arguments it can be shown that this configuration preserves one half of the original supersymmetry.", "paraphrased_abstract": "The construction is based on the type IIA string theory, compacted to four dimensions, on a Calabi-Yau manifold X. The h111120, h110, and h110. We introduce two fractional branes at the singularities of 6 and 7, to form 24 swarms of D6 branes. We introduce two sets of fractional branes of the order of six, and add two sets of fractional branes of the same order, respectively, in the divisors T1 and T2 to the total number of D6 branes. We introduce two sets of fractional branes at the singularities of 6 in the divisors T1 and T2 so that there are 24 swarms of D6 branes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0647, "title": "Testing String Theory with CMB", "abstract": "  Future detection/non-detection of tensor modes from inflation in CMB observations presents a unique way to test certain features of string theory. Current limit on the ratio of tensor to scalar perturbations, r=T/S, is r < 0.3, future detection may take place for r > 10^{-2}-10^{-3}. At present all known string theory inflation models predict tensor modes well below the level of detection. Therefore a possible experimental discovery of tensor modes may present a challenge to string cosmology.   The strongest bound on r in string inflation follows from the observation that in most of the models based on the KKLT construction, the value of the Hubble constant H during inflation must be smaller than the gravitino mass. For the gravitino mass in the usual range, m_{3/2} < O(1) TeV, this leads to an extremely strong bound r < 10^{-24}. A discovery of tensor perturbations with r > 10^{-3} would imply that the gravitinos in this class of models are superheavy, m_{3/2} > 10^{13} GeV. This would have important implications for particle phenomenology based on string theory. ", "machine_abstract": "The cosmic microwave background (CMB) is the remnant radiation left over after the Big Bang, and it provides an important test for theories about how our universe evolved.  In this article we will discuss some recent results on testing string theory using measurements of the temperature fluctuations in the CMB.   We will also describe what these tests tell us about the possible future development of string theory. The cosmic microwave background (CMB; see Figure below ) is the remnant radiation that remains today from when the early universe was only 380,000 years old. It contains information about the physical processes that took place during the first few minutes following the Big Bang. This includes the physics of inflationary expansion as well as the physics of particle production at very high energies. Because of its importance to cosmology, there are many ongoing experiments designed to measure the properties of the CMB more accurately than ever before. These include satellite missions such as WMAP and Planck, which have already produced impressive data sets, and ground-based telescopes like SPT and ACT, which are currently taking new observations.", "paraphrased_abstract": "The CMB is the remnant of the first phase of the universe, and it contains information about the physical processes that took place during the first minutes after the Big Bang, such as the inflationary expansion and the formation of atoms of very high energy. This remnant is the subject of many experiments, mainly on the satellites such as WMAP and Planck, which have already yielded impressive data, and on the ground, the SPT and the ACT, which are currently making observations. In this article, we will describe the recent results of the string theory based on the measurements of the temperature of the CMB, and we will also describe the results of the tests of string theory. The cosmic microwave background (CMB, as shown in Figure 1) is the remnant of the cosmic microwave background, which is still present today, when the early universe was only 380,000 years old. This remnant of microwave background is an important test for theories of the origin of the universe. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3885, "title": "On Classification of Finite Dimensional Complex Filiform Leibniz Algebras (Part 2)", "abstract": "  The paper is devoted to classification problem of finite dimensional complex none Lie filiform Leibniz algebras. Actually, the observations show there are two resources to get classification of filiform Leibniz algebras. The first of them is naturally graded none Lie filiform Leibniz algebras and the another one is naturally graded filiform Lie algebras. Using the first resource we get two disjoint classes of filiform Leibniz algebras. The present paper deals with the second of the above two classes, the first class has been considered in our previous paper. The algebraic classification here means to specify the representatives of the orbits, whereas the geometric classification is the problem of finding generic structural constants in the sense of algebraic geometry. Our main effort in this paper is the algebraic classification. We suggest here an algebraic method based on invariants. Utilizing this method for any given low dimensional case all filiform Leibniz algebras can be classified. Moreover, the results can be used for geometric classification of orbits of such algebras. ", "machine_abstract": "In this article, we continue the classification of finite dimensional complex filiform Leibniz algebras begun in Part 1.  We show that if an algebra is not isomorphic to one of the previously classified ones then it has dimension at least 8 and its derived subalgebra has dimension at most 3. In particular, any such algebra satisfies dim(DerA) = 3 or 4. If dim(DerA)=4, then A is isomorphic either to L4(C), L1(C2), L2(C2), L3(C2), or L5(C2). If dim(DerA )=3, then A is isomorphic to one of the following Lie algebras:  L6(C3), L7(C3), L8(C3), L9(C3), L10(C3), L11(C3), L12(C3), L13(C3), L14(C3), L15(C3), L16(C3), L17(C3), L18(C3), L19(C3), L20(C3), L21(C3), L22(C3), L23(C3), L24(C3), L25(C3), L26(C3), L27(C3), L28(C3), L29(C3), L30(C3), L31(C3), L32(C3), L33(C3), L34(C3), L35(C3), L36(C3), L37(C3), L38(C3), L39(C3), L40(C3), L41(C3), L42(C3), L43(C3), L44(C3), L45(C3), L46(C3), L47(C3), L48(C3), L49(C3), L50(C3), L51(C3), L52(C3), L53(C3), L54(C3), L55(C3), L56(C3), L57(C3), L58(C3), L59(C3), L60(C3), L61(C3), L62(C3), L63(C3", "paraphrased_abstract": "It was also found that, if there is no isomorphism of a given algebra, it is a dimension of 8 and a subalgebra of 3 to 8 with the exception of L5 and L7. This section of the text continues to describe the classification of finite dimensional complexes such as L6 and L7. This section of the paper consists of a classification of the following complex algebras: L6-C3, L7-C3, L8-C3, L9-C3, L10-C3, L11-C3, L12-C3, L14-C3, L15-C3, L14-C3, L14-C3, L14-C3, L15-C3, L16-C3, L18-C3, L19-C3, L21-C3, L23-C3, L23-C3, L23-C3, L24-C3, L23-C3, L24-C3, L27-C3, L27-C3, L29-C3, L3-C3, L4-C3, L4-C3, L4-C3, L4-C3, L-C3, L-C3, L-C3, L-C3, L-C3, L-C3, L-C3, L-C3, L-C3, L-C3, L-C3, L-C3, L-C", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1257, "title": "Detection of GRB 060927 at z = 5.47: Implications for the Use of Gamma-Ray Bursts as Probes of the End of the Dark Ages", "abstract": "  We report on follow-up observations of the GRB 060927 using the ROTSE-IIIa telescope and a suite of larger aperture ground-based telescopes. An optical afterglow was detected 20 s after the burst, the earliest rest-frame detection of optical emission from any GRB. Spectroscopy performed with the VLT about 13 hours after the trigger shows a continuum break at lambda ~ 8070 A produced by neutral hydrogen absorption at z~5.6. We also detect an absorption line at 8158 A which we interpret as SiII at z=5.467. Hence, GRB 060927 is the second most distant GRB with a spectroscopically measured redshift. The shape of the red wing of the spectral break can be fitted by a damped Lyalpha profile with a column density with log(N_HI/cm^-2) ~ 22.5. We discuss the implications of this work for the use of GRBs as probes of the end of the dark ages and draw three main conclusions: i) GRB afterglows originating from z>6 should be relatively easy to detect from the ground, but rapid NIR monitoring is necessary to ensure that they are found; ii) The presence of large HI column densities in some GRBs host galaxies at z>5 makes the use of GRBs to probe the reionization epoch via spectroscopy of the red damping wing challenging; iii) GRBs appear crucial to locate typical star-forming galaxies at z>5 and therefore the type of galaxies responsible for the reionization of the universe. ", "machine_abstract": "We report on the detection by Swift/BAT and Konus-Wind of gamma-ray burst (GRB) 060927, which is one of only two events detected with redshifts greater than five to date.  The redshift was determined using optical spectroscopy obtained with the Very Large Telescope in Chile within three hours after the burst trigger time.  We find that this event has an extremely hard spectrum, similar to other high-redshift bursts observed previously.  Using our current understanding of the physics behind these phenomena we can estimate the total energy released during the prompt emission phase of the burst to be ~10^53 erg.  This value is comparable to or larger than the energies released by some supernovae.  If confirmed, it would make GRB 060927 the most energetic event ever recorded.  In addition, if the host galaxy mass function evolves similarly to what is seen locally, then the progenitor system responsible for producing such an energetic explosion must have been hosted by a very massive galaxy.  These results are consistent with theoretical predictions made about the progenitors of high-z GRBs.  Finally, we discuss how future observations of this object may help us understand the end of cosmic reionization.", "paraphrased_abstract": "We also discuss the possibility of future observations of this object, which may help to explain the ending of cosmic reionization. Then, we present an analysis of the detection of GRB 060927, which is the first of two to be observed with redshifts of five. This is a spectral test, and we find that it is a high-energy event, in the same way as the peaks of supernovae. It is, in fact, the strongest of all the peaks of GRBs, and in fact the strongest of all the high-energy explosions. It is, however, not impossible to conclude that the host galaxy, which is a big one, must be a very large galaxy. In this case, we can estimate the energy of the explosion, which is 1053 erg, equivalent to or greater than the energy of supernovae. In this way, we can see the development of the galaxy and the development of the host star. This hypothesis is supported by the theory of the GRBs of the high-z scale. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1839, "title": "Nurturing Breakthroughs: Lessons from Complexity Theory", "abstract": "  A general theory of innovation and progress in human society is outlined, based on the combat between two opposite forces (conservatism/inertia and speculative herding \"bubble\" behavior). We contend that human affairs are characterized by ubiquitous ``bubbles'', which involve huge risks which would not otherwise be taken using standard cost/benefit analysis. Bubbles result from self-reinforcing positive feedbacks. This leads to explore uncharted territories and niches whose rare successes lead to extraordinary discoveries and provide the base for the observed accelerating development of technology and of the economy. But the returns are very heterogeneous, very risky and may not occur. In other words, bubbles, which are characteristic definitions of human activity, allow huge risks to get huge returns over large scales. We outline some underlying mathematical structure and a few results involving positive feedbacks, emergence, heavy-tailed power laws, outliers/kings/black swans, the problem of predictability and the illusion of control, as well as some policy implications. ", "machine_abstract": "The book Nurturing Breakthroughs by James Watson and Peter Winkler is about how to create breakthroughs in science, technology, engineering or mathematics (STEM). The authors argue that the best way for scientists to make new discoveries is not through individual genius but rather by working together as teams on problems they are passionate about.  They also claim that it helps if you have an idea of what your problem looks like before you start solving it because this will help guide your research efforts. This article describes some of their ideas and provides examples of how these concepts can be applied to physics research. In his book Nurturing Breakdowns, James Watson argues that the most successful people who solve complex problems do so by working with others instead of trying to work alone. He says that when we work individually our brains tend to focus only on one aspect of the problem at hand which may lead us down dead ends while working collaboratively allows us to see all aspects of the problem simultaneously.", "paraphrased_abstract": "He then says that the best way for the scientific community to discover new discoveries is not through a single genius but by the teamwork of others. It is said that when one does work alone, the brain tends to focus only on the one thing that is of importance, and that it may not always be possible to see all aspects of the problem simultaneously. This article describes how to make new discoveries in science, technology, engineering or mathematics (STEM). This book by Watson and Winkler is a book on how to find new breakthroughs in science, technology, engineering or mathematics. Watson and Winkler argue that the best way to discover new breakthroughs is not through individual genius alone, but by working together on problems that interest them. They assert that it is not necessary to solve problems by oneself, but must be able to work together as a team on problems which they are passionate about. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3116, "title": "The metallicity gradient of M 33: chemical abundances of HII regions", "abstract": "  We present spectroscopic observations of a sample of 72 emission-line objects, including mainly HII regions, in the spiral galaxy M 33. Spectra were obtained with the multi-object, wide field spectrograph AF2/WYFFOS at the 4.2m WHT telescope. Line intensities, extinction, and electron density were determined for the whole sample of objects. The aim of the present work was to derive chemical and physical parameters of a set of HII regions, and from them the metallicity gradient. Electron temperatures and chemical abundances were derived for the 14 HII regions where both [OII] and [OIII] emission line fluxes were measured, including the electron temperature sensitive emission line [OIII] 436.3 nm and in a few cases [NII] 575.5 nm. The ionization correction factor (ICF) method was used to derive the total chemical abundances. The presence of abundance gradients was inferred from the radial behaviour of several emission-line ratios, and accurately measured from chemical abundances directly derived in 14 HII regions. The oxygen abundances of our HII regions, located in the radial region from ~2 to ~7.2 kpc, gave an oxygen gradient -0.054+/-0.011 dex/kpc The overall oxygen gradient for M 33 obtained using ours and previous oxygen determinations in a large number of HII regions with direct electron temperature determination as well as abundance in young stars presented a two slope shape: -0.19 dex/kpc for the central regions (R<3kpc), and -0.038dex/kpc for the outer regions (R>=3kpc). ", "machine_abstract": "We present the results on abundance determinations for a sample of H II regions in M33, based on high resolution spectroscopy obtained with VLT/UVES and GIRAFFE spectrographs. The main goal is to study the radial distribution of oxygen abundance across this galaxy. We find that there are no significant differences between the gradients derived using different methods (T e , P method). Our best fit linear relation has slope -0.071 dex/kpc \u00b1 0.005 dex/kpc. This value agrees well within errors with previous studies but it is slightly steeper than those found by other authors. In addition we have studied the behaviour of several \u03b1-elements as a function of galactocentric distance. We do not detect any clear trend neither for Mg nor Si or S elements. However, our data show an increase of [Ne/O] ratio towards the center of the galaxy which could be explained if Ne were produced mainly during type Ia supernovae explosions.     Keywords: Chemical composition", "paraphrased_abstract": "It is also the first to be shown how the radial distribution of the oxygen in this galaxy is dominated by the chemical composition. In addition, we have analyzed the behaviour of certain elements with a distance as a function of the distance of the galaxy, for example, Mg and Si, and Si and S. Our results are not consistent with the previous observations, but are rather higher than those of other authors. Our aim is to investigate the radial distribution of oxygen in this galaxy. Among these we have found no difference in the abundance of oxygen in a single sample of the H II regions of M33. The analysis was carried out by high-resolution spectroscopy, and the spectral curves were compared to one another with a combination of spectroscopic methods. The difference between the two gradients was 0.071 dex/kcc and 0.005 dex/kcc. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1175, "title": "Two-sided optimal bounds for Green function of half-spaces for relativistic $\\alpha$-stable process", "abstract": "  The purpose of this paper is to find optimal estimates for the Green function of a half-space of {\\it the relativistic $\\alpha$-stable process} with parameter $m$ on $\\Rd$ space. This process has an infinitesimal generator of the form $mI-(m^{2/\\alpha}I-\\Delta)^{\\alpha/2},$ where $0<\\alpha<2$, $m>0$, and reduces to the isotropic $\\alpha$-stable process for $m=0$. Its potential theory for open bounded sets has been well developed throughout the recent years however almost nothing was known about the behaviour of the process on unbounded sets. The present paper is intended to fill this gap and we provide two-sided sharp estimates for the Green function for a half-space. As a byproduct we obtain some improvements of the estimates known for bounded sets specially for balls. The advantage of these estimates is a clarification of the relationship between the diameter of the ball and the parameter $m$ of the process.   The main result states that the Green function is comparable with the Green function for the Brownian motion if the points are away from the boundary of a half-space and their distance is greater than one. On the other hand for the remaining points the Green function is somehow related the Green function for the isotropic $\\alpha$-stable process. For example, for $d\\ge3$, it is comparable with the Green function for the isotropic $\\alpha$-stable process, provided that the points are close enough. ", "machine_abstract": "We study the Green function of a relativistic $\\alpha$-ssble process in a half-space with reflecting boundary condition at the boundary, and obtain two-sided estimates on its tail behavior by using the method of reflection principle. The results are applied to the case when the underlying space is $\\mathbb{R}^d$ or $\\mathbb{H}^1(\\mathbb{R})$. We also give an example showing that our upper bound can be strictly smaller than the lower one. Finally we prove some properties of the Green functions of relativistic $\\alpha$-stables processes killed upon leaving a domain. Let $(X_t)_{t \\geq 0}$ be a relativistic $\\alpha$-self-decomposable L\u00e9vy process (see Definition 2 below), which has been introduced independently by Bertoin [1] , Sato [2] and Watanabe [3] . It is known that this process possesses many interesting properties such as self-similarity, infinite divisibility etc. (see e.g., [4] ). In particular, it follows from [5] that there exists a constant $c > 0$ depending only on $\\alpha$ such that  $$P[X_1 = 0] \\leq c \\exp \\left( - \\frac{1}{2} |x|^{2 + \\alpha} \\right). $$", "paraphrased_abstract": "But, as we have already seen, it is known that this is a self-similar process, infinite divisibility, and is well known (see also [3]). We will examine the green function of a relativistic alpha-stable process in a half-space, a reflection-free boundary, and obtain two-sided estimates of its tail by the reflection principle. In particular, it can be seen that there is a constant of c  0 that is proportional to c  0; in particular, it is determined that c  0 = c exp 12||||2||||||||||||||||||||||||||||||||||||2||||()]: - and then - the Green Function of a 'alpha' process, which has been incorporated by Bertoin, Sato, and Watanabe. In the course of this work, we calculate the Green Function of a 'alpha' process in half-space with a reflection at the boundary of the 'alpha' and calculate the tail of", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3662, "title": "Molecular gas in NUclei of GAlaxies (NUGA) VII. NGC4569, a large scale bar funnelling gas into the nuclear region", "abstract": "  This work is part of the NUGA survey of CO emission in nearby active galaxies. We present observations of NGC4569, a member of the Virgo Cluster. We analyse the molecular gas distribution and kinematics in the central region and we investigate a possible link to the strong starburst present at the nucleus. 70% of the 1.1x10^9 Msolar of molecular gas detected in the inner 20\" is found to be concentrated within the inner 800 pc and is distributed along the large scale stellar bar seen in near-infrared observations. A hole in the CO distribution coincides with the nucleus where most of the Halpha emission and blue light are emitted. The kinematics are modelled in three different ways, ranging from the purely geometrical to the most physical. This approach allows us to constrain progressively the physical properties of the galaxy and eventually to emerge with a reasonable fit to an analytical model of orbits in a barred potential. Fitting an axisymmetric model shows that the non-circular motions must be comparable in amplitude to the circular motions (120 km/s). Fitting a model based on elliptical orbits allows us to identify with confidence the single inner Lindblad resonance (ILR) of the large scale bar. Finally, a model based on analytical solutions for the gas particle orbits in a weakly barred potential constrained by the ILR radius reproduces the observations well. The mass inflow rate is then estimated and discussed based on the best fit model solution. The gravitational torques implied by this model are able to efficiently funnel the gas inside the ILR down to 300 pc, although another mechanism must take over to fuel the nuclear starburst inside 100 pc. ", "machine_abstract": "We present new CO(2-1), 13CO(1-0), and C18O(1-0) observations with the IRAM 30m telescope towards the nearby spiral galaxy NGC 4565. The data reveal that this galaxy hosts an extended molecular disk which is warped by interactions with its companion galaxies. We find evidence for two distinct components to the molecular gas distribution; one associated with the main body of the galaxy and another component located along the southern edge of the optical disk. This second component has been previously detected as a dust lane but we show here it also contains significant amounts of molecular gas. In addition, our high resolution maps reveal a prominent central concentration of molecular gas coincident with the position of the AGN. Using these data together with previous results on other galaxies observed within the NUGA survey we investigate how the properties of the molecular gas are related to those of the stars and black holes hosted by each system.", "paraphrased_abstract": "As a result, we present the new observations of the two gases in our 30m telescope towards the nearby spiral galaxy NGC 4565. We show that this galaxy has an extended molecular disk, which is warped by interactions with its companions. Our data suggest that the two gases in the system are composed of two distinct components: one that lies at the base of the galaxy, and another that is located at the southern edge of the disk. This latter component has been previously found to be a dust-lane, but we find that it also contains considerable amounts of gas. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3855, "title": "HST/FGS Parallaxes of AM CVn Stars and Astrophysical Consequences", "abstract": "  We present absolute parallaxes and relative proper motions for five AM CVn stars, which we obtained using the Fine Guidance Sensors on board the Hubble Space Telescope. Our parallax measurements translate into distances d(AM CVn)=606+135-93 pc, d(HP Lib)=197+14-12 pc, d(CR Boo)=337+44-35 pc, d(V803 Cen)=347+32-27 pc, and d(GP Com)=75+2-2 pc. From these distances we estimate the space density of AM CVn stars and suggest that previous estimates have been too high by about an order of magnitude. We also infer the mass accretion rates which allows us to constrain the masses of the donor stars, and we show that relatively massive, semi-degenerate donor stars are favored in all systems except GP Com. Finally, we give updated estimates for their gravitational-wave signals, relevant for future space missions such as the proposed Laser Interferometer Space Antenna (LISA), based on their distances and the inferred masses of the binary components. We show that all systems but GP Com are excellent candidates for detection with LISA. ", "machine_abstract": "We present parallax measurements for four short-period (P orb < 80 min) AM CVn stars using the Fine Guidance Sensor on board HST, which are accurate to better than 1% in distance. We find that all systems have distances consistent with their being located within 20 pc of Earth. The derived absolute magnitudes range between MV = 12.5 and 14.1 mag, corresponding to luminosities of 1030 - 1600 L\u2299 . These values are significantly brighter than those predicted by theoretical models of these objects, indicating either an underestimate of the mass transfer rates or an overestimate of the white dwarf masses. In addition we use our new parallaxes together with previously published data to derive improved estimates of the orbital periods and component masses for three of the targets studied here. Finally, we discuss how our results can be used to test current evolutionary scenarios for this class of binary system.", "paraphrased_abstract": "And, by combining these new observations with previous observations, we estimate the orbital periods and component masses of three targets, which are discussed here. Moreover, we compare these results with other data and discuss how to use them in order to test the evolution of this class of binary systems. We found that the distances of all the systems were within a few centimetres of the Earth. Our data are compared with those of the stars, P orb 80, in the direction of the HST, which are a few hundred kilometer in diameter. We found that the distances of all the stars were more than a quarter of a degree from the Earth. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1593, "title": "Electrical transport and optical studies of ferromagnetic Cobalt doped ZnO nanoparticles exhibiting a metal-insulator transition", "abstract": "  The observed correlation of oxygen vacancies and room temperature ferromagnetic ordering in Co doped ZnO1-o nanoparticles reported earlier (Naeem et al Nanotechnology 17, 2675-2680) has been further explored by transport and optical measurements. In these particles room temperature ferromagnetic ordering had been observed to occur only after annealing in forming gas. In the current work the optical properties have been studied by diffuse reflection spectroscopy in the UV-Vis region and the band gap of the Co doped compositions has been found to decrease with Co addition. Reflections minima are observed at the energies characteristic of Co+2 d-d (tethrahedral symmetry) crystal field transitions, further establishing the presence of Co in substitutional sites. Electrical transport measurements on palletized samples of the nanoparticles show that the effect of a forming gas is to strongly decrease the resistivity with increasing Co concentration. For the air annealed and non-ferromagnetic samples the variation in the resistivity as a function of Co content are opposite to those observed in the particles prepared in forming gas. The ferromagnetic samples exhibit an apparent change from insulator to metal with increasing temperatures for T>380K and this change becomes more pronounced with increasing Co content. The magnetic and resistive behaviors are correlated by considering the model by Calderon et al [M. J. Calderon and S. D. Sarma, Annals of Physics 2007 (Accepted doi: 10.1016/j.aop.2007.01.010] where the ferromagnetism changes from being mediated by polarons in the low temperature insulating region to being mediated by the carriers released from the weakly bound states in the higher temperature metallic region. ", "machine_abstract": "ZnO:Co nanocrystals were synthesized by the sol-gel method followed by annealing at different temperatures (500-700 \u00b0C). The structural, morphological, magnetic and electrical properties have been investigated in detail using X-ray powder diffraction (XRD), scanning electron microscopy (SEM) with energy dispersive spectroscopy (EDS), vibrating sample magnetometer (VSM), Hall effect measurement system and UV-Vis-NIR spectrophotometry techniques. XRD analysis reveals that all samples are single phase wurtzite structure without any impurity peaks. SEM images show that the particles are spherical in shape and their size increases with increasing temperature. VSM measurements reveal that the saturation magnetization decreases gradually as the annealing temperature is increased up to 700 \u00b0C due to the decrease in Co content. It was found that the resistivity shows an abrupt increase around 500 \u00b0C which indicates the occurrence of semiconductor-metal transition.", "paraphrased_abstract": "It was found that there were no defects in the structure of the particles, and that the resistance was increasing, in particular, at the temperature of about 700 \u00b0C. Then, to the effect of the Hall effect, it was observed that the saturation of magnetism declined in the process of annealing up to 700 \u00b0C. Then, the resistance showed an abrupt increase at the 500 \u00b0C temperature. The structures, morphology, magnetic and electrical properties of the alloy were characterized in detail by X-ray diffraction, scanning electron microscopy, EDS, vibrating sample magnetometer, Hall effect, and UV-Vis-NIR spectroscopy. X-ray diffraction revealed that the alloy was of pure wurtzite, without peaks in the composition. X-ray diffraction revealed that all the alloys were single-phase, without peaks, and the alloy was annealed at various temperatures. The resistivity gradually increased at temperatures of 700\u00b0C. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2041, "title": "BVRI Surface Photometry of Isolated Spiral Galaxies", "abstract": "  A release of multicolor broad band (BVRI) photometry for a subsample of 44 isolated spirals drawn from the Catalogue of Isolated Galaxies (CIG) is presented. Total magnitudes and colors at various circular apertures, as well as some global structural/morphological parameters are estimated. Morphology is reevaluated through optical and sharp/filtered R band images, (B-I) color index maps, and archive near-IR JHK images from the Two-Micron Survey. The CAS structural parameters (Concentration, Asymmetry, and Clumpiness) were calculated from the images in each one of the bands. The fraction of galaxies with well identified optical/near-IR bars (SB) is 63%, while a 17% more shows evidence of weak or suspected bars (SAB). The sample average value of the maximum bar ellipticity is 0.4. Half of the galaxies in the sample shows rings. We identify two candidates for isolated galaxies with disturbed morphology. The structural CAS parameters change with the observed band, and the tendencies they follow with the morphological type and global color are more evident in the redder bands. In any band, the major difference between our isolated spirals and a sample of interacting spirals is revealed in the A-S plane. A deep and uniformly observed sample of isolated galaxies is intended for various purposes including (i) comparative studies of environmental effects, (ii) confronting model predictions of galaxy evolution and (iii) evaluating the change of galaxy properties with redshift. ", "machine_abstract": "We present BVRI surface photometry for isolated spiral galaxies in the nearby universe (0.01 < z < 0.1). The sample consists of 12 objects selected by their morphological type, inclination and apparent size to be suitable targets for detailed studies with integral field spectroscopy. We use archival data obtained at the Kitt Peak National Observatory 4m telescope as well as new observations taken during our own observing runs between 2005-2007. Our analysis is based on two-dimensional fitting of exponential disk models using GALFIT. In addition we perform bulge-disk decomposition using two different methods. First, we fit S\u00e9rsic profiles to both components simultaneously. Second, we apply an iterative method where we first subtract the best-fit model from each image before performing another round of fits. This procedure allows us to study the properties of individual galaxy components separately. For all but one object we find that the results are consistent within the uncertainties.", "paraphrased_abstract": "\u201cWe used a method of two-dimensional regressions of the bulge-disk model using Galif and we examined the properties of each component separately. Our study consisted of twelve objects selected from the morphological types, inclination, and apparent size of which we found it to be a suitable candidate for the study of integral field spectroscopy. The spectra of this collection consisted of a few dozen objects which we surveyed with a kitt-poisson, a number of which we compared with observations we obtained at the Kitt-Poisson Observatory in the winter of 2005. We compared the results with the results of a different method: first we simulated the s\u00e9rsic profiles and a second method, which was employed in iterative fashion, and thus reconstituted a slit-disk decomposition. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0652, "title": "Galactic Wind Signatures around High Redshift Galaxies", "abstract": "  We carry out cosmological chemodynamical simulations with different strengths of supernova (SN) feedback and study how galactic winds from star-forming galaxies affect the features of hydrogen (HI) and metal (CIV and OVI) absorption systems in the intergalactic medium at high redshift. We find that the outflows tend to escape to low density regions, and hardly affect the dense filaments visible in HI absorption. As a result, the strength of HI absorption near galaxies is not reduced by galactic winds, but even slightly increases. We also find that a lack of HI absorption for lines of sight (LOS) close to galaxies, as found by Adelberger et al., can be created by hot gas around the galaxies induced by accretion shock heating. In contrast to HI, metal absorption systems are sensitive to the presence of winds. The models without feedback can produce the strong CIV and OVI absorption lines in LOS within 50 kpc from galaxies, while strong SN feedback is capable of creating strong CIV and OVI lines out to about twice that distance. We also analyze the mean transmissivity of HI, CIV, and OVI within 1 h$^{-1}$ Mpc from star-forming galaxies. The probability distribution of the transmissivity of HI is independent of the strength of SN feedback, but strong feedback produces LOS with lower transmissivity of metal lines. Additionally, strong feedback can produce strong OVI lines even in cases where HI absorption is weak. We conclude that OVI is probably the best tracer for galactic winds at high redshift. ", "machine_abstract": "We present the results of an analysis of deep Chandra X-ray Observatory observations of two high redshift galaxies, MS1512-cB58 and APM 08279+5255 (z = 3.91). We find that both sources show evidence for extended soft X-ray emission with luminosities in excess of 1043 erg/sec. The observed properties are consistent with those expected from galactic winds driven by supernovae or active nuclei. In addition to these diffuse components we detect several point-like X-ray sources within each galaxy's field-of-view which may be associated with young supermassive black holes at early stages of their formation. These objects have bolometric luminosities ranging between 1044-1046 erg/sec and appear to lie on tracks similar to those followed by quasars as they evolve through cosmic time. This work is based upon data obtained for the Guaranteed Time Observing program operated by NASA under contract NAS8-39073.", "paraphrased_abstract": "This research is based on data obtained under NASA\u2019s guaranteed time system (NAS 39073). In addition to these, a few point sources of X-rays are observable in each galaxy, which may be associated with young supermassive black holes at the early stage of their formation. These objects have bolometric luminosities in the range of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of thousands of tens of millions of tens of thousands of thousands of years, a total of tens of thousands of years, a total of tens of thousands of years of time and tens of thousands of years of space. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3829, "title": "Gauge symmetry in Kitaev-type spin models and index theorems on odd manifolds", "abstract": "  We construct an exactly soluble spin-$\\frac{1}2$ model on a honeycomb lattice, which is a generalization of Kitaev model. The topological phases of the system are analyzed by study of the ground state sector of this model, the vortex-free states. Basically, there are two phases, A phase and B phase. The behaviors of both A and B phases may be studied by mapping the ground state sector into a general p-wave paired states of spinless fermions with tunable pairing parameters on a square lattice. In this p-wave paired state theory, the A phase is shown to be the strong paired phase, an insulating phase. The B phase may be either gapped or gapless determined by the generalized inversion symmetry is broken or not. The gapped B is the weak pairing phase described by either the Moore-Read Pfaffian state of the spinless fermions or anti-Pfaffian state of holes depending on the sign of the next nearest neighbor hopping amplitude. A phase transition between Pfaffian and anti-Pfaffian states are found in the gapped B phase. Furthermore, we show that there is a hidden SU(2) gauge symmetry in our model. In the gapped B phase, the ground state has a non-trivial topological number, the spectral first Chern number or the chiral central charge, which reflects the chiral anomaly of the edge state. We proved that the topological number is identified to the reduced eta-invariant and this anomaly may be cancelled by a bulk Wess-Zumino term of SO(3) group through an index theorem in 2+1 dimensions. ", "machine_abstract": "We study the gauge structure of quantum spin systems with local interactions, which are described by the so-called Kitaev model. We show that this model is equivalent to an exactly solvable lattice gauge theory (LGT) whose action functional is given by the Chern-Simons term. The equivalence between these two theories allows us to use powerful techniques developed for LGTs such as the index theorem or the localization technique. In particular we prove that the ground state degeneracy of the Kitaev model on any closed 3-manifold M equals the number of independent solutions of certain differential equations called Witten's equation. This result can be viewed as a generalization of the Atiyah-Patodi-Singer index theorem to manifolds with boundary. Finally, using the results obtained above, we derive some exact formulas for the partition function of the Kitaev model defined on surfaces with boundaries. These formulas generalize the celebrated formula due to Witten for the partition function of Yang-Mills fields on Riemann surfaces.", "paraphrased_abstract": "At last, applying the results obtained above, we derive some exact formulas for the partition function of the Kitaev model on closed-ended M3434, that is, the famous formula of Witten for the partition function of Yang-Mills fields on Riemann surfaces. We present a unified model of the sphere-difference of a quantum-spin system with local interactions, which is called the Kitaev model. It is a perfect fit to the simplest of the gauges, which is called the tessellation of Chern-Simons, which is the function of the Chern-Simons term. We show that this model is equal to a definite and stable lattice-gauge theory, whose action is based on the Chern-Simons term. The equivalence of the two theories allows us to apply the latest techniques for LGTs, such as the index theorem or the localization method. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2177, "title": "Unravelling the morphologies of Luminous Compact Galaxies using the HST/ACS GOODS survey", "abstract": "  (Abridged) Luminous Compact Galaxies (LCGs) (M_B < -20, R_{1/2} < 4.5kpc and EW_{0}(OII) >15\\AA) constitute one of the most rapidly evolving galaxy populations over the last ~8 Gyr history of the universe. Due to their inherently compact sizes, any detailed quantitative analysis of their morphologies has proved to be difficult in the past. In this work we use the high angular resolution imaging data, from the HST/ACS GOODS survey, to study the quantitative morphology of a complete sample of LCGs in the redshift range 0.5 < z < 1.2. We have derived structural parameters for a representative sample of 39 LCGs selected from the GOODS-S field, using full 2-dimensional surface brightness profile fitting of the galaxy images in each of the four filters available. B-z color maps are used as an aid in the morphological classification. We then use the rest frame B band bulge flux fraction (B/T) to determine the morphological class of the galaxies. Mergers were essentially identified visually by the presence of multiple maxima of comparable intensity in the rest frame B band images, aided by the color maps to distinguish them from HII regions. We derive the following morphological mix for our sample of intermediate redshift LCGs: Mergers: ~36%, Disk dominated: ~22%, S0: ~20%, Early types: ~7%, Irr/tadpole: ~15%. We establish that our sample LCGs are intermediate mass objects with a median stellar mass of Log(M/M_{\\odot})=10.32. We also derive dust enshrouded SFR values ranging from a few to ~65 $M_{\\odot}$/year using Spitzer 24$\\mu m$ data. We estimate a factor ~11 fall in the comoving number density of blue LCGs from redshifts 0.5 < z < 1.2 to the current epoch. We finally discuss the evolutionary scenario of these LCGs which might give us some clues regarding their descendants in the local universe. ", "machine_abstract": "We present an analysis of morphological properties for a sample of luminous compact galaxies (LCGs) selected in the Hubble Ultra Deep Field and studied with the Advanced Camera for Surveys on board the Hubble Space Telescope. We use GALFIT to fit single S\u00e9rsic profiles to all objects, allowing us to derive their structural parameters such as effective radius Re, S\u00e9rsic index n, axis ratio q, position angle PA, total magnitude mT , and half-light surface brightness SB e . The results show that most of these galaxies are bulge-dominated systems with high central concentration indices (n > 2). In addition, we find that there is no significant correlation between galaxy luminosity or size and its S\u00e9rsic index. Finally, by comparing our results with those obtained for other samples of massive early-type galaxies at different redshifts, we conclude that LCGs have similar sizes but higher concentrations than local elliptical galaxies.", "paraphrased_abstract": "The study was performed in the Hubble Ultra-Dark Field and the Advanced Camera for Surveys was based on the Hubble Space Telescope. The results were compared with those of other samples of early-type galaxies, of different redshifts. Our findings indicated that most of these galaxies were bulge-dominated and that the central concentration was high (n>2). Besides, we found that the proportion of the luminosity of the objects compared with the luminosity of the spheres was not significant. We derived, in particular, from the indices of the S\u00e9rsic Index, which we obtained for all the objects. The structure of the galaxies is determined by the means of the axis, the angle of the plane, the axis angle, the total height mT, the surface brightness of the surface. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3989, "title": "Augmented Sparse Reconstruction of Protein Signaling Networks", "abstract": "  The problem of reconstructing and identifying intracellular protein signaling and biochemical networks is of critical importance in biology today. We sought to develop a mathematical approach to this problem using, as a test case, one of the most well-studied and clinically important signaling networks in biology today, the epidermal growth factor receptor (EGFR) driven signaling cascade. More specifically, we suggest a method, augmented sparse reconstruction, for the identification of links among nodes of ordinary differential equation (ODE) networks from a small set of trajectories with different initial conditions. Our method builds a system of representation by using a collection of integrals of all given trajectories and by attenuating block of terms in the representation itself. The system of representation is then augmented with random vectors, and minimization of the 1-norm is used to find sparse representations for the dynamical interactions of each node. Augmentation by random vectors is crucial, since sparsity alone is not able to handle the large error-in-variables in the representation. Augmented sparse reconstruction allows to consider potentially very large spaces of models and it is able to detect with high accuracy the few relevant links among nodes, even when moderate noise is added to the measured trajectories. After showing the performance of our method on a model of the EGFR protein network, we sketch briefly the potential future therapeutic applications of this approach. ", "machine_abstract": "We propose an algorithm for reconstructing the signaling network in which each node is associated with multiple time series data, and we assume that only some nodes are observed at any given time point. The proposed method consists of two steps: (1) estimating the state vector by solving a sparse reconstruction problem; (2) inferring the edge set using the estimated states as features. We show through numerical experiments on synthetic networks that our approach can accurately recover both the structure and parameters of the underlying network even when only partial information about the system is available. Our results also suggest that the performance of the proposed method depends strongly on how well the sparsity pattern of the true signal is captured by the observation matrix. Finally, we apply this method to analyze the dynamics of protein phosphorylation in yeast cells responding to heat shock stress. In particular, we identify several key proteins involved in regulating the response process. This work was supported by NIH grant R01GM084283-01A1", "paraphrased_abstract": "We are also able to study the dynamics of phosphorylation in yeast cells, in particular phosphorylation of certain key proteins, which are key to the phosphorylation of phosphorus. The present work was supported by the National Institute of Health grant No. GM084283. This work was supported by NIH grant No. R01GM084283. We propose a method for reconstructing the signaling network, in which each node is correlated with a time series, and where the nodes are only seen at a certain time. This method is followed by two steps: (1) estimating the state by solving a sparse reconstruction; (2) estimating the edge by estimating the features of the observed state. We have demonstrated, in our experiment, that our method can accurately and precisely reproduce the structure and the parameters of the network, even if there is only a small information about the system. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0649, "title": "Spherically symmetric problem on the brane and galactic rotation curves", "abstract": "  We investigate the braneworld model with induced gravity to clarify the role of the cross-over length scale \\ell in the possible explanation of the dark-matter phenomenon in astrophysics and in cosmology. Observations of the 21 cm line from neutral hydrogen clouds in spiral galaxies reveal that the rotational velocities remain nearly constant at a value v_c ~ 10^{-3}--10^{-4} in the units of the speed of light in the region of the galactic halo. Using the smallness of v_c, we develop a perturbative scheme for reconstructing the metric in a galactic halo. In the leading order of expansion in v_c, at the distances r \\gtrsim v_c \\ell, our result reproduces that obtained in the Randall-Sundrum braneworld model. This inequality is satisfied in a real spiral galaxy such as our Milky Way for distances r ~ 3 kpc, at which the rotational velocity curve becomes flat, v_c ~ 7 \\times 10^{-4}, if \\ell \\lesssim 2 Mpc. The gravitational situation in this case can be approximately described by the Einstein equations with the so-called Weyl fluid playing the role of dark matter. In the region near the gravitating body, we derive a closed system of equations for static spherically symmetric situation under the approximation of zero anisotropic stress of the Weyl fluid. We find the Schwarzschild metric to be an approximate vacuum solution of these equations at distances r \\lesssim (r_g \\ell^2)^{1/3}. The value \\ell \\lesssim 2 Mpc complies well with the solar-system tests. At the same time, in cosmology, a low-density braneworld with \\ell of this order of magnitude can mimic the expansion properties of the high-density LCDM (lambda + cold dark matter) universe at late times. Combined observations of galactic rotation curves and gravitational lensing can possibly discriminate between the higher-dimensional effects and dark matter. ", "machine_abstract": "We consider a spherically symmetric solution to Einstein's equations in five dimensions with an extra dimension compactified on S 1 /Z 2 . The bulk is assumed to be empty, while matter fields are confined to our four-dimensional world (the \"brane\"). We find that this model can explain the observed flatness of galactic rotation curves without introducing any new particles or exotic forms of energy density. In particular we show how the mass distribution within galaxies may arise naturally as a consequence of the geometry of space-time. This work was supported by NSF grant PHY-0456728. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq  A fundamental question about the nature of dark matter has been whether it consists of one or more species of particle. If so, what are their masses? What interactions do they have with ordinary matter? How much dark matter does each galaxy contain? These questions motivate us to study models for which the dark matter is described by some field theory living on a higher dimensional spacetime manifold.     Here we will focus on a class of solutions where the extra dimension is compactified on a circle $S^1$. Such configurations were first studied in [1] , where it was shown that if the fifth dimension is small compared to the other length scales involved then the gravitational potential felt by observers on the brane is indistinguishable from that produced by a point-like source located at the center of the sphere. However, when the size of the extra dimension becomes comparable to the radius of curvature of the brane, the gravitational force law changes dramatically [2] .     In [3] , Randall and Sundrum showed that such a configuration could provide a natural explanation for the hierarchy between the weak scale and the Planck scale. They considered a 5D anti-de-Sitter space with two 3-branes embedded along its boundary. One of these branes represents our universe, while the second acts like a mirror image of ours. Matter fields are localized near either brane, but gravity propagates freely throughout the entire bulk.", "paraphrased_abstract": "The present paper is supported by the NSF grant PHY-0556728, Number: 0456728. We present here an enumerable, symmetric solution of the equations of Einstein in five dimensions, with an extra dimension compacted in the sphere S 1 / Z 2: The space assumed is empty and the space occupied by the space of matter is contained in our four-dimensional world. The space is occupied by the masses of matter, and the space of matter occupyes the four-dimensional space. We have already seen that, if the fifth dimension is small compared with the other lengths of the equation, the gravitational potential that the observers feel at the bottom of the sphere is indistinguishable from that of a point at the centre of the sphere, but when the extra dimension is reduced to the radius of the curving of the sphere the law of gravity changes dramatically. The following examples are given, and a general solution is presented, namely, the one where the extra dimension is confined on a circle S1; this symmetric solution can explain the observed flatness of the galaxies, without the addition of new particles and exotic energy. These problems are addressed in the context of the structure of spacetime. The fundamental question of dark matter has been whether the matter consists of one or more particles, if it is", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1127, "title": "Redesigning Computer-based Learning Environments: Evaluation as Communication", "abstract": "  In the field of evaluation research, computer scientists live constantly upon dilemmas and conflicting theories. As evaluation is differently perceived and modeled among educational areas, it is not difficult to become trapped in dilemmas, which reflects an epistemological weakness. Additionally, designing and developing a computer-based learning scenario is not an easy task. Advancing further, with end-users probing the system in realistic settings, is even harder. Computer science research in evaluation faces an immense challenge, having to cope with contributions from several conflicting and controversial research fields. We believe that deep changes must be made in our field if we are to advance beyond the CBT (computer-based training) learning model and to build an adequate epistemology for this challenge. The first task is to relocate our field by building upon recent results from philosophy, psychology, social sciences, and engineering. In this article we locate evaluation in respect to communication studies. Evaluation presupposes a definition of goals to be reached, and we suggest that it is, by many means, a silent communication between teacher and student, peers, and institutional entities. If we accept that evaluation can be viewed as set of invisible rules known by nobody, but somehow understood by everybody, we should add anthropological inquiries to our research toolkit. The paper is organized around some elements of the social communication and how they convey new insights to evaluation research for computer and related scientists. We found some technical limitations and offer discussions on how we relate to technology at same time we establish expectancies and perceive others work. ", "machine_abstract": "The purpose of this study is to examine the role that evaluation plays in communication between teachers and students during computer-based learning environments (CBLE). The research question guiding this investigation is: How does evaluation affect student-teacher interaction? This study was conducted with two groups of undergraduate students enrolled in an introductory course on educational technology at a large Midwestern university. Students were asked to complete three tasks using a CBLE called WebQuests, which are designed for use by students working individually or collaboratively. Data collection included audio recordings of group discussions, field notes taken by researchers observing each group's work, and students' written responses to questions posed throughout their participation in the project. Analysis revealed that evaluation played several roles within these interactions including providing feedback about individual performance; clarifying expectations; establishing ground rules; and promoting reflection. These findings suggest that evaluation can be used effectively to enhance student-teacher communication when it occurs frequently enough over time so that both parties have multiple opportunities to respond to one another.", "paraphrased_abstract": "This work was carried out in two groups of undergraduate students who had taken a course on computer science at a large, large university. The subjects were given three tasks in the CBLE, called Webquests. These tasks were completed by students working in pairs or in groups. They were given three tasks in the CBLE, which is for students who work on their own or on a team. These tasks included the following: the observation of individual students, the clarification of expectations, the establishment of ground rules, the reflection of ideas. This study was carried out with the following purpose: the evaluation of students in a computer-based course, in which they were instructed, the question is: how does evaluation affect student-teacher relations? The research question is: How does evaluation affect student-teacher relations? ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1574, "title": "Wavelet transforms in a critical interface model for Barkhausen noise", "abstract": "  We discuss the application of wavelet transforms to a critical interface model, which is known to provide a good description of Barkhausen noise in soft ferromagnets. The two-dimensional version of the model (one-dimensional interface) is considered, mainly in the adiabatic limit of very slow driving. On length scales shorter than a crossover length (which grows with the strength of surface tension), the effective interface roughness exponent $\\zeta$ is $\\simeq 1.20$, close to the expected value for the universality class of the quenched Edwards-Wilkinson model. We find that the waiting times between avalanches are fully uncorrelated, as the wavelet transform of their autocorrelations scales as white noise. Similarly, detrended size-size correlations give a white-noise wavelet transform. Consideration of finite driving rates, still deep within the intermittent regime, shows the wavelet transform of correlations scaling as $1/f^{1.5}$ for intermediate frequencies. This behavior is ascribed to intra-avalanche correlations. ", "machine_abstract": "We study the statistical properties of Barkhausen noise generated by an Ising spin system with random fields and competing interactions at its surface, using wavelets to analyze the time series produced by this model.  We find that the power spectrum of the Barkhausen signal is well described by a stretched exponential function over several decades in frequency space. The stretching exponent depends on both temperature T and magnetic field H. In particular, we show how the stretching exponent can be used as a measure of the degree of disorder in the sample under investigation. Finally, we discuss possible extensions of our work to other types of systems exhibiting avalanche dynamics. Barkhausen noise (BN) has been studied extensively since it was first observed experimentally more than 100 years ago [1] . It consists of bursts of magnetization reversals which occur when a ferromagnetic material is driven through successive metastable states [2] , and is believed to play an important role in determining the coercive force of such materials [3] . The statistics of BN have attracted considerable interest recently [4] - [8] due to their potential application in non-destructive testing [9] . However, despite many experimental studies [10] - [12] there are still open questions about the origin of these fluctuations [13] . For example, while some authors claim that they arise from thermally activated processes [14] others argue that they result from collective effects [15] or even quantum tunneling [16] . A number of theoretical models [17] - [20] have also been proposed to explain the physics behind BN but none of them seems able to reproduce all features simultaneously [21] .", "paraphrased_abstract": "The statistics of the Barkhausen-noise (BN) have gained much interest recently, as it is one of the most promising phenomena in the field of non-destructive testing. However, although many experimental studies have been carried out, they still have not been fully understood. For instance, some argue that it is the result of thermally activated processes, others argue that it is the result of a collective effect, or even of a quantum tunnelling. Many theoretical models have been developed to explain the physics of BN, but none of them can provide a complete picture of all the features. We present the statistical property of Barkhausen noise generated by an Ising spin system with different fields and competing interactions, and examine the time series of this model with a wavelet. The extension of this work is discussed in terms of other avalanches. The statistical properties of Barkhausen noise (BN) have been studied extensively since it was first observed in the field more than 100 years ago, and its origin is disputed. This noise is thought to be essential to the coercive force of such materials. The BN of an Ising spin system is studied in detail, and its field and field are compared. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1765, "title": "Spitzer Mid-Infrared Spectroscopy of Infrared Luminous Galaxies at z~2 II: Diagnostics", "abstract": "  We present mid-IR spectral decomposition of a sample of 48 Spitzer-selected ULIRGs spanning z~1-3 and likely L_IR~10^12-10^13Lsun. Our study aims at quantifying the star-formation and AGN processes in these sources which recent results suggest have evolved strongly between the observed epoch and today. To do this, we study the mid-IR contribution of PAH emission, continuum, and extinction. About 3/4 of our sample are continuum- (i.e. AGN) dominated sources, but ~60% of these show PAH emission, suggesting the presence of star-formation activity. These sources have redder mid-IR colors than typical optically-selected quasars. About 25% of our sample have strong PAH emission, but none are likely to be pure starbursts as reflected in their relatively high 5um hot dust continua. However, their steep 30um-to-14um slopes suggest that star-formation might dominate the total infrared luminosity. Six of our z~2 sources have EW6.2>~0.3um and L_14um>~10^12Lsun (implying L_IR>~10^13Lsun). At these luminosities, such high EW6.2 ULIRGs do not exist in the local Universe. We find a median optical depth at 9.7um of <tau_9.7>=1.4. This is consistent with local IRAS-selected ULIRGs, but differs from early results on SCUBA-selected z~2 ULIRGs. Similar to local ULIRGs about 25% of our sample show extreme obscuration (tau_9.7>~3) suggesting buried nuclei. In general, we find that our sources are similar to local ULIRGs, but are an order of magnitude more luminous. It is not clear whether our z~2 ULIRGs are simply scaled-up versions of local ULIRGs, or subject to fundamentally different physical processes. ", "machine_abstract": "We present Spitzer mid-infrared spectroscopy for a sample of infrared luminous galaxies (LIRGs) and ultra-luminous infrared galaxies (ULIRGs). The sample is drawn from the Great Observatories Origins Deep Survey-North field, which has been observed with both IRAC and MIPS on board Spitzer Space Telescope as part of the GOODS survey program. We use these data to study the physical properties of dusty starbursts in distant galaxies by comparing their rest-frame 5-15 micron spectra against those of local ULIRGs. Our results show that most of our targets are dominated by PAH emission features similar to those seen in nearby ULIRGs. However, we also find some differences between high-z sources and local ULIRGs: 1) Most of the high-redshift objects have weaker silicate absorption feature than local ULIRGs do; 2) High-redshift sources tend to be more compact than local ULIRGS; 3) Some of them may contain an AGN component based on the detection of broad H$\\alpha$ lines or strong continuum emission shortward of 9.7 um.", "paraphrased_abstract": "\u201cSomething in this group of high-z sources we studied is not only similar to that of high-z objects but also very different from those of local ULIRGS: 1) some of these objects have a weaker silicate absorption than those of local ULIRGS; 2) some of these objects contain an AGN component based on the occurrence of long-range spherical H-alpha lines or strong continuum emission as long as 9.7 mm. We are presenting the Spitzer mid-infrared spectroscopy for a small group of bright luminous galaxies and ultra-luminous luminous galaxies, and a subset of superluminous galaxies, and a subset of ultra-luminous galaxies. The sample is taken from the Great Origins Origins field in the North Sea, and the field is observed by the Spitzer space telescope, through the program GOODS. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3158, "title": "SubmilliJansky Transients in Archival Radio Observations", "abstract": "  [ABRIDGED] We report the results of a 944-epoch survey for transient sources with archival data from the Very Large Array spanning 22 years with a typical epoch separation of 7 days. Observations were obtained at 5 or 8.4 GHz for a single field of view with a full-width at half-maximum of 8.6' and 5.1', respectively, and achieved a typical point-source detection threshold at the beam center of ~300 microJy per epoch. Ten transient sources were detected with a significance threshold such that only one false positive would be expected. Of these transients, eight were detected in only a single epoch. Two transients were too faint to be detected in individual epochs but were detected in two-month averages. None of the ten transients was detected in longer-term averages or associated with persistent emission in the deep image produced from the combination of all epochs. The cumulative rate for the short timescale radio transients above 370 microJy at 5 and 8.4 GHz is 0.07 < R < 40 deg^-2 yr^-1, where the uncertainty is due to the unknown duration of the transients, 20 min < t_char < 7 days. A two-epoch survey for transients will detect 1.5 +/- 0.4 transient per square degrees above a flux density of 370 microJy. Two transients are associated with galaxies at z=0.040 and z=0.249. These may be similar to the peculiar Type Ib/c radio supernova SN 1998bw associated with GRB 980428. Six transients have no counterparts in the optical or infrared (R=27, Ks=18). The hosts and progenitors of these transients are unknown. ", "machine_abstract": "We have searched for radio transients with flux densities between 0.1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival data obtained by the Very Large Array (VLA) over the past 20 years. We find that most of these sources are extragalactic, but we also detect several Galactic objects including pulsars, supernova remnants, and flare stars. The majority of our sample is comprised of previously uncatalogued sources; however, we recover many known variable sources such as blazars and gamma-ray burst afterglows. Our results demonstrate the power of combining large amounts of archival VLA data into one coherent dataset. This work was supported by NSF grant AST-0907860. In this Letter, we present an analysis of all available archived Very Large Array (V LA) observations taken since 1990. These data were collected during various observing programs aimed primarily at studying distant galaxies or nearby star forming regions. However, they contain valuable information about fainter transient phenomena occurring within our Galaxy. By searching through more than 10 000 hours of observation time spread across nearly 2000 epochs, we identify thousands of new faint radio sources which appear only once or twice in each epoch's data set. Most of these sources are extragalaxtic, but we also detect numerous Galactic objects including pulsar wind nebulae, supernova remnants, flare stars, and other types of active galactic nuclei. Many of these newly discovered sources are not included in existing catalogs because their low signal-to-noise ratio makes them difficult to detect when observed individually. However, by combining multiple epochs together, we can boost the sensitivity of our survey enough to detect even very weak signals.", "paraphrased_abstract": "Several astronomical and radio observations of a large scale, collected in the last twenty years, are included. In this paper we present an analysis of the observations taken by the VLA during the past twenty years. In the past twenty years the data from the VLA have been collected for studying distant galaxies and nearby stars. We are particularly interested in finding sources of light, especially nebulae, and in pulsars, supernovae, and other nebulae, and these sources were found in many of our earlier observations. The source of light, although not in many instances, was unknown to us, and was therefore not included in our own registries, as they are too low in signal to noise ratio. This was achieved by combining many epochs of archival data. Among these archival data we have found a wide range of new nebulae, with the result that the nebulae of the VLA have a high density of 0.1 to 1 mJy. Most of the nebulae are pulsars, but there are several Galactic objects such as pulsars, supernovae, and flare stars. Most of the nebulae are pulsars, but we also have several Galactic objects, pulsars", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3748, "title": "Clustering Coefficients of Protein-Protein Interaction Networks", "abstract": "  The properties of certain networks are determined by hidden variables that are not explicitly measured. The conditional probability (propagator) that a vertex with a given value of the hidden variable is connected to k of other vertices determines all measurable properties. We study hidden variable models and find an averaging approximation that enables us to obtain a general analytical result for the propagator. Analytic results showing the validity of the approximation are obtained. We apply hidden variable models to protein-protein interaction networks (PINs) in which the hidden variable is the association free-energy, determined by distributions that depend on biochemistry and evolution. We compute degree distributions as well as clustering coefficients of several PINs of different species; good agreement with measured data is obtained. For the human interactome two different parameter sets give the same degree distributions, but the computed clustering coefficients differ by a factor of about two. This shows that degree distributions are not sufficient to determine the properties of PINs. ", "machine_abstract": "We study the clustering coefficients of protein-protein interaction networks (PPINs) and their relationship with network topological properties, such as degree distribution and average path length. We find that PPINs have higher clustering coefficients than random graphs with similar degrees or scale-free networks generated by Barabasi-Albert model. The high clustering coefficient is mainly due to the existence of many triangles in these networks. In addition, we show that the clustering coefficients are correlated positively with the number of proteins but negatively with the number of interactions per protein. These results suggest that there may be some common mechanisms underlying the formation of both triangles and edges between two nodes with large degrees. Finally, we propose an algorithm for identifying functional modules based on local clustering analysis. Our method can identify clusters with different sizes and shapes. Clustering coefficients of protein-protein interactomes are studied. It is found that they are significantly larger than those of random graphs with same degree distributions or scale-free networks generated using Barabasi-Albert preferential attachment rule. High clustering coefficients are mainly caused by the presence of many triangles in these systems. Correlations between clustering coefficients and other topological parameters are also investigated. An algorithm for finding functional modules based on local cluster analysis is proposed.", "paraphrased_abstract": "It is shown that the clustering coefficients of protein-protein interaction networks (PPINs) are significantly greater than those of random graphs of similar degree or scale free networks, as well as that of random graphs with similar degrees or scale free networks, as well as the correlated proportion of interactions per protein, are positively correlated with the number of proteins, and negative for the proportion of interactions per protein. Moreover, we demonstrate that the clustering coefficients are positively correlated with the number of proteins, but not with the number of interactions per protein. The clustering coefficients of proteins-protein interactions are studied, and they are significantly larger than those of random graphs of similar degrees or scale free networks, as well as with Barabasi-Albert-based attachment. We examine the occurrence of the clustering coefficients of proteins-protein interactions and investigate their relationship with the topological properties, such as degree distribution and the average path length. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.429, "title": "The Gemini Deep Planet Survey -- GDPS", "abstract": "  We present the results of the Gemini Deep Planet Survey, a near-infrared adaptive optics search for giant planets and brown dwarfs around nearby young stars. The observations were obtained with the Altair adaptive optics system at the Gemini North telescope and angular differential imaging was used to suppress the speckle noise of the central star. Detection limits for the 85 stars observed are presented, along with a list of all faint point sources detected around them. Typically, the observations are sensitive to angular separations beyond 0.5\" with 5-sigma contrast sensitivities in magnitude difference at 1.6 micron of 9.5 at 0.5\", 12.9 at 1\", 15.0 at 2\", and 16.5 at 5\". For the typical target of the survey, a 100 Myr old K0 star located 22 pc from the Sun, the observations are sensitive enough to detect planets more massive than 2 Mjup with a projected separation in the range 40-200 AU. Second epoch observations of 48 stars with candidates (out of 54) have confirmed that all candidates are unrelated background stars. A detailed statistical analysis of the survey results, yielding upper limits on the fractions of stars with giant planet or low mass brown dwarf companions, is presented. Assuming a planet mass distribution dN/dm m^{-1.2} and a semi-major axis distribution dN/da a^{-1}, the 95% credible upper limits on the fraction of stars with at least one planet of mass 0.5-13 Mjup are 0.28 for the range 10-25 AU, 0.13 for 25-50 AU, and 0.093 for 50-250 AU. The 95% credible interval for the fraction of stars with at least one brown dwarf companion having a semi-major axis in the range 25-250 AU is 0.019 (-0.015/+0.083), irrespective of any assumption on the mass and semi-major axis distributions. The stars HD 14802, HD 166181, and HD 213845 have been resolved into binaries for the first time. ", "machine_abstract": "The Gemini Deep Planet Survey (GDPS) is an ongoing survey for transiting planets around bright stars using the twin 8-meter telescopes at Gemini Observatory in Hawaii and Chile.  The GDPS uses two different techniques to find exoplanets, one that looks for periodic dimming events caused by transits across the face of their host star, and another technique called Doppler spectroscopy which measures tiny shifts in the wavelength of light emitted by the planet as it orbits its parent star.   This data release contains all transit photometry obtained with the GDPS between May 2005 and December 2007 along with some additional follow-up observations made after this time period.    These data are available on the Extrasolar Planets Encyclopedia website at: http://exoplanet.eu/encyclopedia/transit-photometry-from-the-gemini-deep-planet-survey-gdps .  This data set includes more than 1 million individual measurements taken over nearly 1000 nights of observation.  It also includes many thousands of radial velocity measurements collected during the same time span.  In addition there are several hundred high-precision RV measurements made with other facilities such as Keck Observatory and McDonald Observatory.  All these data have been reduced into final form and combined together into a single homogeneous database containing information about each measurement including the date, time, duration, magnitude difference, etc...", "paraphrased_abstract": "There are more than 1 million measurements taken over almost a thousand nights, and then there are a lot of high-precision radial velocity measurements, taken by Keck Observatory and McDonald Observatory. This data has been compiled and compiled into a single and unified database. This is a collection of the transit data from the Gemini Observatory, with two telescopes at 8 meters, at both Hawaii and Chile, and a collection of additional observations. The GDPS consists of two instruments, one of which detects the periodic dimmer of the star in the presence of its host star, and the other of which, by means of the Doppler spectroscopy, measures the changes in the wavelength of the light from the planet that passes on the host star. These measurements are compared with each other, and the date, time, duration, and magnitude of the measurement are given. The GDPS uses two methods to find exoplanets: one uses a halo technique that detects the dimming of the star, and the other, which is called Doppler spectroscopy, which measures the changes in the wavelength of the light from the planet that reaches the parent star. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1945, "title": "Magnetorotational Collapse of Population III Stars", "abstract": "  We perform a series of two-dimensional magnetorotational core-collapse simulations of Pop III stars. Changing the initial distributions of rotation and magnetic fields prior to collapse in a parametric manner, we compute 19 models. By so doing, we systematically investigate how rotation and magnetic fields affect the collapse dynamics and explore how the properties of the black-hole formations and neutrino emissions could be affected. As for the microphysics, we employ a realistic equation of state and approximate the neutrino transfer by a multiflavour leakage scheme. With these computations, we find that the jet-like explosions are obtained by the magnetodriven shock waves if the initial magnetic field is as large as $10^{12}$ G. We point out that the black-hole masses at the formation decrease with the initial field strength, on the other hand, increase with the initial rotation rates. As for the neutrino properties, we point out that the degree of the differential rotation plays an important role to determine which species of the neutrino luminosity is more dominant than the others. Furthermore, we find that the stronger magnetic fields make the peak neutrino luminosities smaller, because the magnetic pressure acts to halt the collapse in the central regions, leading to the suppression of the releasable gravitational binding energies. ", "machine_abstract": "We present the results of our numerical simulations of magnetorotational collapse (MRC) in primordial stars with initial masses between 100 and 1000 M\u2299, which are formed at redshifts z = 20 \u2212 30. We find that for all models considered here, magnetic fields play an important role during the formation process of black holes. The final mass of the central object is determined by the strength of the magnetic field. For weak magnetic fields (B < 10^10 G), we obtain stellar-mass black holes; while for stronger fields (B > 10^{10}G), supermassive black holes form. In addition to this effect on the final mass, magnetic fields also affect the angular momentum distribution inside the collapsing star. This leads to different spin parameters of the resulting black hole depending on its progenitor's initial mass.     Keywords: Black Hole, Primordial Star Formation, Magnetohydrodynamics", "paraphrased_abstract": "We use the numerical simulation of the magnetic field to model the collapse of primordial stars at redshift z = 20  30. In addition to the magnetic field\u2019s effect on the final mass, magnetic field effects on the angular momentum of the crashing star. This results in a varying spin of the resulting black hole according to its corresponding progenitor. In addition to this magnetic field\u2019s influence on the angular momentum, magnetic fields also influence the angle of the exploded star. In the case of a black hole with a weak magnetic field (B  10  10  10  10  10  10  10 ), we see that the resulting black hole has a different spin from the first mass of the parent star. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3931, "title": "The Complexity of Model Checking Higher-Order Fixpoint Logic", "abstract": "  Higher-Order Fixpoint Logic (HFL) is a hybrid of the simply typed \\lambda-calculus and the modal \\lambda-calculus. This makes it a highly expressive temporal logic that is capable of expressing various interesting correctness properties of programs that are not expressible in the modal \\lambda-calculus.   This paper provides complexity results for its model checking problem. In particular we consider those fragments of HFL built by using only types of bounded order k and arity m. We establish k-fold exponential time completeness for model checking each such fragment. For the upper bound we use fixpoint elimination to obtain reachability games that are singly-exponential in the size of the formula and k-fold exponential in the size of the underlying transition system. These games can be solved in deterministic linear time. As a simple consequence, we obtain an exponential time upper bound on the expression complexity of each such fragment.   The lower bound is established by a reduction from the word problem for alternating (k-1)-fold exponential space bounded Turing Machines. Since there are fixed machines of that type whose word problems are already hard with respect to k-fold exponential time, we obtain, as a corollary, k-fold exponential time completeness for the data complexity of our fragments of HFL, provided m exceeds 3. This also yields a hierarchy result in expressive power. ", "machine_abstract": "We study the complexity of model checking higher-order fixpoint logic (HFL) over finite Kripke structures, which is an extension of propositional modal logic with fixpoints and quantification over state variables ranging over infinite domains. We show that HFL satisfiability can be reduced to the problem of deciding whether there exists a solution for a system of linear equations in rational numbers whose coefficients are given by polynomials over integers. This reduction allows us to obtain lower bounds on the computational complexity of HFL satisfiability using known results about solving systems of polynomial equations. In particular, we prove PSPACE-hardness of HFL satisfiability when the number of fixpoint operators occurring in the formula is unbounded or restricted only to existential ones. On the other hand, we provide a simple algorithm for HFL satisfiability based on computing least solutions of certain systems of integer quadratic equations. The running time of this algorithm depends exponentially on the maximum degree of these equations but it does not depend on the number of fixpoint", "paraphrased_abstract": "I am also in the process of studying the complexity of the calculus of the lower-order linear equations, a more sophisticated version of propositional modal logic with fixed points and quantification in state variables in infinite domains. I will show how this can be achieved by reducing the optimum degree of the solution to the optimum degree of the equations to the coefficient of polynomials over integers. The running time of this method is exponential, but does not depend on the number of solvable elements. We are now dealing with the complexity of the calculus of the lower-order linear equations, a method extending the propositional modal logic, with fixed points and quantification over state variables in infinite domains. This algorithm is based on the least-solved system of quadratic equations, and the running time is exponential, but not based on the number of solvable elements. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3943, "title": "Signatures of SUSY Dark Matter at the LHC and in the Spectra of Cosmic Rays", "abstract": "  This thesis discusses the search for supersymmetry at the future Large Hadron Collider (LHC) and the ongoing construction of one of the four large LHC experiments, the Compact Muon Solenoid (CMS), and focuses on the detection of signals from the annihilation of supersymmetric dark matter in the spectra of cosmic rays.   Measurements of cosmic ray antiparticles, such as positrons, can impose strong constraints on the nature of new physics beyond the Standard Model. However, cosmic ray positron measurements are experimentally very challenging due to the vast proton background. A novel approach of positron identification with the space-borne AMS-01 experiment, namely through the detection of bremsstrahlung conversion in a silicon microstrip detector, is introduced. Bremsstrahlung from protons is suppressed by a factor of more than 3*10^6 with respect to positrons. The results of the positron measurement show that the bremsstrahlung approach extends the sensitivity range of AMS-01 to positron momenta up to 50 GeV/c, which is far beyond the original scope of the experiment. At momenta above 8 GeV/c there is indication for a positron overabundance with respect to model predictions for purely secondary positron production. Therefore, the AMS-01 data lend further weight to the hints of a positron overabundance seen in the data from earlier experiments.   The positron fraction measurements from the most recent experiments are combined with the results of this analysis, giving the most precise positron fraction data yet available. ", "machine_abstract": "The Large Hadron Collider (LHC) is expected to produce new particles that could be dark matter candidates, such as supersymmetric partners of quarks or leptons.  In this talk I will discuss how these signatures can be searched for using data collected by the ATLAS experiment during Run 1 of the LHC. The results are presented both in terms of limits on production cross sections and mass exclusion regions. Finally, prospects for future searches with Run 2 data are discussed. This work was performed within the framework of the PhD thesis of M.A.M., supervised by A.S..  Introduction  Dark Matter Candidates  Supersymmetry  ATLAS Experiment  Run 1 Results  Prospects for Run 2 Searches  Conclusions & Outlook   References    Acknowledgements      - - - - - - - -- - - - - - --- - - - - - ---- - - - - - ----- - - - - - ------ - - - - - -------- - - - - - ---------- - - - - - ------------------ - - - - - -------------- - - - - - ----------------------------------- - - - - - ----------------------------------------------------- - - - - - ------------------------------------------------------------------------------------ - - - - -", "paraphrased_abstract": "\"...... - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2135, "title": "Universality of the REM for dynamics of mean-field spin glasses", "abstract": "  We consider a version of a Glauber dynamics for a p-spin Sherrington--Kirkpatrick model of a spin glass that can be seen as a time change of simple random walk on the N-dimensional hypercube. We show that, for any p>2 and any inverse temperature \\beta>0, there exist constants g>0, such that for all exponential time scales, $\\exp(\\gamma N)$, with $\\gamma< g$, the properly rescaled clock process (time-change process), converges to an \\alpha-stable subordinator where \\alpha=\\gamma/\\beta^2<1. Moreover, the dynamics exhibits aging at these time scales with time-time correlation function converging to the arcsine law of this \\alpha-stable subordinator. In other words, up to rescaling, on these time scales (that are shorter than the equilibration time of the system), the dynamics of p-spin models ages in the same way as the REM, and by extension Bouchaud's REM-like trap model, confirming the latter as a universal aging mechanism for a wide range of systems. The SK model (the case p=2) seems to belong to a different universality class. ", "machine_abstract": "We study the relaxation behavior in the Random Energy Model (REM) with an external magnetic field, which is one of the simplest models to describe glassy phenomena. We show that the relaxation time distribution function has a universal form independent of temperature and system size by using numerical simulations on finite-dimensional systems up to dimension d = 8. The universality holds even when we take into account the effect of quenched disorder. This result suggests that the relaxation process in this model can be described as a superposition of many processes each of which obeys the same scaling law. In addition, we find that the relaxation time distribution functions at different temperatures are well fitted by a stretched exponential function. These results suggest that the relaxation process in our model is governed by a broad spectrum of relaxation times. Introduction:-The Random Energy Model(REM), proposed by Derrida [1] ,is one of the simplest models describing glassy phenomena [2] .In this model,the energy landscape consists of N local minima whose energies are randomly distributed according to some probability density P(E).Each minimum corresponds to a metastable state and its lifetime \u03c4i is determined by the Arrhenius law exp[\u2212Ei/kBT ] where Ei denotes the activation energy barrier between two neighboring states i and j.It was shown numerically [3] that the relaxation time distribution follows a power-law decaying function f (\u03c4 ) \u221d 1/\u03c4 1+\u03b1 with \u03b1 \u2248 1.3 \u2212 2 depending on the dimensionality of the system [4] . Recently it was found [5] that the relaxation time distribution also follows a power-law decay if we consider the case without any quenched disorder but with a random initial condition instead.This indicates that the relaxation process in the REM is dominated by activated events over barriers separating different metastable states [6] .However,it should be noted that these studies were performed only for low dimensions such as d \u2264 4 [7, 8] .Therefore,in order to understand the nature of glassy phenomena more clearly,we need to investigate whether or not the above mentioned results hold true in higher dimensions.", "paraphrased_abstract": "But this is not all: the system is quite complicated, and it must be investigated in detail to understand the nature of the reflected glass. This is called the REM of Derrida, and it is the simplest and simplest model of glassy phenomena. In this model the energy landscape consists of local minima of the various energies of the system, each of which corresponds to a stable state, the energy of the Arrhenius law is exp(Ei /kBT) in which  denotes the activation energy barrier between two states i and j. Then, for the sake of the study of the relaxation time, we will take the REM, which is one of the simplest models for the study of the crystalline nature, into account. Moreover, we show that the relaxation time is governed by a broad-spectrum of relaxation times. This results suggest that the crystalline nature of the crystalline nature is governed by many processes, which obey the same scaling law. It is well known that the relaxation time is governed by a broad spectrum of relaxation times. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0009, "title": "XMM-Newton observations of the first unidentified TeV gamma-ray source TeV J2032+4130", "abstract": "  (abridged) The first unidentified very high energy gamma ray source (TeV J2032+4130) in the Cygnus region has been the subject of intensive search for a counterpart source at other wavelengths. A deep ($\\approx 50$ ksec) exposure of TeV J2032+4130 with \\textit{XMM-Newton} has been obtained. The contribution of point sources to the observed X-ray emission from TeV J2032+4130 is subtracted from the data. The point-source subtracted X-ray data are analyzed using blank sky exposures and regions adjacent to the position of TeV J2032+4130 in the field of view covered by the XMM-Newton telescopes to search for diffuse X-ray emission. An extended X-ray emission region with a full width half maximum (FWHM) size of $\\approx 12$ arc min is found. The centroid of the emission is co-located with the position of TeV J2032+4130.The energy spectrum of the emission coinciding with the position and extension of TeV J2032+4130 can be modeled by a power-law model with a photon index $\\Gamma=1.5\\pm0.2_\\mathrm{stat}\\pm0.3_\\mathrm{sys}$ and an energy flux integrated between 2 and 10 keV of $f_{2-10 \\mathrm{keV}} \\approx 7\\cdot 10^{-13}$ ergs/(cm$^2$ s) which is lower than the very high energy gamma-ray flux observed from TeV J2032+4130. We conclude that the faint extended X-ray emission discovered in this observation is the X-ray counterpart of TeV J2032+4130. Formally, it can not be excluded that the extended emission is due to an unrelated population of faint, hot ($k_BT\\approx 10$ keV) unresolved point-sources which by chance coincides with the position and extension of TeV J2032+4130. We discuss our findings in the frame of both hadronic and leptonic gamma-ray production scenarios. ", "machine_abstract": "We report on XMM-Newton and Chandra X-ray Observatory (CXO) observations of the recently discovered high-energy gamma-ray source, TeV J2032+4131. The data show that this object is an active galactic nucleus with a power-law spectrum extending to at least 100 keV. We find no evidence for absorption by intervening material in excess of Galactic values along its line-of-sight. A comparison between our results and those obtained using other instruments suggests that there may be significant variability in both the flux density and spectral index of TeV J2032 + 4131 over timescales as short as one day. This would imply either rapid changes in intrinsic emission or strong Doppler boosting effects due to relativistic motion of the emitting region.     Keywords: Active galactic nuclei, Gamma rays, Variability, X-rays, High energy astrophysics     1. Introduction     In recent years, several new classes of high energy sources have been identified through their detection at very-high energies (E > 10 GeV). These include blazars, radio galaxies, pulsar wind nebulae, supernova remnants, starburst galaxies, galaxy clusters, and possibly even some nearby stars [1] . However, many of these objects are still poorly understood because they lack counterparts at lower frequencies where most of the relevant physical processes occur [2] .   In particular, it has proven difficult to identify the origin of the highest energy photons detected so far [3] , which can reach energies up to 1020 eV [4] . One possible explanation is that such photons are produced during interactions involving extremely energetic particles accelerated within compact regions close to supermassive black holes [5] . Alternatively, they could result from decays of neutral pions created when cosmic ray protons interact with ambient matter [6] . If confirmed, such events would provide important insights into particle acceleration mechanisms near black holes [7, 8] .     Recently, the HESS collaboration reported the discovery of a bright point-like gammaray source located at RA = 20 h 32 m 41 s \u00b1 5\u2032\u2032 and Dec = +39\u00b030\u203200", "paraphrased_abstract": "After the discovery of the HESS collaboration, a new high-energy gamma-ray source, located at RA=20h 32m 41s  5, and Dec=+39\u00b030\u203200, a gamma-ray source with a power of about 100 kHz, has been discovered. Its origins are not known for a long time, and its origins are not known at the lower frequencies, where most of the physical processes are still in progress. Recently, we have reported that a gamma-ray source, a light-weighted gamma-ray, has been found in the XMM-Newton and Chandra CXO, with a spectral flux of about 100 kHz, in which we find no evidence for absorption, but that it has been created by a pair of cosmic rays and is therefore not bound to any known origin. We also discuss the XMM-Newton and Chandra CXO observations of the new high-energy gamma-ray, a gamma-ray of high energy, X-ray, with the result that it is a high-energy gamma-ray with a power-law of 100 kHz, with a spectral polarity of a hundred kHz", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3396, "title": "Lifetime Improvement of Wireless Sensor Networks by Collaborative Beamforming and Cooperative Transmission", "abstract": "  Extending network lifetime of battery-operated devices is a key design issue that allows uninterrupted information exchange among distributive nodes in wireless sensor networks. Collaborative beamforming (CB) and cooperative transmission (CT) have recently emerged as new communication techniques that enable and leverage effective resource sharing among collaborative/cooperative nodes. In this paper, we seek to maximize the lifetime of sensor networks by using the new idea that closely located nodes can use CB/CT to reduce the load or even avoid packet forwarding requests to nodes that have critical battery life. First, we study the effectiveness of CB/CT to improve the signal strength at a faraway destination using energy in nearby nodes. Then, a 2D disk case is analyzed to assess the resulting performance improvement. For general networks, if information-generation rates are fixed, the new routing problem is formulated as a linear programming problem; otherwise, the cost for routing is dynamically adjusted according to the amount of energy remaining and the effectiveness of CB/CT. From the analysis and simulation results, it is seen that the proposed schemes can improve the lifetime by about 90% in the 2D disk network and by about 10% in the general networks, compared to existing schemes. ", "machine_abstract": "In this work, we propose an energy-efficient cooperative transmission scheme for wireless sensor networks (WSNs). The proposed scheme is based on the combination of collaborative beamforming at the source node with cooperative transmission to multiple relay nodes in order to improve lifetime performance. In particular, the source node first transmits data packets using its maximum power level. Then, it switches to lower power levels if no packet has been successfully received within a certain time period. If one or more packets are successfully decoded, then the source node increases its transmit power back up to the previous value. We derive closed-form expressions for outage probability as well as average throughput under Rayleigh fading channels. Our results show that our proposed scheme can significantly increase network lifetime compared to conventional schemes without compromising system reliability. Finally, numerical examples are provided to verify the theoretical analysis. Keywords: Lifetime improvement; Relay selection; Energy efficiency; Outage probability", "paraphrased_abstract": "But the problem is not yet solved, and there is nothing to do. In this study, we are presenting a method to increase the lifespan of a wireless sensor network. Our method is based on the concept of cooperation among the nodes and the transmission of data through the network. We have chosen the first radiance to be the highest and the second radiance to be the lowest, and if no packet is received within a certain time, the second radiance is reduced to the lowest radiance. Then we develop the model of radiance and fading, comparing it with the others. In this way we calculate the number of radiance and the average throughput of the system under Rayleighfading channels. We analyzed the system and showed that it could be significantly improved compared with conventional methods without compromising the system reliability. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0598, "title": "Evolutionary Neural Gas (ENG): A Model of Self Organizing Network from Input Categorization", "abstract": "  Despite their claimed biological plausibility, most self organizing networks have strict topological constraints and consequently they cannot take into account a wide range of external stimuli. Furthermore their evolution is conditioned by deterministic laws which often are not correlated with the structural parameters and the global status of the network, as it should happen in a real biological system. In nature the environmental inputs are noise affected and fuzzy. Which thing sets the problem to investigate the possibility of emergent behaviour in a not strictly constrained net and subjected to different inputs. It is here presented a new model of Evolutionary Neural Gas (ENG) with any topological constraints, trained by probabilistic laws depending on the local distortion errors and the network dimension. The network is considered as a population of nodes that coexist in an ecosystem sharing local and global resources. Those particular features allow the network to quickly adapt to the environment, according to its dimensions. The ENG model analysis shows that the net evolves as a scale-free graph, and justifies in a deeply physical sense- the term gas here used. ", "machine_abstract": "In this work, we propose an evolutionary neural gas model for input categorization and clustering in the context of self-organizing networks. The proposed algorithm is based on the concept that each neuron can be considered as a cluster center with its own weight vector. In addition to updating their weights according to the standard NG learning rule, neurons are also allowed to evolve by applying genetic operators such as crossover and mutation. We show through experiments conducted on benchmark data sets that our approach outperforms other state-of-the-art algorithms including SOMs, GNGs, and EBGs. Finally, we demonstrate how the proposed method can be used to solve real-world problems such as text classification and image segmentation. Keywords: Evolutionary Computation, Self-Organizing Networks, Clustering, Genetic Algorithms, Text Classification, Image Segmentation. 1 Introduction Self-organizing maps (SOMs), originally introduced by Kohonen [1] , have been widely applied in many fields ranging from pattern recognition [2] , speech processing [3] , computer vision [4] , bioinformatics [5] , etc.. However, one major drawback of traditional SOMs lies in the fact that they require users to specify the number of clusters beforehand [6] . To overcome this problem, several extensions of SOMs were developed [7, 8] . Among these extensions, growing neural gas (GNG) [9] has attracted much attention due to its ability to automatically determine the optimal number of clusters during training [10] . Nevertheless, it should be noted that most existing models of self organizing network suffer from two main limitations. First, all nodes in the network share the same set of parameters which makes them unable to capture different characteristics of various categories [11] . Second, there lacks any mechanism to prevent overfitting when dealing with high-dimensional data [12] .", "paraphrased_abstract": ", which is to say, because the network is inherently arranged. There are also some problems to be overcome. The first is that all the nodes in the network share the same parameter, so that they cannot capture the different properties of various categories, and secondly that there is no way of stopping overfitting when dealing with high-dimensional data. In the present work, we propose a new neural network model, which is based on the principle that each neuron can be considered as a center with its own weight vector, and in addition to restraining the standard learning rules, neurons are also able to evolve by combining genetic operators. The proposed method is applied to the corresponding benchmark data. We show how our approach outperforms other neural networks, including SOMs, GNGs and EBGs. We show through experiments that our method outperforms the other systems of the world, including linguistic classification, image classification and classification. In our study, we introduce an evolutionary neural network for input classification and clustering. The main advantage of this network is that it does not need to be specified for every node. This problem, however, is very complicated. For example, the clustering of each neuron is made up of a corresponding entropy of a neural sphere, and it is possible to generate neurons by changing the weights of each neuron, and by ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1575, "title": "QCD Approach to B->D \\pi Decays and CP Violation", "abstract": "  The branching ratios and CP violations of the $B\\to D\\pi$ decays, including both the color-allowed and the color-suppressed modes, are investigated in detail within QCD framework by considering all diagrams which lead to three effective currents of two quarks. An intrinsic mass scale as a dynamical gluon mass is introduced to treat the infrared divergence caused by the soft collinear approximation in the endpoint regions, and the Cutkosky rule is adopted to deal with a physical-region singularity of the on mass-shell quark propagators. When the dynamical gluon mass $\\mu_g$ is regarded as a universal scale, it is extracted to be around $\\mu_g = 440$ MeV from one of the well-measured $B\\to D\\pi$ decay modes. The resulting predictions for all branching ratios are in agreement with the current experimental measurements. As these decays have no penguin contributions, there are no direct $CP$ asymmetries. Due to interference between the Cabibbo-suppressed and the Cabibbo-favored amplitudes, mixing-induced CP violations are predicted in the $B\\to D^{\\pm}\\pi^{\\mp}$ decays to be consistent with the experimental data at 1-$\\sigma$ level. More precise measurements will be helpful to extract weak angle $2\\beta+\\gamma$. ", "machine_abstract": "We present the results on the decay amplitudes for B -> D pi decays in the framework of QCD factorization approach, including both perturbative corrections at one-loop level and nonperturbative contributions from light-cone distribution amplitudes (LCDAs) of pion and kaon. We find that the branching ratios are sensitive to the values of LCDAs parameters. The direct CP asymmetries depend strongly on the relative phase between penguin and tree diagrams. Our numerical results show that the theoretical predictions agree well with experimental data within errors.     Introduction     In recent years there has been great progresses made in understanding heavy flavor physics through experiments performed by BaBar [1] , Belle [2] , CLEO [3] and LHCb [4] Collaborations. Many interesting phenomena have been observed such as large CP violation effects in B mesons [5] . It is therefore important to study these processes theoretically so as to understand their underlying dynamics better. Among them, B \u2192 DPi decays play an essential role since they can be used to extract information about CKM matrix elements Vub and Vtd [6] .     In this work we will calculate the decay amplitudes for B\u2192DPi decays using the method of QCD factorization [7, 8] which was first proposed by Beneke et al [9] . This method takes into account all possible Feynman graphs contributing to the process under consideration and then separates out soft divergences appearing in loop integrals into universal functions called light cone distribution amplitudes(LCDA). These LCDA's contain only non-perturbative information and can be calculated either by lattice simulations or extracted from experiment [10] . After separating out the soft divergences, hard scattering kernels containing collinear singularities remain and need to be evaluated order-by-order in perturbation theory [11] .  Theoretical Framework  B\u2192DPi Decay Amplitudes In the rest frame of B-meson, the differential decay widths for B\u2192DPi can be written as:", "paraphrased_abstract": "A CP violation of B-mesons has recently been discovered, and it is therefore necessary to study the dynamics of these processes in more detail, in order to better understand the underlying dynamics of the system. This work will focus on the decay amplitudes of B-meson - D-pi decays, based on the method of QCD, which was developed by Beneke and et al., it includes all possible Feynman graphs in the study of the process, separates out soft divergences in loop integrals into universal functions called Li-p. These Li-p are only non-persistent data, and can be calculated either by lattice or by experiment. The results are found to be convergent with experimental data in errors. We will discuss the ensuing QCD method, and we will examine the non-persistent correction of the one-loop system, and the non-persistent correction of the light-cone distribution amplitudes (LCDs) of pion and kaon. The inclination of the branches is sensitive to the value of the LCDA. The branching ratios are sensitive to the values of the LCDA. The direct CP errors are strongly affected by the relative phase of the penguin and the tree. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3453, "title": "An Adaptive Strategy for the Classification of G-Protein Coupled Receptors", "abstract": "  One of the major problems in computational biology is the inability of existing classification models to incorporate expanding and new domain knowledge. This problem of static classification models is addressed in this paper by the introduction of incremental learning for problems in bioinformatics. Many machine learning tools have been applied to this problem using static machine learning structures such as neural networks or support vector machines that are unable to accommodate new information into their existing models. We utilize the fuzzy ARTMAP as an alternate machine learning system that has the ability of incrementally learning new data as it becomes available. The fuzzy ARTMAP is found to be comparable to many of the widespread machine learning systems. The use of an evolutionary strategy in the selection and combination of individual classifiers into an ensemble system, coupled with the incremental learning ability of the fuzzy ARTMAP is proven to be suitable as a pattern classifier. The algorithm presented is tested using data from the G-Coupled Protein Receptors Database and shows good accuracy of 83%. The system presented is also generally applicable, and can be used in problems in genomics and proteomics. ", "machine_abstract": "G-protein-coupled receptors (GPCRs) are one of the largest families of cell surface proteins and play an important role in many physiological processes, including vision, olfaction, taste, hormone secretion, neurotransmission, immune response, inflammation, and cancer.  The number of known human GPCR sequences has increased dramatically over recent years due to advances in genome sequencing technologies; however, only about half have been functionally characterized so far. In this study we present a novel computational method that can be used to predict the functional class of uncharacterized GPCRs based on their sequence similarity with other members within each subfamily. Our approach is based on two key ideas: 1) We use a new adaptive strategy to select representative sequences from different subfamilies by considering both the diversity among them as well as their evolutionary relationships; 2) We develop a new scoring scheme which takes into account not only the pairwise sequence similarities but also the structural information between query and template sequences. Using our method, we were able to successfully classify more than 80% of all human GPCRs whose functions had previously been experimentally determined.", "paraphrased_abstract": "In the past two years, the number of human GPCRs has been dramatically increased by the advances in genome sequencing technology, but only half have been successfully described in functional detail. In this paper, we present a new approach for the identification of representatives of different subfamilies, by considering the diversity of the members and their evolutionary relationships; and, secondly, we develop a new scoring system, based on the structural information of the recurrence and recombination of the genes in the corresponding subfamilies. The number of human GPCRs has grown significantly in recent years, in spite of the advances in sequencing technology, but, as a result, only about half of them have been characterized in functional terms. We have developed a new method, using two principal principles: 1) We use a new adaptive strategy, to select representative genes from different subfamilies, analyzing the diversity of their sequences and the evolutionary relationship of their recombinations; 2) We devise a new scoring system that takes into account not only the sequence similarities but also the structural information of the sequences between query and template. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1729, "title": "The Sedentary Survey of Extreme High Energy Peaked BL Lacs III. Results from Optical Spectroscopy", "abstract": "  The multi-frequency Sedentary Survey is a flux limited, statistically well-defined sample of highly X-ray dominated BL Lacertae objects (HBLs) which includes 150 sources. In this paper, the third of the series, we report the results of a dedicated optical spectroscopy campaign that, together with results from other independent optical follow up programs, led to the spectroscopic identification of all sources in the sample. We carried out a systematic spectroscopic campaign for the observation of all unidentified objects of the sample using the ESO 3.6m, the KPNO 4m, and the TNG optical telescopes. We present new identifications and optical spectra for 76 sources, 50 of which are new BL Lac objects, 18 are sources previously referred as BL Lacs but for which no redshift information was available, and 8 are broad emission lines AGNs. We find that the multi-frequency selection technique used to build the survey is highly efficient (about 90%) in selecting BL Lacs objects. We present positional and spectroscopic information for all confirmed BL Lac objects. Our data allowed us to determined 36 redshifts out of the 50 new BL Lacs and 5 new redshifts for the previously known objects. The redshift distribution of the complete sample is presented and compared with that of other BL Lacs samples. For 26 sources without recognizable absorption features, we calculated lower limits to the redshift using a method based on simulated optical spectra with different ratios between jet and galaxy emission. For a subsample of 38 object with high-quality spectra, we find a correlation between the optical spectral slope, the 1.4 GHz radio luminosity, and the Ca H&K break value, indicating that for powerful/beamed sources the optical light is dominated by the non-thermal emission from the jet. ", "machine_abstract": "We present optical spectroscopic observations for the sample of 14 extreme high energy peaked BL Lac objects (EHBLs) selected by Costamante et al. (2013) . The main goal is to study their host galaxy properties and investigate possible differences with respect to lower-energy blazars, which are known to be hosted in elliptical galaxies. We find that all EHBLs have redshifts between 0.1 and 1.0, consistent with previous results on this class of sources. All but one source show evidence of being hosted in spiral or irregular galaxies; only PKS 0537-441 shows an elliptical-like spectrum. This result suggests that there may not exist any significant difference in the hosts of low-and high-energy blazars as previously claimed. However, we note that our sample size is small and further studies will be needed before drawing firm conclusions.     Keywords: Blazar, Host Galaxy", "paraphrased_abstract": "It is known that the low-energy blazars are residing in spiral or irregular galaxies, and PKS 0537-441 is a elliptical spectra. Hence, it may be a case of no difference between the hosts of low-energy blazars and high-energy blazars, as the former has been claiming. However, we are limited in our sample, and it will be necessary to take further measurements before we reach our conclusions. We study the host galaxy and its kinship with low-energy blazars, and we examine the possible differences in host galaxy properties, especially with regard to low-energy blazars, which are known to be in elliptic galaxies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.305, "title": "Optical BVI Imaging and HI Synthesis Observations of the Dwarf Irregular Galaxy ESO 364-G 029", "abstract": "  As part of an effort to enlarge the number of well-studied Magellanic-type galaxies, we obtained broadband optical imaging and neutral hydrogen radio synthesis observations of the dwarf irregular galaxy ESO 364-G 029. The optical morphology characteristically shows a bar-like main body with a one-sided spiral arm, an approximately exponential light distribution, and offset photometric and kinematic centers. The HI distribution is mildly asymmetric and, although slightly offset from the photometric center, roughly follows the optical brightness distribution, extending to over 1.2 Holmberg radii (where mu_B = 26.5 mag/arcsec^2). In particular, the highest HI column densities closely follow the bar, one-arm spiral, and a third optical extension. The rotation is solid-body in the inner parts but flattens outside of the optical extent. The total HI flux F_HI = 23.1 pm 1.2 Jy km/s, yielding a total HI mass M_HI= (6.4 pm 1.7) x 10^8 Msun (for a distance D = 10.8 pm 1.4 Mpc) and a total HI mass-to-blue-luminosity ratio M_HI/L_B = (0.96 pm 0.14) Msun / Lsun,B (distance independent). The HI data suggest a very complex small-scale HI structure, with evidence of large shells and/or holes, but deeper observations are required for a detailed study. Follow-up observations are also desirable for a proper comparison with the Large Magellanic Cloud, where despite an optical morphology very similar to ESO 364-G 029 the HI bears little resemblance to the optical. ", "machine_abstract": "We present optical BVRI imaging, near-infrared JHKs photometry, and radio continuum observations at 1.4 GHz for the dwarf irregular galaxy ESO 364-G 029 (UGC 6456). The new data are combined with existing H\u03b1 spectroscopy to study its star formation history over the past few hundred million years. We find that this galaxy has experienced several bursts of intense star formation in recent times, which have produced large amounts of ionized gas visible as bright knots of emission across most of the face-on disk. These knots appear to be associated with young massive stars formed during each episode of star formation. In addition, we detect an extended component of diffuse ionized gas surrounding these knots. This is likely due to photoionization by hot evolved stars or supernovae remnants. Using our deepest images taken under good seeing conditions, we measure a total stellar mass of M = 2.1 \u00d7 10^7 M_sol within a radius of 5 kpc.", "paraphrased_abstract": "Then, using the deepest observations, and the highest light conditions, we measure a total mass of 2.1  107 M_sol. In the middle of this radius, we find a large, bright, crystalline mass, with a thickness of more than five kpc. We present a detailed analysis of the forming history of this galaxy. During the past hundred million years, a large number of ionized gases have accumulated on the disk of this galaxy, visible in many, but very small, knots of ionization, emitted in a large, sharp, and symmetrical way, with ionization of the innermost part, by the hot atoms of the stars, and by the supernovae. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3572, "title": "The subpulse modulation properties of pulsars at 92 cm and the frequency dependence of subpulse modulation", "abstract": "  A large sample of pulsars has been observed to study their subpulse modulation at an observing wavelength (when achievable) of both 21 and 92 cm using the Westerbork Synthesis Radio Telescope. In this paper we present the 92-cm data and a comparison is made with the already published 21-cm results. We analysed 191 pulsars at 92 cm using fluctuation spectra. The sample of pulsars is as unbiased as possible towards any particular pulsar characteristics. For 15 pulsars drifting subpulses are discovered for the first time and 26 of the new drifters found in the 21-cm data are confirmed. We discovered nulling for 8 sources and 8 pulsars are found to intermittently emit single pulses that have pulse energies similar to giant pulses. It is estimated that at least half of the total population of pulsars have drifting subpulses when observations with a high enough signal-to-noise ratio would be available. It could well be that the drifting subpulse mechanism is an intrinsic property of the emission mechanism itself, although for some pulsars it is difficult or impossible to detect. Drifting subpulses are in general found at both frequencies, although the chance of detecting drifting subpulses is possibly slightly higher at 92 cm. It appears that the youngest pulsars have the most disordered subpulses and the subpulses become more and more organized into drifting subpulses as the pulsar ages. The correlations with the modulation indices are argued to be consistent with the picture in which the radio emission can be divided in a drifting subpulse signal plus a quasi-steady signal which becomes, on average, stronger at high observing frequencies. The measured values of P3 at the two frequencies are highly correlated, but there is no evidence for a correlation with other pulsar parameters. ", "machine_abstract": "We have studied the pulse-to-pulse intensity variations in two pulsars, PSR B0826-34 and PSR B1133+16, using data obtained with the Parkes radio telescope at 1.4 GHz (92cm) and 2.7GHz(36cm). We find that both pulsars show strong evidence for subpulse drift bands which are modulated by an underlying pattern of weaker emission. The observed patterns can be explained as due to interference between different modes of emission within each beam. In addition we find that there is no significant difference in the subpulse modulation properties when comparing observations made at 36cm and those made at 92cm. This suggests that the physical processes responsible for producing these phenomena operate over a wide range of frequencies. These results provide further support for the idea that the phenomenon known as drifting subpulses may arise through some form of plasma instability operating on the open field lines near the magnetic poles of neutron stars.     Keywords: Pulsar", "paraphrased_abstract": "Among these observations, we are able to demonstrate that the phenomenon of the pulsating subpulse is a result of a kind of asymmetrical plasma instability which is observed near the magnetic poles of neutron stars. These results support the hypothesis that the phenomena of the pulsating subpulse are the result of some sort of plasma instability that operates on the open field lines of the neutron stars. The pulsars PSR B0826-34 and PSR B1133+16 are observed with the Parkes radio telescope at 1.4 GHz and 2.7 GHz, respectively. We see that the subpulsars are of high intensity, and the amplitude is influenced by an underlying, weaker pulsation, which can be explained by the interstices of the different spectral modes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2877, "title": "Origin of which-way information and generalization of the Born rule", "abstract": "  The possibility to recover the which-way information, for example in the two slit experiment, is based on a natural but implicit assumption about the position of a particle {\\it before} a position measurement is performed on it. This assumption cannot be deduced from the standard postulates of quantum mechanics. In the present paper this assumption is made explicit and formally postulated as a new rule, the {\\it quantum typicality rule}. This rule correlates the positions of the particles at two different times, thus defining their trajectories. Unexpectedly, this rule is also equivalent to the Born rule with regard to the explanation of the results of statistical experiments. For this reason it can be considered a generalization of the Born rule. The existence of the quantum typicality rule strongly suggests the possibility of a new trajectory-based formulation of quantum mechanics. According to this new formulation, a closed quantum system is represented as a {\\it quantum process}, which corresponds to a canonical stochastic process in which the probability measure is replaced by the wave function and the usual frequentist interpretation of probability is replaced by the quantum typicality rule. ", "machine_abstract": "We present an argument for why quantum mechanics is nonlocal, based on the fact that it allows one to predict with certainty whether or not a measurement will be made in any given experiment. We show how this can lead to superluminal communication between two parties sharing entangled particles by using only local operations and classical communication (LOCC). Finally we generalize the Born rule to allow for arbitrary measurements instead of just von Neumann ones. The usual formulation of quantum mechanics assumes that all experiments are performed under ideal conditions where no errors occur during the preparation of states or the execution of measurements. However, in practice there always exist some experimental imperfections such as decoherence due to environmental noise, imprecision in state preparations, and inaccuracy in measurements. In order to account for these effects, several approaches have been proposed including stochastic Schr\u00f6dinger equations [1] , open systems [2] , and generalized probabilistic theories [3] . Here we consider another approach known as Quantum Bayesianism [4] . In Quantum Bayesianism, the wave function is regarded as representing our knowledge about the system rather than describing its physical properties. This means that when performing a measurement, the outcome is determined by updating our knowledge according to Bayes' theorem [5] . For example, if Alice performs a measurement of spin along the x-axis on her particle, she would update her knowledge accordingly depending on what value was obtained [6] . If Bob also measures his particle's spin along the same axis but obtains different results, then he must perform a new measurement since his knowledge has changed [7, 8] .", "paraphrased_abstract": "The standard approach to quantum mechanics consists in assuming that all experiments are conducted in ideal conditions, in which no error is possible in the preparation of states or the measurements. In order to account for these ill-suited effects, several methods have been developed, such as the stochastic Schr\u00f6dinger equation, the open systems, and the generalized probabilistic theories. We have here a different approach, Quantum Bayesianism, which consists in the notion that the wave function is our knowledge of the system rather than of its physical properties. For instance, if Alice, for example, is measuring the spin of her particle along the x-axis, she will update her knowledge according to the values that it produces. If Bob measures the spin of his particle along the same axis, then he will also measure the spin of his own, for his knowledge has changed. We illustrate that this is the way to make superluminal communication possible between two parties sharing entangled particles. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0094, "title": "Timing and Lensing of the Colliding Bullet Clusters: barely enough time and gravity to accelerate the bullet", "abstract": "  We present semi-analytical constraint on the amount of dark matter in the merging bullet galaxy cluster using the classical Local Group timing arguments. We consider particle orbits in potential models which fit the lensing data. {\\it Marginally consistent} CDM models in Newtonian gravity are found with a total mass M_{CDM} = 1 x 10^{15}Msun of Cold DM: the bullet subhalo can move with V_{DM}=3000km/s, and the \"bullet\" X-ray gas can move with V_{gas}=4200km/s. These are nearly the {\\it maximum speeds} that are accelerable by the gravity of two truncated CDM halos in a Hubble time even without the ram pressure. Consistency breaks down if one adopts higher end of the error bars for the bullet gas speed (5000-5400km/s), and the bullet gas would not be bound by the sub-cluster halo for the Hubble time. Models with V_{DM}~ 4500km/s ~ V_{gas} would invoke unrealistic large amount M_{CDM}=7x 10^{15}Msun of CDM for a cluster containing only ~ 10^{14}Msun of gas. Our results are generalisable beyond General Relativity, e.g., a speed of $4500\\kms$ is easily obtained in the relativistic MONDian lensing model of Angus et al. (2007). However, MONDian model with little hot dark matter $M_{HDM} \\le 0.6\\times 10^{15}\\msun$ and CDM model with a small halo mass $\\le 1\\times 10^{15}\\msun$ are barely consistent with lensing and velocity data. ", "machine_abstract": "We present new results on the timing and lensing properties of two colliding galaxy clusters, Abell 520 (z = 0.2) and Abell 521 (z = 0.3). We find that the collision is occurring at an angle of ~45 degrees with respect to our line-of-sight. The mass distribution in each cluster has been determined by fitting parametric models to strong gravitational lensing data obtained using Hubble Space Telescope images. Using these models we have calculated the expected arrival times for multiply-imaged background galaxies as well as their magnifications. These predictions are compared directly against observations made using the Advanced Camera for Surveys aboard HST. In addition, we use the observed positions of multiple images of lensed quasars behind both clusters to measure the relative velocity between them. Our analysis shows that there is only marginally more than sufficient time available before the collision occurs for the dark matter halos to be accelerated up to the current velocities inferred from the lensing measurements.", "paraphrased_abstract": "There is only some time before the collision for dark matter to accelerate to the speed of which it is inferred by the telescope. The mass distribution in each cluster has been analyzed by fitting a set of parametric models to the skewed spectra of the Hubble Space Telescope. The predicted arrival times and the magnifications are calculated. In addition, we compared the positions of several images of quasars behind the two clusters, and compared them to the observations made by the Advanced Camera for Surveys aboard HST. The mass distribution of the two clusters, Abell 520, Abell 521, is shown to be about 45 degrees. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1991, "title": "Antiproliferative MCR peptides block physical interaction of insulin with retinoblastoma protein (RB) in human lung cancer cells", "abstract": "  Fifteen years ago, a structural analysis of the hormone insulin and the retinoblastoma tumor suppressor protein (RB) revealed that they may physically interact with one another. Subsequently, an RB peptide corresponding to the proposed RB binding site for insulin was found to recognize full-length insulin in vitro. As part of efforts aimed at developing this RB peptide into an anti-cancer drug, this molecule was chemically coupled to a cellular internalization signal and termed \"MCR peptide\". Meanwhile, several such MCR peptide variants have been demonstrated to restrain the proliferation of different human cancer cells in vitro and in vivo. Moreover, one of the MCR peptides coined MCR-10 was shown to be capable of interfering with the complex formation between insulin and RB in HepG2 human hepatoma cells, as monitored by immunofluorescence. This latter result indicating an in vivo association between insulin and RB was confirmed by a follow-up study combining the methods of co-immunoprecipitation and immunoblotting. Here, we provide evidence for the existence of the insulin-RB complex in A549 human non-small cell lung cancer cells. Specifically, we demonstrate this heterodimer by means of a magnetic beads-based immunoprecipitation approach and equally show that this dimer can be disrupted by MCR-4 or MCR-10 each of which is known to possess antiproliferative properties, yet to a much lesser extent by a control peptide. Thus, this investigation has yielded another important proof for the occurrence of the insulin-RB dimer and, furthermore, its validity as a target for antineoplastic MCR peptides. ", "machine_abstract": "The aim of this study was to investigate the effect of novel antimicrobial cyclic peptides, named microbe-derived cationic ring structures (MCRs), on cell proliferation and apoptosis induction in A549 non-small-cell lung carcinoma cells. The results showed that treatment with MCR1 or MCR3 significantly inhibited cell growth by inducing G0/G1-phase arrest and apoptosis through activation of caspase-3/7/9 signaling pathways. In addition, we found that both MCR1 and MCR3 suppressed expression levels of cyclins D1 and E as well as CDK4/6 proteins but increased p21WAF1/cip1 level. Furthermore, our data indicated that MCR1 and MRC3 blocked the binding between insulin-like growth factor 1 receptor (IGF-1R) and retinoblastoma tumor suppressor protein (RB). These findings suggest that MCR1 and 3 may be potential therapeutic agents for treating lung cancers.", "paraphrased_abstract": "And it was found that MCR1 and MCR3 blocked the bind between insulin-like growth factor-1 (IGF-1) and retinoblastoma tumor suppressor proteins. Thus, these results suggest that the effect of MCR1 and MCR3 on cell growth and apoptosis was in a profoundly positive direction. This study was aimed at investigating the effect of new antibacterial cationic ring structures, called cationic ring structures, on cell growth and apoptosis in A549 non-small-cell lung cancer. In addition, we found that MCR1 and MCR3 suppressed the expression of D1 and E, and the expression of CDK4/6 and p21WAF1; but the expression of p21WAF1 and cip1 was increased. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1562, "title": "Galaxy evolution in the infra-red: comparison of a hierarchical galaxy formation model with SPITZER data", "abstract": "  We present predictions for the evolution of the galaxy luminosity function, number counts and redshift distributions in the IR based on the Lambda-CDM cosmological model. We use the combined GALFORM semi-analytical galaxy formation model and GRASIL spectrophotometric code to compute galaxy SEDs including the reprocessing of radiation by dust. The model, which is the same as that in Baugh et al (2005), assumes two different IMFs: a normal solar neighbourhood IMF for quiescent star formation in disks, and a very top-heavy IMF in starbursts triggered by galaxy mergers. We have shown previously that the top-heavy IMF seems to be necessary to explain the number counts of faint sub-mm galaxies. We compare the model with observational data from the SPITZER Space Telescope, with the model parameters fixed at values chosen before SPITZER data became available. We find that the model matches the observed evolution in the IR remarkably well over the whole range of wavelengths probed by SPITZER. In particular, the SPITZER data show that there is strong evolution in the mid-IR galaxy luminosity function over the redshift range z ~ 0-2, and this is reproduced by our model without requiring any adjustment of parameters. On the other hand, a model with a normal IMF in starbursts predicts far too little evolution in the mid-IR luminosity function, and is therefore excluded. ", "machine_abstract": "We present results on the infrared (IR) properties of galaxies obtained by comparing predictions made using a semi-analytic galaxy formation model to observations taken as part of the Spitzer Infrared Nearby Galaxies Survey (SINGS). We find that our model reproduces many observed trends, including the IR luminosity function and the correlation between star formation rate and dust temperature. However, we also identify some discrepancies which suggest that further work is needed before this type of modelling can be used for detailed studies of individual objects or small samples. The main conclusions are summarised below:-The predicted number counts at 24 microns agree well with those measured by Spitzer/MIPS over most of the range probed by SINGS. -Our model predicts too few low-luminosity galaxies compared to the observations; however, these systems may not have been detected due to their extremely faint flux levels. -The predicted distribution of dust temperatures agrees reasonably well with the observations although there appears to be an excess of cold dusty galaxies in the simulations relative to what is seen in the data. -The predicted relationship between far-infrared colour and total infrared luminosity agrees fairly well with the observations but shows signs of being slightly steeper than suggested by the data.", "paraphrased_abstract": "I will say this, for now, that the data presented here are of a very large size, and are to be taken as a whole. In this way, the estimates of the number of galaxies in the infrared spectrum agree fairly well with the measurements, but there are some anomalies in the measurements which suggest that there is still much more work needed in the detailed analysis of individual objects or small samples. In addition, we present a comparison of the number of luminous galaxies in the infrared, with the observations obtained by Spitzer/MIPS in the broader range probed by SINGS. The predicted number of luminous galaxies in the 24 micron range agree fairly well with those of the observations, though they may not be detected due to the extremely low flux. We also present an illustrious comparison of luminous galaxies in the infrared and luminous in the infrared with the observations from the Spitzer/MIPS. Our model predicts too few galaxies of low luminosity compared with the observations, though there are, it seems, some galaxies that are a little colder than those that we observe in the data. The IR luminosity is estimated fairly accurately by the model, but the temperature is too low. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1857, "title": "What is the order of 2D polymer escape transition?", "abstract": "  An end-grafted flexible polymer chain in 3d space between two pistons undergoes an abrupt transition from a confined coil to a flower-like conformation when the number of monomers in the chain, N, reaches a critical value. In 2d geometry, excluded volume interactions between monomers of a chain confined inside a strip of finite length 2L transform the coil conformation into a linear string of blobs. However, the blob picture raises questions on the nature of this escape transition. To check the theoretical predictions based on the blob picture we study 2d single polymer chains with excluded volume interactions and with one end grafted in the middle of a strip of length   2L and width H by simulating self-avoiding walks on a square lattice with the pruned-enriched-Rosenbluth method (PERM). We estimate the free energy, the end-to-end distance, the number of imprisoned monomers, the order parameter, and its distribution. It is shown that in the thermodynamic limit of large N and L but finite L/N, there is a small but finite jump in several average characteristics, including the order parameter. We also present a theoretical description based on the Landau free energy approach, which is in good agreement with the simulation results. Both simulation results and the analytical theory indicate that the 2d escape transition is a weak first-order phase transition. ", "machine_abstract": "We study the escape dynamics of an elastic string in two dimensions confined by a potential well with periodic boundary conditions and subject to thermal noise. We find that there are three different regimes for the escape process depending on the depth of the confining potential, V0.  For small values of V0 (V0 < Vc), we observe a single exponential decay of the survival probability as expected for a first-order phase transition. In this regime, the mean lifetime decreases exponentially with increasing temperature T . For intermediate values of V0 ( Vc < V0 < Vs ), we observe a double-exponential decay of the survival probability indicating a second-order phase transition. The mean lifetime increases linearly with decreasing temperature below Tc , where Tc is the critical temperature at which the system undergoes a continuous phase transition into the metastable state. Finally, for large values of V0 (Vs < V0) no escape occurs within our simulation time window.", "paraphrased_abstract": "We have measured the escape dynamics of an elastic string in two dimensions, which is confined by a potential well, a constant boundary condition and subject to thermal noise. For small values of V0, V0 and Vs, we observe a single exponential decay of the survival probability, a first-order phase transition, and for intermediate values of V0, double-exponential decay of the survival probability, a second-order phase transition. The mean lifetime is exponentially decreasing with decreasing temperature, and below the critical temperature of Tc, the critical temperature at which the system undergoes a continuous transition into a stable state. Then, for large amounts of V0, Vs, no escape is observed in our simulation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1462, "title": "MiniBooNE Results and Neutrino Schemes with 2 sterile Neutrinos: Possible Mass Orderings and Observables related to Neutrino Masses", "abstract": "  The MiniBooNE and LSND experiments are compatible with each other when two sterile neutrinos are added to the three active ones. In this case there are eight possible mass orderings. In two of them both sterile neutrinos are heavier than the three active ones. In the next two scenarios both sterile neutrinos are lighter than the three active ones. The remaining four scenarios have one sterile neutrino heavier and another lighter than the three active ones. We analyze all scenarios with respect to their predictions for mass-related observables. These are the sum of neutrino masses as constrained by cosmological observations, the kinematic mass parameter as measurable in the KATRIN experiment, and the effective mass governing neutrinoless double beta decay. It is investigated how these non-oscillation probes can distinguish between the eight scenarios. Six of the eight possible mass orderings predict positive signals in the KATRIN and future neutrinoless double beta decay experiments. We also remark on scenarios with three sterile neutrinos. In addition we make some comments on the possibility of using decays of high energy astrophysical neutrinos to discriminate between the mass orderings in presence of two sterile neutrinos. ", "machine_abstract": "We present results on the MiniBooNE experiment, which has recently reported an excess in electron neutrino appearance events at low energies ($E < 100 MeV$). We show that this excess can be explained by adding two light sterile neutrinos to the Standard Model. The new states are required to have masses below $2\\times 10^{-3}$ eV/c2 and mix strongly into the active neutrino flavors. In addition we find that there is no need for additional CP violation beyond what is already predicted within the Standard Model. This work was supported by the U.S.     Department of Energy under Contract No. DE-AC02-05CH11231. We thank J. A. Aguilar-Arevalo, S. Babu, D. Blum, C. Boehm, R. Brandt, P. Cardarelli, E. Cholis, T. Creminelli, G. Danilov, K. Enberg, H. Eriksson, L. F. Ferreira, W. Galsterer, V. Grigoriev, N. Gupta, Y. Hanagaki, A. Iwamoto, A. Khodjamirian, A. Kravchuk, A. Lehtinen, A. Marchesini, A. Melnikov, A. Mironov, A. Murayama, A. Oganessian, A. Ostapchenko, A. Petrovich, A. Polukhin, A. Popov, A. Pruneda, A. Reece, A. Ryabtsev, A. Shabalina, A. Smirnov, A. Stepanenko, A. Strumia, A. Tanaka, A. Trubnikov, A. Tsang, A. Voloshin, A. Zatsepin, A. Zhavoronkov, A. Zhitnitsky, A. Zvezdaev, A. Zwickl, A. Zyuzikov, A. Zyuzaev, A.", "paraphrased_abstract": "\u201cThe study was funded by the U.S. Department of Energy under Contract No. DEAC02-03-110. We thank J. Aguilar-Arevalo, S. Babu, C. Boehm, R. Brandt, P. Cardarelli, E. Cholis, T. Creminelli, G. Danilov, K. Enberg, H. Eriksson, W. Galsterer, N. Gupta, N. Gupta, Y. Hanagaki, A. Iwamoto, A. Khodjamirian, A. Kravchuk, A. Marchesini, A. Mironov, A. Oganessian, A. Ostapchenko, A. Polukhin, A. Zatsepin, A. Zhavoronkov, A. Zhukov, A. Zwickl, A. Zyuzikov, A. Zyuzaev, A. Zyuzaev, A. We thank J. Aguilar-Arevalo, S. Babu, C. B., R. Brandt, P. Cardarelli, E. Cholis, T. Creminelli, K. Enberg, H. Eriksson, W. Galsterer, N. Grigor", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1379, "title": "On the generalization of the Costas property in the continuum", "abstract": "  We extend the definition of the Costas property to functions in the continuum, namely on intervals of the reals or the rationals, and argue that such functions can be used in the same applications as discrete Costas arrays. We construct Costas bijections in the real continuum within the class of piecewise continuously differentiable functions, but our attempts to construct a fractal-like Costas bijection there are successful only under slight but necessary deviations from the usual arithmetic laws. Furthermore, we are able, contingent on the validity of Artin's conjecture, to set up a limiting process according to which sequences of Welch Costas arrays converge to smooth Costas bijections over the reals. The situation over the rationals is different: there, we propose an algorithm of great generality and flexibility for the construction of a Costas fractal bijection. Its success, though, relies heavily on the enumerability of the rationals, and therefore it cannot be generalized over the reals in an obvious way. ", "machine_abstract": "We study the problem of finding an optimal set of points on the unit circle that are equidistant to each other and have minimum angular separation between them, known as the Costas array or Costas configuration.  We show how this problem can be formulated as a convex optimization problem with linear constraints over the space of probability measures supported by the unit circle. This formulation allows us to use tools from convex analysis for solving it efficiently. In particular we provide a polynomial time algorithm which computes an approximate solution within any desired accuracy. The proposed method is based on the Frank-Wolfe (FW) algorithm combined with the Sinkhorn-Knopp scaling technique. Finally, numerical experiments demonstrate the efficiency of our approach compared to existing methods. Keywords: Convex Optimization; Probability Measures; Frank Wolfe Algorithm; Scaling Technique; Unit Circle; Costas Array; Costas Configuration.", "paraphrased_abstract": "Hence, we present an inductive solution of the problem, based on the Frank Wolfe algorithm, on the Scaling Technique, a particular solution to the problem of the CostasArray or CostasConsort. This solution is based on the Frank Wolfe algorithm and on the Scaling Technique of the Unit Circle. We present the method for solving the problem of the CostasConsort with linear convex constraints. The proposed method is based on the Frank Wolfe algorithm, and the Sinkhorn-Knopp scaling technique. Then, numerical experiments prove the effectiveness of our method. We examine the problem of finding the optimal set of points on the unit circle, which are equilateral to each other and are of the least angular separation between them, so that they form the Costas Cone. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0978, "title": "X-ray Timing of PSR J1852+0040 in Kesteven 79: Evidence of Neutron Stars Weakly Magnetized at Birth", "abstract": "  The 105-ms X-ray pulsar J1852+0040 is the central compact object (CCO) in SNR Kes 79. We report a sensitive upper limit on its radio flux density of 12 uJy at 2 GHz using the NRAO GBT. Timing using XMM and Chandra over a 2.4 yr span reveals no significant change in its spin period. The 2 sigma upper limit on the period derivative leads, in the dipole spin-down formalism, to an energy loss rate E-dot < 7e33 ergs/s, surface magnetic field strength B_p < 1.5e11 G, and characteristic age tau_c = P/2P-dot > 8 Myr. This tau_c exceeds the age of the SNR by 3 orders of magnitude, implying that the pulsar was born spinning at its current period. However, the X-ray luminosity of PSR J1852+0040, L(bol) ~ 3e33(d/7.1 kpc)^2 ergs/s is a large fraction of E-dot, which challenges the rotation-powered assumption. Instead, its high blackbody temperature, 0.46+/-0.04 keV, small blackbody radius ~ 0.8 km, and large pulsed fraction, ~ 80%, may be evidence of accretion onto a polar cap, possibly from a fallback disk made of supernova debris. If B_p < 1e10 G, an accretion disk can penetrate the light cylinder and interact with the magnetosphere while resulting torques on the neutron star remain within the observed limits. A weak B-field is also inferred in another CCO, the 424-ms pulsar 1E 1207.4-5209, from its steady spin and soft X-ray absorption lines. We propose this origin of radio-quiet CCOs: the B-field, derived from a turbulent dynamo, is weaker if the NS is formed spinning slowly, which enables it to accrete SN debris. Accretion excludes neutron stars born with both B_p < 1e11 G and P > 0.1 s from radio pulsar surveys, where B_p < 1e11 G is not encountered except among very old (tau_c > 40 Myr) or recycled pulsars. Finally, such a CCO, if born in SN 1987A, could explain the non-detection of a pulsar there. ", "machine_abstract": "We report on the discovery and timing analysis of pulsar PSR J1852 + 0040, which is associated with supernova remnant (SNR) Kes 79. The pulsar has spin period P = 1.56 ms and characteristic age \u03c4c = 3 kyrs. We find that its surface magnetic field strength Bs = 2 \u00d7 10^10 G, assuming an inclination angle i = 60 degrees between the rotation axis and line-of-sight to Earth. This value is consistent with theoretical predictions for neutron stars born weakly magnetized. In addition we have detected X-ray pulsations from this source using Chandra observations taken during 2009-2011. These results are presented here along with our timing solution obtained over a span of eight years.     Keywords: Pulsar, Supernova Remnant, X-Ray Pulsars, Chandra Observatory, Radio Pulsar Timing     Introduction     A number of young radio pulsars show very low values of their surface dipole magnetic fields inferred from their spin-down rates. Such objects include Geminga, B1951+32, B1620-26, B1509-58, B0531+21, B1757-24, B1800-21, B1853+01, B1857+09, B1913+16, B1957+50, B2224+65, B2303+46, B2334+61, B0826-34, B1133+16, B1237+25, B1929+10, B1930+42, B1932+29, B1933+16, B1944+43, B1946+35, B1947+36, B1953+50, B1954+28, B1956+54, B1959+20, B1960+03, B1962+14, B1963+27, B1968+18, B1969+22, B1970+38, B1971+02, B1973+51, B1974+14, B1975+28, B1976+44, B1977+47, B1980+12, B1981+24, B1983", "paraphrased_abstract": "In a few cases, a pulsar with a whirlwind P was found to be less than a hundred watts. A single object with a whirlwind P was a tenth of a watt, and a tenth of a watt a hundred watts a hundred and fifty times its watt, a tenth of a watt a watt. A single watt a watt a watt, and a watt a watt. During the period of the supernova, the swarming-out teeming phase was 2.56 ms. The teeming phase was 2.56 m. The teeming phase was 0.66 s. a day, or a year. These teeming phases were 339 5, and 101010101010101010101010101010101010101010  10101010101010, b. b. c. a. B. d. b. c. c. c. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1721, "title": "A Detailed Study of Gas and Star Formation in a Highly Magnified Lyman Break Galaxy at z=3.07", "abstract": "  We report the detection of CO(3-2) emission from a bright, gravitationally lensed Lyman Break Galaxy, LBGJ213512.73-010143 (the 'Cosmic Eye'), at z=3.07 using the Plateau de Bure Interferometer. This is only the second detection of molecular gas emission from an LBG and yields an intrinsic molecular gas mass of (2.4+/-0.4)x10^9 Mo. The lens reconstruction of the UV morphology of the LBG indicates that it comprises two components separated by ~2 kpc. The CO emission is unresolved, and appears to be centered on the intrinsically fainter (and also less highly magnified) of the two UV components. The width of the CO line indicates a dynamical mass of (8+/-2)x10^9csc(i)^2 Mo within the central 2 kpc. Employing mid-infrared observations from Spitzer we derive a stellar mass of ~(6+/-2)x10^9 Mo and a star-formation rate of ~60 Mo/yr, indicating that the molecular gas will be consumed in ~40 Myr. The gas fractions, star-formation efficiencies and line widths suggests that LBGJ213512 is a high-redshift, gas-rich analog of a local luminous infrared galaxy. This galaxy has a similar gas-to-dynamical mass fraction as observed in the submillimeter-selected population, although the gas surface density and star-formation efficiency is a factor of 3x less, suggesting less vigorous activity. We discuss the uncertainties in our conclusions arising from adopting a CO-to-H2 conversion factor appropriate for either the Milky Way or local luminous infrared galaxies. These observations demonstrate that current facilities, when aided by fortuitous gravitational magnification, can study 'ordinary' galaxies at high-redshift and so act as pathfinders for ALMA. ", "machine_abstract": "We present the results of an extensive study of gas dynamics, star formation activity, dust extinction, stellar populations, and black hole accretion properties for a strongly lensed galaxy (A1689-zD1) at redshift 3.07. The lensing magnification factor is ~30\u00c2\u00b15. We use deep near-infrared spectroscopy to measure the kinematics of molecular hydrogen emission lines with high spatial resolution. Our observations reveal that this system consists of two merging galaxies separated by 1 kpc along the line-of-sight. One of these components shows strong H\u00ce\u00b2 emission indicative of active galactic nuclei (AGN). This AGN component has a mass of \u00e2\u02c6\u00bc10^9 M_sol , which corresponds to a supermassive black hole with a mass of \u00e2\u02c6\u00bd\u00e2\u20ac\u201c1 \u00c3\u2014 10^8 M_sol . Using our spatially resolved measurements we find evidence for intense nuclear starbursts on scales as small as 100 pc.", "paraphrased_abstract": "The measurements indicate that this is a large system containing two merging galaxies separated by a radius of only a few hundred kilometers. We find that this system contains two merging galaxies, separated by only one kpc in the line of sight. One of these galaxies is an active galactic nucleus (AGN), which is a mass of 1 \u2013410 \u2013 10 m. The other is a supermassive black hole of 1 \u2013 40 m. We study the gas and gas-dynamics, star formation, dust-extinction, stellar populations, and black hole accretion. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2993, "title": "Room temperature spin polarized magnetic semiconductor", "abstract": "  Alternating layers of granular Iron (Fe) and Titanium dioxide (TiO$_{2-\\delta}$) were deposited on (100) Lanthanum aluminate (LaAlO$_3$) substrates in low oxygen chamber pressure using a controlled pulsed laser ablation deposition technique. The total thickness of the film was about 200 nm. The films show ferromagnetic behavior for temperatures ranging from 4 to $400 ^oK$. The layered film structure was characterized as p-type magnetic semiconductor at $300 ^oK$ with a carrier density of the order of $10^{20} /cm^3$. The undoped pure TiO$_{2-\\delta}$ film was characterized as an n-type magnetic semiconductor. The hole carriers were excited at the interface between the granular Fe and TiO$_{2-\\delta}$ layers similar to holes excited in the metal/n-type semiconductor interface commonly observed in Metal-Oxide-Semiconductor (MOS) devices. The holes at the interface were polarized in an applied magnetic field raising the possibility that these granular MOS structures can be utilized for practical spintronic device applications. ", "machine_abstract": "We report on the room-temperature ferromagnetism in Mn-doped ZnO thin films grown by pulsed laser deposition (PLD). The Curie temperatures are found to be around 300 K for all samples with different doping levels, which is much higher than that reported previously. We also find that the magnetization increases linearly as the applied field decreases and shows hysteresis loops at low fields. These results indicate that the observed ferromagnetic behavior may originate from exchange coupling between localized spins rather than intrinsic ferromagnetism.     In recent years, there has been growing interest in developing new materials for spintronic applications such as nonvolatile memory devices or logic circuits based on the manipulation of electron spins instead of charge carriers1-5 . Among these materials, diluted magnetic semiconductors have attracted considerable attention because they can combine both electronic and magnetic functionalities into one material6-8 .     ZnO-based DMSs have been extensively studied due to their wide band gap energy (3.37 eV), large exciton binding energy (60 meV)9 , high transparency10-12 , and good chemical stability13-15 . However, it remains challenging to achieve room-temperature ferromagnetically ordered states in ZnO-based DMSs16-18 . Although several groups have recently demonstrated room-temperature ferromagnetic ordering in various types of ZnO-based DMS systems19-24 , most of them show relatively small saturation magnetizations25-27 .     Here we report on the observation of room-temperature ferromagnetisms in Mn-doped ZnObased DMSs prepared using pulsed laser deposition28-30 . Our experimental data clearly demonstrate that the dopant concentration plays an important role in determining the Curie temperature31-33 . For example, our sample with x = 0.5% exhibits a Curie temperature of about 300 K while those with lower concentrations exhibit smaller values ranging from 150-250 K34-36 . Moreover, we observe that the magnetization increases almost linearly when decreasing the external magnetic field below 1 T and displays hysteretic behaviors at very low fields. This indicates that the observed ferr", "paraphrased_abstract": "Various studies have been carried out of the diluted magnetic semiconductors of the Xno Group on ZnO, and there have been some significant advances in the area of diluted magnetic semiconductors, in the form of chips, chips, and crystals, but the diluted magnetic materials do not have the ability to bind with each other and have no magnetic property. But recently there has been a great deal of interest in developing new materials for the application of spintronics, i.e., non-volatile memory or logic devices based on the manipulation of electron spins rather than charge carriers, in which, to summarize, they are all based on diluted magnetic materials. These materials, however, are not always of a high temperature, and at very low magnetic fields, they exhibit a ferromagnetic loop, whereas they show no magnetism at all. The ferromagnetic loops, however, are quite small, and they are not observed to be of any real strength, although they are very small. We show here the ferromagnetic behavior of a thin layer of MnO. The ferromagnetic modulus is quite high, with a tenfold increase in the concentration of a charged compound, for example, at a concentration of x, at a temperature of 300 K, for a sample with a x", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0515, "title": "The Kinematic Evolution of Strong MgII Absorbers", "abstract": "  We consider the evolution of strong (W_r(2796) > 0.3A) MgII absorbers, most of which are closely related to luminous galaxies. Using 20 high resolution quasar spectra from the VLT/UVES public archive, we examine 33 strong MgII absorbers in the redshift range 0.3 < z < 2.5. We compare and supplement this sample with 23 strong MgII absorbers at 0.4 < z < 1.4 observed previously with HIRES/Keck. We find that neither equivalent width nor kinematic spread (the optical depth weighted second moment of velocity) of MgII2796 evolve. However, the kinematic spread is sensitive to the highest velocity component, and therefore not as sensitive to additional weak components at intermediate velocities relative to the profile center. The fraction of absorbing pixels within the full velocity range of the system does show a trend of decreasing with decreasing redshift. Most high redshift systems (14/20) exhibit absorption over the entire system velocity range, which differs from the result for low redshift systems (18/36) at the 95% level. This leads to a smaller number of separate subsystems for high redshift systems because weak absorping components tend to connect the stronger regions of absorption. We hypothesize that low redshift MgII profiles are more likely to represent well formed galaxies, many of which have kinematics consistent with a disk/halo structure. High redshift MgII profiles are more likely to show evidence of complex protogalactic structures, with multiple accretion or outflow events. Although these results are derived from measurements of gas kinematics, they are consistent with hierarchical galaxy formation evidenced by deep galaxy surveys. ", "machine_abstract": "We present the kinematics and physical properties of strong Mg II absorbers at z = 1.5 \u2212 3, using high-resolution (R \u2248 45000) spectroscopy obtained with Keck/HIRES. We find that these systems are composed primarily of cool gas clouds in pressure equilibrium with their surroundings; they have typical sizes of 100-200 pc, masses of 10^6\u221210^7 M_sun, and temperatures of ~10 4 K. The majority of our sample show no evidence for bulk motions exceeding 50 km/s relative to their surrounding medium. However, we do detect two outliers which exhibit large velocity shifts between multiple components within each system. These objects may be associated with galactic winds or tidal interactions. Our results suggest that strong Mg II absorbers evolve into galaxies through gravitational collapse on timescales less than one billion years after the Big Bang. This work is based upon observations made with the NASA/ESA Hubble Space Telescope, obtained from the Data Archive at the Space Telescope Science Institute, which is operated by AURA under NASA contract NAS 5-26555.", "paraphrased_abstract": "In the present work, we have obtained observations with the Hubble space telescope, operated by NASA-ESA under contract NAS 5-26555. We present the kinematics and physical properties of these solid-state-cooled helium clouds, which are of the temperature of 15,000 K. The average size of our samples is about 100-200 pc, the masses are around 40-45 ml, and the temperature is about 4 \u00b0C. We do not find any evidence for large-scale movement above a ten-kilometer scale of the surrounding medium. We find two outliers that are velociated by a great flux of the helium within each of them, which may be caused by the tidal wind, or by a tidal wave. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2855, "title": "The generator coordinate method in time-dependent density-functional theory: memory made simple", "abstract": "  The generator coordinate (GC) method is a variational approach to the quantum many-body problem in which interacting many-body wave functions are constructed as superpositions of (generally nonorthogonal) eigenstates of auxiliary Hamiltonians containing a deformation parameter. This paper presents a time-dependent extension of the GC method as a new approach to improve existing approximations of the exchange-correlation (XC) potential in time-dependent density-functional theory (TDDFT). The time-dependent GC method is shown to be a conceptually and computationally simple tool to build memory effects into any existing adiabatic XC potential. As an illustration, the method is applied to driven parametric oscillations of two interacting electrons in a harmonic potential (Hooke's atom). It is demonstrated that a proper choice of time-dependent generator coordinates in conjunction with the adiabatic local-density approximation reproduces the exact linear and nonlinear two-electron dynamics quite accurately, including features associated with double excitations that cannot be captured by TDDFT in the adiabatic approximation. ", "machine_abstract": "The Generator Coordinate Method (GCM) is an efficient approach to calculate the electronic structure and properties of materials with strong electron-phonon coupling, such as polar semiconductors or insulators. In this work we present a new implementation of GCM within Time-Dependent Density Functional Theory (TDDFT), which allows us to study phonon-assisted optical excitations on large systems. The key idea behind our scheme is that it exploits the fact that TDDFT can be formulated as a linear response problem for the Kohn-Sham system, so that the calculation of the ground state wavefunction does not need to be repeated at each step during the self-consistent field iteration. We demonstrate the efficiency of our algorithm by calculating the absorption spectrum of bulk silicon under hydrostatic pressure up to 100 GPa. Our results show good agreement with previous calculations based on supercell approaches.", "paraphrased_abstract": "The results are in accordance with the experimental results of supercells. In our study, we introduce the generator-coil method (GCM) and develop a new solution for the D\u00f6rff-Kohn-Sham system, in which the calculation of the ground state wavefunction is not necessary at each step. The main idea is that our method is applied to the numerical study of phonon-assisted optical excitations on large systems. We demonstrate the efficiency of our method by calculating the absorption spectrum of bulk silicon under hydrostatic pressure up to 100 GPa. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1029, "title": "Inhomogeneities in the Universe and the Fitting Problem", "abstract": "  Observational cosmology provides us with a large number of high precision data which are used to derive models trying to reproduce ``on the mean'' our observable patch of the Universe. Most of these attempts are achieved in the framework of a Friedmann-Lema\\^itre cosmology where large scale homogeneity is assumed. However, we know, from the observation of structures at increasing scales, that these models are only approximations of a smoothed or averaged inhomogeneous underlying patern. Anyhow, when modelling the Universe, the usual method is to use continuous functions representing the kinematical scalars of the velocity field, implicitly assuming that they represent volume averages of the corresponding fine-scale inhomogeneous quantities, then put them into the Einstein equations which are solved to give the model and its dependance upon a number of parameters arbitrarily defined. In General Relativity, such a method is very much involved since the equations which determine the metric tensor and the quantities calculated from it are highly nonlinear. The question raised by the method consisting of determining the parameters of an a priori assumed FLRW model from observational data is the ``fitting problem'' brought to general attention by Ellis and Stoeger in the 80's. This problem has recently experienced a reniewed attention due to the amount of available data and the increase of the minimum scale at which homogeneity can be assumed. We propose a discussion of this issue in the light of the latest developments of observational and theoretical cosmology. ", "machine_abstract": "The fitting problem is one of the most important problems in cosmology, which has been studied for more than 50 years.  The main goal of this work was to study the effect of different types of initial conditions on the evolution of density perturbations in an expanding universe with a positive cosmological constant (dark energy).  We have used two methods to solve numerically the Einstein equations coupled to the matter fields:  the standard method based on the expansion of the metric tensor into spherical harmonics; and the new method developed by us that uses the expansion of the metric into Chebyshev polynomials.  In both cases we have considered three types of initial conditions: Gaussian random fields generated using the Zel'dovich approximation, Gaussian random fields generated using a fast Fourier transform algorithm, and white noise.  Our results show that all these models give similar predictions at late times when they are evolved up to z = 0.5.  However, there are some differences between them at early times.  These differences can be explained as follows:  the Zel'dovich model predicts larger fluctuations compared to other models because it does not take into account the pressure term in the fluid equation; the white noise model gives smaller fluctuations due to its statistical properties; finally, the Gaussian random fields obtained via the fast Fourier transform algorithm predict intermediate values.  This result shows that the choice of the initial conditions may affect significantly the final value of the power spectrum of primordial density fluctuations predicted by inflationary theories.", "paraphrased_abstract": "We use two ways of solving the equations for the equator and the matter, the standard method for constructing the metric tensor into spherical harmonics, and the new method for constructing the metric into Chebyshev polynomials. We have applied the methods of the fitting problem to the calculations of the Einstein equations coupled with matter; we use the standard method of introducing the metric tensor into spherical harmonics, and the new method, developed by us, that incorporates the metric into the Chebyshev polynomials. The results of the fitting process are a long-standing and well-studied problem of cosmology, and have been studied for more than fifty years. The Fitting Problem is an important problem of cosmology, and it has been studied for more than fifty years. This fit problem is a cosmological problem that has been studied for more than fifty years. Its effect on the growth of density in an expanding universe is a matter of great importance. This result is indicated by the fact that the selection of the initial conditions influences the final values of the power spectrum of the primordial density. The results show that the choice of the initial conditions influences the final values of the kinetic energy spectrum of primordial density. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0673, "title": "The Thurston norm via Normal Surfaces", "abstract": "  Given a triangulation of a closed, oriented, irreducible, atoroidal 3-manifold every oriented, incompressible surface may be isotoped into normal position relative to the triangulation. Such a normal oriented surface is then encoded by non-negative integer weights, 14 for each 3-simplex, that describe how many copies of each oriented normal disc type there are. The Euler characteristic and homology class are both linear functions of the weights. There is a convex polytope in the space of weights, defined by linear equations given by the combinatorics of the triangulation, whose image under the homology map is the unit ball, B, of the Thurston norm.   Applications of this approach include (1) an algorithm to compute B and hence the Thurston norm of any homology class, (2) an explicit exponential bound on the number of vertices of B in terms of the number of simplices in the triangulation, (3) an algorithm to determine the fibred faces of B and hence an algorithm to decide whether a 3-manifold fibres over the circle. ", "machine_abstract": "In this talk, we will discuss the relationship between the Thurston norm and normal surfaces in 3-manifolds.  We will begin by recalling some basic definitions about the Thurston norm on homology classes of surface bundles over circles (or equivalently, mapping tori).  Then, using techniques developed by Gabai, we will show that any class with positive Thurston norm can be represented by an embedded surface which is incompressible and not boundary-parallel.  Finally, we will give examples to illustrate how these results are used to study the topology of hyperbolic 3-manifolds. The Thurston norm was introduced in 1982 as part of the solution to the Geometrization Conjecture for Haken manifolds.   It assigns each element of $H_1(M;\\mathbb{R})$ a non-negative real number, where $M$ is a closed orientable 3-manifold.  This assignment satisfies certain natural properties, including additivity under connected sums and multiplicativity under finite covers.  In particular, if $M_1$ and $M_2$ are two closed orientable 3-manfolds whose fundamental groups have isomorphic subgroups of finite index, then there exists a constant $C > 0$ such that $||[\\phi]||_{Th} = C ||[\\psi]||_{Th}$ whenever $\\phi : \\pi_1 M_1 \\to \\pi_1 M_2 \\cong H_3$ is a homomorphism inducing isomorphism on all subgroups of finite index.    If $\\mu \\in H_1(M;\\mathbb{R};\\mathbb{Z})$ represents a fibered knot or link in a closed oriented 3-manifold, then its Thurston norm measures the complexity of the associated fibration.  For example, if $L \\subseteq M$ is a fibered torus knot or link, then $||\\mu||_{Th} = 1$ if and only if $L$ bounds a Seifert surface in $M$.  On the other hand, if $L$ does not bound a Seifert surface in...", "paraphrased_abstract": "\"If the subseteq M is a torus knot or a link, then'mu  Th = 1 if and only if mu  Th = 1 if mu  Th = 1 if mu th = 1 if mu th is not bound to a Seifert surface in mu h. We will begin by recollecting some basic definitions about the Thurston norm on homology classes of surface bundles over circles (or equivalent to a tori), then introducing our theory that the Thurston norm, which is based on a positive integer, satisfies certain natural properties, namely, if the element is in the closed form, and if the subtype is in the closed form, then the corresponding Thurston norm is satisfied. The Thurston norm is a real number for every element of the form H_1 (M]\u2014the closed form of the form M\u2014and, by this means, all of the closed form of the form M\u2014which has a positive and positive phi, if the form is not symmetrical and not linear. This is a rule introduced in 1982 to correct the Geometrization Conjecture for the Haken Man", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.148, "title": "Magnetic structure of Sm2IrIn8", "abstract": "  The magnetic structure of the intermetallic antiferromagnet Sm2IrIn8 was determined using x-ray resonant magnetic scattering (XRMS). Below TN = 14.2, Sm2IrIn8 has a commensurate antiferromagnetic structure with a propagation vector (1/2,0,0). The Sm magnetic moments lie in the ab plane and are rotated roughly 18 degrees away from the a axis. The magnetic structure of this compound was obtained by measuring the strong dipolar resonant peak whose enhancement was of over two orders of magnitude at the L2 edge. At the L3 edge both quadrupolar and dipolar features were observed in the energy line shape. The magnetic structure and properties of Sm2IrIn8 are found to be consistent with the general trend already seen for the Nd-, Tb- and the Ce-based compounds from the RmMnIn3m+2n family (R = rare earth; M=Rh or Ir, m = 1, 2; n = 0, 1), where the crystalline electrical field (CEF) effects determine the direction of magnetic moments and the TN evolution in the series. The measured Neel temperature for Sm2IrIn8 is slightly suppressed when compared to the TN of the parent cubic compound SmIn3. ", "machine_abstract": "We have performed neutron powder diffraction experiments on the intermetallic compound Sm2IrIn8 in order to determine its magnetic structure and compare it with that proposed for YbMgGaO4, another member of this family of compounds. The results show that Sm2IrIn8 has an antiferromagnetic ordering at TN = 3.5 K with moments aligned along the c-axis. This is similar to what was found previously for YbMgGaO4 but different than the theoretical prediction based on band-structure calculations which suggested that the ordered moment should be perpendicular to the c-axis.  We also find evidence for a structural phase transition near T* ~ 80 K where there are changes in both the lattice parameters as well as the unit cell volume. These results suggest that the low temperature crystal structure may not correspond exactly to the high-temperature tetragonal symmetry predicted by theory. Finally we present specific heat data showing clear anomalies associated with both the magnetic ordering and the structural phase transition.", "paraphrased_abstract": "Nevertheless, we have also obtained detailed spectral data, showing the clear anomalies of both magnetic ordering and transitions. We have subsequently tested the magnetic order of Sm2Irin8 in a metallic alloy, and compared it with YbMgGaO4, a compound of the same family. In addition, we have observed a phase transition near T  80 K, where the lattice parameters are shifted to the right, and the volume is also reduced. This suggests that the crystal structure of the lower temperature may not correspond exactly to the high temperature tetragonal symmetry predicted by the theory. We have tested the magnetic order of Sm2Irin8 with the help of X-rays and X-rays from a variety of different materials, such as YbMgGaO4 and YbMgO4 in our experiments. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1591, "title": "Flavour-Dependent Type II Leptogenesis", "abstract": "  We reanalyse leptogenesis via the out-of-equilibrium decay of the lightest right-handed neutrino in type II seesaw scenarios, taking into account flavour-dependent effects. In the type II seesaw mechanism, in addition to the type I seesaw contribution, an additional direct mass term for the light neutrinos is present. We consider type II seesaw scenarios where this additional contribution arises from the vacuum expectation value of a Higgs triplet, and furthermore an effective model-independent approach. We investigate bounds on the flavour-specific decay asymmetries, on the mass of the lightest right-handed neutrino and on the reheat temperature of the early universe, and compare them to the corresponding bounds in the type I seesaw framework. We show that while flavour-dependent thermal type II leptogenesis becomes more efficient for larger mass scale of the light neutrinos, and the bounds become relaxed, the type I seesaw scenario for leptogenesis becomes more constrained. We also argue that in general, flavour-dependent effects cannot be ignored when dealing with leptogenesis in type II seesaw models. ", "machine_abstract": "We study the possibility that leptons acquire their masses through an effective dimension-5 operator, which is suppressed by some heavy scale M . We show how this scenario can be realized in supersymmetric theories with extra dimensions and discuss its phenomenological consequences for neutrino physics.     In particular we consider the case where the lepton number violating interactions are localized on a brane at y = 0 while the Standard Model fields live in the bulk. The resulting mass matrix has two zero eigenvalues corresponding to the light active neutrinos and one massive eigenvalue m ~ 10 GeV associated with the sterile neutrino. This leads to interesting predictions for neutrinoless double beta decay as well as for cosmology. For example, if the Majorana phase vanishes then there will be no contribution to neutrinoless double-beta decay but the sterile neutrino may still contribute significantly to dark matter production during inflation or reheating.", "paraphrased_abstract": "It turns out that if Majorana is lost, the neutrino will not have to decay, but the sterile neutrino may have to a great extent in order to produce dark matter, whether by compression or by heat. Thus we consider the possibility that the neutrino acquires its masses by a powerful dimension-5 operator, which is suppressed by some enormous M. We show how this scenario is achieved in supersymmetric theories with extra dimensions, and we discuss its phenomenological implications for neutrino physics. In particular, we consider the case where the errant interaction of the leptons is localized on a brane at y = 0, while the standard model fields live in the bulk. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.012, "title": "Effective inverse spectral problem for rational Lax matrices and applications", "abstract": "  We reconstruct a rational Lax matrix of size R+1 from its spectral curve (the desingularization of the characteristic polynomial) and some additional data. Using a twisted Cauchy--like kernel (a bi-differential of bi-weight (1-nu,nu)) we provide a residue-formula for the entries of the Lax matrix in terms of bases of dual differentials of weights nu and 1-nu respectively. All objects are described in the most explicit terms using Theta functions. Via a sequence of ``elementary twists'', we construct sequences of Lax matrices sharing the same spectral curve and polar structure and related by conjugations by rational matrices. Particular choices of elementary twists lead to construction of sequences of Lax matrices related to finite--band recurrence relations (i.e. difference operators) sharing the same shape. Recurrences of this kind are satisfied by several types of orthogonal and biorthogonal polynomials. The relevance of formulae obtained to the study of the large degree asymptotics for these polynomials is indicated. ", "machine_abstract": "We consider the effective inverse spectral problems (EISP) associated with rational Lax matrices, which are generalizations of classical EISP in terms of Jacobi matrices. We show that these new EISPs can be reduced to some special cases of classical EISP by using an explicit formula for the coefficients of the characteristic polynomial of a rational matrix. As examples we study two classes of rational Lax matrices: one is related to the Toda lattice hierarchy and another is connected with the modified Volterra lattice hierarchy. In particular, we give a complete description on all solutions of the corresponding EISPs. Finally, as applications of our results, we present several interesting properties about the spectra of these rational Lax matrices. The research was supported by NSFC under Grant No.11571040. Keywords: Effective inverse spectral problem; Rational Lax matrices; Toda lattice hierarchy; Modified Volterra lattice hierarchy", "paraphrased_abstract": "Moreover, we present in this paper some interesting properties about the spectra of these modal Lax matrices. The study is supported by the NSFC under grant No. 11571040. During this research we studied two classes of modal Lax matrices: one is related to the Toda hierarchy and the other to the modified Volterra hierarchy. In particular, we introduce a complete description of the solutions of the corresponding equations. As a result, we show that these new equations can be reduced to special cases of classical equations by using a formula to define the coefficients of a particular rational matrix. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0272, "title": "A Comparison between Anomalous 6-cm H$_2$CO Absorption and CO(1-0) Emission in the L1204/S140", "abstract": "  We report observations of the dust cloud L1204 with the Onsala 25-m telescope in the 6 cm (1$_{11}-1_{10}$) transition of \\htco. The observed region includes the   S140 H${\\alpha}$ arc. This spectral line is seen here in absorption against the cosmic microwave background, indicating the presence of widespread warm molecular gas at intermediate densities. Overall, the distributions of H$_2$CO and CO (taken from the literature) are fairly similar, though significant differences exist at small scales. Most notably, while the CO peak is nearly coincident with the S140 H${\\alpha}$ arc, the maximum H$_2$CO absorption is clearly separated from it by a full 10$'$ beam ($\\sim$ 3 pc). We argue that these differences result from differing abundances and excitation requirements. The CO(1-0) line is more optically thick and more biased towards warm gas than the H$_2$CO 6 cm line. On the other hand, formaldehyde is more easily photodissociated and is, therefore, a poorer tracer of the molecular gas located immediately behind Photon Dominated Regions. ", "machine_abstract": "We present new observations of molecular hydrogen (H_2CO) absorption toward the low-mass protostar IRAS 16293-2422, which is associated with two outflows driven by different components of this binary system. The main component drives an east-west bipolar flow that has been traced over more than 1000 AU using SiO emission lines observed at high angular resolution. We have detected anomalously strong absorption features near the systemic velocity of the source for both ortho- and para-H_2CO transitions. These are likely due to self-absorption within the dense gas surrounding the central protostars. In addition, we find evidence for blueshifted absorption features in the para-H_2CO line profiles that may be tracing infalling material along the axis of one of the outflow lobes. Finally, we compare our results with previous studies of carbon monoxide (CO) emission towards the same region.", "paraphrased_abstract": "We present a new study of the ozone resembling hydrogen (H2CO) absorption into the low-mass protostar IRAS 16293-2422, and it is a two-part system whose axis is a circular one. In addition, we observe blue-shifted absorption of the ozone layer in the para-O2 layer which traces the origin of the material along the axis of the axis of the system. We conclude that this ozone resembling hydrogen was absorbed into the air through the thin gas of the central protostars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1514, "title": "Searching for Gravitational Radiation from Binary Black Hole MACHOs in the Galactic Halo", "abstract": "  The Laser Interferometer Gravitational Wave Observatory (LIGO) is one of a new generation of detectors of gravitational radiation. The existence of gravitational radiation was first predicted by Einstein in 1916, however gravitational waves have not yet been directly observed. One source of gravitation radiation is binary inspiral. Two compact bodies orbiting each other, such as a pair of black holes, lose energy to gravitational radiation. As the system loses energy the bodies spiral towards each other. This causes their orbital speed and the amount of gravitational radiation to increase, producing a characteristic ``chirp'' waveform in the LIGO sensitive band. In this thesis, matched filtering of LIGO science data is used to search for low mass binary systems in the halo of dark matter surrounding the Milky Way. Observations of gravitational microlensing events of stars in the Large Magellanic Cloud suggest that some fraction of the dark matter in the halo may be in the form of Massive Astrophysical Compact Halo Objects (MACHOs). It has been proposed that low mass black holes formed in the early universe may be a component of the MACHO population; some fraction of these black hole MACHOs will be in binary systems and detectable by LIGO. The inspiral from a MACHO binary composed of two 0.5 solar mass black holes enters the LIGO sensitive band around 40 Hz. The chirp signal increases in amplitude and frequency, sweeping through the sensitive band to 4400 Hz in 140 seconds. By using evidence from microlensing events and theoretical predictions of the population an upper limit is placed on the rate of black hole MACHO inspirals in the galactic halo. ", "machine_abstract": "We present results on gravitational radiation searches using data collected by the LIGO and Virgo detectors during their second observing run (O2). We search for signals from binary black holes with masses between 10$^{-3}$ M$\\sim10^2$M, which are expected to be abundant in galactic halos if they exist as massive compact halo objects (MACHOs) predicted by some theories of galaxy formation. The analysis is performed over a large range of source parameters including mass ratio, spin magnitudes and orientations, sky position, distance, inclination angle, eccentricity, and time of coalescence. No significant signal candidates were found above background noise levels. Upper limits on the rate density of binary black hole mergers are set at 90% confidence level within different ranges of component masses. These upper limits can be used to constrain theoretical models of binary black hole merger rates. This work was supported by NSF grants PHY-0960291, PHY-1125897, PHY-1214379, PHY-1430152, and PHY-1505155.", "paraphrased_abstract": ", p. 143, p. 14379, p. 14379, p. 14916, p. 14917, p. 14917. We present data from the LIGO and Virgo detectors during their second observing run (O2). We look for signals from binary black holes of mass tens of thousands, which would be a hazard in the spherical halo, if they are derived from massive compact haloes in a galaxy, as derived from MACHOs, which are expected to be abundant in the galactic halos, if they exist in the form of gigantic compact haloes, as predicted by some theories of galaxy formation. The upper limit for the accursed density of the binary black hole is tenfold, in the range of mass and mass, and in the range of tenfold. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1281, "title": "Gauge-Higgs Unification and Radiative Electroweak Symmetry Breaking in Warped Extra Dimensions", "abstract": "  We compute the Coleman Weinberg effective potential for the Higgs field in RS Gauge-Higgs unification scenarios based on a bulk SO(5) x U(1)_X gauge symmetry, with gauge and fermion fields propagating in the bulk and a custodial symmetry protecting the generation of large corrections to the T parameter and the coupling of the Z to the bottom quark. We demonstrate that electroweak symmetry breaking may be realized, with proper generation of the top and bottom quark masses for the same region of bulk mass parameters that lead to good agreement with precision electroweak data in the presence of a light Higgs. We compute the Higgs mass and demonstrate that for the range of parameters for which the Higgs boson has Standard Model-like properties, the Higgs mass is naturally in a range that varies between values close to the LEP experimental limit and about 160 GeV. This mass range may be probed at the Tevatron and at the LHC. We analyze the KK spectrum and briefly discuss the phenomenology of the light resonances arising in our model. ", "machine_abstract": "We study the possibility that electroweak symmetry breaking is triggered by strong dynamics at an ultraviolet (UV) fixed point, as suggested by gauge-Higgs unification models with warped extra dimensions. We show how this scenario can be realized within the framework of composite Higgs models based on strongly-coupled gauge theories. In particular we consider two different realizations of such scenarios: one where the Higgs arises as a pseudo-Nambu-Goldstone boson associated to spontaneous breaking of approximate global symmetries; another where it emerges as a bound state of new fermions charged under the Standard Model gauge group. The latter case leads to novel signatures for Higgs production through gluon fusion which are potentially observable at future colliders. Finally, we discuss possible implications of these results for cosmology. Gauge-Higgs unification provides a compelling explanation for why the weak scale is so much smaller than any other mass scale in nature [1] . It also offers a natural solution to the hierarchy problem between the Planck and TeV scales [2] , since quantum corrections to the Higgs potential are cut off at the UV scale [3] . In order to realize this idea in practice, however, several challenges must be overcome [4] : i) the Higgs should arise naturally out of some strongly coupled sector; ii) the Higgs couplings to SM particles should agree with experiment; iii) there should exist a mechanism to generate masses for all SM fields without introducing large hierarchies among them. These issues have been addressed recently using the Randall-Sundrum model [5] , where the Higgs field lives on the IR brane while gravity propagates into the bulk [6] - [8] . This setup allows for a calculable description of the Higgs physics [9] , but introduces additional complications due to the presence of Kaluza-Klein gravitons [10] .", "paraphrased_abstract": "These difficulties have recently been considered by the Randall-Sundrum model, in which the Higgs' field is on the IR brane, while gravity spreads in the bulk. This setup is sufficient to represent the Higgs physics, but also introduces additional difficulties due to the presence of Kaluza-Klein gravitons. We consider two different interpretations of this model: one is that the Higgs emerges as a pseudo-Nambu-Goldstone boson which causes the breaking of the global symmetries; the other, that the Higgs become bound to a new fermions charged under the gauge group; and the latter is followed by new signatures of Higgs production by gluon fusion which might be observed at future colliders. We present a study of the possibility that electroweak symmetry is broken by strong dynamics at a UV fixed point, as proposed by the gauge-Higgs unification model with warped dimensions. The gauge-Higgs unification of a weak sphere offers a natural solution to the hierarchy of Planck and TeV scales, as the Higgs potential is cut off at the UV-line. In particular, we consider two different interpretations of the possibility, one of which is that the Higgs is a pseudo-Nambu-Goldstone boson", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2243, "title": "Post-Newtonian gravitational radiation and equations of motion via direct integration of the relaxed Einstein equations. V. Evidence for the strong equivalence principle to second post-Newtonian order", "abstract": "  Using post-Newtonian equations of motion for fluid bodies valid to the second post-Newtonian order, we derive the equations of motion for binary systems with finite-sized, non-spinning but arbitrarily shaped bodies. In particular we study the contributions of the internal structure of the bodies (such as self-gravity) that would diverge if the size of the bodies were to shrink to zero. Using a set of virial relations accurate to the first post-Newtonian order that reflect the stationarity of each body, and redefining the masses to include 1PN and 2PN self-gravity terms, we demonstrate the complete cancellation of a class of potentially divergent, structure-dependent terms that scale as s^{-1} and s^{-5/2}, where s is the characteristic size of the bodies. This is further evidence of the Strong Equivalence Principle, and supports the use of post-Newtonian approximations to derive equations of motion for strong-field bodies such as neutron stars and black holes. This extends earlier work done by Kopeikin. ", "machine_abstract": "We present evidence that the strong equivalence principle (SEP) holds in general relativity at least up to second post Newtonian order, by showing that the SEP is satisfied if it holds at first PN order.  We show this by directly integrating the relaxed Einstein field equations with an appropriate source term which incorporates the effects of gravitational waves generated by accelerating bodies. The results are consistent with those obtained previously using perturbation theory. This work was supported by NSF grant PHY-0456747. In this article we will consider only isolated systems where no external forces act on any body. For such systems the total energy E = T + U , where T is the kinetic energy and U is the potential energy associated with all interactions between particles within the system. If there were no gravitational fields then the total energy would be conserved since the net force acting on each particle vanishes. However when gravitational fields exist they can exchange energy with matter through emission or absorption of gravitational waves.", "paraphrased_abstract": "In the present article we consider isolated systems where no external forces exert themselves on any body. The total energy in a system is E. T is the kinetic energy, U is the potential energy of all the particles in the system. The total energy in a system is E. T x U, where E is the kinetic energy, U is the potential energy of all the particles in the system, and we are referring to the principle of strong equivalence, which applies in general to the second post Newtonian order. We prove that the principle of strong equivalence holds in general relativity, at least to the second post Newtonian order. This is proved by a direct calculation of the equivalence of the equations of the Einstein field, combining with an appropriate term with the effects of gravitational waves produced by the ebullition of the ebullition. This is in agreement with the results obtained in perturbation theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1124, "title": "One- and two-component bottle-brush polymers: simulations compared to theoretical predictions", "abstract": "  Scaling predictions and results from self-consistent field calculations for bottle-brush polymers with a rigid backbone and flexible side chains under good solvent conditions are summarized and their validity and applicability is assessed by a comparison with Monte Carlo simulations of a simple lattice model. It is shown that under typical conditions, as they are also present in experiments, only a rather weak stretching of the side chains is realized, and then the scaling predictions based on the extension of the Daoud-Cotton blob picture are not applicable.   Also two-component bottle brush polymers are considered, where two types (A,B) of side chains are grafted, assuming that monomers of different kind repel each other. In this case, variable solvent quality is allowed for, such that for poor solvent conditions rather dense cylinder-like structures result. Theories predict ``Janus Cylinder''-type phase separation along the backbone in this case. The Monte Carlo simulations, using the pruned-enriched Rosenbluth method (PERM) then are restricted to rather short side chain length. Nevertheless, evidence is obtained that the phase separation between an A-rich part of the cylindrical molecule and a B-rich part can only occur locally. The correlation length of this microphase separation can be controlled by the solvent quality. This lack of a phase transition is interpreted by an analogy with models for ferromagnets in one space dimension. ", "machine_abstract": "We study the conformational properties of one- and two-component bottlebrush polymers in good solvent conditions by means of Monte Carlo (MC) simulations, using an off-lattice model with freely jointed chains. We compare our results for the radius of gyration Rg(N), end-to-end distance Ree(N), persistence length P(N), and contour length Lc(N) as functions of chain length N to those obtained within the framework of the worm-like-chain (WLC) theory. The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over several orders of magnitude in chain lengths. In particular, we find that the persistence length scales linearly with the number of monomers per backbone segment, which agrees well with recent experimental findings on bottle-brush polyelectrolytes.     Keywords: Polymer brushes, Persistence length", "paraphrased_abstract": "In a word, a sphere of gyration Rg is a sphere of gyration Rg is a sphere of gyration Rg, Rg is the distance from the end to the end, a length of Pcn, persistence length Pcn, a contour length Lccn is an extension of the worm-like chain. We compare the effect of the WLC on the number of chains of a single chain, namely, that the persistence length is linearly proportional to the number of monomers of the backbone, which is in agreement with recent experiments in polyelectrolytes. Then, we analyzed the conformal properties of one- and two-component bottle-brush polymers in a liquid solution, using a chain-free MC. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1605, "title": "Discovery of two candidate pulsar wind nebulae in very-high-energy gamma rays", "abstract": "  We present the discovery of two very-high-energy gamma-ray sources in an ongoing systematic search for emission above 100 GeV from pulsar wind nebulae in survey data from the H.E.S.S. telescope array. Imaging Atmospheric Cherenkov Telescopes are ideal tools for searching for extended emission from pulsar wind nebulae in the very-high-energy regime. H.E.S.S., with its large field of view of 5 degrees and high sensitivity, gives new prospects for the search for these objects. An ongoing systematic search for very-high-energy emission from energetic pulsars over the region of the Galactic plane between -60 degrees < l < 30 degrees, -2 degrees < b < 2 degrees is performed. For the resulting candidates, the standard H.E.S.S. analysis was applied and a search for multi-wavelength counterparts was performed. We present the discovery of two new candidate gamma-ray pulsar wind nebulae, HESS J1718-385 and HESS J1809-193. H.E.S.S. has proven to be a suitable instrument for pulsar wind nebula searches. ", "machine_abstract": "We report the discovery of two new TeV PWN candidates, HESS J1825-137 and HESS J1857+026, using data taken with the High Energy Stereoscopic System (H.E.S.S.) between 2004 and 2007. The sources are spatially coincident with extended radio emission that is likely to be associated with supernova remnants G18.0-0.7 and CTB 37A respectively. Both objects show hard power-law spectra extending up to at least 100 GeV. We discuss possible scenarios for their origin as well as implications on our understanding of particle acceleration mechanisms within PWNe. Keywords: Very high energy gamma ray astronomy, Pulsar Wind Nebula, Supernova Remnant, Particle Acceleration. 1 Introduction Pulsar Wind Nebulae (PWNe) are believed to be powered by relativistic winds ejected from young rotation-powered pulsars [1] . These winds interact with surrounding material creating shocks which accelerate particles to extremely high energies [2] , resulting in synchrotron radiation observed across the electromagnetic spectrum [3] . The detection of high-energy photons emitted by these systems can provide important information about the physical processes occurring inside them [4] . In particular, observations above 10 GeV have been used to study the spectral properties of several known PWNe [5] . However, only one object has so far been detected beyond 30 GeV [6] . This lack of detections may be due to the fact that most current instruments were not designed specifically for this purpose or because they operate under unfavourable observing conditions such as large zenith angles [7, 8] .", "paraphrased_abstract": "The spectral properties of the PWNe are very interesting and can be used to estimate the physical processes in them. The detection of these photons by these systems can provide important information about the physical processes in them, but so far they have not been detected above 30 GeV. In addition, the PWNe have been used in conjunction with the high-energy spectrograph of the High Energy X-ray Surveyor, with a view to studying the spectral properties of several PWNe. These objects are connected by radio waves, which may have associated with supernova remnants G.18.0 and CTB 37. We discuss possible causes of their origin and their possible implications for the study of particle acceleration in PWNe. Pulsar Wind Nebulae are believed to be generated by relativistic winds, emitted by young pulsars, in turn, by the Rotating Winds, and by the Relativity, which, causing them to accelerate to super-high energies, generate synchrotron radiation, emitted by the Radiation of the World. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1643, "title": "Two-dimensional Ising model with competing interactions and its application to clusters and arrays of $\\pi$-rings and adiabatic quantum computing", "abstract": "  We study planar clusters consisting of loops including a Josephson $\\pi$-junction ($\\pi$-rings). Each $\\pi$-ring carries a persistent current and behaves as a classical orbital moment. The type of particular state associated with the orientation of orbital moments at the cluster depends on the interaction between these orbital moments and can be easily controlled, i.e. by a bias current or by other means. We show that these systems can be described by the two-dimensional Ising model with competing nearest-neighbor and diagonal interactions and investigate the phase diagram of this model. The characteristic features of the model are analyzed based on the exact solutions for small clusters such as a 5-site square plaquette as well as on a mean-field type approach for the infinite square lattice of Ising spins. The results are compared with spin patterns obtained by Monte Carlo simulations for the 100 $\\times$ 100 square lattice and with experiment. We show that the $\\pi$-ring clusters may be used as a new type of superconducting memory elements. The obtained results may be verified in experiments and are applicable to adiabatic quantum computing where the states are switched adiabatically with the slow change of coupling constants. ", "machine_abstract": "We study the two-dimensional (2D) Ising model on square lattices with competing nearest-neighbor ferromagnetic interaction J1, next-nearest neighbor antiferromagnetic interaction J2, and external magnetic field H. We find that for small values of J2 there is no phase transition at finite temperature T in this system. However, when J2 increases beyond some critical value, we observe two successive transitions between paramagnetic state and ordered states as T decreases. The first-order transition line ends at tricritical point where second-order transition starts. In addition, we show that the ground-state energy per spin can be used to predict whether or not an array of \u03c0-rings will exhibit macroscopic degeneracy. Finally, by using the exact solution of 2D Ising model, we propose a new scheme for implementing adiabatic quantum computing which may lead to faster algorithms than those based on conventional methods. Two-dimensional Ising models have been studied extensively over past decades due to their rich physics and wide applications ranging from statistical mechanics [1] , condensed matter physics [2] , and biophysics [3] . Recently, it has also attracted much attention because of its potential use in designing efficient quantum computers [4] . In this work, we consider the following Hamiltonian, where Sx = \u00b1 1 are classical spins located at each site x of a square lattice; J1 > 0 denotes the strength of nearest-neighbor ferromagnetic coupling while J2 < 0 represents the strength of next-nearest neighbor antifermomagnetic coupling. Here, h stands for the magnitude of external magnetic field applied along positive z-direction. It should be noted that our results remain valid even if one replaces the term \u2212J2Sx2Sy2 with \u2212J2(Sx1 + Sy1)(Sx2 + Sy2). This is because both terms give rise to same contribution after summation over all sites y such that |y| = 2.", "paraphrased_abstract": "I will use a two-dimensional (2D) two-dimensional sizing model on a square ring of a square, with the same two-dimensional sizing model J2, the next-dimensional sizing model H, and the external magnetic field H. In this study we study the two-dimensional Hamiltonian (or J2 Sx2 Sy2), where Sx is the first spin, and J1 is the second, which is the third. For small values of J2 there is no transition at T, but with increasing T the transitions between paramagnetic and ordered are observed, and the first order transition is completed at the tricritical point, and the second order transition starts at the tricritical point. We will be able to find out the exact solution of the J2Sx2Sy2 S2 S2 S2 S2 S2 S2 S2 S2 S2 0. The results of this experiment are: the two-dimensional (2D) sing model is applied to the square ring with competing ferromagnetic ferromagnetic ferromagnetic ferromagnetic ferromagnetic ferromagnetic ferromagnetic ferromagnetic ferromagnetic ferromagnetic ferromagnetic ferromagnetic", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1154, "title": "Information, information processing and gravity", "abstract": "  I discuss fundamental limits placed on information and information processing by gravity. Such limits arise because both information and its processing require energy, while gravitational collapse (formation of a horizon or black hole) restricts the amount of energy allowed in a finite region. Specifically, I use a criterion for gravitational collapse called the hoop conjecture. Once the hoop conjecture is assumed a number of results can be obtained directly: the existence of a fundamental uncertainty in spatial distance of order the Planck length, bounds on information (entropy) in a finite region, and a bound on the rate of information processing in a finite region. In the final section I discuss some cosmological issues related to the total amount of information in the universe, and note that almost all detailed aspects of the late universe are determined by the randomness of quantum outcomes. This paper is based on a talk presented at a 2007 Bellairs Research Institute (McGill University) workshop on black holes and quantum information. ", "machine_abstract": "The author considers the problem of gravitational interaction between bodies in terms of their informational content. The main idea is that the gravitational field can be considered as an ensemble of gravitons which carry information about the source body. Gravitational waves are treated as carriers of information on the state of motion of gravitating objects. It is shown how this approach allows one to explain some phenomena observed in astrophysics (the Pioneer anomaly) and cosmology (dark energy). In addition, it is proposed to use the concept of \"information potential\" for describing the evolution of the universe. This article was published by the journal Classical and Quantum Gravity Volume 27, Issue 14, pages 5993-6010, November 2010. DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is taken directly from the original publication.     Abstract     We consider the problem of gravitational interaction among bodies in terms of their information content. The main idea here is that the gravitational field may be viewed as an ensemble of gravitons/quanta carrying information about the source body; gravitational waves are then seen as carriers of information regarding the state of motion of the gravitating objects. This viewpoint enables us to provide explanations for certain phenomena observed in astrophysical settings (e.g., the Pioneer anomaly), as well as in cosmological contexts (e.g., dark energy). Moreover, we propose using the notion of \u201cinformation potential\u201d to describe the evolution of the Universe.", "paraphrased_abstract": "The following text is a direct translation of the original article. The article is based on the notion of the resemblance of gravitational fields to information. This is in view of the fact that gravitational fields are an ensemble of gravitons and quanta that contain information about the receptacle; gravitational waves are considered to be carriers of information about the state of the receptacle. The author explains the problem of gravitational interaction between bodies by considering the information content. The receptacle is regarded as an ensemble of gravitons which contain information about the source body; the gravitational waves are regarded as carriers of information about the motion of the receptacle. In this way, we explain the phenomena observed in astrophysics (the Pioneer anomaly) and in cosmology (the dark energy). This approach can be used to explain certain phenomena observed in astronomy (the Pioneer anomaly), and in cosmology (the dark energy). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1189, "title": "The Kinematics and Dynamics of the Globular Clusters and the Planetary Nebulae of NGC 5128", "abstract": "  A new kinematic and dynamic study of the halo of the giant elliptical galaxy, NGC 5128, is presented. From a spectroscopically confirmed sample of 340 globular clusters and 780 planetary nebulae, the rotation amplitude, rotation axis, velocity dispersion, and the total dynamical mass are determined for the halo of NGC 5128. The globular cluster kinematics were searched for both radial dependence and metallicity dependence by subdividing the globular cluster sample into 158 metal-rich ([Fe/H] > -1.0) and 178 metal-poor ([Fe/H] < -1.0) globular clusters. Our results show the kinematics of the metal-rich and metal-poor subpopulations are quite similar. The kinematics are compared to the planetary nebula population where differences are apparent in the outer regions of the halo. The total mass of NGC 5128 is found using the Tracer Mass estimator (Evans et al. 2003), to determine the mass supported by internal random motions, and the spherical component of the Jeans equation to determine the mass supported by rotation. We find a total mass of (1.0+/-0.2) x 10^(12) Msun from the planetary nebulae data out to a projected radius of 90 kpc and (1.3+/-0.5) x 10^(12) Msun from the globular clusters out to a projected radius of 50 kpc. Lastly, we present a new and homogeneous catalog of known globular clusters in NGC 5128. This catalog combines all previous definitive cluster identifications from radial velocity studies and HST imaging studies, as well as 80 new globular clusters from a study of M.A. Beasley et al. (2007, in preparation). ", "machine_abstract": "We present new spectroscopic observations for two globular clusters (GCs) in the nearby galaxy NGC 5128, which is known as Centaurus A. The GCs are located at projected distances of ~3 kpc to ~10 kpc from the nucleus of this elliptical galaxy. We have obtained high-resolution spectra with Gemini/GMOS-S on three different nights during 2013-14. These data allow us to measure radial velocities accurate to better than 1 km/sec for both GCs. In addition we also obtain line-of-sight velocity dispersions using these same GMOS-S data. For one cluster, we find that its systemic velocity agrees well with previous measurements by other authors. However, our measurement for the second cluster differs significantly from previously published values. This discrepancy may be due to contamination from an underlying stellar population or possibly because it has been misclassified as a GC.", "paraphrased_abstract": "In one case, we are able to find the system-to-system ratio more agreeable than the previous estimates, but the second, which is considerably less agreeable, we will find more inaccuracies, that is, a difference of over one and a half points. The new spectral data for the two globular clusters, in the nearby galaxy of NGC 5128, which is known as Centaurus A, have been analyzed. In the spectral data of the two clusters, we have measured the radial velocity with the Gemini system and the GMOS-S. The spectral data of the two clusters, namely the radial velocity of the first cluster, are very agreeable. For the second cluster, we have obtained the measurement at a resolution of one kilometer. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0977, "title": "Stellar Evolution in NGC 6791: Mass Loss on the Red Giant Branch and the Formation of Low Mass White Dwarfs", "abstract": "  We present the first detailed study of the properties (temperatures, gravities, and masses) of the NGC 6791 white dwarf population. This unique stellar system is both one of the oldest (8 Gyr) and most metal-rich ([Fe/H] ~ 0.4) open clusters in our Galaxy, and has a color-magnitude diagram (CMD) that exhibits both a red giant clump and a much hotter extreme horizontal branch. Fitting the Balmer lines of the white dwarfs in the cluster, using Keck/LRIS spectra, suggests that most of these stars are undermassive, <M> = 0.43 +/- 0.06 Msun, and therefore could not have formed from canonical stellar evolution involving the helium flash at the tip of the red giant branch. We show that at least 40% of NGC 6791's evolved stars must have lost enough mass on the red giant branch to avoid the flash, and therefore did not convert helium into carbon-oxygen in their core. Such increased mass loss in the evolution of the progenitors of these stars is consistent with the presence of the extreme horizontal branch in the CMD. This unique stellar evolutionary channel also naturally explains the recent finding of a very young age (2.4 Gyr) for NGC 6791 from white dwarf cooling theory; helium core white dwarfs in this cluster will cool ~3 times slower than carbon-oxygen core stars and therefore the corrected white dwarf cooling age is in fact ~7 Gyr, consistent with the well measured main-sequence turnoff age. These results provide direct empirical evidence that mass loss is much more efficient in high metallicity environments and therefore may be critical in interpreting the ultraviolet upturn in elliptical galaxies. ", "machine_abstract": "We present new optical photometry for the open cluster NGC 6791, obtained with the Wide Field Camera 3 (WFC3) aboard HST as part of program GO-12775 (PI: A. Dotter). The data cover an area of 0.5 deg2 around the cluster center at high spatial resolution (0.1 arcsec/pixel), allowing us to study individual stars down to V = 20 mag. We use these observations together with archival WFC3/UVIS images taken under programs GO-10775 (PI: J. Kalirai) and GO-11775 (PI: S. Casagrande) to derive accurate stellar parameters for more than 1000 red giant branch (RGB) stars in this cluster. Our analysis shows that RGB mass loss is very efficient among low-mass stars, leading to the formation of white dwarfs with masses below 0.45 M . This result has important implications for our understanding of the evolution of low-mass stars near the end of their lives.     Keywords: Open clusters", "paraphrased_abstract": "The results show that the gamma-loss of the RGB was very efficient in the formation of white dwarfs with masses below 0.45 M. This result could be of use to us in our understanding of the development of low-mass stars in their final stages. In this study, we show that a small number of white dwarfs with a mass below 0.45 M was found in the open cluster NGC 6791. Our data are based on a new, high-resolution survey of the innermost region of the cluster at a resolution of a pixel per pixel (a resolution of a single arcsec). This data are used in conjunction with the archival UVIS images of the old program GO 10775 (J. Kalirai) and GO 11775 (S. Casagrande) to determine the precise nature of the star, and the elongated range of the universe, to the extent that a white dwarf forming white dwarfs. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2345, "title": "On Predicting the Solar Cycle using Mean-Field Models", "abstract": "  We discuss the difficulties of predicting the solar cycle using mean-field models. Here we argue that these difficulties arise owing to the significant modulation of the solar activity cycle, and that this modulation arises owing to either stochastic or deterministic processes. We analyse the implications for predictability in both of these situations by considering two separate solar dynamo models. The first model represents a stochastically-perturbed flux transport dynamo. Here even very weak stochastic perturbations can give rise to significant modulation in the activity cycle. This modulation leads to a loss of predictability. In the second model, we neglect stochastic effects and assume that generation of magnetic field in the Sun can be described by a fully deterministic nonlinear mean-field model -- this is a best case scenario for prediction. We designate the output from this deterministic model (with parameters chosen to produce chaotically modulated cycles) as a target timeseries that subsequent deterministic mean-field models are required to predict. Long-term prediction is impossible even if a model that is correct in all details is utilised in the prediction. Furthermore, we show that even short-term prediction is impossible if there is a small discrepancy in the input parameters from the fiducial model. This is the case even if the predicting model has been tuned to reproduce the output of previous cycles. Given the inherent uncertainties in determining the transport coefficients and nonlinear responses for mean-field models, we argue that this makes predicting the solar cycle using the output from such models impossible. ", "machine_abstract": "The solar cycle is an important phenomenon in space weather, which can have significant effects on Earth and its technological infrastructure. The prediction of future solar activity has been studied for many years with varying degrees of success.  In this work we use mean-field models to predict the next solar cycle (24) based on data from previous cycles. We find that our model predicts the amplitude of the current cycle well but underestimates the length by about one year. Our results suggest that the Sun will enter into minimum phase around 2013-14 followed by maximum phase around 2017-18. This prediction agrees reasonably well with other recent predictions made by different groups. However, it should be noted that there are large uncertainties associated with these predictions due to the chaotic nature of the solar dynamo process. Future observations may help reduce some of these uncertainties. Keywords: Space Weather, Solar Dynamo, Mean Field Modeling, Prediction", "paraphrased_abstract": "Aspects of the solar cycle are very important to the sky and may have a major effect on the environment. We have studied the solar cycle for a long time, but we have found that it is very ill-defined. Its duration is a little less than the forecast for the previous one. The prediction is quite reasonable. However, the forecast is ambiguous and uncertain. The solar cycle has been studied for a long time and is not without its difficulties. The predictions are made, however, based on the results of the previous one. We predict a amplitude of the current cycle well, but we overestimate the length of the current cycle by a year. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4156, "title": "Hidden Symmetry of the CKM and Neutrino Mapping Matrices", "abstract": "  We propose that the smallness of the light quark masses is related to the smallness of the T violation in hadronic weak interactions. Accordingly, for each of the two quark sectors (\"upper\" and \"lower\") we construct a 3\\times 3 mass matrix in a bases of unobserved quark states, such that the \"upper\"and \"lower\" basis states correspond exactly via the $W^\\pm$ transitions in the weak interaction. In the zeroth approximation of our formulation, we assume T conservation by making all matrix elements real. In addition, we impose a \"hidden symmetry\" (invariance under simultaneous translations of all three basis quark states in each sector), which ensures a zero mass eigenstate in each sector.   Next, we simultaneously break the hidden symmetry and T invariance by introducing a phase factor e^{i\\chi} in the interaction for each sector. The Jarlskog invariant J_{CKM}, as well as the light quark masses are evaluated in terms of the parameters of the model. We find a simple relation with J_{CKM}=(m_dm_s/m_b^2)^{1/2}A\\lambda^3\\cos(\\chi/2), with A and \\lambda the Wolfenstein parameters. Setting J_{CKM}=3.08 \\times 10^{-5}, m_b=4.7GeV, m_s=95MeV, A=0.818 and \\lambda=0.227, we find m_d\\cos^2(\\chi/2) \\simeq 2.4MeV, consistent with the accepted value m_d=3-7MeV.   We make a parallel proposal for the lepton sectors. With the hidden symmetry and in the approximation of T invariance, both the masses of e and \\nu_1 are zero. The neutrino mapping matrix V_\\nu is shown to be of the same Harrison-Scott form which is in agreement with experiments. We also examine the correction due to T violation, and evaluate the corresponding Jarlskog invariant {\\cal J}_\\nu. ", "machine_abstract": "We present an explicit construction for the CKM matrix, VCKM, in terms of the neutrino mixing matrices UPMNS and UMNS. The latter are defined as unitary transformations that diagonalize the Majorana mass matrices MPMNS and MMNS respectively. We show how this mapping can be used to derive relations between observables associated with quarks and leptons. In particular we find that the Jarlskog invariant JCPV is related to the lepton CP violating phase $\\Delta$ by $JCPV = 2 \\Delta \\cos(2 \\theta_{13})$ where $\\theta_{13}$ denotes the Cabibbo angle. This relation holds independently of whether or not there exists a Dirac CP violating phase in the PMNS matrix. Furthermore it allows us to relate the quark and lepton unitarity triangles. Finally we discuss possible implications on models beyond the Standard Model. Introduction: It has been known since the work of Kobayashi-Maskawa [1] , that the observed pattern of flavor changing neutral currents (FCNCs), which occur at tree level in the standard model (SM), requires new physics contributions [2] . These FCNC processes have so far only been observed within experimental uncertainties consistent with SM predictions [3] . In order to explain these results one usually assumes that the underlying theory respects some form of approximate symmetry [4] . One possibility would be to assume that the Yukawa couplings respect such symmetries [5] . However, if the fermion masses arise through spontaneous breaking of global symmetries [6] then the resulting effective Lagrangian will contain higher dimensional operators [7] . Such operators may induce large effects in FCNC processes [8] . Alternatively, one could consider extensions of the SM gauge group [9] .", "paraphrased_abstract": "Observable phenomena in the CKM of the VCKM are those resulting from the equation of the neutrino mixing matrices UMNS and MPMNS. These two transformations are the result of the normalization of the Majorana mass matrices MPMNS and MMNS, respectively. As a result, the CKM of the VCKM has a corresponding XVIII representation of the nucleus of the CPV, whose invariance is a derivative of the finite element, a delta, whose invariance is the angle of the Cabibbo. This relation is independent of the presence of Dirac CP violations in the PMNS. We therefore introduce the SM gauge. The results are usually assumed to be in accordance with some approximate symmetry, for the simplest physics involving CPs, which occur in the tree level of the SM, but in the simplest physics they have been only observed in experimental uncertainties consistent with SM predictions. Hence, we present an explicit construction of the VCKM, VCKM, in terms of the neutrino mixing matrices UPMNS and UMNS, defining, in a single line, the Majorana mass matrices MPMNS and MMNS. We prove that the CPV of", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1499, "title": "Top production at the Tevatron/LHC and nonstandard, strongly interacting spin one particles", "abstract": "  In this note, we consider possible constraints from $t \\bar t$ production on the gauge bosons of theories with an extended strong interaction sector such as axigluons or flavour universal colorons. Such constraints are found to be competitive with those obtained from the dijet data. The current $t \\bar t$ data from the Tevatron rule out axigluon masses ($m_A$) up to 900 GeV and 850 GeV at 2 $\\sigma$ and 4 $\\sigma$ levels respectively. For the case of flavour universal colorons the data rule out a mass ($m_C$) below 800 GeV (780 GeV) at the $2 (4) \\sigma $ level and also the mass range between 900 GeV to 2.1 TeV at 2 $\\sigma$ level, for $\\cot \\xi = 1$, where $\\xi$ is the mixing angle. For $\\cot \\xi =2$ on the other hand, the excluded range is $m_C \\lsim 950 (920)$ GeV and $m_C \\gsim 1.02 (1.15 \\lsim m_C \\lsim 1.8)$ TeV at $2 \\sigma$ ($4 \\sigma $) level. We point out that for higher axigluon/coloron masses, even for the dijet channel, the limits on the coloron mass, for $\\cot \\xi = 1$, may be different than those for the axigluon. We also compute the expected forward-backward asymmetry for the case of the axigluons which would allow it to be discriminated against the SM as also the colorons. We further find that at the LHC, the signal should be visible in the $t \\bar t$ invariant mass spectrum for a wide range of axigluon and coloron masses that are still allowed. We point out how top polarisation may be used to further discriminate the axigluon and coloron case from the SM as well as from each other. ", "machine_abstract": "We propose to search for new physics in events with two energetic jets and large missing transverse energy (MET) using data collected by the CDF experiment during Run II of Fermilab's Tevatron Collider. The MET is due to the presence of weakly-interacting stable massive particles that escape detection. We consider models where these particles are produced via interactions mediated by heavy gauge bosons associated with an extended electroweak symmetry group SU(2)xSU(3). In this class of models there exist exotic states such as color octets or triplets which can decay into pairs of quarks and/or gluons leading to final state signatures similar to those expected from supersymmetric theories. These models predict cross sections larger than Standard Model backgrounds over most of the parameter space considered here. This analysis will be performed on 1 fb-1 of integrated luminosity recorded by CDF between 2002-2007 corresponding to about 2 million events.", "paraphrased_abstract": "The experiment will be performed on 1 fb-1 of the combined luminosity recorded by CDF between 2002 and 2007 for a total of two million events. The MET is caused by the presence of a few stable particles, but they are unable to be detected, and are accounted for by weakly-interacting, stable particles which do not pass through detection. This MET is based on the hypothesis that, in an event of two energetic jets, large MET is absent, and the results are interpreted in terms of the MET and the MET, which are based on the assumption that the MET is due to the presence of a few heavy-gauge bosons of a long symmetry, with an extended symmetry of SU(2)xSU(3). In this sphere, exotic states, such as color octets or a triplet, are given to decay, and, in a final state, the observable state of the system is given to the observable state. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3871, "title": "Anomalies and de Sitter radiation from the generic black holes in de Sitter spaces", "abstract": "  Robinson-Wilczek's recent work shows that, the energy momentum tensor flux required to cancel gravitational anomaly at the event horizon of a Schwarzschild-type black hole has an equivalent form to that of a (1+1)-dimensional blackbody radiation at the Hawking temperature. Motivated by their work, Hawking radiation from the cosmological horizons of the general Schwarzschild-de Sitter and Kerr-de Sitter black holes, has been studied by the method of anomaly cancellation. The result shows that the absorbing gauge current and energy momentum tensor fluxes required to cancel gauge and gravitational anomalies at the cosmological horizon are precisely equal to those of Hawking radiation from it. It should be emphasized that the effective field theory for generic black holes in de Sitter spaces should be formulated within the region between the event horizon (EH) and the cosmological horizon (CH), to integrate out the classically irrelevant ingoing modes at the EH and the classically irrelevant outgoing modes at the CH, respectively. ", "machine_abstract": "We study the thermodynamics of the Schwarzschild-de Sitter (SdS) black hole by using the tunneling method, which is based on the WKB approximation to quantum gravity. We find that there are two kinds of Hawking radiations for this black hole: one is thermal radiation with temperature T = 1/2\u03c0r0, where r0 is the radius of event horizon; another is nonthermal radiation with temperature T = 0. The latter can be regarded as the effect of the cosmological constant \u039b. In addition, we also calculate the entropy of the SdS black hole and show that it agrees well with Bekenstein-Hawking entropy when the mass M \u226b 1/(8\u03c0G\u039b). Finally, we discuss some possible applications of our results. It has been shown recently that the existence of the cosmological constant leads to new phenomena in physics [1] . For example, the vacuum fluctuations around an electrically charged particle lead to spontaneous emission [2] , while those around a neutral particle result in stimulated emission [3] . Moreover, the presence of the cosmological constant may affect the evaporation process of black holes [4] . In fact, the effects of the cosmological constant have already been studied extensively in the literature [5] - [8] . However, most works focus only on static or stationary black holes. Recently, the authors [9] investigated the thermodynamic properties of the Reissner-Nordstr\u00f6m-de Sitter (RNdS) black hole by applying the tunneling method [10] . They found that besides the usual thermal radiation, there exists another kind of Hawking radiation whose temperature vanishes at the end point of evaporation. This phenomenon was interpreted as the effect of the negative pressure due to the cosmological constant [11] . Furthermore, they showed that the entropy of the RNdS black hole agrees very well with BekensteinHawking entropy [12] if its charge Q satisfies Q \u226b G/(2\u03c0\u2113), where \u2113 denotes the radius of the event horizon.", "paraphrased_abstract": "It was shown that the existence of the cosmological constant was the cause of new phenomena in physics. The vacuum fluctuations around an electrically charged particle produced a spontaneous excitation, while the emission around a neutral particle produced a slurred excitation. We have studied the effect of the cosmological constant in the study of the Schwarzschild-de-Stater black hole by the method of tunneling. The method was developed on the basis of the WKB approximation to quantum gravity. We have found that the thermal radiation of the SdS black hole is also quite high and is equal to the Bekenstein-Hawking excitation. In addition, we have calculated the entropy of the SdS black hole, which is very close to the Bekenstein-Hawking excitation if the mass M satisfies Q G/(2l)l, where l is the radius of the event horizon. This excitation is a result of the cosmological constant, which is known to be a major factor in physics, for example, the fluctuations of a charged particle lead to the spontaneous emission of energy, while those around the neutral particle stimulate the emission. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1976, "title": "Information-Based Asset Pricing", "abstract": "  A new framework for asset price dynamics is introduced in which the concept of noisy information about future cash flows is used to derive the price processes. In this framework an asset is defined by its cash-flow structure. Each cash flow is modelled by a random variable that can be expressed as a function of a collection of independent random variables called market factors. With each such \"X-factor\" we associate a market information process, the values of which are accessible to market agents. Each information process is a sum of two terms; one contains true information about the value of the market factor; the other represents \"noise\". The noise term is modelled by an independent Brownian bridge. The market filtration is assumed to be that generated by the aggregate of the independent information processes. The price of an asset is given by the expectation of the discounted cash flows in the risk-neutral measure, conditional on the information provided by the market filtration. When the cash flows are the dividend payments associated with equities, an explicit model is obtained for the share-price, and the prices of options on dividend-paying assets are derived. Remarkably, the resulting formula for the price of a European call option is of the Black-Scholes-Merton type. The information-based framework also generates a natural explanation for the origin of stochastic volatility. ", "machine_abstract": "The information-based asset pricing model is an extension to the traditional CAPM (Capital Assets Pricing Model) that incorporates additional factors into its analysis.  The first factor, market risk premium, represents the excess return investors demand for holding risky assets over and above their returns on low-risk investments such as Treasury bills or bonds.   The second factor, size effect, captures differences in expected returns between small-cap stocks and large-cap stocks.   The third factor, value effect, measures the difference in returns between high book-to-market ratio stocks and low book-to-market ratio stocks; this factor has been shown to be particularly important during periods when interest rates are falling.    This article provides a brief overview of these three factors along with some examples of how they can affect investment decisions. The Capital Assets Pricing Model (CAPM), developed by Sharpe (1964), Lintner (1965), Mossin (1966), and Black (1972), is one of the most widely used models in finance today.  It states that the expected rate of return on any given security should equal the sum of two components - the risk-free rate plus a risk premium associated with the level of systematic risk inherent in each security.  In other words, if you hold a portfolio consisting only of risk-free securities then your expected return will simply be the risk-free rate; however, if you hold a diversified portfolio containing both risky and non-risky securities then your expected rate of return will increase proportionately with the amount of risk you take on.  For example, suppose we have a hypothetical investor who holds a portfolio consisting of 50% U.S. Treasury bills and 50% Standard & Poor\u2019s 500 Index Funds.  If the current yield on 10-year Treasuries is 5% per year while the S&P 500 Index earns 10% annually,...", "paraphrased_abstract": "It is called the capital pricing model, and it is used in finance today. It is based on the theory that the expected rate of return of a given security should be equal to the sum of the two components, the risk-free rate and the risk premium for the risk of a systematic risk. If the current yield of the 10-year Treasury bill is 5%, and the S&P 500 index is 5%, then the expected return will be 5%. Moreover, if you have a portfolio consisting of only risk-free securities, then the expected return will be the risk-free rate, while if you have a portfolio consisting of risk-free securities, the expected return will be a certain proportional increase in the proportion of the risks that you take. In this article we have described the three factors, with an example of how they can affect the decisions of an investor. The CAPM, as an extension of the CAPM, was developed by Sharpe, Lintner, Mossin, and Black in 1972, and has been applied to finance since the beginning. The first factor, the market risk premium, represents the excess returns of the investors requesting more risky assets than the returns of the lower-risk investments, such as Treasury bills or notes, and the second factor, the market size, represents the difference between the returns of small- and large-cap stocks. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2259, "title": "Non-equilibrium coherence dynamics in one-dimensional Bose gases", "abstract": "  Low-dimensional systems are beautiful examples of many-body quantum physics. For one-dimensional systems the Luttinger liquid approach provides insight into universal properties. Much is known of the equilibrium state, both in the weakly and strongly interacting regime. However, it remains a challenge to probe the dynamics by which this equilibrium state is reached. Here we present a direct experimental study of the coherence dynamics in both isolated and coupled degenerate 1d Bose gases. Dynamic splitting is used to create two 1d systems in a phase coherent state. The time evolution of the coherence is revealed in local phase shifts of the subsequently observed interference patterns. Completely isolated 1d Bose gases are observed to exhibit a universal sub-exponential coherence decay in excellent agreement with recent predictions by Burkov et al. [Phys. Rev. Lett. 98, 200404 (2007)]. For two coupled 1d Bose gases the coherence factor is observed to approach a non-zero equilibrium value as predicted by a Bogoliubov approach. This coupled-system decay to finite coherence is the matter wave equivalent of phase locking two lasers by injection. The non-equilibrium dynamics of superfluids plays an important role in a wide range of physical systems, such as superconductors, quantum-Hall systems, superfluid Helium, and spin systems. Our experiments studying coherence dynamics show that 1d Bose gases are ideally suited for investigating this class of phenomena. ", "machine_abstract": "We study the non-equilibrium evolution of an interacting onedimensional Bose gas initially prepared in a coherent state with finite particle number fluctuations and show that it exhibits universal features, which are independent of microscopic details such as interactions or initial conditions. We find that the system evolves into a stationary state characterized by non-vanishing density-density correlations at all distances. The time dependence of these correlations is governed by a single parameter, which we identify with the inverse temperature of the final equilibrium state. This allows us to determine this temperature directly from experimental data without any fitting parameters. Our results provide new insights into the nonequilibrium physics of quantum many-body systems and may be tested experimentally using ultracold atoms trapped in optical lattices.  I. INTRODUCTORY REMARK The recent development of techniques for trapping and manipulating cold atomic gases has opened up exciting possibilities for studying strongly correlated quantum matter far from thermal equilibrium [1] . In particular, experiments have demonstrated how isolated quantum systems can evolve towards their ground states [2] , while being driven out of equilibrium by sudden changes in external control parameters [3] . In this work, we consider the case where the system is suddenly quenched across a phase transition [4] . For example, if the particles were originally confined to a harmonic trap, they would expand freely after switching off the confining potential [5] . Alternatively, the system could be initialized in its ground state [6] before undergoing a rapid change in some other parameter (e.g., magnetic field) [7, 8] . In both cases, the subsequent relaxation process will depend crucially on whether the system was initially prepared close to equilibrium [9] or not [10] . If the latter situation applies, then the system typically relaxes towards a metastable state [11] whose properties cannot be inferred from those of the original equilibrium ensemble [12] .", "paraphrased_abstract": "And yet we do not know whether the system is in equilibrium or not, and whether it is in equilibrium. Then, as we have seen, the system will become unstable, and the properties of the equilibrium ensemble will not be discernible. In this work we investigate the non-equilibrium evolution of a single-dimensional Bose gas initially prepared in a coherent state with finite particle density fluctuations, and show that it exhibits universal properties, independent of any specific factors, such as the interactions or the initial conditions. In addition, we show that the equilibrium of the system is governed by a single parameter, the inverse temperature of the equilibrium state. In this case, we are dealing with the case where the system is suddenly quenched by a phase transition. For example, if the particles were originally confined to a harmonic trap, they would subsequently expand free when the magnetic field was switched off. In this case, the equilibrium of the system depends crucially on whether it is first formed in equilibrium or not. The first state is chosen as the equilibrium state, and the second one is stabilized, and is of no appreciable nature, and cannot be inferred from the equilibrium of the original equilibrium. The recent development of atom trapping and manipulating cold atoms has enabled us to study the non-equilibrium of an insoluble atom ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2588, "title": "Gamma-ray probe of cosmic-ray pressure in galaxy clusters and cosmological implications", "abstract": "  Cosmic rays produced in cluster accretion and merger shocks provide pressure to the intracluster medium (ICM) and affect the mass estimates of galaxy clusters. Although direct evidence for cosmic-ray ions in the ICM is still lacking, they produce gamma-ray emission through the decay of neutral pions produced in their collisions with ICM nucleons. We investigate the capability of the Gamma-ray Large Area Space Telescope (GLAST) and imaging atmospheric Cerenkov telescopes (IACTs) for constraining the cosmic-ray pressure contribution to the ICM. We show that GLAST can be used to place stringent upper limits, a few per cent for individual nearby rich clusters, on the ratio of pressures of the cosmic rays and thermal gas. We further show that it is possible to place tight (<~10%) constraints for distant (z <~ 0.25) clusters in the case of hard spectrum, by stacking signals from samples of known clusters. The GLAST limits could be made more precise with the constraint on the cosmic-ray spectrum potentially provided by IACTs. Future gamma-ray observations of clusters can constrain the evolution of cosmic-ray energy density, which would have important implications for cosmological tests with upcoming X-ray and Sunyaev-Zel'dovich effect cluster surveys. ", "machine_abstract": "We present the first measurement of the gamma-ray emission associated with the hot gas in galaxy clusters using data collected by Fermi Large Area Telescope (LAT). We find that the observed gamma-ray luminosity is consistent with theoretical predictions based on the assumption that the energy density of relativistic particles follows closely that of thermal plasma, as expected for hadronic models where cosmic rays are accelerated at shocks driven into intracluster medium by mergers or AGN activity. The inferred cosmic ray pressure is found to be comparable to the thermal pressure within r500 but decreases more rapidly outside this radius. This result suggests that cosmic rays may play an important role in regulating cluster dynamics over large scales. In addition, we use our results together with those obtained previously by Suzaku X-ray observatory to constrain the fraction of nonthermal pressure support in galaxy clusters. Our analysis indicates that the contribution of cosmic rays to total pressure can reach up to 30%-40% inside r500 while it drops below 10% beyond 2r500. Finally, we discuss how these findings could affect constraints on dark matter annihilation cross section derived from observations of galaxy clusters.", "paraphrased_abstract": "We then investigate the correlation between these observations with Suzaku's X-ray observatory. Our results indicate that the concentration of cosmic rays is about 30% to 40%, but in the upper part of the range it falls to less than 10%. This shows that the cosmic rays play a key role in the emanation of dark matter. We show that the gamma ray emission of the hot gas is in accordance with the predictions of the relativistic particles that the density of rays is comparable to that of the thermal plasma, as if the rays are accelerated by the evaporation of the intracluster medium from mergers or AGNs. In this work we present the first measurement of the gamma rays in the hot gas in clusters with data collected by Fermi. The gamma ray fluxes are similar to those of the thermal plasma, and the pressure is lower outside this radius. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1848, "title": "Recent progresses in the simulation of small-scale magnetic fields", "abstract": "  New high-resolution observations reveal that small-scale magnetic flux concentrations have a delicate substructure on a spatial scale of 0.1''. Its basic structure can be interpreted in terms of a magnetic flux sheet or tube that vertically extends through the ambient weak-field or field-free atmosphere with which it is in mechanical equilibrium. A more refined interpretation comes from new three-dimensional magnetohydrodynamic simulations that are capable of reproducing the corrugated shape of magnetic flux concentrations and their signature in the visible continuum. Furthermore it is shown that the characteristic asymmetric shape of the contrast profile of facular granules is an effect of radiative transfer across the rarefied atmosphere of the magnetic flux concentration. I also discuss three-dimensional radiation magnetohydrodynamic simulations of the integral layers from the top of the convection zone to the mid-chromosphere. They show a highly dynamic chromospheric magnetic field, marked by rapidly moving filaments of stronger than average magnetic field that form in the compression zone downstream and along propagating shock fronts. The simulations confirm the picture of flux concentrations that strongly expand through the photosphere into a more homogeneous, space filling chromospheric field. Future directions in the simulation of small-scale magnetic fields are indicated by a few examples of very recent work. ", "machine_abstract": "The origin and evolution of cosmic magnetic fields are one of the most important unsolved problems in astrophysics, which is closely related to many other fundamental issues such as star formation, galaxy formation and structure formation. In this talk I will review our recent work on simulating primordial magnetic fields with different physical processes involved. The first part focuses on the generation of seed magnetic fields during inflation by quantum fluctuations. We show that these seeds can be amplified significantly after reheating due to the magnetohydrodynamic turbulence driven by the decaying inflaton perturbations. Then we discuss how these seeds evolve into large-scale coherent magnetic fields through various mechanisms including inverse cascade, dynamo action and turbulent pumping. Finally, we present some possible observational signatures for future detection. This talk was given at the International Conference on Computation & Theory (ICCT) held in Beijing, China between September 24-27, 2014.", "paraphrased_abstract": "And the first part is a study of the evolution of primordial magnetic fields. It will be based on the theory of quantum fluctuations and will be the first. Then we will explain how these seeds will become large-scale coherent magnetic fields through various mechanisms, including the inverse cascade, the dynamo action, and the turbulent pumping. This is the first work I presented in the international conference on computation and theory, held in Beijing, China, from September 24-27, 2014. The origin and evolution of cosmic magnetic fields is a major problem of astrophysics, and it is closely related to many other fundamental problems, such as star formation, galaxy formation, and structure. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0224, "title": "Evaporation of buffer gas-thermalized anions out of a multipole rf ion trap", "abstract": "  We identify plain evaporation of ions as the fundamental loss mechanism out of a multipole ion trap. Using thermalized negative Cl- ions we find that the evaporative loss rate is proportional to a Boltzmann factor. This thermodynamic description sheds new light on the dynamics of particles in time-varying confining potentials. It specifically allows us to extract the effective depth of the ion trap as the activation energy for evaporation. As a function of the rf amplitude we find two distinct regimes related to the stability of motion of the trapped ions. For low amplitudes the entire trap allows for stable motion and the trap depth increases with the rf field. For larger rf amplitudes, however, rapid energy transfer from the field to the ion motion can occur at large trap radii, which leads to a reduction of the effective trapping volume. In this regime the trap depth decreases again with increasing rf amplitude. We give an analytical parameterization of the trap depth for various multipole traps that allows predictions of the most favorable trapping conditions. ", "machine_abstract": "We report on the evaporation of buffer-gas thermalized ions in a linear quadrupole ion trap (QIT). The QIT is filled with helium buffer gas at pressures between 0 and 1 mbar, which leads to temperatures up to 1000 K for trapped ions. We evaporate the ions by lowering the temperature of the surrounding helium bath down to 300 K within less than one second. This results in a significant reduction of the number density inside the QIT without affecting its trapping properties significantly. In this way we are able to reduce the number of stored ions by more than two orders of magnitude while keeping their kinetic energy below 10 eV per charge state. Our experimental findings agree well with theoretical predictions based on rate equations describing the time evolution of the number densities of all relevant species involved.     Introduction     Multipole radio-frequency ion traps have been used extensively over the past decades as mass spectrometers [1] . They provide high resolution and sensitivity [2] , but they suffer from space-charge effects when storing large numbers of ions [3] . Space charge can be reduced by cooling the ions [4] or by removing them selectively [5] . Cooling requires sophisticated laser systems [6] that may not always be available. Selective removal has been demonstrated using pulsed electric fields [7, 8] , collisions with neutral atoms [9] , photoionization [10] , electron impact ionization [11] , and resonant photodissociation [12] .   In our experiment, we use selective removal via rapid heating of the helium buffer gas [13] . Heating the helium causes the ions to lose their kinetic energy rapidly through elastic collisions [14] . As a result, the ions escape the trap volume before they gain enough energy to cause space charge problems [15] . A similar approach was recently reported [16] where the authors heated the helium buffer gas directly instead of indirectly via the ions [17] .     Herein, we present detailed measurements of the process of evaporative cooling of buffer gas-thermalised ions in a linear quadrupolar ion trap (QIT) [18] . We show how the number density of the ions decreases exponentially after switching off the helium flow into the vacuum chamber containing the", "paraphrased_abstract": "... The ion traps that have been used for the last few decades are now becoming more and more popular, and their accuracy and sensitivity are of great importance, but they also have a hazard, which prevents them from storing any ions. Thus it is not always possible to make the ion traps cool, so they are not always possible, and if the ion traps are cooled, they are removed. The process of evaporating gas-based ions in a quadrupole ion trap is described. It is a gas-filled ion trap filled with helium at a pressure of 0 to 1 mbar, at which the trapped ions reach a temperature of 900 K. This temperature is then reduced to 300 K, which is about half the time the number of the ions is evaporated. In this way, the number of the ions decreases by about two orders of magnitude, but the charge of the ion is not high enough to increase the kinetic energy of the ion. We show here how the ion traps with helium are thermalized in a linear quadrupole ion trap. In the past, multipole ion traps have been used for mass spectrometry, but they have a problem with space", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2461, "title": "GRB blastwaves through wind-shaped circumburst media", "abstract": "  Context:A significant fraction of progenitors for long gamma-ray bursts (GRBs) are believed to be massive stars. The investigation of long GRBs therefore requires modeling the propagation of ultra-relativistic blastwaves through the circumburst medium surrounding massive stars. We simulate the expansion of an isotropic, adiabatic relativistic fireball into the wind-shaped medium around a massive GRB progenitor. The circumburst medium is composed of a realistically stratified stellar wind zone up to its termination shock, followed by a region of shocked wind characterized by a constant density. Aims: We followed the evolution of the blastwave through all its stages, including the extremely rapid acceleration up to a Lorentz factor 75 flow, its deceleration by interaction with stellar wind, its passage of the wind termination shock, until its propagation through shocked wind. Methods: We used the adaptive mesh refinement versatile advection code to follow the evolution of the fireball. Results: We show that the acceleration from purely thermal to ultra-relativistic kinetic regimes is abrupt and produces an internally structured blastwave. We resolved the structure of this ultra-relativistic shell in all stages, thanks to the adaptive mesh. We comment on the dynamical roles played by forward and reverse shock pairs in the phase of interaction with the free stellar wind and clearly identify the complex shock-dominated structure created when the shell crosses the terminal shock. Conclusion: We show that in our model where the terminal shock is taken relatively close to the massive star, the phase of self-similar deceleration of Blandford-McKee type can only be produced in the constant density, shocked wind zone. ", "machine_abstract": "We present the results of our analysis on GRB 080916C, one of the most energetic bursts ever detected by Swift/BAT and Fermi/GBM. We find that this burst is consistent with being produced in an off-axis jet viewed at an angle \u03b8 ~ 60\u00b0 to its axis. The observed light curve can be explained as emission from two components: (1) A bright component which peaks early during the prompt phase; it has a duration T90 = 1 s and a fluence F\u03b3 = 2\u00d710\u22126 erg cm\u22122. (2) An extended tail lasting for several hundred seconds after the end of the prompt phase; it contains about half of the total energy emitted by the source. Using detailed modeling we show that both these features are naturally reproduced if the burst was generated within a dense stellar wind environment surrounding a Wolf-Rayet star. In particular, we demonstrate how the density profile of such winds leads to a double-peaked structure in the time integrated spectrum of the burst.", "paraphrased_abstract": "There are two types of light emission: one brighter component which is emitted at the start of the activity; it has a duration of T90 of s, and a fluence of F = 2106 erg cm2; the second component, which emitted several hundred seconds after the activity was complete, is about half the energy of the source. We prove that the observed light can be explained by two types of emitted light: (1) a bright emitted light that peaks in the activity early, with a duration T90 of s, and a fluence F of 2106 erg cm2; (2) a long tail, some hundred seconds after the activity has finished, which contains about half of the energy of the source. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3661, "title": "Virtual photons in imaginary time: Computing exact Casimir forces via standard numerical-electromagnetism techniques", "abstract": "  We describe a numerical method to compute Casimir forces in arbitrary geometries, for arbitrary dielectric and metallic materials, with arbitrary accuracy (given sufficient computational resources). Our approach, based on well-established integration of the mean stress tensor evaluated via the fluctuation-dissipation theorem, is designed to directly exploit fast methods developed for classical computational electromagnetism, since it only involves repeated evaluation of the Green's function for imaginary frequencies (equivalently, real frequencies in imaginary time). We develop the approach by systematically examining various formulations of Casimir forces from the previous decades and evaluating them according to their suitability for numerical computation. We illustrate our approach with a simple finite-difference frequency-domain implementation, test it for known geometries such as a cylinder and a plate, and apply it to new geometries. In particular, we show that a piston-like geometry of two squares sliding between metal walls, in both two and three dimensions with both perfect and realistic metallic materials, exhibits a surprising non-monotonic ``lateral'' force from the walls. ", "machine_abstract": "We present an efficient method for computing the vacuum energy and force between two parallel plates using only standard numerical electromagnetism methods, without resorting to any approximations or special treatments such as analytic continuation into complex frequencies. The key idea is that we can use the Feynman-Kac formula to express the vacuum expectation value of the stress tensor at finite temperature T = 1/\u03b2 (where \u03b2 is the inverse temperature) in terms of the corresponding quantity at zero temperature but with one additional term involving the time evolution operator over a period of length \u03b2 . We then show how this expression may be evaluated efficiently by expressing it in terms of the Green's function of Maxwell's equations on a periodic domain. This allows us to compute the vacuum energy and force exactly within our computational framework, which consists of solving the vector wave equation numerically on a rectangular grid. Our results are compared against those obtained previously using other approaches, including analytic continuation into complex frequencies and the PFA.", "paraphrased_abstract": "We present a method for the computation of the vacuum force and the force of two parallel plates by means of a simple formula of the Feynman-Kac equation. This formula is based on the T \u20131/ ( is the inverse temperature) and adds an additional term: the time evolution operator is a constant length. We then apply this formula to the corresponding value of the stress tensor at a finite temperature T = 1/ ( is the inverse temperature) in terms of the equivalent value of the corresponding value at zero temperature, but adds an additional term to it: the Green function of Maxwell\u2019s equations. The results we present are compared with those obtained by other methods, namely the analytic continuation of complex frequencies, and the PFA. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3401, "title": "A model for the Globular Cluster extreme anomalies", "abstract": "  In spite of the efforts made in the latest years, still there is no comprehensive explanation for the chemical anomalies of globular cluster stars. Among these, the most striking is oxygen depletion, which reaches values down to [O/Fe]~-0.4 in most clusters, but in M13 it goes down to less than [O/Fe]~ - 1. In this work we suggest that the anomalies are due to the super position of two different events: 1) PRIMORDIAL SELF-ENRICHMENT: this is asked to explain the oxygen depletion down to a minimum value [O/Fe]~ -0.4; 2) EXTRA MIXING IN A FRACTION OF THE STARS ALREADY BORN WITH ANOMALOUS COMPOSITION: these objects, starting with already low [O/Fe], will reduce the oxygen abundance down to the most extreme values. Contrary to other models that invoke extra mixing to explain the chemical anomalies, we suggest that it is active only if there is a fraction of the stars in which the primordial composition is not only oxygen depleted, but also extremely helium rich (Y~ 0.4), as found in a few GCs from their main sequence multiplicity. We propose that the rotational evolution (and an associated extra mixing) of extremely helium rich stars may be affected by the fact that they develop a very small or non existent molecular weight barrier during the evolution. We show that extra mixing in these stars, having initial chemistry that has already been CNO processed, affects mainly the oxygen abundance, and to a much smaller extent if affects the sodium abundance. The model also predicts a large fluorine depletion concomitant with the oxygen depletion, and a further enhancement of the surface helium abundance, which reaches values close to Y=0.5 in the computed models. We stress that, in this tentative explanation, those stars that are primordially O--depleted, but ARE NOT extremely helium rich do not suffer deep extra mixing. ", "machine_abstract": "We present an explanation to the observed anomalies in globular cluster luminosity functions (GCLFs) and mass-to-light ratios, based on the assumption that these clusters are composed by two different populations with distinct chemical compositions.  We show how this hypothesis can be tested using photometric data obtained at near-infrared wavelengths. The proposed scenario is able to explain all the main observational features associated to GCLFs:  i) the presence of a peaked distribution; ii) its width; iii) the existence of a tail towards high luminosities; iv) the lack of low-luminosity stars. In addition, it also explains why some GCs have very large values of M/LV . Finally we discuss possible implications of our results regarding the formation history of globulars. Keywords: Globular cluster, Mass-to-light ratio, Luminosity function, Chemical composition, Near-infrared", "paraphrased_abstract": "The proposed solution explains all the most obvious occurrences in globular clusters: i) a peaked distribution, ii) a width, iii) a crest towards high luminosities, iv) the absence of low luminosities. Then we discuss the possible implications of our results for the formation of globular clusters. We present an explanation of the observed variations in the globular cluster luminosity and mass-to-light ratios based on the assumption that globular clusters are composed of two different populations with different chemical compositions. We prove this hypothesis by analyzing photometric data at the wavelengths of the visible spectrum. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2004, "title": "TrES-3: A Nearby, Massive, Transiting Hot Jupiter in a 31-Hour Orbit", "abstract": "  We describe the discovery of a massive transiting hot Jupiter with a very short orbital period (1.30619 d), which we name TrES-3. From spectroscopy of the host star GSC 03089-00929, we measure T_eff = 5720 +- 150 K, logg=4.6 +- 0.3, and vsini < 2 km/s, and derive a stellar mass of 0.90 +- 0.15 M_sun. We estimate a planetary mass of 1.92 +- 0.23 M_Jup, based on the sinusoidal variation of our high-precision radial velocity measurements. This variation has a period and phase consistent with our transit photometry. Our spectra show no evidence of line bisector variations that would indicate a blended eclipsing binary star. From detailed modeling of our B and z photometry of the 2.5%-deep transits, we determine a stellar radius 0.802 +- 0.046 R_sun and a planetary radius 1.295 +- 0.081 R_Jup. TrES-3 has one of the shortest orbital periods of the known transiting exoplanets, facilitating studies of orbital decay and mass loss due to evaporation, and making it an excellent target for future studies of infrared emission and reflected starlight. ", "machine_abstract": "We report the discovery and characterization of TrES-3b, an extrasolar planet with mass M = 1.3 MJup orbiting its host star every 31 hours at 0.081 AU (1.7 stellar radii). The planet is one of only two known transiting planets that are more massive than Saturn but less massive than Neptune; it has a radius R = 2.2 RJup and effective temperature T eff = 2400 K. We find no evidence for additional bodies in this system using radial velocity measurements taken over three years. This planet's proximity to Earth makes it an excellent target for atmospheric studies.     Keywords: Extrasolar planet - Discovery - Transit photometry - Radial velocities - Atmosphere - Planetary systems         Introduction     In recent years there have been several discoveries of giant exoplanets with orbital periods shorter than four days. These short-period planets are particularly interesting because they may be tidally locked into synchronous rotation about their axes, which would lead to strong day-night contrasts on their surfaces. Furthermore, these planets' atmospheres will experience extreme conditions due to high temperatures and intense radiation fields. As such, understanding how planetary atmospheres respond under these circumstances can provide important insights into processes occurring within our own Solar System as well as other planetary systems.     Here we present the discovery and initial characterization of TrES-3b; a hot Jupiter with a period P = 3.09 d discovered by the transit method. Using follow-up observations made with the Spitzer Space Telescope, we show that TrES-3b orbits close enough to its parent star so that tidal forces should synchronize the planet's spin axis with its orbital angular momentum vector. However, we do not detect any significant infrared excess emission associated with the planet itself or its host star, indicating that either the planet does not possess a large amount of dusty material surrounding it and/or that the planet is too cool to produce detectable thermal emission beyond 4 microns.", "paraphrased_abstract": "A few years ago, some large exoplanets with shorter orbits were discovered, and they were of a particularly large size, and a relatively low mass, and their orbits were synchronized with their orbital angular momentum, which, on the contrary, gave the impression that the tidal forces were sufficiently small and that the angular momentum was distorted, so that there was not a large amount of dust around the planet, and that there was a high level of thermal evaporation, and that the atmosphere was extremely sensitive to the influence of the sun. This was the first exoplanets of the planets with orbits less than four days, and they were of particularly interest because they were tidally locked up in a synchronized rotation around the axes, which was accompanied by a strong night-night contrast. In addition, the atmosphere was in danger from the high temperatures and the intense radiation, and the atmosphere could not be understood under such circumstances, but it was in our Solar System and other planetary systems. During this study we also found the transitory Jupiter of TrES-3b, a hot Jupiter with a period of P = 3.09 d. We also found that it was close to the Earth. Using Spitzer\u2019s tidal spectra, we found that TrES-3", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1891, "title": "The influence of AGN nuclear parameters on the FRI/FRII dichotomy", "abstract": "  We have investigated the influence of nuclear parameters such as black hole (BH) mass and photoionizing luminosity on the FRI/FRII transition in a sample of nearby (z<0.2) 3CR radio galaxies. The sample was observed with medium-resolution, optical spectroscopy and contains some galaxies with unpublished velocity dispersion measurements and emission-line fluxes. Measured velocity dispersions are 130-340 km/s with a mean of 216 km/s. Converting to BH mass, we find that the BH mass distribution is identical for FRIs and FRIIs, with a mean of approximately 2.5x10^8 Msun. We convert [OII] and [OIII] emission-line luminosities to photoionizing luminosity under the assumption that the gas is ionized by the nuclear UV continuum. Most of the galaxies with FRI morphology and/or low-excitation emission-line spectra have progressively lower BH masses at lower photoionizing (and jet) luminosities. This agrees with the Ledlow-Owen relation which states that the radio luminosity at the FRI/FRII transition depends on the optical luminosity of the host, L_radio ~ L_optical^1.8, because both L_radio and L_optical relate to AGN nuclear parameters. When recasting the Ledlow-Owen relation into BH mass versus photoionizing and jet luminosity, we find that the recasted relation describes the sample quite well. The FRI/FRII transition occurs at approximately an order of magnitude lower luminosity relative to the Eddington luminosity than the soft-to-hard transition in X-ray binaries. This difference is consistent with the Ledlow-Owen relation, which predicts a weak BH mass dependence in the transition luminosity. We conclude that the FRI/FRII dichotomy is caused by a combination of external and nuclear factors, with the latter dominating. ", "machine_abstract": "We study how the properties of active galactic nuclei (AGNs) affect their radio morphologies, and in particular whether they can explain the observed difference between Fanaroff-Riley type I (FRI) and II (FRII). We use high-resolution hydrodynamical simulations to follow the growth of supermassive black holes (SMBHs), which are fed by cold gas accretion at rates that depend on the SMBH mass and its environment. The resulting jets are simulated using relativistic magnetohydrodynamics with an adiabatic index of 4/3. Our results show that FRIs have lower jet powers than FRIIs for similar BH masses because of differences in the accretion rate onto the central BH. This is consistent with observations showing that FRIs typically reside in less massive galaxies compared to FRIIs. In addition, we find that FRIs produce more collimated jets due to higher magnetic field strengths close to the BH horizon.", "paraphrased_abstract": "The elongated jets are generated in a manner similar to those of the FRI and the IRI, and the elongated jets are created due to the higher magnetic field near the elongated horizon. Using the most detailed hydrodynamics, we study the growth of supermassive black holes (SMBHs), which are accreted by cold gas at rates different from that of the SMBH and its surroundings. This is consistent with our observations that the IBGs are usually found in a smaller number of massive galaxies than the IBGs. Our research investigates the effect of active galactic nuclei on their radio morphology, especially whether the difference between the IBG and the IIBG is explained by the difference between FRI and IBG. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0903, "title": "New possible properties of atomic nuclei investigated by non linear methods: Fractal and recurrence quantification analysis", "abstract": "  For the first time we apply the methodologies of nonlinear analysis to investigate atomic matter. We use these methods in the analysis of Atomic Weights and of Mass Number of atomic nuclei. Using the AutoCorrelation Function and Mutual Information we establish the presence of nonlinear effects in the mechanism of increasing mass of atomic nuclei considered as a function of the atomic number. We find that increasing mass is divergent, possibly chaotic. We also investigate the possible existence of a Power Law for atomic nuclei and, using also the technique of the variogram, we conclude that a fractal regime could superintend to the mechanism of increasing mass for nuclei. Finally, using the Hurst exponent, evidence is obtained that the mechanism of increasing mass in atomic nuclei is in the fractional Brownian regime. The most interesting results are obtained by using Recurrence Quantification Analysis (RQA). New recurrences, psudoperiodicities, self-resemblance and class of self-similarities are identified with values of determinism showing oscillating values indicating the presence of more or less stability during the process of increasing mass of atomic nuclei. In brief, new regimes of regularities are identified for atomic nuclei that deserve to be studied by future researches. In particular an accurate analysis of binding energy values by nonlinear methods is further required. ", "machine_abstract": "The present work is devoted to the investigation of new possible nuclear properties using nonlinear methods, namely fractal dimension (FD) and recurrence quantification analysis (RQA). The FD was calculated for different mass number A in order to study its dependence on the system size. It has been found that there exists an optimal value of the scaling parameter which provides the best results for each nucleus separately. This fact indicates that the structure of the studied systems can be characterized as multifractals with non-trivial behavior. In addition we have shown that the obtained values are very close to those predicted by the percolation theory. We also applied RQA method to investigate the temporal evolution of the considered systems. Our calculations show that the complexity of the time series increases with increasing mass number A. Moreover it turns out that this increase follows closely the predictions made within the framework of the Random Matrix Theory. Finally we have compared our results with experimental data available for some light nuclei.", "paraphrased_abstract": "This finding indicates that the structure of the studied systems is multi-fractal with a non-trivial character. We also developed a new approach to studying the structure of the atoms, namely the fractal dimension, the recurrence dimension, and the RQA, for a variety of mass A. This method was tested for different masses to see how it is affected by the size of the atom. The results show that the structure of the atoms increases with the increase in the number of atoms. We have also developed a recurrence dimension, which is used to estimate the complexity of the time series of the atoms, and in the same way as predicted by the Random Matrix Theory. In this study, we examined the properties of the nuclear system by using nonlinear methods, such as fractal dimension and recurrence quantification. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3759, "title": "A modified cable formalism for modeling neuronal membranes at high frequencies", "abstract": "  Intracellular recordings of cortical neurons in vivo display intense subthreshold membrane potential (Vm) activity. The power spectral density (PSD) of the Vm displays a power-law structure at high frequencies (>50 Hz) with a slope of about -2.5. This type of frequency scaling cannot be accounted for by traditional models, as either single-compartment models or models based on reconstructed cell morphologies display a frequency scaling with a slope close to -4. This slope is due to the fact that the membrane resistance is \"short-circuited\" by the capacitance for high frequencies, a situation which may not be realistic. Here, we integrate non-ideal capacitors in cable equations to reflect the fact that the capacitance cannot be charged instantaneously. We show that the resulting \"non-ideal\" cable model can be solved analytically using Fourier transforms. Numerical simulations using a ball-and-stick model yield membrane potential activity with similar frequency scaling as in the experiments. We also discuss the consequences of using non-ideal capacitors on other cellular properties such as the transmission of high frequencies, which is boosted in non-ideal cables, or voltage attenuation in dendrites. These results suggest that cable equations based on non-ideal capacitors should be used to capture the behavior of neuronal membranes at high frequencies. ", "machine_abstract": "We present an extension to the cable model that allows one to account for membrane dynamics in neurons with active conductances over a wide range of time scales and spatial dimensions, including those relevant to subthreshold electrogenesis. The proposed approach is based on a reformulation of the cable equation as a system of first-order differential equations describing voltage changes along the axon or dendrite. This formulation leads naturally to a generalization of the concept of space constants to include both passive and active components of the membrane impedance. We show how this generalized cable formalism can be used to describe propagation of action potentials through a single compartment neuron with Hodgkin-Huxley type currents. Finally we demonstrate its utility by applying it to study spatiotemporal patterns of activity in networks of coupled neurons. Neurons are highly specialized cells capable of generating electrical signals known as action potentials (APs). These APs propagate down the length of the cell's axon toward synaptic terminals where they trigger release of neurotransmitters into the synapse. In turn these transmitters bind to receptors located on the postsynaptic side of the synapse initiating signaling cascades which ultimately lead to generation of new APs. Thus information transfer between neurons occurs via propagating APs across chemical synapses.", "paraphrased_abstract": "The axons of the neurons are highly complex and capable of sending electrical signals known as action potentials (APs) to the neurons. APs propagate along the length of the axon to the synaptic terminals, where they bind to the neurons, forming a signaling cascade, and thus the information is transmitted across chemical synapses. The extension of the cable equation is then presented as a system of differential equations of voltages across the axon or dendrite, and as a result, we generalize the concept of space constants to include both passive and active components of the membrane impedance. We demonstrate the utility of this generalized cable formalism by applying it to a single neuron, in which the Hodgkin-Huxley currents are produced. This generalized cable formalism is shown to be applied to the phenomenon of a single compartment neuron, with a Hodgkin-Huxley current. We show how the generalized cable formalism can be applied to the study of synapse-related activity. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0968, "title": "Non-adiabatic transport in a quantum dot turnstile", "abstract": "  We present a theoretical study of the electronic transport through a many-level quantum dot driven by time-dependent signals applied at the contacts to the leads. If the barriers oscillate out of phase the system operates like a turnstile pump under a finite constant bias, as observed in the experiments of Kouwenhoven {\\it et al.} [Phys. Rev. Lett. {\\bf 67}, 1626 (1991)]. The time-dependent currents and their averages over succesive pumping periods are computed from the Keldysh formalism for tight-binding models. The calculation considers a sudden application of the pumping potentials at $t=0$ which leads to transient features of the time-dependent and averaged currents during the first pumping cycles which turn out to be important in the high-frequency regime. We show that in the transient regime the efficiency of the system as a pump is rather poor because it mainly absorbs charge from both leads in order to fill the levels located below the bias window. Under a finite bias and a low-frequency pumping signal the charge transferred across the system depends on the number of levels located within the bias window. The internal charge dynamics and the role of energy sidebands are investigated. The so called satellite peaks of the averaged current are observed also in the transient regime. ", "machine_abstract": "We study the non-adiabatic current through an open quantum dot connected to two leads with different chemical potentials, which is driven by time-dependent gate voltages applied on both sides of the dot. We show that this system can be described as a \"turnstile\" where electrons are pumped between the leads when the driving frequencies match certain resonances. The effect is robust against disorder and dephasing. This work was supported by NSERC (Canada) and CIFAR (Canadian Institute for Advanced Research). In recent years there has been growing interest in studying electron pumps based on semiconductor nanostructures such as quantum dots or carbon nanotubes [1, 2] . These devices have potential applications ranging from metrology [3] , single-electron transistors [4] , and spintronics [5] . In these systems, charge carriers are transported across the device via sequential tunneling processes [6] . A number of theoretical studies [7, 8] have shown that it is possible to achieve high efficiency in these devices even at room temperature [9] . However, most previous works focused only on adiabatic pumping [10] , i.e., the case where the frequency of the external drive is much smaller than all other relevant energy scales [11] . Recently, several experiments [12, 13] reported large currents generated by nonadiabatic pumping [14, 15] . It remains unclear whether these results can be explained within existing theories [16] . Here we consider a simple model of a quantum dot connected to two metallic leads [see Fig. 1(a) ] [17] . The dot level is modulated periodically by applying oscillating gate voltages V L/R = \u00b1V 0 cos \u03c9t on each side of the dot [18] . When the modulation period T \u2261 2\u03c0/\u03c9 matches one of the dwell times \u03c4 n = \u03c0 /[2(E F \u2212 E n )] associated with the discrete levels E n of the isolated dot, electrons will be transferred coherently between the left and right leads [19] . Here E F denotes the Fermi energy of the leads [20] . As illustrated schematically in Figs. 1(b-c), depending on", "paraphrased_abstract": "In recent years, interest has grown to study electron-pump devices made of carbon nanotubes or carbon nanotubes, which are suitable for electron-pump devices, for example, in the field of metrology, for single electron-voltages and spintronics. Moreover, there has been considerable interest in electro-hydro-electric devices based on semiconductor nanostructures such as quantum dots or carbon nanotubes, for which electrons are carried by the channel through a series of successive tunnelings. We have studied the non-adiabatic current in an open quantum dot connected to two metal leads, which have different chemical potentials, and which, in accordance with the constant amplitude of the external voltage, carries the electrons between the leads at certain resonances. In this way, we show that the nonadiabatic current is not adiabatic, but is produced by an open quantum dot, connected to two leads of different chemical potentials, which are driven by time-varying voltages on both sides of the dot, and that the voltage on both sides is regulated by a slew of amplitudes, as well as by a resonant current, that is, by a certain amount of energy at a certain moment. This is a typical example of a crystalline crystalline dot connected to two leads of different chemical potential", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4381, "title": "Holography in Three-dimensional Kerr-de Sitter Space with a Gravitational Chern-Simons Term", "abstract": "  The holographic description of the three-dimensional Kerr-de Sitter space with a gravitational Chern-Simons term is studied, in the context of dS/CFT correspondence. The space has only one (cosmological) event horizon and its mass and angular momentum are identified from the holographic energy-momentum tensor at the asymptotic infinity. The thermodynamic entropy of the cosmological horizon is computed directly from the first law of thermodynamics, with the usual Hawking temperature, and it is found that the usual Gibbons-Hawking entropy is modified. It is remarked that, due to the gravitational Chern-Simons term, (a) the results go beyond analytic continuation from AdS, (b) the maximum-mass/N-bound conjecture may be violated, and (c) the three-dimensional cosmology is chiral. A statistical mechanical computation of the entropy, from a Cardy-like formula for a dual CFT at the asymptotic boundary, is discussed. Some technical difference in the Chern-Simons energy-momentum tensor, from literatures is remarked also. ", "machine_abstract": "We study the holographic entanglement entropy for three-dimensional de Sitter space with gravitational Chern-Simons term by using the replica trick and the covariant phase-space method. We find that there is no logarithmic correction to the entanglement entropy, which agrees with previous results obtained via other methods. In addition, we show that the first-order corrections are proportional to the square root of the volume enclosed by the entangling surface. Finally, we calculate the second-order corrections and obtain an expression containing two terms. One of them has been previously found in Ref. [Phys. Rev. D 98 (2018) 084011] while another one is new. The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points. This result shows that the gravitational Chern-Simons coupling constant plays a role similar to the Newton's constant in four dimensions.", "paraphrased_abstract": "Xuande has been known to the whole of the world, and has been found in the Library of Congress, as well as the Library of Congress. The result is that the gravitational Chern-Simons coupling constant is in four dimensions a similar constant to Newton's constant. The result is that the gravitational Chern-Simons coupling constant has the same role as the Newton constant. We have obtained a holographic entanglement entropy in three dimensions, in which we have applied the principle of the replica and the covariant phase-space, and find that there is no logarithmic correction to the entanglement entropy, which is in keeping with the previous results obtained by other methods. The second-order corrections are compared with the square root of the volume which entangles the entangling surface. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2756, "title": "The nature of a broad line radio galaxy: Simultaneous RXTE and Chandra HETG observations of 3C 382", "abstract": "  We present the results from simultaneous chandra and rxte observations of the X-ray bright Broad-Line Radio Galaxy (BLRG) 3C 382. The long (120 ks) exposure with chandra HETG allows a detailed study of the soft X-ray continuum and of the narrow component of the Fe Kalpha line. The rxte PCA data are used to put an upper limit on the broad line component and constrain the hard X-ray continuum. A strong soft excess below 1 keV is observed in the time-averaged HETG spectrum, which can be parameterized with a steep power law or a thermal model. The flux variability at low energies indicates that the origin of the soft excess cannot be entirely ascribed to the circumnuclear diffuse emission, detected by chandra on scales of 20-30 arcsec (22-33 kpc). A narrow (sigma<90 eV) Fe Kalpha line (with EW< 100 eV) is observed by the chandra HEG. Similar values for the line parameters are measured by the rxte PCA, suggesting that the contribution from a broad line component is negligible. The fact that the exposure is split into two observations taken three days apart allows us to investigate the spectral and temporal evolution of the source on different timescales. Significant flux variability associated with spectral changes is observed on timescales of hours and days. The spectral variability is similar to that observed in radio-quiet AGN ruling out a jet-dominated origin of the X-rays. ", "machine_abstract": "We present the results of simultaneous X-ray (Chandra) and radio (RXTE )observations of the Broad Line Radio Galaxy, 3C382 . The data were taken on 2001 September 24-25 UT during an outburst in which the source was detected at radio frequencies as high as 22 GHz. We find that the X-ray spectrum is well described by a power law with photon index \u0393 = 1.7 \u00b1 0.1 modified by photoelectric absorption consistent with N_H = 2 x 1022 cm-2. There are no significant spectral changes between the two epochs observed. In addition to the continuum emission we detect several narrow lines including Fe K\u03b1 , He-like Si XIII , S XV and Ar XVII . These features appear blueshifted relative to their rest wavelengths indicating bulk motion towards us along our line-of-sight. Using these velocities together with estimates for the mass of the central black hole derived from optical measurements we estimate the distance of the emitting material from the center of the AGN to be ~10 light days.", "paraphrased_abstract": "We have now presented the results of simultaneous X-ray and radio measurements of the Broad Line Radio Galaxy, 3C382. Using these data and the estimates for the mass of the central black hole, we estimate a distance of 10 light days from the center of the AGN. Besides the spectral spectrum, we observe a few narrow bands, including Fe K, He-like Si XIII, S XV and Ar XVII. They appear blue, shifted in the direction of their surroundings, and show the movement of the bulk along the horizon. We present the results of the simultaneous X-ray and radio measurements of the Broad-Line Radio Galaxy, 3C382. The data were collected on September 24 to 25 in 2001, in which the source was seen at radio frequencies of up to 22 GHz. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0022, "title": "Exact half-BPS Type IIB interface solutions I: Local solution and supersymmetric Janus", "abstract": "  The complete Type IIB supergravity solutions with 16 supersymmetries are obtained on the manifold $AdS_4 \\times S^2 \\times S^2 \\times \\Sigma$ with $SO(2,3) \\times SO(3) \\times SO(3)$ symmetry in terms of two holomorphic functions on a Riemann surface $\\Sigma$, which generally has a boundary. This is achieved by reducing the BPS equations using the above symmetry requirements, proving that all solutions of the BPS equations solve the full Type IIB supergravity field equations, mapping the BPS equations onto a new integrable system akin to the Liouville and Sine-Gordon theories, and mapping this integrable system to a linear equation which can be solved exactly. Amongst the infinite class of solutions, a non-singular Janus solution is identified which provides the AdS/CFT dual of the maximally supersymmetric Yang-Mills interface theory discovered recently. The construction of general classes of globally non-singular solutions, including fully back-reacted $AdS_5 \\times S^5$ and supersymmetric Janus doped with D5 and/or NS5 branes, is deferred to a companion paper. ", "machine_abstract": "We present the first exact local solution to the equations of motion for type II B supergravity in AdS 5 \u00d7 S 5 . The solution is supported by two independent harmonic functions, one on each side of an arbitrary surface \u03a3 which divides space into three regions. We show that this solution preserves eight real supercharges and has vanishing central charge. It can be interpreted as a bound state of N = 4 SYM theory with gauge group SU(N)\u00d7SU(N), where the number of degrees of freedom scales like O(N 2 ) at large N. In addition we find a new class of solutions describing interfaces between different vacua of the same field theory. These are obtained by taking appropriate limits of our general solution. They preserve four supercharges and have non-vanishing central charges. One particular member of this family describes a supersymmetric Janus-like configuration interpolating between two distinct conformal fixed points of the same field theory.  Introduction  The study of holographic duals of strongly coupled quantum systems has been greatly advanced over recent years through the use of string/M-theory [1, 2] . A particularly interesting application of these ideas involves studying non-conformal theories using their dual description in terms of gravitational backgrounds [3, 4] . In order to construct such models it is necessary to solve the equations of motion associated with the relevant supergravity or gauged supergravity theory. This problem becomes more tractable when considering specific classes of solutions preserving some fraction of the original supersymmetry [5] , since only certain combinations of fields may then appear [6] . For example, if one considers configurations preserving all but one of the original supersymmetries (BPS states), then the resulting system will depend upon just five scalar fields [7, 8] . However, even in this case finding explicit solutions remains difficult [9] . One approach to solving BPS-type problems is to consider special cases where the geometry admits additional symmetries [10] . An important subclass of such solutions arises when the internal manifold M 6 factorises into a product of two spaces M 3 \u00d7 M 3 [11] . In this", "paraphrased_abstract": "The study of the holographic duals of strongly coupled, quantum systems has been greatly advanced over the past few years through the application of string/Mtheory. A particularly interesting application of these theories is to develop the dual description of gravitational backgrounds. To reach this logical solution, it is necessary to find a solution for a purely sub-symmetrical problem, a solution in which the internal manifold M6 is multiplied by the product of two spaces M 3  M3. The solution is in the form of two independent harmonic functions, one on each side of an arbitrary surface, divided by three regions. This solution preserves eight supercharges and no central charge. One of these solutions consists in a supersymmetric Janus-like configuration, which is derived from two distinct conformal fixed points of the same field theory. In this regard, the difficulties of defining the exact solution are difficult to find. A new type of solution is based on a special case where the geometry permits additional symmetries, which, in the course of the solution, may be resolving the simplest and most difficult problem of the time. The first solution of the equations of motion of type II B is given by AdS 5, S 5, S 5, and is supported by two independent harmonic functions, one on each side of a surface, which divides space", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2822, "title": "Homogenized spectral problems for exactly solvable operators: asymptotics of polynomial eigenfunctions", "abstract": "  Consider a homogenized spectral pencil of exactly solvable linear differential operators $T_{\\la}=\\sum_{i=0}^k Q_{i}(z)\\la^{k-i}\\frac {d^i}{dz^i}$, where each $Q_{i}(z)$ is a polynomial of degree at most $i$ and $\\la$ is the spectral parameter. We show that under mild nondegeneracy assumptions for all sufficiently large positive integers $n$ there exist exactly $k$ distinct values $\\la_{n,j}$, $1\\le j\\le k$, of the spectral parameter $\\la$ such that the operator $T_{\\la}$ has a polynomial eigenfunction $p_{n,j}(z)$ of degree $n$. These eigenfunctions split into $k$ different families according to the asymptotic behavior of their eigenvalues. We conjecture and prove sequential versions of three fundamental properties: the limits $\\Psi_{j}(z)=\\lim_{n\\to\\infty} \\frac{p_{n,j}'(z)}{\\la_{n,j}p_{n,j}(z)}$ exist, are analytic and satisfy the algebraic equation $\\sum_{i=0}^k Q_{i}(z) \\Psi_{j}^i(z)=0$ almost everywhere in $\\bCP$. As a consequence we obtain a class of algebraic functions possessing a branch near $\\infty\\in \\bCP$ which is representable as the Cauchy transform of a compactly supported probability measure. ", "machine_abstract": "We study the asymptotic behavior as $|x|\\to\\infty$ of solutions to certain homogenized spectral problems associated with Schr\u00f6dinger-type operators on unbounded domains in $R^n$, $n\\geq 3$.  We show that if the potential is periodic and satisfies some mild decay conditions at infinity then there exists an explicit formula for the leading term of this expansion which involves only the first two terms of the Taylor series of the potential about zero. This result generalizes previous results obtained by one of us (S.V.) for potentials satisfying stronger decay assumptions near infinity.  The proof relies heavily on the fact that the spectrum of these operators consists entirely of eigenvalues of finite multiplicity. The main idea behind our approach is to use the method of matched asymptotic expansions together with suitable integral representations of the solution to obtain information about its large-$x$ behavior. In particular we prove that under appropriate hypotheses on the coefficients of the operator the solution has an exponential decay rate at infinity.", "paraphrased_abstract": "The main idea of this method is to use a method of matched asymptotically symmetrical expansion with suitable integrals of the solution, and thus obtain information about the large x behavior. We study the asymptotically symmetrical behavior of solutions to the homogeneous spectral problems of Schr\u00f6dinger operators on the unbounded domains of Rn, Ngeq 3 in which the potential is periodic and satisfying some decay conditions, then there exists an explicit formula for the leading term of the expansion, which is the first two terms of the Taylor series of the potential at zero. This result generalizes the earlier results of one of us (S.) in the case of potentials satisfying the stronger decay conditions near infinity. This proof relies on the fact that the spectrum of the operators consists exclusively of finite numbers. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3985, "title": "Dirichlet sigma models and mean curvature flow", "abstract": "  The mean curvature flow describes the parabolic deformation of embedded branes in Riemannian geometry driven by their extrinsic mean curvature vector, which is typically associated to surface tension forces. It is the gradient flow of the area functional, and, as such, it is naturally identified with the boundary renormalization group equation of Dirichlet sigma models away from conformality, to lowest order in perturbation theory. D-branes appear as fixed points of this flow having conformally invariant boundary conditions. Simple running solutions include the paper-clip and the hair-pin (or grim-reaper) models on the plane, as well as scaling solutions associated to rational (p, q) closed curves and the decay of two intersecting lines. Stability analysis is performed in several cases while searching for transitions among different brane configurations. The combination of Ricci with the mean curvature flow is examined in detail together with several explicit examples of deforming curves on curved backgrounds. Some general aspects of the mean curvature flow in higher dimensional ambient spaces are also discussed and obtain consistent truncations to lower dimensional systems. Selected physical applications are mentioned in the text, including tachyon condensation in open string theory and the resistive diffusion of force-free fields in magneto-hydrodynamics. ", "machine_abstract": "We study the relationship between Dirichlet sigma-models on Riemannian manifolds with boundary, and mean curvature flows in the interior of these manifolds.  We show that if the initial metric is conformally equivalent to the standard round sphere then there exists an explicit solution for all time which converges exponentially fast towards the round sphere as $t \\to \\infty$.   In particular we prove that this holds true even when the manifold has corners or conical singularities at its boundary.    The results are obtained by combining techniques from geometric analysis (in particular heat kernel estimates) and functional analysis (in particular spectral theory).    Keywords: Mean Curvature Flow, Sigma Models, Conformal Geometry, Geometric Analysis, Functional Analysis. 1 Introduction   Let $(M^n,g_0)$ be a compact smooth Riemannian n-manifold with nonempty boundary $\\partial M = \\varnothing$. Consider the following evolution equation for metrics $g_t$ on $M$:  $$\\displaystyle g_{t+1} = e^{-2tH_g}g_t$$ where $H_g$ denotes the mean curvature vector field of the hypersurface $M^n$ equipped with the metric $g$. This is called the Ricci-DeTurck flow [DeT84], since it can also be written as:  $$\\displaystyle g^{ij}_{t} = e^{-2tH_{g}}g^{ij}_0 + 2e^{-2tH_{g}}\\Gamma_{ij}^{k}\\nabla_k \\log g_{0j}$$ Here, $\\Gamma_{ij}^{k}$ denote the Christoffel symbols associated to the metric $g$. If one starts with a metric $g$ such that the scalar curvature $R_g$ satisfies $R_g > 0$ everywhere, then the above flow preserves positivity of the scalar curvature. Moreover, if the initial metric is Einstein, i.e., $R_g = c g_0$ for some constant $c>0$, then the flow will preserve Einstein-ness.", "paraphrased_abstract": "And as a consequence of the fact that the initial metric is equal to the round sphere, the solution is immediately obvious to the round sphere as a t-to-infty-slope. Then the result is a curve of the scalar inclination. It is also a scalar curve of the scalar inclination, which, on all sides, satisfies the curvature of the scalar inclination. Thus the above equation preserves the positivity of the scalar inclination. It is called the Ricci-DeTurck-Geometry, which, by the way, is also called the Ricci-DeTorck-Geometry, because it is a similar flow. It is a function of geometric analysis (in particular heat-hilt) and spectral analysis. In this, we converge on the results of the ratio between the metric G and the mean inclination. This equation is known as the Ricci-DeTurck-Geometry, because it can be written as: 2tHggg100 log g101. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.393, "title": "The KATRIN sensitivity to the neutrino mass and to right-handed currents in beta decay", "abstract": "  The aim of the KArlsruhe TRItium Neutrino experiment KATRIN is the determination of the absolute neutrino mass scale down to 0.2 eV, with essentially smaller model dependence than from cosmology and neutrinoless double beta decay. For this purpose, the integral electron energy spectrum is measured close to the endpoint of molecular tritium beta decay. The endpoint, together with the neutrino mass, should be fitted from the KATRIN data as a free parameter. The right-handed couplings change the electron energy spectrum close to the endpoint, therefore they have some effect also to the precise neutrino mass determination. The statistical calculations show that, using the endpoint as a free parameter, the unaccounted right-handed couplings constrained by many beta decay experiments can change the fitted neutrino mass value, relative to the true neutrino mass, by not larger than about 5-10 %. Using, incorrectly, the endpoint as a fixed input parameter, the above change of the neutrino mass can be much larger, order of 100 %, and for some cases it can happen that for large true neutrino mass value the fitted neutrino mass squared is negative. Publications using fixed endpoint and presenting large right-handed coupling effects to the neutrino mass determination are not relevant for the KATRIN experiment. ", "machine_abstract": "The Karlsruhe Tritium Neutrino experiment (KATRIN) is currently under construction at the Karlsruhe Institute for Technology, Germany. It will measure the electron energy spectrum near the endpoint with unprecedented precision by using an electrostatic retardation spectrometer combined with a magnetic filter. The measurement aims at determining the absolute value of the effective electron antineutrino mass mneff with a statistical uncertainty of 0.2 eV/c2 or better. In this contribution we present results on the expected sensitivities of KATRIN to mneff as well as to possible contributions from right-handed currents in beta-decay. We show that these sensitivities are mainly determined by systematic uncertainties which can be controlled within the experimental accuracy required. This allows us to conclude that KATRIN has excellent prospects to determine mneff precisely enough to test whether it agrees with the predictions of standard model extensions such as supersymmetry.", "paraphrased_abstract": "The kinetics of the kinetics of mneff are analyzed in this study. We have shown that the kinetics of mneff are mainly driven by the definite uncertainties, which can be controlled within the limits of the experimental accuracy. The K-TRIN experiment, currently under construction at the Karlsruhe Institute of Technology, is being developed at the Karlsruhe Institute of Technology. It is an experimental device that measures the electron energy of the atom with the highest precision and by means of an electrostatic retardation spectrometer with a magnetic filter. The K-TRIN experiment is intended to find out the absolute value of the electron antineutrino mass mnef with a statistical uncertainty of 0.2 eV/c2 or better. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1844, "title": "Role of the rho meson in the description of pion electroproduction experiments at JLab", "abstract": "  We study the p(e,e' pi+)n reaction in the framework of an effective Lagrangian approach including nucleon, pi and rho meson degrees of freedom and show the importance of the rho-meson t-pole contribution to sigmaT, the transverse part of cross section. We test two different field representations of the rho meson, vector and tensor, and find that the tensor representation of the rho meson is more reliable in the description of the existing data. In particular, we show that the rho-meson t-pole contribution, including the interference with an effective non-local contact term, sufficiently improves the description of the recent JLab data at invariant mass W less 2.2 GeV and Q2 less 2.5 GeV2/c2. A ``soft'' variant of the strong piNN and rhoNN form factors is also found to be compatible with these data. On the basis of the successful description of both the sigmaL and sigmaT parts of the cross section we discuss the importance of taking into account the sigmaT data when extracting the charge pion form factor Fpi from sigmaL. ", "machine_abstract": "We present results on the role played by the rho meson in describing pion electroproduction data obtained with the CLAS detector at Jefferson Lab (JLab). The analysis is performed within an effective field theory approach, where we use chiral perturbation theory to describe the interaction between pions and nucleons up to next-to-leading order. We then introduce vector-meson degrees of freedom through the hidden gauge formalism. In particular, we consider contributions coming from one-loop diagrams involving rho mesons as well as tree-level processes mediated by rho mesons. Our theoretical framework allows us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single-pion production off protons. Using this model, we are able to reproduce experimental data for all these observables simultaneously. Finally, we discuss how our findings can be used to extract information about the properties of the rho meson. This work was supported by the U.S.    Department of Energy under Contract No. DE-SC0012704.  PACS numbers: 12.38.Mh", "paraphrased_abstract": "In the present work, we are using the rho meson as a model, and the rho meson is also an important tool for the study of electroproduction. We describe this in the form of a field theory based on chiral perturbation. This theory permits us to calculate pion-pion interaction, and then we introduce vector-meson degrees of freedom. In particular, we present a rho meson-based kinetics, based on rho mesons, and on the rho meson in the tree. We are able to describe all the observed results simultaneously. We present here the role of the rho meson in the electrochemical data obtained with the CLAS spectrometer in Jefferson Laboratory (JLab). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4543, "title": "Constraining the Geometry of the Neutron Star RX J1856.5-3754", "abstract": "  RX J1856.5-3754 is one of the brightest, nearby isolated neutron stars, and considerable observational resources have been devoted to its study. In previous work, we found that our latest models of a magnetic, hydrogen atmosphere matches well the entire spectrum, from X-rays to optical (with best-fitting neutron star radius R=14 km, gravitational redshift z_g~0.2, and magnetic field B~4x10^12 G). A remaining puzzle is the non-detection of rotational modulation of the X-ray emission, despite extensive searches. The situation changed recently with XMM-Newton observations that uncovered 7 s pulsations at the 1% level. By comparing the predictions of our model (which includes simple dipolar-like surface distributions of magnetic field and temperature) with the observed brightness variations, we are able to constrain the geometry of RX J1856.5-3754, with one angle < 6 deg and the other angle = 20-45 deg, though the solutions are not definitive given the observational and model uncertainties. These angles indicate a close alignment between the rotation and magnetic axes or between the rotation axis and the observer. We discuss our results in the context of RX J1856.5-3754 being a normal radio pulsar and a candidate for observation by future X-ray polarization missions such as Constellation-X or XEUS. ", "machine_abstract": "We present new observations and analysis of the neutron star (NS) candidate RX J1856.5-37543, which is one of only two NSs with measured parallaxes to date.  We use these measurements along with optical photometry in order to constrain its radius and distance as well as the interstellar extinction towards it. Our results are consistent with previous estimates for this source's radius but we find that our best-fit model has an unusually large value of A V = 1.6 \u00b1 0.2 mag compared to other nearby stars. This suggests that there may be additional absorption by dust associated with the system or possibly even circumstellar material around the NS itself. The latter possibility would have important implications for understanding how such objects form. Subject headings: Black holes - accretion disks - binaries -X-ray sources: general - Stars: neutron - Parallaxes: fundamental constants - Stellar radii", "paraphrased_abstract": "\u2018It\u2019s all very well and good to have black holes, accretion disks, binaries, and X-ray sources. Stars: neutrons \u2013 Parallaxes: fundamental constants \u2013 Spectroscopic radii \u2013 Stellar radii \u2013 Observations of the neutron star RX J1856.5-37543, which is one of the few NSs to have measured parallaxes. We have used these measurements to constrain its radius and distance and to constrain the extinction to it by the circumstellar medium. We present the results of the measurements and analysis of the candidate NS, RX J1856.5-37543, one of only two NSs with parallaxes that we have measured. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0855, "title": "A discrete computer network model with expanding dimensions", "abstract": "  Complex networks with expanding dimensions are studied, where the networks may be directed and weighted, and network nodes are varying in discrete time in the sense that some new nodes may be added and some old nodes may be removed from time to time. A model of such networks in computer data transmission is discussed. Each node on the network has fixed dimensionality, while the dimension of the whole network is defined by the total number of nodes. Based on the spectacular properties of data transmission on computer networks, some new concepts of stable and unstable networks differing from the classical Lyapunov stability are defined. In particular, a special unstable network model, called devil network, is introduced and discussed. It is further found that a variety of structures and connection weights affects the network stability substantially. Several criteria on stability, instability, and devil network are established for a rather general class of networks, where some conditions are actually necessary and sufficient. Mathematically, this paper makes a first attempt to rigorously formulate a fundamental issue of modeling discrete linear time-varying systems with expanding dimensions and study their basic stability property. ", "machine_abstract": "We present an algorithm for the construction of a discrete computer network in which each node is connected to all nodes within its distance, and also to some other nodes at larger distances. The number of additional connections per node increases exponentially as one moves away from it along any path. We show that this property can be used to construct efficient routing tables for such networks. Our results are based on a new concept called \"expanding dimension\". This concept allows us to define a natural hierarchy among the nodes of our network. In particular, we prove that if there exists a polynomial time algorithm for finding shortest paths between two given nodes then there must exist another polynomial time algorithm for solving the same problem but restricted only to pairs of nodes whose distance belongs to a certain range. Finally, we discuss possible applications of these ideas to distributed computing problems. Keywords: Computer network, Expanding Dimension, Hierarchy, Routing", "paraphrased_abstract": "Then, based on a new concept, extending the dimensions, we propose to re-invent a natural hierarchy among the nodes of the network. We show how to establish a new hierarchy between the nodes of our network. We prove that if there is a polynomial time for finding the shortest path between two nodes, there must be another polynomial time for solving this problem, limited to the number of nodes that are within a certain range. We propose an algorithm to construct a discrete network, in which each node connects to the other nodes, and to the other nodes at a larger distance. In this way we are able to provide efficient routing for such networks. We present an algorithm for building a discrete computer network in which each node connects to the others, and at a larger distance, and at the same time to the distances between other nodes. We show how this property can be exploited to build up efficient routing tables for such networks. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.447, "title": "Dynamic response of Ag monolayers adsorbed on Au(100) upon an oscillatory variation of the chemical potential: A Monte Carlo simulation study", "abstract": "  Based on the fact that the underpotential electrochemical deposition of Ag atoms on the $Au(100)$ surface exhibits sharp first-order phase transitions at well-defined values of the (coexistence) chemical potential ($\\mu_{coex}$), we performed extensive simulations aimed at investigating the hysteretic dynamic behavior of the system close to coexistence upon the application of a periodic signal of the form $\\mu(t) = \\mu_{coex} + \\mu_{o}sin(2\\Pi t/\\tau) $, where $\\mu_{o}$ and $\\tau$ are the amplitude and the period of the sweep, respectively. For relatively short periods and small enough amplitudes the system becomes trapped either at low or high Ag coverage states, as evidenced by recording hysteresis loops. This scenario is identified as dynamically ordered states (DOS), such that the relaxation time $(\\tau_{relax})$ of the corresponding metastable state obeys $\\tau_{relax} > \\tau $. On the other hand, by properly increasing $\\mu_{o}$ or/and $\\tau$, one finds that the $Ag$ coverage gently follows the external drive (here $\\tau_{relax} < \\tau $) and the system is said to enter into dynamically disordered states (DDS), where hysteresis loops show almost symmetric limiting cycles. This symmetry breaking between limiting cycles driven by an external signal is discussed in terms of the concept of (out-of-equilibrium) Dynamic Phase Transitions between DOS and DDS, similar to those encountered when a magnetic system is placed in the presence of a variable external magnetic field. However, a careful finite-size scaling study reveals that, at least at $T = 300K$, the $Ag/Au(100)$ system does not exhibit true second-order phase transitions but rather a crossover behavior between states. A diagram showing the location of the ordered and disordered states in the $\\mu$ versus $\\tau$ plane is obtained and discussed. ", "machine_abstract": "We have studied by means of molecular dynamics simulations how the dynamic behavior of silver atoms deposited on gold (100) is affected when the substrate temperature and the coverage are varied in such a way that the average number of nearest neighbors per atom changes between 1 to 4. We find that, for low coverages, the system behaves as if it were made up of isolated particles with no interaction among them. However, at higher coverages we observe collective effects which lead to the formation of ordered structures. The results obtained show that these structures can be classified into two different categories depending on whether they are formed by one or more layers of silver atoms. In particular, we found that the structure of the first layer depends strongly on the substrate temperature while the second layer shows only small variations. Finally, our calculations indicate that the third layer forms a disordered arrangement of silver atoms. This work was supported by DGESIC under project PB98-0443-C02-01", "paraphrased_abstract": "The first layer, for instance, is strongly dependent on the temperature of the ground, while the second layer shows only minor variations. These results indicate that the third layer is a disordered structure of the silver atoms. We have studied, by means of simulations, the behaviour of the silver atoms deposited on gold 102 on the basis of the temperature and the coverage. The temperature is varied by a factor of four, from 1 to 4; we found that the structure of the first layer depends strongly on the temperature of the earth, while the second layer shows only slight variations. We have shown that the structure of the third layer depends on the temperature of the earth, while the third layer is the disordered arrangement of silver atoms. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1024, "title": "High-energy threshold reaction rates on 0.8 GeV proton-irradiated thick Pb-target", "abstract": "  This works presents results of activation-aided determination of threshold reaction rates in 92 209Bi, natPb, 197Au, 181Ta, 169Tm, natIn, 93Nb, 64Zn, 65Cu, 63Cu, 59Co, 19F, and 12C samples and in 121 27Al samples. All the samples were aligned with the proton beam axis inside and outside the demountable 92-cm thick Pb target of 15-cm diameter assembled of 23 4-cm thick discs. The samples were placed on 12 target disks to reproduce the long axis distribution of protons and neutrons. In June 2006, the target was exposed for 18 hours to a 800-MeV proton beam extracted from the ITEP U-10 accelerator. The proton fluence and the proton beam shape were determined using the 27Al(p,x)7Be monitor reaction. The reaction rates were determined by the direct gamma-spectrometry techniques. In total, 1196 gamma-spectra have been measured, and about 1500 reaction rates determined. The measured reaction rates were simulated by the MCNPX code using the following databases: ENDF/B6 for neutrons below 20 MeV, MENDL2 for 20-100 MeV neutrons, and MENDL2P for proton cross sections up to 200 MeV. An acceptable agreement of simulations with experimental data has been found. ", "machine_abstract": "The high-energy threshold reaction rates for the reactions induced by protons with energies up to 800 MeV are calculated using the statistical model code TALYS-1.6 and compared with experimental data obtained at JINR-Dubna in the framework of the project \"Study of nuclear processes under extreme conditions\". The results show that the theoretical predictions agree well with experiment within uncertainties, except for some cases where there is an overestimation or underestimation of the measured cross sections.     Keywords: Reaction rate, Thick target, Proton beam, Statistical model, Nuclear physics, Cross section measurement, High energy threshold reaction rate calculation, Extreme condition study, Projectile fragmentation, Heavy ion collision, Energy deposition, Radiation damage, Lead target, Inverse kinematics, Fragmentation function, Neutron emission probability distribution, Excitation functions, Stopping power, Energy loss straggling, Energy transfer", "paraphrased_abstract": "Statistical model, Nuclear physics, cross-section measurement, Excitation rate, Excitations, Electrons, Electrons, energy loss, Electrons. In these experiments the theoretical prediction was confirmed and there were few differences in the measured cross-sections. Hence, it was found that the theoretical predictions were quite in agreement with the experimental data. Hence, in the experiments, the theoretical predictions were well accepted. The redox rate, the thick target, the protons\u2019 beam, the statistical model, nuclear physics, the cross-sections were measured, and the ratios of the cross-sections were compared to the experimental data from JINR-Dubna. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0641, "title": "Information diffusion epidemics in social networks", "abstract": "  The dynamics of information dissemination in social networks is of paramount importance in processes such as rumors or fads propagation, spread of product innovations or \"word-of-mouth\" communications. Due to the difficulty in tracking a specific information when it is transmitted by people, most understanding of information spreading in social networks comes from models or indirect measurements. Here we present an integrated experimental and theoretical framework to understand and quantitatively predict how and when information spreads over social networks. Using data collected in Viral Marketing campaigns that reached over 31,000 individuals in eleven European markets, we show the large degree of variability of the participants' actions, despite them being confronted with the common task of receiving and forwarding the same piece of information. This have a profound effect on information diffusion: Firstly, most of the transmission takes place due to super-spreading events which would be considered extraordinary in population-average models. Secondly, due to the different way individuals schedule information transmission we observe a slowing down of the spreading of information in social networks that happens in logarithmic time. Quantitative description of the experiments is possible through an stochastic branching process which corroborates the importance of heterogeneity. Since high variability of both the intensity and frequency of human responses are found in many other activities, our findings are pertinent to many other human driven diffusion processes like rumors, fads, innovations or news which has important consequences for organizations management, communications, marketing or electronic social communities. ", "machine_abstract": "We study information diffusion on networks with multiple communities, where each community has its own set of nodes and edges. We propose an epidemic model that captures the effect of both global and local interactions between users within different communities. The proposed model is based on two key ideas: (1) we assume that there are some influential individuals who can spread information to their neighbors faster than others; and (2) we allow for cross-influences among neighboring communities through these influential individuals. Our main goal is to understand how the structure of the underlying network affects the spreading process. In particular, our contributions include:  1. We develop a novel mathematical framework to analyze the dynamics of information diffusion under the proposed epidemic model. 2. We show that if all communities have similar sizes then the number of infected nodes at time t grows as O(t log n), where n denotes the total number of nodes in the network. 3. We prove that if one community dominates the other ones by size then the number of infected individuals grows exponentially fast. 4. Finally, we provide extensive numerical experiments to validate our theoretical results.", "paraphrased_abstract": "The model we propose is based on two basic ideas: 1) we assume that there are some influential individuals who spread information faster than other people, and 2) we allow for the influence of others, through these influential persons. We study the dynamics of information ebb and flow in a network of many people. We present an ebb and flow model of information ebb and flow, based on two main points: (1) we assume that some influential people are more willing to spread information than others, and (2) we allow for the influence of others through these influential people. We also present a numerical experiment to verify the theoretical results. In this paper, we introduce a new mathematical framework for the study of the dynamics of information ebb and flow, and show that if all the people in the network have similar sizes, the number of infected people in time t grows as O(t) log n, where n is the number of infected people. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1228, "title": "Near- and Far-Infrared Counterparts of Millimeter Dust Cores in the Vela Molecular Ridge Cloud D", "abstract": "  The aim of this paper is to identify the young protostellar counterparts associated to dust millimeter cores of the Vela Molecular Ridge Cloud D through new IR observations (H_2 narrow-band at 2.12 micron and N broad band at 10.4 micron) along with an investigation performed on the existing IR catalogues. The association of mm continuum emission with infrared sources from catalogues (IRAS, MSX, 2MASS), JHK data from the literature and new observations, has been established according to spatial coincidence, infrared colours and spectral energy distributions. Only 7 out of 29 resolved mm cores (and 16 out of the 26 unresolved ones) do not exhibit signposts of star formation activity. The other ones are clearly associated with: far-IR sources, H_2 jets or near-IR objects showing a high intrinsic colour excess. The distribution of the spectral indices pertaining to the associated sources is peaked at values typical of Class I objects, while three objects are signalled as candidates Class 0 sources. We remark the high detection rate (30%) of H_2 jets driven by sources located inside the mm-cores. They appear not driven by the most luminous objects in the field, but rather by less luminous objects in young clusters, testifying the co-existence of both low- and intermediate-mass star formation. The presented results reliably describe the young population of VMR-D. However, the statistical evaluation of activity vs inactivity of the investigated cores, even in good agreement with results found for other star forming regions, seems to reflect the limiting sensitivity of the available facilities rather than any property intrinsic to the mm-condensations. ", "machine_abstract": "We present near-infrared (NIR) observations toward two millimeter dust cores, MM1 and MM2, located at the edge of the Vela Molecular Ridge cloud D. The NIR data were obtained with Subaru/Suprime-Cam on 2005 December 8-9 under photometric conditions. We detected several point sources associated with each core. In addition to these point-like objects, we found extended emission around MM1 that is likely due to scattered light by interstellar grains. Using our NIR data together with previous infrared and radio continuum surveys, we investigated the nature of the observed objects. Our results are summarized as follows:  1. Two brightest point-like objects within MM1 have been identified as young stellar objects based on their spectral energy distributions.  2. One of them shows an excess flux over its photosphere in K-band which suggests the presence of circumstellar disks or envelopes.  3. Another object has no significant excess but exhibits strong H$\\alpha$ emission line suggesting active accretion onto the central star.  4. Several other fainter point-like objects show neither excess nor H$\\alpha$ emission lines indicating they may be background stars.  5. Extended emission was also seen around MM1.", "paraphrased_abstract": "Using the same methods as our previous observations of the infrared and radio continuum, we examined the nature of the objects that were detected. The following table is a summary of the observations that have been made on the MM1 and MM2 cores. The two objects of interest were, as they were, young stellar objects. One was a star with an excess of energy in the K band, indicating that there were rims of disks and envelopes in the sphere. Two other objects, of a tinier type, did not show any excess, but had a strong halo. This was a feature that could be seen as an interstellar grain. We identified several haloes, with the following conclusions: One is a star with a large amount of energy, which is a young star, in a very young galaxy, with a spectral energy of 7, a very young star with the spectral energy of 8; another, not very large, had a very large H-alpha line, which could indicate that the star was accreted on a central star. Two other stars, which are haloes, have also been detected, that are, observable at high altitude, have been observed around MM1. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3942, "title": "Separability Criterion for multipartite quantum states based on the Bloch representation of density matrices", "abstract": "  We give a new separability criterion, a necessary condition for separability of $N$-partite quantum states. The criterion is based on the Bloch representation of a $N$-partite quantum state and makes use of multilinear algebra, in particular, the matrization of tensors. Our criterion applies to {\\it arbitrary} $N$-partite quantum states in $\\mathcal{H}=\\mathcal{H}^{d_1}\\otimes \\mathcal{H}^{d_2} \\otimes ... \\otimes \\mathcal{H}^{d_N}.$ The criterion can test whether a $N$-partite state is entangled and can be applied to different partitions of the $N$-partite system. We provide examples that show the ability of this criterion to detect entanglement. We show that this criterion can detect bound entangled states. We prove a sufficiency condition for separability of a 3-partite state, straightforwardly generalizable to the case $N > 3,$ under certain condition. We also give a necessary and sufficient condition for separability of a class of $N$-qubit states which includes $N$-qubit PPT states. ", "machine_abstract": "We present an explicit criterion to determine whether or not two given multipartite quantum states are separable, i.e., can be written as convex combinations of product states. The criterion is formulated in terms of the Bloch representation of the corresponding density matrices and it relies only on local measurements performed by each party. We show that our method provides a necessary condition for separability which is strictly weaker than other known criteria. Finally we illustrate its usefulness with some examples. Introduction:-The problem of determining if a given state belongs to the set of separable states has been extensively studied during last years [1] . In particular, several authors have proposed different methods to solve this problem [2] - [4] , but none of them seems to provide a complete solution yet. Recently, Vidal et al [5] introduced a new approach to study separability problems using the Bloch representation [6] of the density matrix associated to any pure state. This technique allows one to obtain simple conditions for separability which involve only local measurements made by each party involved in the system under consideration. However, these results do not apply directly when dealing with mixed states since they require the knowledge of all possible pure-state decompositions of such states. Here we will use another version of the Bloch representation [7] to derive a general criterion for separability applicable also to mixed states. Our main result consists of showing that there exists at least one decomposition into pure states compatible with the Bloch representation of every separable state. As a consequence, we prove that the criterion presented here constitutes a necessary condition for separabilty which is strictly weaker than previous ones [8] . Preliminaries:-In what follows we consider N-partite systems described by Hilbert spaces H 1 ,H 2 ...H N . A generic element |\u03c8\u27e9 \u2208 H = \u2211 N i=1 H i is called a pure state vector while \u03c1 \u2208 D(H) denotes a density operator acting on H. Any density operator can always be expressed in terms of its spectral decomposition [9]  where {|\u03bb\u27e9} \u03bb=1,...,d is an orthonormal basis of eigenvectors of \u03c1 and p \u03bb \u2265 0 (p", "paraphrased_abstract": "In our next section we will consider the N-partite systems of Hilbert spaces H1, H2, H3; a generic element  H=H=i, is called a pure state vector,  D(H) is a density operator on H. We have been experimenting with a new method to study the polarity of two states in the sense that we can, through a simple calculation of the local polarity of each state, but we will use another method, the Bloch representation, to produce the polarity of all the states, but it is difficult to solve it in this way, as it is necessary to know all the states of the state. Here we will use another way of solving the problem of polarity, which is also applicable to mixed states. We will present the general polarity of polarity of polarity, which is strictly weaker than the previous ones. In this work, Vidal and et al have introduced a new method for the study of polarity, based on the Bloch representation of the density matrix of each polarity. We are using this method to obtain a polarity that is strictly weaker than the other ones. We show that our method provides a polarity that is primarily weaker than other known criteria. The main result is that there is at least one polarity that is compatible with", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0849, "title": "Imaging the Cosmic Matter Distribution using Gravitational Lensing of Pregalactic HI", "abstract": "  21-cm emission from neutral hydrogen during and before the epoch of cosmic reionisation is gravitationally lensed by material at all lower redshifts. Low-frequency radio observations of this emission can be used to reconstruct the projected mass distribution of foreground material, both light and dark. We compare the potential imaging capabilities of such 21-cm lensing with those of future galaxy lensing surveys. We use the Millennium Simulation to simulate large-area maps of the lensing convergence with the noise, resolution and redshift-weighting achievable with a variety of idealised observation programmes. We find that the signal-to-noise of 21-cm lens maps can far exceed that of any map made using galaxy lensing. If the irreducible noise limit can be reached with a sufficiently large radio telescope, the projected convergence map provides a high-fidelity image of the true matter distribution, allowing the dark matter halos of individual galaxies to be viewed directly, and giving a wealth of statistical and morphological information about the relative distributions of mass and light. For instrumental designs like that planned for the Square Kilometer Array (SKA), high-fidelity mass imaging may be possible near the resolution limit of the core array of the telescope. ", "machine_abstract": "We present an analysis of gravitational lensing by large-scale structure in the universe, as traced by neutral hydrogen (HI) at high redshifts z > 6. We use numerical simulations to show that this effect is detectable with future radio telescopes such as SKA and ngVLA. The signal-to-noise ratio for detecting these effects depends on the angular resolution of the telescope used; we find that it can be improved significantly if one uses multiple frequency channels instead of single-frequency data. This technique could provide valuable information about dark matter halos at early times when they were still forming their first stars. In addition, our results suggest that the cosmic web may have been denser than previously thought. Finally, we discuss how this method could be applied to detect primordial black holes. Introduction -Gravitational lensing has become a powerful tool for studying the distribution of mass in the Universe. It allows us to probe structures which are too distant or small to be detected directly through other means. For example, galaxy clusters act like lenses, magnifying background galaxies behind them. By measuring the distortion caused by lensing, one can infer properties of the cluster's dark matter halo [1] . Similarly, weak gravitational lensing measurements allow astronomers to map out the total projected mass density field over large areas of sky [2] . In recent years there has been growing interest in applying gravitational lensing techniques to study high-redshift objects [3] , including the epoch of reionization [4] . However, most previous studies focused only on the lensing produced by visible matter, such as galaxies and quasars [5] . Here we consider another source of lensing: the intergalactic medium (IGM). At very high redshift, before galaxies formed, the IGM was filled with neutral hydrogen gas [6] . As time passed, some fraction of this gas became ionized due to ultraviolet radiation emitted by young stars [7, 8] . But even today, much of the IGM remains neutral [9] . Since the IGM contains more mass than any individual galaxy [10] , its contribution to lensing should not be ignored [11] . The goal of this", "paraphrased_abstract": "During the first year of the last century, a great deal of interest has been made to study the distribution of mass in the Universe, especially at high redshifts, when they are the first stars. This technique has been applied to many structures that are inaccessible to other astronomers. For example, galaxies act as lenses, exposing behind them their starry backsides, which gives an indication of the dark matter in their halo. Then, by measuring the distortion of the halo caused by halo, it can be interpreted to be a halo of dark matter, which in turn could be interpreted as a halo of dark matter. In addition, the data can be used to map the mass density of the cosmic web, which, we conclude, is denser than ever. In our experiments we have shown that haloes of large scale can be studied with HI at high redshifts. The distance and spatial resolution of our instrument will vary. However, the relative quality of our instrument will be greatly improved by the use of multiple frequency channels instead of one single frequency. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2047, "title": "Comment on the statistical analysis in \"A new experimental limit for the stability of the electron\" by H.V. Klapdor-Kleingrothaus, I.V. Krivosheina and I.V. Titkova", "abstract": "  We point out the evident errors in statistical analysis of the data, performed by Klapdor-Kleingrothaus et al in recently published paper to establish a limit on the stability of electron with respect to the decay into $\\nu+\\gamma$. We performed the reestimation of the sensitivity of the experimental setups to the 256 keV gamma emitted in the hypothetical electron decay, and show that in [1] the limits on the electron stability and charge nonconservation parameter $\\epsilon_{e\\nu\\gamma}^{2}$ are overestimated by at least a factor of 5. We have revealed evident errors in the statistical analysis, performed by Klapdor-Kleingrothaus et al in a recently published paper [1] to establish a limit on the stability of electron with respect to the decay into $\\nu+\\gamma$. The performed reestimation of the sensitivity of the experimental setups to the 256 keV gamma emitted in the hypothetical electron decay, has shown that the limits on the electron stability and charge nonconservation parameter $\\epsilon_{e\\nu\\gamma}^{2}$ presented in [1], have been overestimated by at least a factor of 5. ", "machine_abstract": "We have read with interest the recent preprint [1] . In this work we find that the authors' claim to set an upper bound on the lifetime of the electron is not justified because they did not take into account all relevant decay channels.     We would like to comment briefly on their treatment of backgrounds as well as their choice of cuts used to select events. The main source of background comes from radiative Bhabha scattering e+e-\u2192e+e-\u03b3 which has been studied extensively at LEP2 [2] , where it was found to be negligible compared to other sources such as two-photon processes or four-fermion final states (e.g., W pair production). This process can only contribute if one photon escapes detection; however, since photons are emitted almost collinearly with electrons/positrons, the probability of missing both photons is very small. Furthermore, the cross section for this process decreases rapidly when the invariant mass of the lepton pairs increases [3] .   The authors also use a cut on the total energy of the event, Evis>10 GeV, which removes most of these events. They do mention that there may still be some residual contamination due to radiative Bhabhas but argue that this will be suppressed by requiring the presence of additional jets. However, even though the jet multiplicity distribution does decrease slightly after applying this requirement, the effect is too small to compensate for the loss of signal efficiency caused by removing events with low visible energies.    In addition, the authors state that the contribution from radiative Bhabhas should be included in the systematic uncertainty estimate. However, this statement is misleading given that the quoted systematic error already includes contributions from many different sources including those related to the modelling of initial-state radiation [4] .    Finally, we note that the authors present results obtained using Monte Carlo simulations performed with PYTHIA 6 [5] . It is known [6] that this generator underestimates the number of high-multiplicity...", "paraphrased_abstract": "And finally, the authors report the results of Monte Carlo simulations with PYTHIA 6. This generator can only produce some of the high-multiplicity... It is known that this generator underestimates the number of high-multiplicity... We have read the recent preprints. In the preprint, we have noticed that the authors\u2019 claim to have an upper limit on the lifetime of the electron is not well-founded, because they have not taken into account all the relevant decay mechanisms. In the present work, we find that the authors have omitted the contributions of radiative Bhabha, which have been observed in LEP-2, and which are negligible in comparison with other sources, such as two-photon processes or four-fermion final states (e.g., W-edimentation). This process can only be contributed if the detector is not detecting one of the electrons, but since it is almost collinear with electrons and positrons, the probability of missing both the electrons is very small. We shall briefly comment on the effects of background and the choice of background cuts. The authors have argued that the contribution of radiative Bhabhas is not justified in the calculation of the lifetime of electrons. However, the authors declare that a certain amount of radiation, e+e-e-e-e-", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1301, "title": "Massive gravity and structure formation", "abstract": "  We study the growth of cosmological perturbations in the model of Lorentz-violating massive gravity. The Friedman equation in this model acquires an unconventional term due to the Lorentz-breaking condensates which has the equation of state w = -1 / (3 gamma) with gamma being a free parameter taking values outside of the range [0,1/3]. Apart from the standard contributions, the perturbations above the Friedmann background contain an extra piece which is proportional to an arbitrary function theta(x) of the space coordinates. This function appears as an integration constant and corresponds to a non-propagating scalar mode which may, however, become dynamical with the account of the higher-derivative corrections. For -1 < gamma < 0 and gamma = 1 the ``anomalous'' perturbations grow slower than the standard ones and thus the model is compatible with observations. Whether the model is experimentally acceptable at other values of \\gamma depends on the value of the function theta(x) at the beginning of the radiation-dominated epoch. ", "machine_abstract": "We study the evolution of density fluctuations in massive gravity, which is an extension to general relativity that includes terms quadratic in curvature invariants. We show that this theory predicts a scale-invariant spectrum of primordial perturbations with amplitude given by $\\Delta^2 R = (4\\pi)^{-3/2} H_0^2 \\sim 10^{-20}$ at horizon crossing during inflation.  This value agrees well with current observations of the cosmic microwave background anisotropy. The same mechanism also leads to a suppression of power on small scales due to nonlinear effects. In addition, we find that massive gravity can lead to significant modifications to the growth rate of large-scale structures compared to standard \u039bCDM cosmology. These results are consistent with recent measurements of galaxy clustering using data from the Sloan Digital Sky Survey. Finally, we discuss how these predictions could be tested observationally through future surveys such as Euclid or LSST. Massive gravity is an extension to general", "paraphrased_abstract": "Then we discuss how this theory could be tested with future observations from Euclid or LSST. Massive gravity is an extension of the general relativity, and in addition includes the terms quadratic and quadratic in curvature. We show that it enables the formation of large-scale structures, in a way that is incomparably more than that of ordinary cosmology. We show that it entails a large-scale spectrum of primordial perturbations, whose amplitude was given by the amplitude of R = (R = (R)) 3 h1  sim10. This value is consistent with the recent measurements of the Sloan Digital Sky Survey. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.127, "title": "The Cosmic Code Comparison Project", "abstract": "  Current and upcoming cosmological observations allow us to probe structures on smaller and smaller scales, entering highly nonlinear regimes. In order to obtain theoretical predictions in these regimes, large cosmological simulations have to be carried out. The promised high accuracy from observations make the simulation task very demanding: the simulations have to be at least as accurate as the observations. This requirement can only be fulfilled by carrying out an extensive code validation program. The first step of such a program is the comparison of different cosmology codes including gravitation interactions only. In this paper we extend a recently carried out code comparison project to include five more simulation codes. We restrict our analysis to a small cosmological volume which allows us to investigate properties of halos. For the matter power spectrum and the mass function, the previous results hold, with the codes agreeing at the 10% level over wide dynamic ranges. We extend our analysis to the comparison of halo profiles and investigate the halo count as a function of local density. We introduce and discuss ParaView as a flexible analysis tool for cosmological simulations, the use of which immensely simplifies the code comparison task. ", "machine_abstract": "The cosmic code comparison project is an effort to compare the results produced by different cosmological codes, and thereby test their accuracy.  The goal is to produce a set of simulated data that can be used as input for any number of codes, and then have each code run on this same dataset.   This will allow us to determine how well these codes agree with one another in terms of both the physical quantities they predict (e.g., matter density profiles) and also the statistical properties of those predictions (e.g., power spectra).   We are currently working towards producing a large suite of simulations covering a wide range of parameter space, including dark energy models, modified gravity theories, neutrino masses, and primordial non-Gaussianity. In addition we plan to make available a variety of observational datasets which can be compared against the simulation outputs using standard techniques such as cross-correlation functions or likelihood analysis. The cosmic code comparison project is funded through NASA's Astrophysics Theory Program under grant NNX09AC84G.", "paraphrased_abstract": "He is currently a senior scientist with a PhD in Astrophysics at the NASA Astrophysics Laboratory, under the auspices of NASA sextant, with a view to comparing the predictions of the different cosmic codes. In addition, he is working on a number of simulations of a wide variety of parameters, such as dark energy, modified gravity, neutrino masses, and primordial non-Gaussianity. In addition, he is preparing a number of experimental datasets, namely: dark energy, modified gravity, neutrino masses, primordial non-Gaussianity. The aim is to generate a set of simulations for any number of codes, and then to test these simulations against this same dataset. We are currently working on a large number of simulations covering a wide range of parameter space: dark energy, modified gravity, neutrino masses, primordial non-Gaussianity. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.216, "title": "Characterization of Instrumental Phase Stability", "abstract": "  Atmospheric water vapor causes significant undesired phase fluctuations for the Submillimeter Array (SMA) interferometer, particularly in its highest frequency observing band of 690 GHz. One proposed solution to this atmospheric effect is to observe simultaneously at two separate frequency bands of 230 and 690 GHz. Although the phase fluctuations have a smaller magnitude at the lower frequency, they can be measured more accurately and on shorter timescales due to the greater sensitivity of the array to celestial point source calibrators at this frequency. In theory, we can measure the atmospheric phase fluctuations in the 230 GHz band, scale them appropriately with frequency, and apply them to the data in 690 band during the post-observation calibration process. The ultimate limit to this atmospheric phase calibration scheme will be set by the instrumental phase stability of the IF and LO systems. We describe the methodology and initial results of the phase stability characterization of the IF and LO systems. ", "machine_abstract": "We present an analysis method for characterizing the stability of optical phase in astronomical instruments, based on the measurement and characterization of fringe contrasts obtained with different integration times.  We show that this method can be used to characterize both short-term (< 1 hour) and long-term (> 24 hours) instrumental instabilities. The results are compared against those obtained using other methods such as power spectral density or Allan variance measurements. This new technique is applied to data taken at the Palomar Observatory Interferometer during commissioning runs in 2007-2008. It allows us to identify specific sources of instability which could not have been detected by previous techniques. In particular we find that the main source of instability comes from atmospheric turbulence effects rather than mechanical vibrations. Finally, we demonstrate how our method can also be used to measure the coherence time of the atmosphere. Keywords: Fringe contrast, Optical interferometry, Instrumentation, Atmospheric turbulence", "paraphrased_abstract": "He further demonstrates how our method can be used to determine the time to the coherence of the atmosphere. This new method is applied to the observations made by the Palomar Interferometer, which was completed during the summer of 2007-2008. We use a method for assessing the stability of the optical horizon in astronomical instruments by measuring and evaluating fringe contrasts, which are obtained by varying the time of the integration, to detect and quantify the short- and long-term, at most 24 hours. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1594, "title": "Measurement of the top-quark mass using missing $E_T$+jets events with secondary vertex $b$-tagging at CDF II", "abstract": "  We present a measurement of the top-quark mass in $p\\bar p$ collisions at \\hbox{$\\sqrt{s}=1.96$} TeV which uses events with an inclusive signature of missing transverse energy and jets. % The event selection is sensitive to $t\\bar t\\to W^+bW^-\\bar b\\to l\\nu b q q^\\prime \\bar b$ independent of the lepton flavor, and results in a large acceptance for $W\\to\\tau\\nu$ decays. % All-hadronic $t\\bar t$ decays and events with identified electrons or muons are vetoed to provide a statistically independent sample with respect to all previous measurements. % The top-quark mass is inferred from the distribution of the scalar sum of all jet transverse energies and the missing transverse energy. Using 311 pb$^{-1}$ of integrated luminosity recorded by the Collider Detector at Fermilab, we measure a top-quark mass $m_t = 172.3 ^{+10.8}_{-9.6}$(stat.)$\\pm 10.8$(syst.) GeV/$c^2$. While the uncertainty on $m_t$ is larger than that of other measurements, the result is statistically uncorrelated with those of other methods, and thus can help to reduce the overall $m_t$ uncertainty when combined with other existing measurements. ", "machine_abstract": "We present measurements of the top quark mass in final states containing one or two jets and large missing transverse energy, based on data collected by the Collider Detector at Fermilab during Run I (1992-1996). The analysis uses an iterative likelihood fit to extract the signal yield as a function of the assumed top quark mass for each jet multiplicity bin. We find mtop = 174.3 +/- 1.7 GeV/c2(stat) +/-3.1(syst), where the systematic uncertainty is dominated by uncertainties associated with modeling of initial state radiation and gluon splitting.  These results are consistent with previous measurements performed by D0 and CDF experiments. This measurement supersedes our previously published result.  The full text can be found at: http://arxiv.org/abs/0907.4189v1  # Measurement of the top-quarks mass using missing E_T+jets events with secondary-vertex b-tagging at CDF-II", "paraphrased_abstract": "The full text is available at: http://arxiv.org/abs/0907.489v1, and the measurement superseded our previous measurement. The analysis used the iterative likelihood-fitting method to extract the molar ratio of the molar mass for each atom of the jets, corresponding to the results of our previous experiments. We calculate the mass of the top quarks in final states containing a few jets and large missing transverse energies. We have obtained the mass of the top quark in terms of the absence of jets in the primary sphere, which is a very high one. We find the molar ratio of 174.3 GeV/c2 vs 3.1 syst, where the systematic uncertainty is dominated by the uncertainties arising from the first state radiation and the gluon splitting. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1516, "title": "Compton Scattering of Fe K alpha Lines in Magnetic Cataclysmic Variables", "abstract": "  Compton scattering of X-rays in the bulk flow of the accretion column in magnetic cataclysmic variables (mCVs) can significantly shift photon energies. We present Monte Carlo simulations based on a nonlinear algorithm demonstrating the effects of Compton scattering on the H-like, He-like and neutral Fe K alpha lines produced in the post-shock region of the accretion column. The peak line emissivities of the photons in the post-shock flow are taken into consideration and frequency shifts due to Doppler effects are also included. We find that line profiles are most distorted by Compton scattering effects in strongly magnetized mCVs with a low white dwarf mass and high mass accretion rate and which are viewed at an oblique angle with respect to the accretion column. The resulting line profiles are most sensitive to the inclination angle. We have also explored the effects of modifying the accretion column width and using a realistic emissivity profile. We find that these do not have a significant overall effect on the resulting line profiles. A comparison of our simulated line spectra with high resolution Chandra/HETGS observations of the mCV GK Per indicates that a wing feature redward of the 6.4 keV line may result from Compton recoil near the base of the accretion column. ", "machine_abstract": "We report on the first detection of Compton scattering of iron K-alpha lines (6.4 keV) by hot plasma in magnetic cataclysmic variables (mCVs). The observations were made with Suzaku satellite, which has an unprecedented combination of high sensitivity and energy resolution at 6.4 keV for mCVs. We detected significant excess emission above continuum around 6.4 keV in three out of four targets observed. This is interpreted as Compton-scattered line photons produced in the accretion disk coronae surrounding white dwarfs. In one target, we found that the scattered component was significantly stronger than the direct component. Our results suggest that the geometry of the X-ray emitting region may be more complex than previously thought.  Keywords: Accreting compact objects, Hot plasma, Iron K-shell lines, Compton scattering, Suzaku, Cataclysmic variable", "paraphrased_abstract": "The X-ray is expected to be absorbed by the hot plasma, the iron K-alpha lines, and the fusion of the two gases, the composition of which is a unique, and a unique, phenomenon, of the magnetic cataclysms (mCVs). We have taken the first step in detecting fusion of the fusion of the two gases of iron at 6.4 keV. The results of our experiments indicate that the fusion of fusion and fusion is a complicated system, and the geometry of fusion and fusion may be more complicated than previously believed. We have analyzed the results of the first detection of fusion of iron at 6.4 keV by a hot plasma, which has a resolution of 6.4 keV. We have taken advantage of the Suzaku satellite for the detection of fusion at 6.4 keV. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3743, "title": "The Suzaku Observation of the Nucleus of the Radio-Loud Active Galaxy Centaurus A: Constraints on Abundances of the Accreting Material", "abstract": "  A Suzaku observation of the nucleus of the radio-loud AGN Centaurus A in 2005 has yielded a broadband spectrum spanning 0.3 to 250 keV. The net exposure times after screening were: 70 ks per X-ray Imaging Spectrometer (XIS) camera, 60.8 ks for the Hard X-ray Detector (HXD) PIN, and 17.1 ks for the HXD-GSO. The hard X-rays are fit by two power-laws of the same slope, absorbed by columns of 1.5 and 7 * 10^{23} cm^{-2} respectively. The spectrum is consistent with previous suggestions that the power-law components are X-ray emission from the sub-pc VLBI jet and from Bondi accretion at the core, but it is also consistent with a partial covering interpretation. The soft band is dominated by thermal emission from the diffuse plasma and is fit well by a two-temperature VAPEC model, plus a third power-law component to account for scattered nuclear emission, jet emission, and emission from X-ray Binaries and other point sources. Narrow fluorescent emission lines from Fe, Si, S, Ar, Ca and Ni are detected. The Fe K alpha line width yields a 200 light-day lower limit on the distance from the black hole to the line-emitting gas. Fe, Ca, and S K-shell absorption edges are detected. Elemental abundances are constrained via absorption edge depths and strengths of the fluorescent and diffuse plasma emission lines. The high metallicity ([Fe/H]=+0.1) of the circumnuclear material suggests that it could not have originated in the relatively metal-poor outer halo unless enrichment by local star formation has occurred. Relative abundances are consistent with enrichment from Type II and Ia supernovae. ", "machine_abstract": "We report results of X-ray observations with Suzaku satellite of the nucleus of radio-loud active galaxy Centaurus A (NGC 5128). The data were obtained in 2006 and 2007, when the source was observed for about 100 ks each time. We find that the spectrum is well described by an absorbed power-law model modified at low energies by photoelectric absorption due to cold gas. No significant emission lines are detected above 3 keV. Using this spectral model we derive constraints on abundances of heavy elements in the accretion flow onto supermassive black hole located in the center of NGC 5128. In particular, we show that iron abundance relative to solar value cannot be higher than 1.5-2 times its solar value. This result agrees with previous findings based on Chandra observation but contradicts earlier XMM-Newton measurement which suggested much larger overabundance of iron. Our analysis also shows that the absorbing material has relatively high column density NH ~ 2 x 1023 cm-2 .", "paraphrased_abstract": "The X-ray observations were taken from Suzaku's satellite at the epoch of the active galaxy Centaurus A, where a source was supposing for more than 100 ks. In the first place, we conclude that the abundance of iron in the accretion flows to the supermassive black hole located in the center of NGC 5128 is equal to that of the sun. In this second place, we confirm the observations made by Chandra, and confirm the results of X-rays from Newton, which indicated a large abundance of iron. Moreover, we also prove that the absorbing material has a density of NH 2x1023 cm-2. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.041, "title": "Curvature Inspired Cosmological Scenario", "abstract": "  Using modified gravity with non-linear terms of curvature, $R^2$ and $R^{(r +2)}$ (with $r$ being the positive real number and $R$ being the scalar curvature), cosmological scenario,beginning at the Planck scale, is obtained. Here, a unified picture of cosmology is obtained from $f(R)-$ gravity. In this scenario, universe begins with power-law inflation, followed by deceleration and acceleration in the late universe as well as possible collapse of the universe in future. It is different from $f(R)-$ dark energy models with non-linear curvature terms assumed as dark energy. Here, dark energy terms are induced by linear as well as non-linear terms of curvature in Friedmann equation being derived from modified gravity.It is also interesting to see that, in this model, dark radiation and dark matter terms emerge spontaneously from the gravitational sector. It is found that dark energy, obtained here, behaves as quintessence in the early universe and phantom in the late universe. Moreover, analogous to brane-tension in brane-gravity inspired Friedmann equation, a tension term $\\lambda$ arises here being called as cosmic tension. It is found that, in the late universe, Friedmann equation (obtained here) contains a term $- \\rho^2/2\\lambda$ ($\\rho$ being the phantom energy density) analogous to a similar term in Friedmann equation with loop quantum effects, if $\\lambda > 0$ and brane-gravity correction when $\\lambda < 0.$ ", "machine_abstract": "We propose an alternative scenario for the evolution of our universe, which is based on the idea that the expansion rate of the universe may be driven by its curvature rather than dark energy. We show how this can lead to a viable cosmology with no need for dark energy and without any fine tuning problems associated with other models in the literature. In particular we find that:  The model has a number of interesting features including:  This work was supported by the Australian Research Council (ARC) Discovery Project DP0877481. Any opinions expressed are those of the authors only. 1 Introduction. The discovery of accelerated cosmic expansion [1, 2] , as well as the recent detection of gravitational waves [3] have led to renewed interest in understanding the nature of gravity at large scales [4] . A possible explanation for these phenomena could lie within the framework of modified theories of gravity [5] . In order to explain the observed acceleration of the universe it seems necessary to introduce some form of \"dark energy\" [6] into Einstein's field equations [7, 8] . However, there appears to be little agreement amongst theorists about what exactly constitutes dark energy [9] or whether it should even exist [10] . Furthermore, if one assumes that dark energy exists then it must be extremely finely tuned [11] so that it behaves like a cosmological constant [12] over many orders of magnitude [13] . It also remains unclear why such a small value of vacuum energy density would arise naturally [14] . Another possibility is that the apparent accelerating behaviour of the universe arises due to quantum effects [15] . For example, loop quantum gravity [16] predicts that space-time becomes discrete [17] leading to corrections to the Friedmann equation [18] . These corrections become significant when the scale factor reaches values close to the Planck length [19] . Other approaches include string theory [20] where the extra dimensions of spacetime [21] provide another source of potential modifications [22] .", "paraphrased_abstract": "The discovery of accelerated cosmic expansion and the recent detection of gravitational waves have increased the interest in the nature of gravity at large scales. A possible explanation for the observed increase of gravity is the use of some type of dark energy, namely, the idea of which is \"dark energy\", namely, that it is not dark energy, but \"dark energy\"; it is assumed, however, to be a highly refined cosmological constant, a constant of many orders of magnitude; and also it is unclear why a small vacuum density naturally arises. In particular, we show that: - our model has several interesting features, including: - the \"dark energy\" is explained by the \"black energy\" and not by the dark energy. This implies that, for the \"dark energy\" to exist, it must be very carefully balanced, like a cosmological constant, for many orders of magnitude; and, in particular, there is no clear reason why this \"black energy\" has been so low. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0361, "title": "Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo Codes", "abstract": "  It has been observed that particular rate-1/2 partially systematic parallel concatenated convolutional codes (PCCCs) can achieve a lower error floor than that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can only be identified by means of an exhaustive search, whilst convergence towards low bit error probabilities can be problematic when the systematic output of a rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we present and study a family of rate-1/2 partially systematic PCCCs, which we call pseudo-randomly punctured codes. We evaluate their bit error rate performance and we show that they always yield a lower error floor than that of their rate-1/3 parent codes. Furthermore, we compare analytic results to simulations and we demonstrate that their performance converges towards the error floor region, owning to the moderate puncturing of their systematic output. Consequently, we propose pseudo-random puncturing as a means of improving the bandwidth efficiency of a PCCC and simultaneously lowering its error floor. ", "machine_abstract": "Turbo codes are powerful error correcting codes that have been adopted in several wireless standards, such as WiMax and 3GPP-LTE. However, their performance is limited by an error floor at high signal-to-noise ratios (SNRs). In this work we propose pseudo-random puncturing for turbo codes which can be used to lower the error floor. The proposed technique exploits the correlation between parity bits generated by different component encoders within a turbo code. We show through simulation results that our method outperforms existing techniques with respect to both coding gain and bit error rate (BER) performance. Finally, we present some practical considerations on how to implement the proposed scheme efficiently.     Turbo codes were invented by Berrou et al., [1] and they are powerful error-correcting codes that have been widely adopted in many communication systems including WiMAX [2] , DVB-S2 [3] , and 3GPP-LTE [4] . Turbo codes achieve near Shannon limit performance over additive white Gaussian noise channels [5] . However, there exists an error floor [6] - [8] when using turbo codes due to the inherent structure of these codes [9] . This limits the use of turbo codes in applications where low BERs are required [10] . Several methods have been proposed to reduce or eliminate the error floor [11]- [16] . These include interleaver design [17] , iterative decoding [18] , soft-decision feedback [19] , irregular repeat accumulate [20] , and concatenated codes [21] .", "paraphrased_abstract": "The turbo codes were invented by Berrou et al., and are powerful error-correcting codes, whose usage is well established in wireless communication, especially in WiMAX, DVB-S2 and 3GPP-LTE. They are of a kind that can be applied to many wireless systems, like WiMAX and 3GPP-LTE, but they have a disadvantage: they are not able to overcome the error-proneness at high SNRs. We present a new approach, based on pseudo-random puncturing, to reduce the error-proneness of turbo codes. It is possible to impose the turbo codes on high SNRs by combining the encoding, the decoding, and the encoding. But there is a difficulty in lowering the error-proneness of turbo codes, because of the structure of the codes, a limitation that prevents them from being used in the application where low BERs are required. We present in this paper a method of pseudo-random puncturing for turbo codes which, in addition to the regular puncture, has the advantage of reducing the error-proneness. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.338, "title": "Lorentzian and signature changing branes", "abstract": "  General hypersurface layers are considered in order to describe brane-worlds and shell cosmologies. No restriction is placed on the causal character of the hypersurface which may thus have internal changes of signature. Strengthening the results in our previous letter [1], we confirm that a good, regular and consistent description of signature change is achieved in these brane/shells scenarios, while keeping the hypersurface and the bulk completely regular. Our formalism allows for a unified description of the traditional timelike branes/shells together with the signature-changing, or pure null, ones. This allows for a detailed comparison of the results in both situations. An application to the case of hypersurface layers in static bulks is presented, leading to the general Robertson-Walker geometry on the layer --with a possible signature change. Explicit examples on anti de Sitter bulks are then studied. The permitted behaviours in different settings ($Z_{2}$-mirror branes, asymmetric shells, signature-changing branes) are analysed in detail. We show in particular that (i) in asymmetric shells there is an upper bound for the energy density, and (ii) that the energy density within the brane vanishes when approaching a change of signature. The description of a signature change as a `singularity' seen from within the brane is considered. We also find new relations between the fundamental constants in the brane/shell, its tension, and the cosmological and gravitational constants of the bulk, independently of the existence or not of a change of signature. ", "machine_abstract": "We study the dynamics of Lorentzian brane-worlds with time dependent bulk cosmological constant in the context of Randall-Sundrum type II models, where we allow for both positive and negative values of the bulk cosmological constant. We show that there are two branches of solutions corresponding to either an expanding or contracting universe on the brane depending upon whether the bulk cosmological constant is positive or negative respectively. In addition, we find that these solutions can be smoothly connected by a branch of static solutions which correspond to Minkowski space-time on the brane. Finally, we also consider the possibility of having a change of signature across the brane and discuss how this affects our results. The main motivation behind studying braneworld scenarios comes from the fact that they provide us with a natural framework within which one may address some of the fundamental issues related to quantum gravity such as the hierarchy problem [1] . However, it has been shown recently [2] , that even if one considers only classical gravitational effects, then the standard four dimensional Friedmann equations cannot be recovered unless certain conditions are satisfied. These include the requirement that the energy density of matter fields localized on the brane should satisfy the weak energy condition (WEC) [3] . In order to overcome this difficulty, several authors have considered different approaches [4] - [8] . For example, in ref. [6] , the authors studied the case when the bulk cosmological constant changes sign at late times thereby allowing them to obtain realistic cosmologies without violating WEC. On the other hand, in ref. [7] , the authors showed that it was possible to recover the standard Friedmann equation provided the stress-energy tensor satisfies the null energy condition (NEC).", "paraphrased_abstract": "I have been a long time since I have been in the field and I have been fascinated by the possibilities of a 'brane', which explains the reason for the existence of the brane. However, the main motivation of the study of brane systems is that they give us a natural frame of reference for some of the most fundamental questions of quantum gravity, such as the hierarchy problem. But recently, however, it has been observed that even for classical gravitational effects, the standard Friedmann equation cannot be recovered without certain conditions; in this case, the density of the localized matter, localized on the brane, should satisfy the weak energy condition (NEC). Thus, the authors have made the argument that, if the WEC is satisfied, the Friedmann equation is possible, provided that the stress-energy tensor is in compliance with the NEC., i.e., the value of the bulk cosmological constant increases at a very late time, and this allows the world to grow and contract without violating the WEC. Moreover, we have shown that the solution is smoothly connected to the fixed one, which corresponds to the Minkowski spacetime. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1273, "title": "Multicolor observations of the afterglow of the short/hard GRB 050724", "abstract": "  New information on short/hard gamma-ray bursts (GRBs) is being gathered thanks to the discovery of their optical and X-ray afterglows. However, some key aspects are still poorly understood, including the collimation level of the outflow, the duration of the central engine activity, and the properties of the progenitor systems. We want to constrain the physical properties of the short GRB 050724 and of its host galaxy, and make some inferences on the global short GRB population. We present optical observations of the afterglow of GRB 050724 and of its host galaxy, significantly expanding the existing dataset for this event. We compare our results with models, complementing them with available measurements from the literature. We study the afterglow light curve and spectrum including X-ray data. We also present observations of the host galaxy. The observed optical emission was likely related to the large flare observed in the X-ray light curve. The apparent steep decay was therefore not due to the jet effect. Available data are indeed consistent with low collimation, in turn implying a large energy release, comparable to that of long GRBs. The flare properties also constrain the internal shock mechanism, requiring a large Lorentz factor contrast between the colliding shells. This implies that the central engine was active at late times, rather than ejecting all shells simultaneously. The host galaxy has red colors and no ongoing star formation, consistent with previous findings on this GRB. However, it is not a pure elliptical, and has some faint spiral structure. GRB 050724 provides the most compelling case for association between a short burst and a galaxy with old stellar population. It thus plays a pivotal role in constraining progenitors models, which should allow for long delays between birth and explosion. ", "machine_abstract": "We report on multiwavelength observations of the X-ray, optical and radio afterglows of the short-hard gamma-ray burst (GRB) 050724 detected by Swift satellite at 07:24:06 UT on 24 July 2005. The prompt emission lasted for about 1 s with an average photon energy E = 300 keV in the 15-350 keV band. We find that the temporal decay index is ~1.2 between 10s to 1000s post-burst time scale which indicates that this event belongs to the class of ultra-long GRBs. In addition we also detect a possible rebrightening feature around 100s post-burst time-scale. Our spectral analysis shows that the spectrum can be fitted well using both single power-law model as well as broken power law model. However, the best fit parameters are found to be consistent within their errors when compared with each other. Using our multi-wavelength data set, we estimate the total energetics associated with this event to be ~3 x 1044 erg.", "paraphrased_abstract": "We spectroscopically estimate the total energy in the vicinity of the event to be 3 x 1044 ergs. The measured energy in this event is a combination of X-ray, optical, and radio afterglows from a GRB of 400 Hz, which was detected by Swift at the time of 24 UT on 24 July 2005. Our data show that the spectral distribution of this event can be compared with that of the single power law and the broken power law. In the case of the broken power law, we find that the best fit is a very consistent one. We present the first observation of X-ray, optical, and radio afterglows of the short-dark gamma-ray glimmer (Glimmer) 050724, detected by Swift by the Swift satellite at 08:00 UT on the 24th of July 2005. We show that the decay index is 1.2 between the intervals of 10s and 1000s, and that this glimmer is of a kind of extreme Glimmer. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.114, "title": "Depletion effects in smectic phases of hard rod--hard sphere mixtures", "abstract": "  It is known that when hard spheres are added to a pure system of hard rods the stability of the smectic phase may be greatly enhanced, and that this effect can be rationalised in terms of depletion forces. In the present paper we first study the effect of orientational order on depletion forces in this particular binary system, comparing our results with those obtained adopting the usual approximation of considering the rods parallel and their orientations frozen. We consider mixtures with rods of different aspect ratios and spheres of different diameters, and we treat them within Onsager theory. Our results indicate that depletion effects, and consequently smectic stability, decrease significantly as a result of orientational disorder in the smectic phase when compared with corresponding data based on the frozen--orientation approximation. These results are discussed in terms of the $\\tau$ parameter, which has been proposed as a convenient measure of depletion strength. We present closed expressions for $\\tau$, and show that it is intimately connected with the depletion potential. We then analyse the effect of particle geometry by comparing results pertaining to systems of parallel rods of different shapes (spherocylinders, cylinders and parallelepipeds). We finally provide results based on the Zwanzig approximation of a Fundamental--Measure density--functional theory applied to mixtures of parallelepipeds and cubes of different sizes. In this case, we show that the $\\tau$ parameter exhibits a linear asymptotic behaviour in the limit of large values of the hard--rod aspect ratio, in conformity with Onsager theory, as well as in the limit of large values of the ratio of rod breadth to cube side length, $d$, in contrast to Onsager approximation, which predicts $\\tau\\sim d^3$. ", "machine_abstract": "We study the effect of depletants on the phase behavior of binary mixtures of hard rods and hard spheres by computer simulation. We find that for sufficiently large concentrations of depletant, the system undergoes an isostructural transition to a new smectic phase with a different layer spacing than found in pure systems. The results are explained within mean-field theory using a free energy functional approach. DOI: 10.1063/1.3189000 I. INTRODUCTIO N The addition of small particles (depletants) to colloidal suspensions can lead to dramatic changes in their structure [1] . In particular, it has been shown experimentally [2] , theoretically [3] , and computationally [4] that adding nonadsorbing polymer chains or other types of depletants leads to the formation of ordered structures such as crystals [5] . In this work we consider the case where the depletants are spherical particles which do not adsorb onto the surfaces of the larger colloids [6] . This type of interaction between the two species gives rise to a rich variety of phenomena [7, 8] including liquid crystal phases [9] . Here we focus our attention on one aspect of these mixtures -the effect of depletants upon the smectic phases formed by mixtures of hard rods and spheres [10] . Smectics are mesophases characterized by layers of parallel oriented molecules separated by regions of disordered fluid [11] . These materials have attracted considerable interest because they exhibit many properties similar to those observed in biological membranes [12] . For example, both smectic A and C phases form spontaneously when lipids are mixed with water [13] . It was recently suggested [14] that smectic phases may also be present in certain protein solutions [15] .", "paraphrased_abstract": ", ed. - \"It is said that in some lipids the lipids will bind with water and lipids bind with water. This is a great interest in these materials, as they have all the properties of biological membranes. \" These materials, especially when they are dissolved in water, are often dissolved in water. In our study we study the effect of depletants on the structure of binary solids, rods and spheres. The depletants are spherical, and they do not bind to the surface of the larger solids, and the result is that the smectic phase undergoes a transformation in a different way from the normal crystals. We present here one aspect of these mixtures, namely, the effect of depletants on the structure of the smectic phases of the solids of hard rods and spheres. In this study, we consider the case of spherical, which does not adsorb on the surfaces of the larger solids. The study shows that in the presence of depletants, the system undergoes a dramatic transition to a new smectic phase, with a different thickness than in pure systems. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1305, "title": "Where post-Newtonian and numerical-relativity waveforms meet", "abstract": "  We analyze numerical-relativity (NR) waveforms that cover nine orbits (18 gravitational-wave cycles) before merger of an equal-mass system with low eccentricity, with numerical uncertainties of 0.25 radians in the phase and less than 2% in the amplitude; such accuracy allows a direct comparison with post-Newtonian (PN) waveforms. We focus on one of the PN approximants that has been proposed for use in gravitational-wave data analysis, the restricted 3.5PN ``TaylorT1'' waveforms, and compare these with a section of the numerical waveform from the second to the eighth orbit, which is about one and a half orbits before merger. This corresponds to a gravitational-wave frequency range of $M\\omega = 0.0455$ to 0.1. Depending on the method of matching PN and NR waveforms, the accumulated phase disagreement over this frequency range can be within numerical uncertainty. Similar results are found in comparisons with an alternative PN approximant, 3PN ``TaylorT3''. The amplitude disagreement, on the other hand, is around 6%, but roughly constant for all 13 cycles that are compared, suggesting that only 4.5 orbits need be simulated to match PN and NR waves with the same accuracy as is possible with nine orbits. If, however, we model the amplitude up to 2.5PN order, the amplitude disagreement is roughly within numerical uncertainty up to about 11 cycles before merger. ", "machine_abstract": "We present the first results for gravitational waves generated by binary black holes in full general relativity, using both post-post-Newtonian (PPN) theory and numerical relativity. We find that our PPN-calculated waveform agrees with the numerical one to within 1% at frequencies above 100 Hz. This is an improvement over previous calculations which used only PN theory or numerical relativity alone.     The agreement between these two methods provides us with confidence that we can use this hybrid approach as part of future searches for gravitational waves produced during mergers of compact objects such as neutron stars and/or black holes. In addition, it allows us to test whether current models are accurate enough to detect gravitational waves emitted by merging binaries containing supermassive black holes. These tests will be performed on data collected by LIGO/VIRGO detectors when they become operational later this year. Finally, we show how the hybrid method could also be useful for testing alternative theories of gravity.", "paraphrased_abstract": "We will show how the hybrid method could be useful for testing other theories of gravity. The first result is a new-to-classical wave, a hybrid, a fusion of pre-post-Newtonian theory and numerical relativity. This joint work shows that the wave is compatible with the numerical one at the frequencies above 100 Hz. We also prove that our hybrid method can be used for the investigation of merging binaries, in which black holes and neutron stars meet. This is an improvement from the previous calculation, which used only the fusion theory and the numerical one. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3031, "title": "The Cornell Caltech Atacama Telescope", "abstract": "  Cornell University, the California Institute for Technology, and the Jet Propulsion Laboratory are jointly studying the construction of a 25 m diameter telescope for submillimeter astronomy on a high mountain in northern Chile. This Cornell Caltech Atacama Telescope (CCAT) will combine high sensitivity, a wide field of view, and a broad wavelength range to provide an unprecedented capability for deep, large area, multi-color submillimeter surveys to complement narrow field, high resolution studies with ALMA. CCAT observations will address fundamental themes in contemporary astronomy, notably the formation and evolution of galaxies, the nature of the dark matter and dark energy that comprise most of the content of the universe, the formation of stars and planets, the conditions in circumstellar disks, and the conditions during the early history of the Solar system. The candidate CCAT site, at 5600 m in northern Chile, enjoys superb observing conditions. To accommodate large format bolometer cameras, CCAT is designed with a 20 arcmin field of view. CCAT will incorporate closed loop active control of its segmented primary mirror to maintain a half wavefront error of 10 mum rms or less. Instrumentation under consideration includes both short (650 mum-200 mum) and long (2 mm-750 mum) wavelength bolometer cameras, direct detection spectrometers, and heterodyne receiver arrays. The University of Colorado, a Canadian university consortium, and the UK Astronomy Technology Centre on behalf of the UK community are pursuing participation in the CCAT consortium. When complete early in the next decade, CCAT will be the largest and most sensitive facility of its class as well as the highest altitude astronomical facility on Earth. ", "machine_abstract": "The Cornell Caltech Atacama: The Cornell Caltech Atacame Observatory (CCAT) is an optical and infrared telescope under construction in Chile's northern desert, near the town of San Pedro de Atacama.  CCAT will be one of the largest telescopes ever built with a diameter of 24 meters.   It was designed to have unprecedented sensitivity for detecting faint objects such as distant galaxies or exoplanets around other stars.    CCAT will also provide high angular resolution images that can resolve individual stars within these distant galaxies. CCAT will use adaptive optics technology to correct for atmospheric distortions caused by turbulence in Earth's atmosphere. This will allow it to produce sharper images than are possible today using conventional techniques. CCAT will operate at wavelengths ranging from 0.35 microns to 16 microns. CCAT will consist of two main components:  A large primary mirror made out of silicon carbide.", "paraphrased_abstract": "Is a new telescope, in the northern desert, constructed by Cornell University in the northern desert. Its objective is to discover in the faint, such as the distant galaxies or the exoplanets around other stars. Its spectra will be in the order of ten times as sharp as is possible today with the traditional methods. Its primary lens is made of silicon carbide, its main body is of glass. Its main ring is of a single element: the primary mirror is a chromalite of about two meters in diameter, its focal point is a central ring of about four kilometres in length, and the telescope is located at the southern edge of the desert of the San Pedro de Atacama. The telescope is the largest of its kind in the world, with a diameter of 24 meters. It is designed to measure the distance between distant galaxies and the exoplanets around other stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2258, "title": "Growth mechanisms and structure of fullerene-like carbon-based thin films: superelastic materials for tribological applications", "abstract": "  In this chapter we review our findings on the bonding structure and growth mechanisms of carbon-based thin solid films with fullerene-like (FL) microstructure. The so-called FL arrangements arise from the curvature and cross-linking of basal planes in graphitic-like structures, partially resembling that of molecular fullerenes. This three-dimensional superstructure takes advantage of the strength of planar pi bonds in sp2 hybrids and confers the material interesting mechanical properties, such as high hardness, high elastic recovery, low-friction and wear-resistance. These properties can be tailored by controlling the curvature, size and connectivity of the FL arrangements, making these materials promising coatings for tribological applications. We have focused our interest mostly on carbon nitride (CNx) since nitrogen promotes the formation of FL arrangements at low substrate temperatures and they are emerging over pure carbon coatings in tribological applications such as protective overcoats in magnetic hard disks. We address structural issues such as origin of plane curvature, nature of the cross-linking sites and sp2 clustering, together with growth mechanisms based on the role of film-forming precursors, chemical re-sputtering or concurrent ion assistance during growth. ", "machine_abstract": "Fullerenes are spherical molecules composed entirely of sp2-hybridized carbons, which can be arranged in different ways to form various allotropes with unique properties. The most common is the C60 molecule (buckminsterfullerene), but other forms such as nanotubes or graphene have also been reported. Fullerenes were first discovered by Kroto et al., who received the Nobel Prize in Chemistry in 1996 for their work on them.  In this study we report on the growth mechanism and structural characterization of novel carbon-based nanomaterials that consist of an amorphous network of interconnected nanoparticles resembling fullerenes. These structures grow spontaneously when evaporating carbon atoms onto silicon substrates at temperatures between 600-700 \u00b0C under ultrahigh vacuum conditions. We show how these new carbon nanostructures exhibit remarkable mechanical properties including high elasticity and hardness combined with low friction coefficients making them ideal candidates for use as protective coatings against wear and corrosion.", "paraphrased_abstract": "This paper is about the growth and the structure of a new type of carbon-based composites made from amorphous carbon-atoms, of the kind commonly known as p-hybridized carbon, with a crystalline structure like a fullerene. This new composite consists of an amorphous network of interconnected particles, resembling fullerenes, that grows spontaneously on the silicon substrate, at temperatures of 600-700 \u00b0C, under ultra-high vacuum. The structure of these nanostructures has been shown to be of remarkable mechanical properties, with high elasticity, hardness and low friction, which make them ideal as a protective coating against wear and tear. In this work we describe the growth and structural properties of new carbon nanostructures, consisting of amorphous network of interconnected particles, like fullerenes, with a structure based on the synthesis of amorphous molecules, containing the spherical carbon, in which the carbon is attached in a single atom, and in which the carbon is attached in two different forms, the most common of which is the C60 molecule (buckminsterfullerene), but other kinds are also known, such as nanotubes and graphene. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0806, "title": "Interval Conjectures for level Hilbert functions", "abstract": "  We conjecture that the set of all Hilbert functions of (artinian) level algebras enjoys a very natural form of regularity, which we call the {\\em Interval Conjecture} (IC): If, for some positive integer $\\alpha $, $(1,h_1,...,h_i,...,h_e)$ and $(1,h_1,...,h_i+\\alpha ,...,h_e)$ are both level $h$-vectors, then $(1,h_1,...,h_i+\\beta ,...,h_e)$ is also level for each integer $\\beta =0,1,..., \\alpha .$ In the Gorenstein case, i.e. when $h_e=1$, we also supply the {\\em Gorenstein Interval Conjecture} (GIC), which naturally generalizes the IC, and basically states that the same property simultaneously holds for any two symmetric entries, say $h_i$ and $h_{e-i}$, of a Gorenstein $h$-vector.   These conjectures are inspired by the research performed in this area over the last few years. A series of recent results seems to indicate that it will be nearly impossible to characterize explicitly the sets of all Gorenstein or of all level Hilbert functions. Therefore, our conjectures would at least provide the existence of a very strong - and natural - form of order in the structure of such important and complicated sets.   We are still far from proving the conjectures at this point. However, we will already solve a few interesting cases, especially when it comes to the IC, in this paper. Among them, that of Gorenstein $h$-vectors of socle degree 4, that of level $h$-vectors of socle degree 2, and that of non-unimodal level $h$-vectors of socle degree 3 and any given codimension. ", "machine_abstract": "We prove the following interval conjectures on the values of certain level Hilbert functions, which are defined as follows.  Let $(R,m)$ be an Artinian local ring with maximal ideal $m$ and residue field $k$. For each integer $n$ let $R_n$ denote the localization of $R$ at the prime ideals containing $m^n$. The nth level Hilbert function $H_n(R)$ is then defined to be the number of elements in $k^n/m^{n+1}$.  Theorem 1 (Main theorem). If $(R,m,k)$ satisfies Serre's condition $S_2$, then there exists a positive constant $c$ such that  $$H_n(R) \\geq c n^2 + cn - 2c \\tag{1}$$ for all integers $n \\geq 1$. Corollary 1. If $(R, m, k)$ satisfies Serre\u2019s condition $S_2$, then we have $$H_n(R)  \\geq  \\frac{n}{2}(n-1)^2 + \\frac{n}{2}(n-1),\\tag{2}$$ for all integers $1 \\leq n \\leq 4$. Corollary 2. If $(R_1, m_1, k_1), (R_2, m_2, k_2)$ satisfy Serre\u2019s condition $S_{2}$, then we have $$\\max_{1\\leq i\\leq 2} H_i(R_i) \\geq \\frac{1}{2}(\\max_{1\\leq i \\leq 2} H_i(R_1)+H_i(R_2)).\\tag{3}$$", "paraphrased_abstract": "Theorem 1 (main theorem). Let \u2018R,\u2019 in the sense of an artinian local ring with the maximum ideal, m, and the residue field, k. Let \u2018R,\u2019 in the sense of \u2018R,\u2019 in the sense of \u2018R,\u2019 in the sense of \u2018R\u2019, where \u2018R\u2019 is the localization of \u2018R\u2019 at the peaks of \u2018k\u2019, and in the sense of \u2018h\u2019. In this sense, \u2018h\u2019 is a non-negative constant of \u2018c\u2019, so that \u2018h\u2019 = \u2018h\u2019 = \u2018h\u2019 = \u2018h\u2019 for all integers \u2018n\u2019 \u2013 \u2018h\u2019.\u2019 Then the \u2018nth\u2019 level Hilbert function \u2018H\u2019n\u2019,\u2018\u2018the number of elements of \u2018k\u2019/ \u2018h\u2019+1\u2019. Corollary 2. If \u2018h\u2019 is satisfied by \u2018h\u2019,\u2019 then \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019, \u2018h\u2019 and \u2018h\u2019 are satisfied by \u2018h\u2019. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1773, "title": "The Stokes phenomenon in the confluence of the hypergeometric equation using Riccati equation", "abstract": "  In this paper we study the confluence of two regular singular points of the hypergeometric equation into an irregular one. We study the consequence of the divergence of solutions at the irregular singular point for the unfolded system. Our study covers a full neighborhood of the origin in the confluence parameter space. In particular, we show how the divergence of solutions at the irregular singular point explains the presence of logarithmic terms in the solutions at a regular singular point of the unfolded system. For this study, we consider values of the confluence parameter taken in two sectors covering the complex plane. In each sector, we study the monodromy of a first integral of a Riccati system related to the hypergeometric equation. Then, on each sector, we include the presence of logarithmic terms into a continuous phenomenon and view a Stokes multiplier related to a 1-summable solution as the limit of an obstruction that prevents a pair of eigenvectors of the monodromy operators, one at each singular point, to coincide. ", "machine_abstract": "In this article, we study the confluence of the second order linear differential equations with variable coefficients and their solutions by applying the method of successive approximations to find the solution of the confluent hypergeometric equation (CHGE). We also use the concept of the Stokes phenomenon for finding the singularities of the CHGE's solutions at infinity. The results are illustrated numerically through examples. Keywords: Confluence of differential equations; Stokes phenomenon; Hypergeometric functions. 1 Introduction In mathematics, the confluent hypergeometic function is defined as the sum of all possible products of n terms taken from an infinite series of complex numbers [1] . It has been used extensively in many fields such as physics [2] , engineering [3] , computer science [4] , statistics [5] , etc., because it can be applied to solve various types of problems [6] . The confluent hypergeometric function is denoted by ${}_1F_1$ or ${_2F_1}$ depending on whether the first argument is equal to one or not [7, 8] . For example, if the first argument equals one then the function is called the Kummer confluent hypergeometric function:", "paraphrased_abstract": "And so we have, to give you an idea of the type of confluence, the confluence of differential equations with variable coefficients and their solutions, a method for the successive approximation of the confluence hypergeometric equation. The confluence hypergeometric function is characterized by the sufficiency of a first argument and the sufficiency of a second argument. In our case the solution is the confluence hypergeometric function. The confluence hypergeometric function is called the Kummer confluence hypergeometric function: It is the sum of the products of the first argument of an infinite series of complex numbers. In this respect, the confluence of the differential equations with variable coefficients is investigated by using the method of successive approximations. The confluence of the two equations is described in a mathematical form and is referred to by the name of a hypergeometric function. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4683, "title": "Valley Dependent Optoelectronics from Inversion Symmetry Breaking", "abstract": "  Inversion symmetry breaking allows contrasted circular dichroism in different k-space regions, which takes the extreme form of optical selection rules for interband transitions at high symmetry points. In materials where band-edges occur at noncentral valleys, this enables valley dependent interplay of electrons with light of different circular polarizations, in analogy to spin dependent optical activities in semiconductors. This discovery is in perfect harmony with the previous finding of valley contrasted Bloch band features of orbital magnetic moment and Berry curvatures from inversion symmetry breaking [Phys. Rev. Lett. 99, 236809 (2007)]. A universal connection is revealed between the k-resolved optical oscillator strength of interband transitions, the orbital magnetic moment and the Berry curvatures, which also provides a principle for optical measurement of orbital magnetization and intrinsic anomalous Hall conductivity in ferromagnetic systems. The general physics is demonstrated in graphene where inversion symmetry breaking leads to valley contrasted optical selection rule for interband transitions. We discuss graphene based valley optoelectronics applications where light polarization information can be interconverted with electronic information. ", "machine_abstract": "We report on the valley dependent optoelectronic properties in monolayer WSe2, which is an inversion symmetry breaking semiconductor with strong spin-orbit coupling and large exciton binding energy. We show that circularly polarized light can be used to control the valley polarization of photoexcited carriers by optical pumping at room temperature. The valley polarization lifetime is found to be about 1 ns for both electrons and holes under weak excitation conditions. This work opens up new opportunities for exploring novel valleytronic devices based on 2D materials.     Valleytronics has been proposed as one promising approach towards realizing spin-based electronics beyond conventional silicon technology1-5 . Recently, it was shown that the valley degree of freedom could also play important roles in many other physical phenomena such as phonon transport6 , thermoelectricity7-10 , and superconductivity11-13 .      Monolayer transition metal dichalcogenides (TMDCs) are emerging two-dimensional semiconductors14-17 with broken inversion symmetry18-20 due to their unique layered structure21-23 . They have attracted great attention because they exhibit remarkable electronic24-26 , mechanical27-29 , thermal30-32 , and optical33-35 properties. Moreover, TMDCs possess high carrier mobility36-38 , making them ideal candidates for future valleytronic applications39-41 .     Here we demonstrate valley-dependent optoelectronic properties of monolayer WSe2 using time-resolved photoluminescence spectroscopy42-45 . By exciting WSe2 with circularly polarized light, we observe that the valley polarization lifetimes of photo-excited carriers are around 1ns for both electrons and holes46-48 . Our results provide direct evidence for valleydependent optoelectronic processes in this material system49-51 .", "paraphrased_abstract": "\u201cThese results suggest that a 'valley-free' state of light could be used to control the valley-free state of electrons and holes. These results indicate that the polarization of electrons and holes is a valley-free state. This discovery opens up new possibilities for introducing spin-based electronics to 2D materials., such as phonon transport and thermoelectricity, and superconductivity. This TMDC is a new and advanced synthesis of two-dimensional semiconductors of a two-dimensional shape, a structure of a curved shape, a dense and crystalline structure, and is highly conductive. The conductive and conductive properties of the WSe2 are inseparably correlated with the magnetic and magnetic properties of the material, making them an excellent candidate for synthesis of the valley-complex. The study of the valley-complexity of WSe2 is presented in time and space, and the results indicate that WSe2 is a strong and symmetrical inversion-semice with a strong magnetic and conductive coupling. Moreover, the conductive and conductive properties of the WSe2 are highly favorable to the synthesis of the magnetic and conductive materials. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2813, "title": "Theory of anomalous diffusive reaction rates on realistic self-affine fractals", "abstract": "  In this letter, a theoretical method for the analysis of diffusive flux/current to limited scale self-affine random fractals is presented and compared with experimentally measured electrochemical current for such roughness. The theory explains the several experimental findings of the temporal scale invariance as well as deviation from this of current transients in terms of three dominant fractal parameters for the limited-length scales of roughness. This theoretical method is based on limited scale power-law characterization of the interfacial roughness spectrum and the solution of diffusion equation under the diffusion-limited boundary conditions on rough interfaces. More broadly, these calculations challenges the earlier belief that the anomalous behavior is solely dependent on fractal dimension of roughness and highlight the potential to apply this equation for the scale invariant roughness determination. Finally, the validity of theoretical result is tested with extensive experimental data. ", "machine_abstract": "We present the theory for anomalous diffusion in terms of fractional Fokker-Planck equations and apply it to study the time evolution of chemical reactions occurring on realistic self-affined fractals, such as porous media or biological tissues. We show that the rate at which reactants are consumed is determined by the geometry of the medium through an effective fractal dimension D(t) that evolves with time according to a nonlinear differential equation. The solution of this equation depends on the initial conditions and can be obtained numerically using standard methods. In particular we find that if the initial distribution has compact support then the system reaches equilibrium after some characteristic relaxation time t*. For times larger than t* the consumption rate becomes independent of the initial condition and coincides with the one predicted by classical mean field theories. This result suggests that the dynamics of chemical reactions in complex environments may be described by simple models based only on geometrical information about the environment.", "paraphrased_abstract": "The existence of this assemblage of conditions demonstrates that the dynamics of chemical reactions in complex environments can be described by simple mathematical models, based only on geometric information. We show that the rate at which the reaction occurs is influenced by the geometry of the medium through an effective dimension D(t) which is subtracted from the initial condition, and grows up with time according to a differential differential equation. This equation depends on the initial conditions, and can be computed with standard methods. We find that if the initial distribution is compact, then the system reaches equilibrium after a characteristic period t*. For longer periods, the consumption rate is independent of the initial conditions and converges with the one predicted by the classic mean field theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2466, "title": "Is diversity good?", "abstract": "  Prominent ethical and policy issues such as affirmative action and female enrollment in science and engineering revolve around the idea that diversity is good. However, even though diversity is an ambiguous concept, a precise definition is seldom provided. We show that diversity may be construed as a factual description, a craving for symmetry, an intrinsic good, an instrumental good, a symptom, or a side effect. These acceptions differ vastly in their nature and properties. The first one cannot lead to any action and the second one is mistaken. Diversity as intrinsic good is a mere opinion, which cannot be concretely applied; moreover, the most commonly invoked forms of diversity (sexual and racial) are not intrinsically good. On the other hand, diversity as instrumental good can be evaluated empirically and can give rise to policies, but these may be very weak. Finally, symptoms and side effects are not actually about diversity. We consider the example of female enrollment in science and engineering, interpreting the various arguments found in the literature in light of this polysemy.   Keywords: ethics, policy, higher education, female students, minority students, affirmative action ", "machine_abstract": "The question is not new, but it has been recently revived by the publication of two papers in Nature and Science that claim to show that biodiversity loss leads to ecosystem collapse.  The authors argue that this finding should be taken seriously because ecosystems are essential for human well-being.   They also point out that there have been many previous studies showing that biodiversity loss can lead to declines in ecosystem functioning (e.g., productivity) without necessarily causing an abrupt change in state or collapse.    In this article we review these recent findings on biodiversity-ecosystem function relationships as well as some earlier results suggesting that biodiversity may sometimes enhance rather than reduce ecosystem stability.  We conclude with a discussion about how our understanding of biodiversity-ecosystem function interactions could be improved through further research. Biodiversity loss is one of humanity's greatest challenges today. It threatens the sustainability of natural resources used directly by humans such as food production systems and water supply, and indirectly via changes in climate regulation and disease transmission pathways. There is growing concern over the rate at which species extinction rates are increasing globally due to anthropogenic activities including habitat destruction, pollution, overexploitation, and invasive alien species1\u20133. This situation has led to calls for urgent action to conserve biological diversity4\u20136. However, despite widespread recognition of the importance of conserving biodiversity7\u201310, there remains considerable uncertainty regarding its role in maintaining ecosystem functions11\u201313. A number of theoretical models suggest that biodiversity loss will cause reductions in ecosystem functioning14\u201316. For example, Tilman et al. (1997)17 showed theoretically that reducing plant species richness would decrease primary productivity in grassland communities. Similarly, Naeem & Li (1998)18 found experimentally that removing species from soil microcosms reduced decomposition rates. These predictions were supported by numerous subsequent empirical studies19\u201322.", "paraphrased_abstract": "A significant concern is that the deficiency of biodiversity threatens the ecological stability of natural resources, which are directly used by humans, for food and water, and indirectly by climate and disease transmission. It is not new, but it is recently resurrected by two studies in Nature and Science that have shown that the loss of species leads to the collapse of the ecosystem. We will examine these results, and then discuss the implications of our research on the relationship between biodiversity and the functioning of ecosystems. Moreover, a few previous studies have shown that the loss of species leads to the decline of the system and the collapse of the ecosystem. In this sense, the authors say, it is important to take note of this, because the function of the ecosystem is essential for the human well-being. This is a matter of no small concern for humanity; it is an important challenge to the balance of natural resources, both directly in the production of food and water, and indirectly through the regulation of climate and disease transmission. In this regard, the question is not new; however, two recent papers have been published in Nature and Science, arguing that the loss of biodiversity leads to the collapse of the ecosystem. The authors say that the hypothesis is serious; because the ecosystem is necessary for human health, and therefore the importance of the organism is greatly emphasized. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2636, "title": "Linkage between Accretion Disks and Blazars", "abstract": "  The magnetic field in an accretion disk is estimated assuming that all of the angular momentum within prescribed accretion disk radii is removed by a jet. The magnetic field estimated at the base of the jet is extrapolated to the blazar emission region using a model for a relativistic axisymmetric jet combined with some simplifying assumptions based on the relativistic nature of the flow. The extrapolated magnetic field is compared with estimates based upon the synchrotron and inverse Compton emission from three blazars, MKN 501, MKN 421 and PKS 2155-304. The magnetic fields evaluated from pure synchrotron self- Compton models are inconsistent with the magnetic fields extrapolated in this way. However, in two cases inverse Compton models in which a substantial part of the soft photon field is generated locally agree well, mainly because these models imply magnetic field strengths which are closer to being consistent with Poynting flux dominated jets. This comparison is based on estimating the mass accretion rate from the jet energy flux. Further comparisons along these lines will be facilitated by independent estimates of the mass accretion rate in blazars and by more detailed models for jet propagation near the black hole. ", "machine_abstract": "We present the results of our analysis on the correlation between blazar activity and accretion disk luminosity in active galactic nuclei (AGN). We use data for all AGNs with available information about their black hole mass, bolometric luminosity, radio flux density at 5 GHz, and redshift taken from the literature. The sample consists of 1248 objects including both BL Lacertae-type and flat-spectrum radio quasars. Our main result is that there exists an anti-correlation between the radio emission produced by jets and the optical/UV radiation emitted by disks. This effect can be explained if we assume that the jet power decreases as the disk luminosity increases. In addition to this general trend, we find evidence for two separate branches in the distribution of sources along the radio-optical plane. One branch corresponds to low-luminosity AGNs whose radio emission correlates positively with the disk luminosity; another one includes high-luminosity AGN where the radio emission anticorrelates strongly with the disk luminosity.", "paraphrased_abstract": "There is an effect in which the radiance of jets is countered by the radiance of the disk. One effect is that the radiance of the disk is positively proportioned with the radiance of the disk. Then, for the case of high-radiance AGNs, the radiance of the disk is strongly proportioned with the radiance of the disk. In our study, we present data for all AGNs with available information about their mass, mass, bolometric luminosity, radio flux density at 5 GHz, and redshift, based on literature. We present our results on the correlation of jets and optical and UV radiation in active Galactic nuclei. We obtain data for 1248 AGNs of various types, with a total of 1248 AGNs, and with a variety of radio quasars, both BL Lacertae-type and BL-flat-spectrum. We then present two separate lines of evidence about the distribution of sources along the radio optic plane. One line corresponds to low-light AGNs whose radio emitted positively correlates with the disk luminosity; another one describes high-light AGNs, in which the radio emitted strongly. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.286, "title": "Cryptoplanet update", "abstract": "  We have had several talks recently reviewing 11 years of exoplanet discoveries through radial velocity variations, or from transits, or from microlensing. More than 200 exoplanets have been found, including some around pulsars that we do not discuss here.   My physical definition for a planet is a roughly spherical, self-gravitating body more massive than 10**26 g formed from the leftover material in a protostellar disk after the protostar forms. Radiation from the protostar pushes the inner wall of the disk outward. The material agglomerates and forms planets in radial sequence. The outer planets are formed slowly by classical dynamical mechanisms acting in the snow zone. Planets have dense cores because of agglomeration.   Not one of the exoplanets discovered thus far is a planet. They are cryptoplanets formed from matter ejected by protostars. When protostars have excessive infall at high latitudes, they partially balance angular momentum through outflow at the equator as they spin up. The ejected matter is trapped in the magnetic torus formed between the star and the disk, like a tokamak. The tokamak eventully reconnects and magnetic compression forms self-gravitating remnants trapped and compressed by a closed spherical magnetic field, spheromaks. Cooled spheromaks are cryptoplanets. They orbit near the star. They can merge with each other or fall into the star or be ejected. They can grow by accreting gas. They have a low density core and abundances characteristic of the protostar. Their masses, radii, densities, and orbits are random, and are inconsistent with the parameters for planets. They tend to have lower density than planets. ", "machine_abstract": "The Cryptoplanet project is an ongoing effort to collect and archive data on the world's cryptocurrencies, including Bitcoin (BTC), Ethereum (ETH) and Litecoin (LTC). The goal of this project is to provide researchers with access to historical information about these currencies in order to facilitate research into their underlying technologies.  This article describes how we collected our dataset for analysis as well as some preliminary results that have been obtained using it. We also describe plans for future work. In recent years there has been growing interest among academics in studying virtual currency systems such as Bitcoin [1] . One reason for this interest is that many believe that Bitcoin's success will lead to the development of new types of digital payment systems [2] , which could potentially be used by millions of people around the world [3] . In addition to its potential use as a means of exchange, Bitcoin may also serve as a platform for other applications [4] . For example, one can imagine a system where users pay each other directly via Bitcoins without having to rely on third parties like banks or credit card companies [5] . Another possible application would involve storing Bitcoins in cold storage [6] so they are not vulnerable to theft or loss due to hacking attacks [7, 8] . Finally, Bitcoin transactions might even be used to settle financial contracts [9] . Despite all of these exciting possibilities, however, little academic research has been done on Bitcoin itself [10] . Most existing studies focus instead on related topics such as mining [11] , price prediction [12] , transaction processing [13] , and security [14] .", "paraphrased_abstract": "The study of the cryptographic system has been increasing in recent years, and the main reason for this interest is that there will be an increase in the number of users in the world. The cryptocurrencies are used for payment, and this may be used to pay a foreigner, for example, to pay a foreigner without having to rely on third parties, like banks or credit cards. Another possibility is to put the coins in cold storage so that they can never be stolen or lost, and if there is any risk of theft or theft, they may be retrieved and used in the settlement of transactions. The present article presents our research, and describes some of the results obtained. However, the most important question remains: what is the significance of Bitcoin? The existence of Bitcoin, the very concept of digital money, is attracting the attention of many academics. There is an enormous need to study the various kinds of cryptocurrencies that are like Bitcoin, and many have tried to use them. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.257, "title": "Outer jet X-ray and radio emission in R Aquarii: 1999.8 to 2004.0", "abstract": "  Chandra and VLA observations of the symbiotic star R Aqr in 2004 reveal significant changes over the three to four year interval between these observations and previous observations taken with the VLA in 1999 and with Chandra in 2000. This paper reports on the evolution of the outer thermal X-ray lobe-jets and radio jets. The emission from the outer X-ray lobe-jets lies farther away from the central binary than the outer radio jets, and comes from material interpreted as being shock heated to ~10^6 K, a likely result of collision between high speed material ejected from the central binary and regions of enhanced gas density. Between 2000 and 2004, the Northeast (NE) outer X-ray lobe-jet moved out away from the central binary, with an apparent projected motion of ~580 km s^-1. The Southwest (SW) outer X-ray lobe-jet almost disappeared between 2000 and 2004, presumably due to adiabatic expansion and cooling. The NE radio bright spot also moved away from the central binary between 2000 and 2004, but with a smaller apparent velocity than of the NE X-ray bright spot. The SW outer lobe-jet was not detected in the radio in either 1999 or 2004. The density and mass of the X-ray emitting material is estimated. Cooling times, shock speeds, pressure and confinement are discussed. ", "machine_abstract": "We present new results on the outer jets of the symbiotic star, R Aqr (=V1016 Cyg). We have analyzed archival Chandra data obtained between 1999 August 31 and 2000 September 30 as well as XMM-Newton observations taken between 2001 October 24 and 2002 November 3. The analysis shows that both jets are still active at least up to 2004 January 1. In addition we report on an optical spectroscopic campaign carried out with the Nordic Optical Telescope during 2003 December 10-17 which revealed no significant changes compared to previous campaigns. Finally, we discuss our findings within the context of current models for the formation of bipolar nebulae around evolved stars. Keywords: Symbiosis, Jets, Bipolar Nebulae, Stellar Winds, Mass Ejection, Binary Star Systems, Chandra Observatory, XMM-Newton Observatory, R Aquarius, V1016 Cyg", "paraphrased_abstract": "\u2018Symbiosis, Jets, Bipolar Nebulae, Mass Ejection, Binary Stars, Chandra Observatory, XMM-Newton Observatory, R Aquarius, V1016 Cyg. We present new results on the outer jets of the symbiotic star, R Aquarius, V1016 Cyg. Our analysis reveals that these jets are active to the end of 2004. We have also analyzed the Chandra data from 1999 to the end of 2000, and XMM-Newton data from 2001 to the end of 2002. The analysis shows that they are active up to the end of 2004. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0783, "title": "Non-cooperative games for spreading code optimization, power control and receiver design in wireless data networks", "abstract": "  This paper focuses on the issue of energy efficiency in wireless data networks through a game theoretic approach. The case considered is that in which each user is allowed to vary its transmit power, spreading code, and uplink receiver in order to maximize its own utility, which is here defined as the ratio of data throughput to transmit power. In particular, the case in which linear multiuser detectors are employed at the receiver is treated first, and, then, the more challenging case in which non-linear decision feedback multiuser receivers are adopted is addressed. It is shown that, for both receivers, the problem at hand of utility maximization can be regarded as a non-cooperative game, and it is proved that a unique Nash equilibrium point exists. Simulation results show that significant performance gains can be obtained through both non-linear processing and spreading code optimization; in particular, for systems with a number of users not larger than the processing gain, remarkable gains come from spreading code optimization, while, for overloaded systems, the largest gainscome from the use of non-linear processing. In every case, however, the non-cooperative games proposed here are shown to outperform competing alternatives. ", "machine_abstract": "In this thesis we study the problem of optimizing resource allocation in wireless networks by using non-cooperative game theory. We consider three different problems: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. In each case, we formulate an optimization problem as a noncooperative game between users competing to maximize their own utility functions. Then, we propose distributed algorithms that converge to Nash equilibria of these games. Finally, we evaluate our proposed schemes through extensive simulations on both static and mobile scenarios.     Keywords: Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium. 1 Introduction     The rapid growth of wireless communication has led to increased demand for high quality services such as voice over IP (VoIP), video streaming, online gaming etc., which require efficient use of limited resources available at base stations or access points. To meet this growing demand, researchers have been working towards developing new techniques to improve the performance of existing wireless systems while maintaining low cost and energy consumption [1] . One promising approach is to optimize resource allocations among users in order to increase overall system throughput [2] , reduce interference [3] , minimize transmission delay [4] , and/or enhance fairness [5] .   The main challenge faced when designing resource allocation strategies lies in the fact that there are usually multiple conflicting objectives [6] . For example, maximizing total user satisfaction may lead to unfair distribution of resources across users [7]; increasing spectral efficiency can cause severe inter-user interference [8] ; minimizing transmission delays may result in poor channel utilization [9] . Therefore, it becomes necessary to develop novel approaches that strike a balance between various conflicting goals [10] .     This work was supported in part by NSF under Grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.", "paraphrased_abstract": "And to cope with the increased demand for wireless services, it is necessary to use the accumulated resources to make the most of them. In the present study, we study the problem of resource allocation in wireless networks by applying non-cooperative game theory. We consider three different problems: (1) code optimization, (2) power control, (3) receiver design. In each case, we introduce a non-cooperative game of games with competing players to improve the efficiency of their respective services. Hence, we propose a novel method of balancing these competing games, and we demonstrate this in detail. Then, we propose distributed algorithms that are able to achieve the Nash Equilibria of these games. The main difficulty of balancing resource allocations in wireless networks is that there are often conflicting goals, such as maximizing the total rate of satisfaction, maximizing the spectral efficiency of the wireless systems, minimizing the transmission delay, maximizing the efficiency of the wireless networks. In the present study, we propose three different methods: (1) code optimization, (2) power control, (3) receiver design. The three strategies are: (1) code optimization, (2) power control, (3) receiver design. Then, we propose the distributed algorithms to make the games Nash Equilibrium. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2762, "title": "Complementarity in the Bohr-Einstein Photon Box", "abstract": "  The photon box thought experiment can be considered a forerunner of the EPR-experiment: by performing suitable measurements on the box it is possible to ``prepare'' the photon, long after it has escaped, in either of two complementary states. Consistency requires that the corresponding box measurements be complementary as well. At first sight it seems, however, that these measurements can be jointly performed with arbitrary precision: they pertain to different systems (the center of mass of the box and an internal clock, respectively). But this is deceptive. As we show by explicit calculation, although the relevant quantities are simultaneously measurable, they develop non-vanishing commutators when calculated back to the time of escape of the photon. This justifies Bohr's qualitative arguments in a precise way; and it illustrates how the details of the dynamics conspire to guarantee the requirements of complementarity. In addition, our calculations exhibit a ``fine structure'' in the distribution of the uncertainties over the complementary quantities: depending on \\textit{when} the box measurement is performed, the resulting quantum description of the photon differs. This brings us close to the argumentation of the later EPR thought experiment. ", "machine_abstract": "We present an experimental investigation into complementarity between position and momentum measurements on single photons using a modified version of the original Einstein-Bohr photon box experiment.  The results show that, for this particular measurement scheme, there is no violation of Bell's inequality or any other form of nonlocality. We also demonstrate how our setup can be used to investigate quantum contextuality by performing two different experiments with identical settings but opposite outcomes. In one case we observe violations of Bell inequalities while in the other they are not violated. This shows that the observed behavior cannot be explained within classical physics and demonstrates quantum contextuality. Quantum mechanics predicts that certain physical quantities such as position and momentum do not have simultaneous well-defined values. Instead these quantities exist only as probability distributions which evolve continuously over time according to Schr\u00f6dinger's equation. However, it has been shown that if both position and momentum were measured simultaneously then their respective probabilities would interfere destructively resulting in a zero probability of measuring either quantity at its most probable value [1] . This phenomenon known as Heisenberg uncertainty principle leads to the concept of complementarity: the impossibility of observing all properties of a system simultaneously [2] . In 1964 John Bell showed that local hidden variable theories could not explain some predictions made by quantum mechanics [3] , leading to the formulation of Bell's theorem [4] . Since then many experiments have been performed to test whether quantum mechanical predictions violate Bell's theorem [5] . These tests typically involve entangled particles [6] where each particle carries information about the state of another distant particle [7, 8] . If the particles are separated far enough so that they never interact again after being created, then the correlations between them must be due solely to quantum effects [9] .", "paraphrased_abstract": "At the end of 1964, John Bell, a fellow in the field of hidden variables, made it clear that some of the predictions of quantum mechanics could not be proved to be true, led to the development of the Bell theorem. Since then many experiments have been carried out to see whether the predictions of quantum mechanics violate the Bell theorem. The tests usually involve entangled particles which contain information on the state of another particle. If the particles are separated far enough from each other that they never interact again, the correlation between them must be due solely to quantum effects. The Bell theorem was proposed in 1964 and was confirmed in the field by John Bell, who insisted that local hidden variable theories of the field could not be explained by the quantum mechanics, and therefore he developed Bell\u2019s theorem. We have proved that this is not the case in classical physics, and that the phenomenon cannot be explained in terms of quantum mechanics. We present an experimental study of the coexistence of position and momentum on a single photon, using the modified version of the Einstein-Bohr experiment. The results show that this measurement technique is not a violation of Bell\u2019s inequality, nor of any other type of nonlocality. We present two examples of this phenomenon: one is a violation of Bell\u2019s inequality, while the other is not a violation of Bell\u2019s inequality", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3583, "title": "Relaxation of a dewetting contact line Part 2: Experiments", "abstract": "  The dynamics of receding contact lines is investigated experimentally through controlled perturbations of a meniscus in a dip coating experiment. We first characterize stationary menisci and their breakdown at the coating transition. It is then shown that the dynamics of both liquid deposition and long-wavelength perturbations adiabatically follow these stationary states. This provides a first experimental access to the entire bifurcation diagram of dynamical wetting, confirming the hydrodynamic theory developed in Part 1. In contrast to quasi-static theories based on a dynamic contact angle, we demonstrate that the transition strongly depends on the large scale flow geometry. We then establish the dispersion relation for large wavenumbers, for which we find that sigma is linear in q. The speed dependence of sigma is well described by hydrodynamic theory, in particular the absence of diverging time-scales at the critical point. Finally, we highlight some open problems related to contact angle hysteresis that lead beyond the current description. ", "machine_abstract": "We present new experimental results on the relaxation dynamics of a liquid film that is pulled off an inclined solid substrate by gravity and capillarity. The experiments are performed in a microgravity environment aboard the International Space Station (ISS). We find that, for sufficiently large pulling speeds, the relaxation process can be described as a succession of three stages. In stage I, the contact angle decreases rapidly to its equilibrium value at which point the contact line stops moving. Stage II starts when the contact line has stopped moving; during this stage, the contact angle remains constant while the height profile of the free surface continues evolving towards its final shape. Finally, in stage III, the contact angle increases again until it reaches its initial value. This behavior is explained using a simple model based on lubrication theory. Our analysis shows that the relaxation time scales with the inverse of the pulling speed. For small pulling speeds, we observe deviations from our theoretical predictions due to inertial effects.     Liquid films play important roles in many natural phenomena such as rain drops falling down a window or water evaporating into air. Dewetting processes occur frequently in nature but also have applications in industrial processes like coating technologies where thin films need to be removed from substrates. Understanding how these systems evolve over time requires knowledge about their relaxation dynamics after they have been perturbed out of equilibrium. Here, we study experimentally the relaxation of a liquid film that was pulled off an inclined solid wall by gravity and capillary forces.", "paraphrased_abstract": "The dewetting of water and moisture are common in nature, but they are also useful in the industrial sector, where thin film is needed to be removed from a surface. Liquids play a vital role in many natural phenomena, such as rain falling down a window or water evaporating into the air. We present experimental results on the relaxation of a liquid film that is sucked out of an inclined wall by gravity and capillary forces. We show that the relaxation of a liquid film sucked out of an inclined wall is three stages. In the first phase the contact angle falls to the equilibrium value and then the contact angle decreases to its equilibrium value. In the second phase the contact angle is steady and the surface gradually develops into a higher dimension. Finally, in the third phase the contact angle increases again to its initial value. We show that the relaxation time is proportionate to the force of the pull. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1587, "title": "Possible X-ray diagnostic for jet/disk dominance in Type 1 AGN", "abstract": "  Using Rossi X-ray Timing Explorer Seyfert 1 and 1.2 data spanning 9 years, we study correlations between X-ray spectral features. The sample consists of 350 time-resolved spectra from 12 Seyfert 1 and 1.2 galaxies. Each spectrum is fitted to a model with an intrinsic powerlaw X-ray spectrum produced close to the central black hole that is reprocessed and absorbed by material around the black hole. To test the robustness of our results, we performed Monte Carlo simulations of the spectral sample. We find a complex relationship between the iron line equivalent width (EW) and the underlying power law index (Gamma). The data reveal a correlation between Gamma and EW which turns over at Gamma <~ 2, but finds a weak anti-correlation for steeper photon indices. We propose that this relationship is driven by dilution of a disk spectrum (which includes the narrow iron line) by a beamed jet component and, hence, could be used as a diagnostic of jet-dominance. In addition, our sample shows a strong correlation between the reflection fraction (R) and Gamma, but we find that it is likely the result of modeling degeneracies. We also see the X-ray Baldwin effect (an anti-correlation between the 2-10 keV X-ray luminosity and EW) for the sample as a whole, but not for the individual galaxies and galaxy types. ", "machine_abstract": "We present the results of an analysis of Chandra observations of two nearby radio-quiet quasars, PG 1211+143 and Mrk 335 (z=0.034). We find that both sources show evidence for significant variability on timescales as short as hours to days. The observed flux changes are consistent with those expected if these objects have relativistic jets which precess around their axes at small angles to our line-of-sight. In addition we detect rapid flaring activity in the soft X-rays during one observation of each source. This is most likely due to thermal instabilities within the accretion disk. Finally, we discuss how this behaviour may be used to diagnose whether or not an active galactic nucleus has a dominant jet component. Keywords: Active Galactic Nuclei, Quasars, Jet Precession, Flare Activity, Accretion Disk Instability, Chandra Observatory", "paraphrased_abstract": "It is also possible to see the temperature in the accretion disk, which has a tendency to cool down in the cold. The present work presents a study of the Chandra observations of two nearby radio-quiet quasars, PG 1211 & 143, and Mr. 335. We see that the flux is significant and varies from one hour to the next. The corresponding flux changes are the same as the ones we would expect if the objects have relativistic jets that protrude around their axes at very short intervals. We have detected, therefore, a rapid flounce in the soft X-rays of the two quasars. This flounce, which could be attributed to the accretion disk instability, was a result of the thermal instabilities of the accretion disk. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2062, "title": "Nonholonomic Ricci Flows: III. Curve Flows and Solitonic Hierarchies", "abstract": "  The geometric constructions are elaborated on (semi) Riemannian manifolds and vector bundles provided with nonintegrable distributions defining nonlinear connection structures induced canonically by metric tensors. Such spaces are called nonholonomic manifolds and described by two equivalent linear connections also induced in unique forms by a metric tensor (the Levi Civita and the canonical distinguished connection, d-connection). The lifts of geometric objects on tangent bundles are performed for certain classes of d-connections and frame transforms when the Riemann tensor is parametrized by constant matrix coefficients. For such configurations, the flows of non-stretching curves and corresponding bi-Hamilton and solitonic hierarchies encode information about Ricci flow evolution, Einstein spaces and exact solutions in gravity and geometric mechanics. The applied methods were elaborated formally in Finsler geometry and allows us to develop the formalism for generalized Riemann-Finsler and Lagrange spaces. Nevertheless, all geometric constructions can be equivalently re-defined for the Levi Civita connections and holonomic frames on (semi) Riemannian manifolds. ", "machine_abstract": "We study the curve flows associated with nonholonomic solitonic hierarchies in terms of nonlinear connections (N-connections) on fiber bundles, which are generated by horizontal lifts of vector fields along curves. We show that these N-connection structures can be derived as solutions to certain classes of generalized Einstein equations for metrics depending on higher order derivatives of time variables. The corresponding geometric objects are defined by using the so-called anholonomy coefficients of N-connections. In particular, we consider the case when the base manifold is an arbitrary smooth n-dimensional Riemannian space $(M^n,g)$ equipped with a family of local coordinates $x^{\\mu} = (x^1,..., x^n)$. Then any point $p\\in M^n$ has its natural coordinate neighborhood $U_p\\subset M^n$ such that all points $q\\in U_p$ have their own sets of coordinates $\\bar{x}_q = \\bar{x^1}_q, ...,\\bar{x}_n_q$ related to each other via some diffeomorphism $\\Phi_q : U_p\\to V_q\\subset \\mathbb R^{n+1}$. This allows us to define the following class of metric tensors on $(M^n, g)$:  $$ G_{\\alpha\\beta}(x) := g_{\\alpha\\beta} + \\sum\\limits_{q=1}^{n} h^{q}_{\\alpha\\beta}\\bigl(x, \\bar{x}_q\\bigr) $$ where $h^{q}_{\\alpha\\beta}(p, \\bar{x}_q)$ are functions of two arguments satisfying certain conditions. These conditions guarantee that the tensor field $G_{\\alpha\\beta}$ defines a pseudo-Riemannian metric on $M$.", "paraphrased_abstract": "He has the property of defining the metric tensor 'G' alpha-beta' on the basis of two conditions: 'The tensor 'G' alpha-beta' 'g' '' (x, 'G' alpha-beta'), 'and 'amounts' 'x' and 'amounts '2' 'x','suppose' a certain number of 'n''--for 'the 'n'--is 'N'--on the basis of 'M'.'' These are the results of 'anholonomy' of solitonic hierarchies, and they imply the existence of 'N'-connected 'n'-connected'suppose' (or \"N'-connected ') of 'M' and 'M'. They are the results of some equation of generalized Einstein in a time-varying metric. We study how the nonholonomic solitonic hierarchies develop in the form of N-connections, a type of non-linear connection that arises from the vertical slackening of vector fields along the axis. Then we consider a arbitrary smooth", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2311, "title": "Multidimensional fluid motions with planar waves", "abstract": "  In the classical one-dimensional solution of fluid dynamics equations all unknown functions depend only on time t and Cartesian coordinate x. Although fluid spreads in all directions (velocity vector has three components) the whole picture of motion is relatively simple: trajectory of one fluid particle from plane x=const completely determines motion of the whole plane. Basing on the symmetry analysis of differential equations we propose generalization of this solution allowing movements in different directions of fluid particles belonging to plane x=const. At that, all functions but an angle determining the direction of particle's motion depend on t and x only, whereas the angle depends on all coordinates. In this solution the whole picture of motion superposes from identical trajectories placed under different angles in 3D space. Orientations of the trajectories are restricted by a finite relation possessing functional arbitrariness. The solution describes three-dimensional nonlinear processes and singularities in infinitely conducting plasma, gas or incompressible liquid. ", "machine_abstract": "We consider the motion of an incompressible, inviscid fluid in two dimensions under the influence of gravity and surface tension. The flow is assumed to be irrotational except for a finite number of singularities at which point vorticity may accumulate. We show that if the initial data are sufficiently small then there exists a unique global solution whose velocity field has only one component along each line parallel to the x-axis. This result generalizes previous results on two-dimensional flows by allowing more complicated wave patterns such as those generated by water waves or nonlinear optics.     Keywords: Fluids; Vortices; Water waves; Nonlinear optics. 1 Introduction     In this work we study the dynamics of fluids governed by the Euler equations in two space dimensions. Our main goal is to prove existence of solutions having certain special properties. More precisely, let us assume that the initial velocity field u0(x) = (u0x(x), u0y(x)) satisfies the following conditions:", "paraphrased_abstract": "This is a study of the dynamics of fluids in two dimensions. Our main goal is to prove that there are solutions of some special properties. We consider the motion of an incompressible, inviscid fluid in two dimensions under the influence of gravity and surface tension. We assume a rotational motion, and there are no finite number of singularities at which the vorticity accumulates. The following conditions are given: The first velocity of u0 = (u0x), u0y =. This is a general result and an extension of our earlier studies on two dimensions by introducing more complex wave patterns, such as water waves or nonlinear optics. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2437, "title": "A theorem about relative entropy of quantum states with an application to privacy in quantum communication", "abstract": "  We prove the following theorem about relative entropy of quantum states. \"Substate theorem: Let rho and sigma be quantum states in the same Hilbert space with relative entropy S(rho|sigma) = Tr rho (log rho - log sigma) = c. Then for all epsilon > 0, there is a state rho' such that the trace distance ||rho' - rho||_t = Tr sqrt{(rho' - rho)^2} <= epsilon, and rho'/2^{O(c/epsilon^2)} <= sigma.\"   It states that if the relative entropy of rho and sigma is small, then there is a state rho' close to rho, i.e. with small trace distance ||rho' - rho||_t, that when scaled down by a factor 2^{O(c)} `sits inside', or becomes a `substate' of, sigma. This result has several applications in quantum communication complexity and cryptography. Using the substate theorem, we derive a privacy trade-off for the set membership problem in the two-party quantum communication model. Here Alice is given a subset A of [n], Bob an input i in [n], and they need to determine if i in A.   \"Privacy trade-off for set membership: In any two-party quantum communication protocol for the set membership problem, if Bob reveals only k bits of information about his input, then Alice must reveal at least n/2^{O(k)} bits of information about her input.\"   We also discuss relationships between various information theoretic quantities that arise naturally in the context of the substate theorem. ", "machine_abstract": "We prove that the relative entropy between two density matrices is equal to the difference between their von Neumann entropies, and we show how this result can be used for proving security against eavesdropping attacks on quantum key distribution protocols.  We also present some examples where our results are applied. The relative entropy between two density operators $\\rho$ and $\\sigma$ is defined as $$D(\\rho||\\sigma) = \\sum_i p_i \\log \\frac{p_i}{\\sigma^{*}}(\\sigma(\\hat{\\psi}_i))$$   where $(\\hat{\\psi_i},p_1,...,p_n)$ is any complete set of eigenvectors of both $\\rho$ and $\\sigma^\u2217 \\sigma$ such that $\\sum_ip_i = 1$.   In particular, if one considers the case when $\\rho$ is the state sent by Alice (the sender), while $\\sigma$ is the state received by Bob (the receiver), then the quantity $D(\\rho||\\sigma)$ represents Eve's uncertainty about what was transmitted through the channel.", "paraphrased_abstract": "A... For instance, if you take the case that Alice sent the...., while.... - sigma...  (hoi),  (hoi), sum i p log frac hoi,  sigma  - (hoi)(hoi, hoi, hosigma - ) -. - In this sense the relative entropy between two density matrices is equal to the difference between the von Neumann entropy and the relative entropy between the two density matrices. We present an example of the results. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0267, "title": "Near-Infrared Spectra of the Black Hole X-Ray Binary, A0620-00", "abstract": "  We present broadband NIR spectra of A0620-00 obtained with SpeX on the IRTF. The spectrum is characterized by a blue continuum on which are superimposed broad emission lines of HI and HeII and a host of narrower absorption lines of neutral metals and molecules. Spectral type standard star spectra scaled to the dereddened spectrum of A0620-00 in K exceed the A0620-00 spectrum in J and H for all stars of spectral type K7V or earlier, demonstrating that the donor star, unless later than K7V, cannot be the sole NIR flux source in A0620-00. In addition, the atomic absorption lines in the K3V spectrum are too weak with respect to those of A0620-00 even at 100% donor star contribution, restricting the spectral type of the donor star in A0620-00 to later than K3V. Comparison of the A0620-00 spectrum to scaled K star spectra indicates that the CO absorption features are significantly weaker in A0620-00 than in field dwarf stars. Fits of scaled model spectra of a Roche lobe-filling donor star to the spectrum of A0620-00 show that the best match to the CO absorption lines is obtained when the C abundance is reduced to [C/H] = -1.5. The donor star contribution in the H waveband is determined to be 82+-2%. Combined with previous published results from Froning & Robinson (2001) and Marsh et al. (1994), this gives a precise mass for the black hole in A0620-00 of M_BH = 9.7+-0.6 M_solar. ", "machine_abstract": "We present near-infrared (NIR) spectroscopy of the black hole X-ray binary system A0620-00 obtained with the Subaru Telescope and Gemini Observatory in 2001-2002. The NIR spectrum shows strong emission lines originating from highly ionized species such as FeXXV-XXVI, NeIX-XI, MgXI-XII, SiXIV-XVII, SXV-XXIII, ArXVIII-XXIV, CaXIX-XXVIII, and NiXXI-XXIV. We find that these features are well reproduced by our photoionization model calculations for an accretion disk irradiated by hard X-rays emitted from the central source. In addition to the above mentioned ions, we also detect absorption lines due to neutral hydrogen and helium at wavelengths longer than 1 micron. These absorptions originate from the interstellar medium along the line-of-sight toward this object.     Keywords: Near-infrared spectroscopy", "paraphrased_abstract": "At the same time, we also detected absorptions of neutral hydrogen and helium at the wavelength of less than 1 micron. Then, with the help of the optical techniques, the ionization is carried out for the central source. The spectral observations of the black hole binary system A0620 have been collected by the Subaru Telescope and the Gemini Observatory in 2001 and 2002. We show that these ionizations are very well represented in the calculations of our photoionization model for an accretion disk emitted by the central source. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2464, "title": "Scaling Relations of Compressible MHD Turbulence", "abstract": "  We study scaling relations of compressible strongly magnetized turbulence. We find a good correspondence of our results with the Fleck (1996) model of compressible hydrodynamic turbulence. In particular, we find that the density-weighted velocity, i.e. $u \\equiv \\rho^{1/3} v$, proposed in Kritsuk et al. (2007) obeys the Kolmogorov scaling, i.e. $E_{u}(k)\\sim k^{-5/3}$ for the high Mach number turbulence. Similarly, we find that the exponents of the third order structure functions for $u$ stay equal to unity for the all the Mach numbers studied. The scaling of higher order correlations obeys the She-Leveque (1994) scalings corresponding to the two-dimensional dissipative structures, and this result does not change with the Mach number either. In contrast to $v$ which exhibits different scaling parallel and perpendicular to the local magnetic field, the scaling of $u$ is similar in both directions. In addition, we find that the peaks of density create a hierarchy in which both physical and column densities decrease with the scale in accordance to the Fleck (1996) predictions. This hierarchy can be related ubiquitous small ionized and neutral structures (SINS) in the interstellar gas. We believe that studies of statistics of the column density peaks can provide both consistency check for the turbulence velocity studies and insight into supersonic turbulence, when the velocity information is not available. ", "machine_abstract": "We present the results of high-resolution numerical simulations of compressible magnetohydrodynamic (MHD) turbulence in periodic boxes with different sizes and resolutions, ranging from $(2^{10} \\sim 2^{12})^3$ to $(4^{10} \\sim 4^{12})^3$ grid points at resolution levels between 64 3 and 512 3 . We find that the scaling relations for kinetic energy spectra are consistent with those predicted by Kolmogorov's theory when normalized appropriately.  The magnetic field spectrum follows a power law over several decades in wavenumber space, but deviates significantly from the prediction of idealized MHD turbulence theories such as Iroshnikov-Kraichnan or Goldreich-Sridhar models. In addition, we show that the anisotropy of velocity fluctuations is negligible compared to that of magnetic fields. Finally, we demonstrate that our simulation data can be used to test theoretical predictions on the statistics of Alfv\u00e9nic modes.", "paraphrased_abstract": "In the magnetic field the power law of the magnetohydrodynamics is based on the law of the power law for several decades, but the model for idealized MHD turbulence, such as Iroshnikov-Kraichnan or Goldreich-Sridhar, is significantly reduced. The kinetic energies of kinetic energy are in accordance with those predicted by Kolmogorov, if normalized in accordance with the theory. We present the results of numerical simulations of a compression of the magnetohydrodynamics in periodic boxes, at varying scales and resolutions, from 210 sim 212 to 410 sim 412, at resolutions from 64 3 to 512 3. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0404, "title": "Detection of Crab Giant Pulses Using the Mileura Widefield Array Low Frequency Demonstrator Field Prototype System", "abstract": "  We report on the detection of giant pulses from the Crab Nebula pulsar at a frequency of 200 MHz using the field deployment system designed for the Mileura Widefield Array's Low Frequency Demonstrator (MWA-LFD). Our observations are among the first high-quality detections at such low frequencies. The measured pulse shapes are deconvolved for interstellar pulse broadening, yielding a pulse-broadening time of 670$\\pm$100 $\\mu$s, and the implied strength of scattering (scattering measure) is the lowest that is estimated towards the Crab nebula from observations made so far. The sensitivity of the system is largely dictated by the sky background, and our simple equipment is capable of detecting pulses that are brighter than $\\sim$9 kJy in amplitude. The brightest giant pulse detected in our data has a peak amplitude of $\\sim$50 kJy, and the implied brightness temperature is $10^{31.6}$ K. We discuss the giant pulse detection prospects with the full MWA-LFD system. With a sensitivity over two orders of magnitude larger than the prototype equipment, the full system will be capable of detecting such bright giant pulses out to a wide range of Galactic distances; from $\\sim$8 to $\\sim$30 kpc depending on the frequency. The MWA-LFD will thus be a highly promising instrument for the studies of giant pulses and other fast radio transients at low frequencies. ", "machine_abstract": "We report on the detection and analysis of giant pulses (GPs) from the Crab pulsar using data recorded with the MWA-LFD prototype system at frequencies between 80 and 240 MHz. The GPs are detected by searching for excess power in short time intervals, which is then compared to that expected from background noise fluctuations. We find that the number of GP candidates increases as we lower the frequency below 150 MHz. This increase can be explained if the emission mechanism responsible for producing GPs has an intrinsic spectral index steeper than -1.5. In addition, we show that the pulse width distribution changes significantly when going from high to low radio frequencies. Finally, we present results showing how the flux density of individual GPs varies across our observing bandwidth. These observations provide new insights into the physical processes occurring within the magnetosphere of this energetic pulsar. Keywords: Radio astronomy, Pulsar", "paraphrased_abstract": "Hence we present a detailed study of the electromagnetic spectrum in the Crab pulsar, and the detection and analysis of the giant pulses (GPs) from the Crab pulsar, using the LFD prototype, ranging from 80 to 240 MHz. GPs are detected from a search for excess power, which is compared with the background noise. As a result, we show that the flux density of individual GPs is systematically varied over our observation range. This is particularly relevant to the physical processes occurring within the magnetosphere of this energetic pulsar. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1427, "title": "The origin of the molecular emission around the southern hemisphere Re 4 IRS - HH 188 region", "abstract": "  We present SEST observations of the molecular environment ahead of the southern Herbig-Haro object 188 (HH188), associated with the low-mass protostar Re4 IRS. We have also used the SuperCosmos Halpha survey to search for Halpha emission associated with the Re4 IRS - HH188 region. The aim of the present work is to study the properties of the molecular gas and to better characterize this southern star forming region. We mapped the HCO+ 3-2 and H13CO+ 1-0 emission around the YSO and took spectra of the CH3OH 2(0)-1(0)A+ and 2(-1)-1(-1)E and SO 6(5)-5(4) towards the central source. Column densities are derived and different scenarios are considered to explain the origin of the molecular emission. HCO+ arises from a relatively compact region around the YSO; however, its peak emission is displaced to the south following the outflow direction. Our chemical analysis indicates that a plausible scenario is that most of the emission arises from the cold, illuminated dense gas ahead of the HH188 object. We have also found that HH188, a high excitation object, seems to be part of a parsec scale and highly collimated HH system. Re4 IRS is probably a binary protostellar system, in the late Class 0 or Class I phase. One of the protostars, invisible in the near-IR, seems to power the HH188 system. ", "machine_abstract": "We present new observations in the mid-infrared (MIR) and near-infrared (NIR), as well as archival data, to study the physical conditions of the gas surrounding the young stellar object Re 4 IRS located at the center of the reflection nebula NGC 1333 IRAS 2. We find that the MIR spectrum is dominated by strong ro-vibrational lines of H$_2$O and CO, while the NIR spectrum shows prominent Brackett-\u03b3 line emission with an equivalent width of ~1 \u00c5. The observed fluxes are consistent with those expected for a T Tauri star surrounded by a dense circumstellar disk heated by accretion shocks. In addition, we detect several forbidden transitions of [Fe II] , [S III] , [N II] , and [C II] . These results suggest that the central source has recently undergone a burst of enhanced mass loss activity which may be related to its recent transition into the main sequence phase.", "paraphrased_abstract": "Besides, a few forbidden transitions are found. These signals suggest that the central source has recently experienced a high degree of mass loss and that it is about to undergo a transition. In addition, there are two forbidden transitions: fe II, fe III, fe N, and fe C. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0209, "title": "Chandra Observations of Supernova 1987A", "abstract": "  We have been monitoring Supernova (SN) 1987A with {\\it Chandra X-Ray Observatory} since 1999. We present a review of previous results from our {\\it Chandra} observations, and some preliminary results from new {\\it Chandra} data obtained in 2006 and 2007. High resolution imaging and spectroscopic studies of SN 1987A with {\\it Chandra} reveal that X-ray emission of SN 1987A originates from the hot gas heated by interaction of the blast wave with the ring-like dense circumstellar medium (CSM) that was produced by the massive progenitor's equatorial stellar winds before the SN explosion. The blast wave is now sweeping through dense CSM all around the inner ring, and thus SN 1987A is rapidly brightening in soft X-rays. At the age of 20 yr (as of 2007 January), X-ray luminosity of SN 1987A is $L_{\\rm X}$ $\\sim$ 2.4 $\\times$ 10$^{36}$ ergs s$^{-1}$ in the 0.5$-$10 keV band. X-ray emission is described by two-component plane shock model with electron temperatures of $kT$ $\\sim$ 0.3 and 2 keV. As the shock front interacts with dense CSM all around the inner ring, the X-ray remnant is now expanding at a much slower rate of $v$ $\\sim$ 1400 km s$^{-1}$ than it was until 2004 ($v$ $\\sim$ 6000 km s$^{-1}$). ", "machine_abstract": "The Chandra X-ray Observatory has observed the supernova remnant (SNR) produced by SN1987A in the Large Magellanic Cloud for over ten years, providing an unprecedented view into this young and energetic object.  The observations have revealed that the blast wave is interacting with dense circumstellar material surrounding the progenitor star at velocities up to 1000 km/sec.  This interaction produces bright knots of emission which are seen as moving outward through the shell of the remnant.  These knots appear to be composed primarily of oxygen-rich ejecta mixed with shocked interstellar gas.  In addition, there appears to be a large amount of hot plasma trapped behind the forward shock front.  We present here new results on these features based on our analysis of data obtained during the first year of the Chandra mission. The Chandra X-ray Observatory has observed  the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years . It provides an unprecedented view into this y oung and en erg i c obj ect .  T he obse rvations ha ve reve al ed tha t th e b las t wa ve is interactin g wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star-at-rou nd -velocities up to 1 000 k m/ sec . Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene -rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .", "paraphrased_abstract": "This supernova has been observed in the Large Magellan Cloud for more than ten years. This is the first time we have seen this object. We have observed it in the midst of the large magellan cloud, which stretches for ten years. It gives us an unprecedented look into this young and energetic object. X-rays from the Chandra Observatory have been observing the supernova that was created by SN1987 in the Great Magellanic Cloud. We have shown that the bursting ray shattered in the dense circumstellar material surrounding the progenitor star at a velocity of up to 1000 km/sec. The swarms produced in this way are mainly of oxygen-rich ejecta and some gaseous interstellar gas. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1771, "title": "Performance of the Charge Injection Capability of Suzaku XIS", "abstract": "  A charge injection technique is applied to the X-ray CCD camera, XIS (X-ray Imaging Spectrometer) onboard Suzaku. The charge transfer inefficiency (CTI) in each CCD column (vertical transfer channel) is measured by the injection of charge packets into a transfer channel and subsequent readout. This paper reports the performances of the charge injection capability based on the ground experiments using a radiation damaged device, and in-orbit measurements of the XIS. The ground experiments show that charges are stably injected with the dispersion of 91eV in FWHM in a specific column for the charges equivalent to the X-ray energy of 5.1keV. This dispersion width is significantly smaller than that of the X-ray events of 113eV (FWHM) at approximately the same energy. The amount of charge loss during transfer in a specific column, which is measured with the charge injection capability, is consistent with that measured with the calibration source. These results indicate that the charge injection technique can accurately measure column-dependent charge losses rather than the calibration sources. The column-to-column CTI correction to the calibration source spectra significantly reduces the line widths compared to those with a column-averaged CTI correction (from 193eV to 173eV in FWHM on an average at the time of one year after the launch). In addition, this method significantly reduces the low energy tail in the line profile of the calibration source spectrum. ", "machine_abstract": "We have investigated charge injection capability (CIC) of Suzaku X-ray Imaging Spectrometer (XIS). The CIC is an important feature for high energy resolution spectroscopy, which can be used to reduce the effect of pile-up in bright sources and improve the spectral quality at low energies. We performed detailed analysis on the performance of CIC by using Crab observations with different count rates. By comparing the results obtained with and without CIC we found that CIC improves the spectral fitting significantly especially below 1 keV where the pile-up fraction becomes large when the source flux increases. This improvement was confirmed also by analyzing simulated data generated by xisrmfgen and xissimarfgen. Finally, we applied this method to analyze the observation of Mrk 421 during its flare state. We successfully detected emission lines such as O VII K\u03b1 line and Fe XXV K\u03b1 line from the spectrum taken with CIC enabled.  These results show that CIC works well even under strong pile-up conditions.", "paraphrased_abstract": "He compared this with the xis-ciqtafgen, and xis-simarfgen were compared with the xis-ciqtafgen. The xis-ciqtafgen data were also compared, and the difference between CIC and without CIC was substantial. The xis-ciqtafgen data was confirmed by a simulation of xis-ciqtafgen and xis-ciqtafgen data. We analyzed CIC performance in Suzaku's X-ray-irradiometer with the purpose of reducing the effect of aggregation at the high energy of the light sources, and improving the spectral quality at low energies. The X-ray spectrometer Suzaku uses is an important feature of high-efficiency spectroscopy. It can reduce aggregation in bright sources, and improve the spectral quality at low energies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.235, "title": "The time evolution of cosmological redshift as a test of dark energy", "abstract": "  The variation of the expansion rate of the Universe with time produces an evolution in the cosmological redshift of distant sources (for example quasar Lyman-$\\alpha$ absorption lines), that might be directly observed by future ultra stable, high-resolution spectrographs (such as CODEX) coupled to extremely large telescopes (such as European Southern Observatory's Extremely Large Telescope, ELT). This would open a new window to explore the physical mechanism responsible for the current acceleration of the Universe. We investigate the evolution of cosmological redshift from a variety of dark energy models, and compare it with simulated data. We perform a Fisher matrix analysis and discuss the prospects for constraining the parameters of these models and for discriminating among competing candidates. We find that, because of parameter degeneracies, and of the inherent technical difficulties involved in this kind of observations, the uncertainties on parameter reconstruction can be rather large unless strong external priors are assumed. However, the method could be a valuable complementary cosmological tool, and give important insights on the dynamics of dark energy, not obtainable using other probes. ", "machine_abstract": "We propose to use the time evolution of cosmological redshifts in order to probe the nature of dark energy, which is one of the most important problems in modern physics and astronomy. We show that this method can be used for testing various models of dark energy by using only two parameters (the present-day values of Hubble constant H0 and deceleration parameter q0). The proposed method does not require any additional information about the universe beyond what we already know today. This makes it possible to perform an independent check on the results obtained with other methods such as supernovae Ia observations or cosmic microwave background anisotropy measurements. In particular, our analysis shows that the current data are consistent with the standard \u039bCDM model at 1\u03c3 level but do not rule out some alternative models like quintessence or phantom fields. Finally, we discuss how future surveys could improve the constraints on these models. Cosmological redshifts play an important role in modern astrophysics and cosmology because they provide us with valuable information about the expansion history of the Universe. However, their interpretation requires knowledge of the underlying theory describing the dynamics of space-time. For example, if we assume general relativity then cosmological redshifts can be interpreted as due to the Doppler effect caused by the recession velocities of distant galaxies [1] . On the other hand, if we consider modified gravity theories then cosmological redshifting may have different physical origins [2] . In recent years there has been growing interest in studying the possibility of probing the nature of dark energy through its effects on cosmological redshifts [3] - [8] . Dark energy is currently believed to dominate the content of the Universe [9] , however its exact origin remains unknown [10] . It is usually described within the framework of Einstein's field equations by introducing a new component into the stress-energy tensor [11] . Its presence leads to accelerated expansion of the Universe [12] , which manifests itself in the form of observed...", "paraphrased_abstract": "And it is now known that the dark energy is present in the cosmological redshifts, and that it has its origins in the tensor of the stress energy. This is known as the Doppler effect, which is caused by the retreating of distant galaxies. Then we can imply that cosmological redshifts are the result of the gravitational effect of the universe, which is known in general relativity. This effect is attributed to the Doppler effect of the apocalypse of distant galaxies, whereas in the case of gravity-inspired theories, cosmological redshifts have different physical origins. In recent years there has been a great interest in studying the nature of dark energy in cosmology, because it has the potential to give us useful information about the expansion of the universe. Then we will show how our results will be different from those of other cosmological models such as the CDM and the phantom fields. In this paper we will test the theoretical properties of the dark energy by examining cosmological redshifts. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3917, "title": "Ly-alpha Emission-Line Galaxies at z = 3.1 in the Extended Chandra Deep Field South", "abstract": "  We describe the results of an extremely deep, 0.28 deg^2 survey for z = 3.1 Ly-alpha emission-line galaxies in the Extended Chandra Deep Field South. By using a narrow-band 5000 Anstrom filter and complementary broadband photometry from the MUSYC survey, we identify a statistically complete sample of 162 galaxies with monochromatic fluxes brighter than 1.5 x 10^-17 ergs cm^-2 s^-1 and observers frame equivalent widths greater than 80 Angstroms. We show that the equivalent width distribution of these objects follows an exponential with a rest-frame scale length of w_0 = 76 +/- 10 Angstroms. In addition, we show that in the emission line, the luminosity function of Ly-alpha galaxies has a faint-end power-law slope of alpha = -1.49 +/- 0.4, a bright-end cutoff of log L^* = 42.64 +/- 0.2, and a space density above our detection thresholds of 1.46 +/- 0.12 x 10^-3 h70^3 galaxies Mpc^-3. Finally, by comparing the emission-line and continuum properties of the LAEs, we show that the star-formation rates derived from Ly-alpha are ~3 times lower than those inferred from the rest-frame UV continuum. We use this offset to deduce the existence of a small amount of internal extinction within the host galaxies. This extinction, coupled with the lack of extremely-high equivalent width emitters, argues that these galaxies are not primordial Pop III objects, though they are young and relatively chemically unevolved. ", "machine_abstract": "We present spectroscopic redshifts for Lyman alpha emission-line galaxies (LAEs) selected with narrow-band imaging data taken by Subaru/Suprime-Cam and VLT/VIMOS on the basis of their strong rest-frame ultraviolet emission lines, including Ly-alpha line. We have obtained spectra for 16 LAEs using Keck/DEIMOS over an area of 0.5 deg2 centered around the Hubble Ultra Deep Field (HUDF). The median redshift is found to be $3.10 \\pm 0.01$. This value agrees well with that estimated from photometric redshifts based on multi-band optical-to-infrared SEDs constructed from deep ground-based observations as well as those derived from Spitzer/MIPS 24 micron fluxes. Our sample consists of two sub-samples; one has been observed only once while another was observed twice or more times. No significant difference between these samples are seen except for the fraction of objects showing double peaks in their Ly-alpha profiles.", "paraphrased_abstract": "One sample, one observed only once, was observed twice, and the second was observed twice or more. Our sample consists of two subsamples: one was observed only once, while another was observed twice or more. We derived the redshifts of 16 of the spectroscopic galaxies from a survey by Subaru/Suprime and VLT/VIMOS with the use of their high-resolution ultraviolet spectra and the strong alpha line. The redshift is calculated from multi-band optical-to-infrared spectroscopic measurements taken by Subaru/Suprime and VLT/VIMOS from their ultraviolet spectra, with the corresponding alpha line, and of the spherical alpha line. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3229, "title": "The Long Period, Massive Binaries HD 37366 and HD 54662: Potential Targets for Long Baseline Optical Interferometry", "abstract": "  We present the results from an optical spectroscopic analysis of the massive stars HD 37366 and HD 54662. We find that HD 37366 is a double-lined spectroscopic binary with a period of 31.8187 +/- 0.0004 days, and HD 54662 is also a double lined binary with a much longer period of 557.8 +/- 0.3 days. The primary of HD 37366 is classified as O9.5 V, and it contributes approximately two-thirds of the optical flux. The less luminous secondary is a broad-lined, early B-type main-sequence star. Tomographic reconstruction of the individual spectra of HD 37366 reveals absorption lines present in each component, enabling us to constrain the nature of the secondary and physical characteristics of both stars. Tomographic reconstruction was not possible for HD 54662; however, we do present mean spectra from our observations that show that the secondary component is approximately half as bright as the primary. The observed spectral energy distributions (SEDs) were fit with model SEDs and galactic reddening curves to determine the angular sizes of the stars. By assuming radii appropriate for their classifications, we determine distance ranges of 1.4 - 1.9 and 1.2 - 1.5 kpc for HD 37366 and HD 54662, respectively. ", "machine_abstract": "We present the first results on two massive binary systems with periods longer than 100 days that are potential targets for future space-based optical interferometers such as SIM or TPF-I. The primary stars in these binaries have masses between 8 to 12 Msun and their companions are less massive (1-5 Msun). We used high resolution spectroscopy obtained at McDonald Observatory to determine orbital parameters for both systems. In addition we determined stellar atmospheric properties using photometric data taken by Hipparcos satellite and spectroscopic observations made at McDonald Observatory. Our analysis shows that the components of HD 37362 system are very similar while those of HD 54672 differ significantly. This is consistent with our expectations based on evolutionary models which predict that the more massive star will evolve faster and become redder. These results demonstrate that it may be possible to use astrometric measurements combined with radial velocity curves to measure directly mass loss rates due to winds driven by radiation pressure.", "paraphrased_abstract": "The first results are presented on two large binary systems with a duration of more than 100 days, and are therefore likely candidates for a new optical interferometer, such as SIM or TPF-I. We use high-resolution spectroscopy to calculate the orbital parameters of both systems. We present the results of the spectral analyses of Hipparcos, a satellite of McDonald Observatory, and the spectral observations of McDonald Observatory. The main stars of these two systems have mass in the range of eight to twelve Msun, and the companions are less than five. The spectral parameters of these two systems are not very different from each other. This is in agreement with the predictions of evolutionary models, which predict that the more massive star will grow redder and grow more. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2851, "title": "Bounds on quark mass matrices elements due to measured properties of the mixing matrix and present values of the quark masses", "abstract": "  We obtain constraints on possible structures of mass matrices in the quark sector by using as experimental restrictions the determined values of the quark masses at the $M_Z$ energy scale, the magnitudes of the quark mixing matrix elements $V_{\\rm ud}$, $V_{\\rm us}$, $V_{\\rm cd}$, and $V_{\\rm cs}$, and the Jarlskog invariant $J(V)$. Different cases of specific mass matrices are examined in detail. The quality of the fits for the Fritzsch and Stech type mass matrices is about the same with $\\chi^2/{\\rm dof}=4.23/3=1.41$ and $\\chi^2/{\\rm dof}=9.10/4=2.28$, respectively. The fit for a simple generalization (one extra parameter) of the Fritzsch type matrices, in the physical basis, is much better with $\\chi^2/{\\rm dof}=1.89/4=0.47$. For comparison we also include the results using the quark masses at the 2 GeV energy scale. The fits obtained at this energy scale are similar to that at $M_Z$ energy scale, implying that our results are unaffected by the evolution of the quark masses from 2 to 91 GeV. ", "machine_abstract": "We derive bounds on the absolute values of all entries in the up- and down-quark mass matrices, using only information about the CKM-matrix and current experimental data for the masses of quarks. We find that these bounds are much stronger than those obtained previously by other authors.     The results presented here can be used as input parameters for future studies of CP violation within the Standard Model or its extensions. They also provide useful constraints on models with extra dimensions where quarks propagate into higher-dimensional bulk spaces.  I. INTRODUCTORY REMARK The Cabibbo-Kobayashi-Maskawa (CKM) [1] quark mixing matrix V is an unitary 3 x 3 complex matrix which describes how quarks mix among themselves after electroweak symmetry breaking. It contains nine independent real parameters, three angles \u03b812 , \u03b823 , \u03b813 and six phases \u03c61 , \u03c62 ,...", "paraphrased_abstract": "- The Cabibbo-Kobayashi-Maskawa-Maskawa (CKM) matrix, which is a unitary, three-dimensional, three-dimensional matrix of quarks, which contains nine independent parameters, three angles, 12, 23, 13 and six phases, 1, 2, 6 -, 2 -. The results of this study may be used to test the CP violation of the Standard Model, and to test the other dimensions where the quarks are disposed. The results of this study can be used for further studies of the CP violation in the Standard Model, and for additional models with extra dimensions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.112, "title": "\"Illusion of control\" in Minority and Parrondo Games", "abstract": "  Human beings like to believe they are in control of their destiny. This ubiquitous trait seems to increase motivation and persistence, and is probably evolutionarily adaptive. But how good really is our ability to control? How successful is our track record in these areas? There is little understanding of when and under what circumstances we may over-estimate or even lose our ability to control and optimize outcomes, especially when they are the result of aggregations of individual optimization processes. Here, we demonstrate analytically using the theory of Markov Chains and by numerical simulations in two classes of games, the Minority game and the Parrondo Games, that agents who optimize their strategy based on past information actually perform worse than non-optimizing agents. In other words, low-entropy (more informative) strategies under-perform high-entropy (or random) strategies. This provides a precise definition of the \"illusion of control\" in set-ups a priori defined to emphasize the importance of optimization. ", "machine_abstract": "We study the effect of \"illusion of control\" on minority games with different number of players, as well as on Parrondo's paradoxical games. We show that for all these cases there is an optimal value of illusion of control which maximizes the performance of the system. The results are obtained by using numerical simulations based on Monte Carlo method. In particular we find that the optimal values of illusion of control depend strongly on the number of players involved in each game.  I. INTRODUCTIO N  A. Illusion of Control (IC) The term \"illusion of control\" was first introduced by Langer [1] . It refers to situations where people tend to overestimate their ability to influence events or outcomes [2] , even when they have no real control [3] . In recent years this concept has been applied to many fields such as: gambling [4] , stock markets [5] , sports [6] , health [7] , education [8] etc., showing its importance in human behavior [9] - [11] . B. Minority Game (MG) MGs were proposed by Challet and Zhang [12] as models of financial markets. They consist of agents who make decisions according to some strategy at discrete time steps. At every step one agent makes a decision among two options, called spin-up and spindown. If more than half of the agents choose the same option then it wins; otherwise it loses. Agents can change their strategies during the course of play [13] . There exist several variants of MGs: single-agent [14] , multi-agent [15] , continuous-time [16] , quantum [17] , evolutionary [18] , co-evolutionary [19] , spatially extended [20] , and others [21] - [23] . C. Parrondo's Paradoxical Games (PPGs) Parrondo's paradoxical games [24] are simple two-player games played between a player A and B [25] . Each player plays against his opponent with a certain probability p i = 1 \u2212 q i , where 0 < p i , q i \u2264 1 [26] . When both players use the same strategy s i \u2208 {\u22121, 1}, the expected return per round is zero [27] . However if", "paraphrased_abstract": "Illusion of Control (IIC) - The term \"IIC\" was first introduced by Langer [1] - and was applied to many different situations such as gambling, stock exchanges, sports, health, education, etc., showing its importance in human behaviour, - - \"IIC\" - We study the effect of the IC on the Minority Games, the Games of Parrondo, and the Games of Parrondo - the Games of Paradox, which are simple two-player games between two players A and B. Each player plays against a player of a certain probability p = 1 - q - i - 1 - i - i, where p = p - q - i - 1 - 1 - 0 - 1 - 1 - 0 - 0 - 0 - 1 - 1, when a player plays the same strategy s i - i - i, - i - i, - i - i - 1 - - a player plays with a certain probability, p i - p - i - 1, p - p - i - i - 1, 0 ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0166, "title": "A CLT for Information-theoretic statistics of Gram random matrices with a given variance profile", "abstract": "  Consider a $N\\times n$ random matrix $Y_n=(Y_{ij}^{n})$ where the entries are given by $$ Y_{ij}^{n}=\\frac{\\sigma_{ij}(n)}{\\sqrt{n}} X_{ij}^{n} $$ the $X_{ij}^{n}$ being centered, independent and identically distributed random variables with unit variance and $(\\sigma_{ij}(n); 1\\le i\\le N, 1\\le j\\le n)$ being an array of numbers we shall refer to as a variance profile. We study in this article the fluctuations of the random variable $$ \\log\\det(Y_n Y_n^* + \\rho I_N) $$ where $Y^*$ is the Hermitian adjoint of $Y$ and $\\rho > 0$ is an additional parameter. We prove that when centered and properly rescaled, this random variable satisfies a Central Limit Theorem (CLT) and has a Gaussian limit whose parameters are identified. A complete description of the scaling parameter is given; in particular it is shown that an additional term appears in this parameter in the case where the 4$^\\textrm{th}$ moment of the $X_{ij}$'s differs from the 4$^{\\textrm{th}}$ moment of a Gaussian random variable. Such a CLT is of interest in the field of wireless communications. ", "machine_abstract": "We consider the information-theoretic limit theorems (CLTs) for linear spectral statistics of large dimensional Gram random matrices, which are defined as follows.  Let $(X_1,\\ldots, X_n)$ be an iid sequence of zero-mean complex Gaussian vectors in $\\mathbb{C}^d$ and let $Y=XX^*$ denote its covariance matrix. We assume that the distribution of $X$ is invariant under unitary transformations on each coordinate axis. Then we define the Gram random matrix $G_n$ by $$G_n = Y^{1/2}Y^{1/2}$$ where $Y^{1/2}$ denotes the square root of $Y$. The main purpose of this work is to establish central limit theorems for linear spectral statistics of the form $\\displaystyle{\\sum_{j=1}^n f_j(\\lambda_j)}$, where $\\lambda_j$'s are the eigenvalues of $G$ and $f_j$'s are functions satisfying some regularity conditions. In particular, we show that if $f$ has bounded variation then the limiting distribution of $\\displaystyle{\\sum_{j=0}^n f_j(\\lambda_i)}$ can be expressed explicitly using the Stieltjes transform of the Marchenko-Pastur law.", "paraphrased_abstract": "I have been writing this book because I am interested in the theory of the central limit theorems of linear spectral statistics of large dimensions, that is, a totality of the length of a gram, a length of one hundred and fifty feet, and a bounded scale of a metric function, a function satisfying some regularity conditions. I will show that if the metric function is bounded, then the limit of the metric function - bdj = Y1/2y1/2$ is explicitly expressed by the Stieltjes transform of the Marchenko-Pastur law. Firstly, the central limit theorems for linear statistics of large scales are obtained, that is, the central limit theorems for a metric function - m1nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2283, "title": "Studies of Thermally Unstable Accretion Disks around Black Holes with Adaptive Pseudo-Spectral Domain Decomposition Method I. Limit-Cycle Behavior in the Case of Moderate Viscosity", "abstract": "  We present a numerical method for spatially 1.5-dimensional and time-dependent studies of accretion disks around black holes, that is originated from a combination of the standard pseudo-spectral method and the adaptive domain decomposition method existing in the literature, but with a number of improvements in both the numerical and physical senses. In particular, we introduce a new treatment for the connection at the interfaces of decomposed subdomains, construct an adaptive function for the mapping between the Chebyshev-Gauss-Lobatto collocation points and the physical collocation points in each subdomain, and modify the over-simplified 1-dimensional basic equations of accretion flows to account for the effects of viscous stresses in both the azimuthal and radial directions. Our method is verified by reproducing the best results obtained previously by Szuszkiewicz & Miller on the limit-cycle behavior of thermally unstable accretion disks with moderate viscosity. A new finding is that, according to our computations, the Bernoulli function of the matter in such disks is always and everywhere negative, so that outflows are unlikely to originate from these disks. We are encouraged to study the more difficult case of thermally unstable accretion disks with strong viscosity, and wish to report our results in a subsequent paper. ", "machine_abstract": "We have studied thermally unstable accretion disks by using an adaptive pseudo-spectral domain decomposition method (PSDDM). We found that limit-cycles appear when the viscosity is moderate, and they are suppressed as the viscosity decreases. The results obtained here agree well with those reported previously for non-adaptive PSDDMs.     Keywords: black hole, disk instability, limit cycle, spectral methods, viscous flow     1 Introduction     In recent years there has been considerable interest in studying the nonlinear behavior of thermally unstable accretion flows onto black holes because such instabilities may be responsible for some observed phenomena associated with active galactic nuclei (AGNs) or quasars [1, 2] . For example, it was suggested that the rapid variability of AGNs/quasars could result from thermal-viscous instabilities [3 -5] , which can lead to limit cycles [6] . However, previous studies on this subject were limited mainly due to computational difficulties [7 -9] .     In order to overcome these difficulties we developed recently an adaptive pseudospectral domain decomposition method (APSDDM), which allows us to study numerically the nonlinear evolution of thermally unstable accretions more efficiently than before [10, 11] . Using APSDDM we investigated the effects of various physical parameters on the nonlinear evolution of thermallly unstable accretion disks [11] . It turned out that the effect of magnetic fields on the nonlinear evolution depends strongly on their strength; while weak magnetic fields tend to enhance the growth rate of the limit-cycle amplitude, strong ones suppress them [12] .  In this work we continue our investigation into the nonlinear evolution of thermaly unstable accretion disks by considering the case where the viscosity parameter \u03b1 = 0.1. This value of \u03b1 corresponds roughly to the situation expected at distances of about 10 Schwarzschild radii from the central black hole [13] .", "paraphrased_abstract": "We have investigated the nonlinearity of accretion disks, based on a method of APSDDM, that is, an adaptive pseudo-spectral domain method, which allows us to study the nonlinearity of accretion disks more precisely than ever before. We have taken an approach to the study of the nonlinearity of accretion disks by using a method of APSDDM that can be used to calculate the nonlinearity of accretion disks in the case of a material of a certain thickness, i.e. a density of one hundred thousand ml. We found that the maximum density was attained when the density of a material was moderate and that the density was diminished as the thickness decreased. We also found that the maximal density of a material was lowered by the decreasing density of the material, and we found that the maximal density was heightened by the decrease of the material. The latter, according to our own calculations, is more accurate than any other. We have recently developed a modified version of the APSDDM, which enables us to study the nonlinear evolution of accretion disks. We have examined the nonlinearity of the accretion disks in the presence of a magnetic field, which increases the rate of limit-time and reduces the speed of limit-", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3266, "title": "Rotation and activity of pre-main-sequence stars", "abstract": "  We present a study of rotation (vsini) and chromospheric activity (Halpha EW) based on an extensive set of high-resolution optical spectra obtained with MIKE on the 6.5m Magellan Clay telescope. Our targets are 74 F-M dwarfs in the young stellar associations Eta Cha, TW Hydrae, Beta Pic, and Tuc-Hor, spanning ages from 6 to 30 Myr. While the Halpha EW for most F and G stars are consistent with pure photospheric absorption, most K and M stars show chromospheric emission. By comparing Halpha EW in our sample to results in the literature, we see a clear evolutionary sequence: Chromospheric activity declines steadily from the T Tauri phase to the main sequence. Using activity as an age indicator, we find a plausible age range for the Tuc-Hor association of 10-40 Myr. Between 5 and 30 Myr, we do not see evidence for rotational braking in the total sample, thus angular momentum is conserved, in contrast to younger stars. This difference indicates a change in the rotational regulation at 5-10 Myr, possibly because disk braking cannot operate longer than typical disk lifetimes, allowing the objects to spin up. The rotation-activity relation is flat in our sample; in contrast to main-sequence stars, there is no linear correlation for slow rotators. We argue that this is because young stars generate their magnetic fields in a fundamentally different way from main-sequence stars, and not just the result of a saturated solar-type dynamo. By comparing our rotational velocities with published rotation periods for a subset of stars, we determine ages of 13 (7-20) Myr and 9 (7-17} Myr for the Eta Cha and TWA associations, respectively, consistent with previous estimates. Thus we conclude that stellar radii from evolutionary models by Baraffe et al. (1998) are in agreement with the observed radii within +-15%. (abridged) ", "machine_abstract": "We present the results of an analysis of high-resolution optical spectra obtained with the Subaru Telescope for a sample of young, low-mass (0.5-2 Msun) PMS stars in Orion Nebula Cluster. We find that most of these objects show significant periodic variability on timescales ranging between 1 day to several months. The observed periods are consistent with those expected if the photometric variations result from rotational modulation due to starspots and/or plages. In addition we detect strong H\u03b1 emission lines which indicate chromospheric activity. Our observations suggest that many of our targets have relatively rapid rotation rates compared to older field dwarfs at similar masses. This is likely caused by tidal locking during their formation process. Finally, we also report the detection of two new spectroscopic binaries among our target list. These systems will be useful as benchmarks for testing theoretical models of binary evolution.", "paraphrased_abstract": "We show that the targets, many of which have small, light-colored, elliptic stars, have relatively rapid rotations compared with the dwarfs, who are more similar in mass. This may be due to the formation of the stars in the form of tidal tidal locks. We have also shown the detection of two new binaries. The results of this analysis are given, as well as the results of the spectroscopic analysis of the spectra of stars of a low density (0.5-2 Msun) in the Orion nebulae. We have also observed, in a few cases, a very large fluctuation of the spectra, between one day and several months. These fluctuations are mainly due to the oblique rotation of the stars, and by tidal locking, which is likely to be a consequence of the tidal locking of the stars during their formation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2187, "title": "Light curves and colours of the faint Uranian irregular satellites Sycorax, Prospero, Stephano, Setebos and Trinculo", "abstract": "  After the work of Gladman et al. (1998), it is now assessed that many irregular satellites are orbiting around Uranus. Despite many studies have been performed in past years, very few is know for the light-curves of these objects and inconsistencies are present between colours derived by different authors. This situation motivated our effort to improve both the knowledge of colours and light curves. We present and discuss time series observations of Sycorax, Prospero, Stephano, Setebos and Trinculo, five faint irregular satellites of Uranus, carried out at VLT, ESO Paranal (Chile) in the nights between 29 and 30 July, 2005 and 25 and 30 November, 2005. We derive light curves for Sycorax and Prospero and colours for all of these these bodies. For Sycorax we obtain colours B-V =0.839 +/- 0.014, V-R = 0.531 +/- 0.005 and a light curve which is suggestive of a periodical variation with period about 3.6 hours and amplitude about 0.067 +/- 0.004 mag. The periods and colours we derive for Sycorax are in agreement with our previous determination in 1999 using NTT. We derive also a light-curve for Prospero which suggests an amplitude of about 0.2 mag and a periodicity of about 4 hours. However, the sparseness of our data, prevents a more precise characterization of the light-curves, and we can not determine wether they are one-peaked or two-peaked. Hence, these periods and amplitudes have to be considered preliminary estimates. As for Setebos, Stephano and Trinculo the present data do not allow to derive any unambiguous periodicity, despite Setebos displays a significant variability with amplitude about as large as that of Prospero. Colours for Prospero, Setebos, Stephano and Trinculo are in marginal agreement with the literature. ", "machine_abstract": "We present new photometric observations in the visible range for five faint irregular satellites of Uranus (Sycorax, Prospero,  Stephano, Setebos; Trinculo) obtained with the 1-m telescope at the South African Astronomical Observatory during two observing runs on September 24-25th 2010 and March 28-29th 2011. The data were reduced using standard procedures within IRAF. We used aperture photometry to extract magnitudes of these objects relative to comparison stars observed simultaneously.  These results are presented together with previously published light-curves and colour indices. All our measurements are consistent with previous ones but we have significantly improved their accuracy by reducing systematic errors due to atmospheric conditions or instrumental effects. Our analysis shows that all five satellites exhibit very similar behaviour - they are redder than predicted by models of irradiated icy bodies and show no significant phase variations over several years. This suggests that they may be covered by dark material which is not exposed directly to solar radiation.", "paraphrased_abstract": "But our measurements have a certain improvement, since we have reduced the systematic errors of the weather or the instrumentation, and have a greater accuracy than the previous measurements. They are all based on similar spectral phenomenology, but they are remarkably similar in their behaviour, because they are redder than the ones predicted by the models of icy bodies, and they have no noticeable change over several years. This suggests that they are covered by dark material which is not exposed to sun rays. We present new measurements of the visible range for five strange, irregular satellites of Uranus: Sycorax, Prospero, Stephano, Setebos, Trinculo, taken with the astronomy-scope of the South African Astronomy Observatory, on September 24, 25, and 29 March 2011. These data were processed according to standard procedures in IRAF. We extracted the magnitude of the five satellites in relation to the other five astronomers. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0227, "title": "Self assembly of a model multicellular organism resembling the Dictyostelium slime molds", "abstract": "  The evolution of multicellular organisms from monocellular ancestors represents one of the greatest advances of the history of life. The assembly of such multicellular organisms requires signalling and response between cells: over millions of years these signalling processes have become extremely sophisticated and refined by evolution, such that study of modern organisms may not be able to shed much light on the original ancient processes . Here we are interested in determining how simple a signalling method can be, while still achieving self-assembly. In 2D a coupled cellular automaton/differential equation approach models organisms and chemotaxic chemicals, producing spiralling aggregation. In 3D Lennard-Jones-like particles are used to represent single cells, and their evolution in response to signalling is followed by molecular dynamics. It is found that if a single cell is able to emit a signal which induces others to move towards it, then a colony of single-cell organisms can assemble into shapes as complex as a tower, a ball atop a stalk, or a fast-moving slug. The similarity with the behaviour of modern Dictyostelium slime molds signalling with cyclic adenosine monophosphate (cAMP) is striking. ", "machine_abstract": "The authors describe how they have created an artificial life form that self-assembles into a structure similar to the fruiting body of the Dictyostelium discoideum, or \"dictyos\" for short.  The dictyos is made up of thousands of identical cells connected by elastic filaments and can be grown in Petri dishes with nutrients added.   It has been shown to grow and divide indefinitely without any external control signals (such as hormones).   This work was done at Harvard University's Wyss Institute for Biologically Inspired Engineering.   ... Self-assembly refers to the spontaneous organization of matter into ordered structures such as crystals, snowflakes, and living organisms like bacteria colonies and animal tissues. In this study we report on our efforts toward creating a synthetic multicellular system capable of autonomous growth and division through self-assembly. We designed a minimal cell based on a spherical water-in-oil emulsion droplet containing a single microtubule-based cytoskeleton surrounded by a lipid membrane. These cells are able to attach to each other via flexible polymeric tethers and assemble into three-dimensional aggregates called \u201cdictyos\u201d which resemble the fruiting bodies formed by the social amoeba Dictyostelium discoidium. Our results demonstrate that these simple cellular units can autonomously organize themselves into complex 3D shapes reminiscent of natural systems.", "paraphrased_abstract": "The author describes how they have made a synthetic system of organisms capable of self-assembled, amorphous crystals, snowflakes, and living organisms like bacteria, worms, and animal tissues. The work was performed at Harvard University\u2019s Wyss Institute for Biologically Inspired Engineering. The study concerns a minimal cell, made of a spherical water-soluble emulsion containing a single microtubule surrounded by a lipid membrane. The cells bind together, connect to each other, tether them, and in turn form three-dimensional aggregates called dictyos, a type of dative organism that has the structure of the fruiting body of the social amoeba Dictyostelium discoideum. Its structure consists of thousands of identical cells, linked by elastic threads, which can be grown in Petri dishes with the help of some nutrients. This organism can grow and divide indefinitely without the aid of external signals (such as hormones). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4399, "title": "Spitzer-IRAC GLIMPSE of high mass protostellar objects. I Infrared point sources and nebulae", "abstract": "  The GLIMPSE archive was used to obtain 3.6--8.0micron, point source photometry and images for 381 massive protostellar candidates lying in the Galactic mid-plane. The colours, magnitudes and spectral indicies of sources in each of the 381 target fields were analysed and compared with the predictions of 2D radiative transfer model simulations. Although no discernable embedded clusters were found in any targets, multiple sources or associations of redenned young stellar objects were found in many sources indicating multiplicity at birth. The spectral index ($\\alpha$) of these point sources in 3.6--8.0mum bands display large values of $\\alpha$=2--5. A color-magnitude analog plot was used to identify 79 infrared counterparts to the HMPOs. Compact nebulae are found in 75% of the detected sources with morphologies that can be well described by core-halo, cometary, shell-like and bipolar geometries similar to those observed in ultra-compact HII regions. The IRAC band SEDs of the IR counterparts of HMPOs are best described to represent YSOs with a mass range of 8--20\\msun in their Class I stages when compared with 2D radiative transfer models. They also suggest that the high $\\alpha$ values represent reprocessed star/star+disk emission that is arising in the dense envelopes. Thus we are witnessing the luminous envelopes around the protostars rather than their photospheres or disks. We argue that the compact infrared nebulae likely reflect the underlying physical structure of the dense cores and are found to imitate the morphologies of known UCHII regions. Our results favour models of continuuing accretion involving both molecular and ionised accretion components to build the most massive stars rather than purely molecular rapid accretion flows. ", "machine_abstract": "We present the results of an infrared survey of the Galactic plane using Spitzer/InfraRed Array Camera (IRAC) data in four mid-infrared bands centered at 3.6, 4.5, 5.8, and 8 microns. The IRAC observations cover about one third of the entire sky with |b| < 1 degree. We have identified more than 100 thousand candidate young stellar objects (YSOs), including both Class 0/I YSOs and Classical T Tauri stars (CTTSs). About half of these are associated with previously known star-forming regions or open clusters. Most of them show extended emission around their central cores which is likely to be due to circumstellar disks and/or envelopes. A large fraction of our sample shows bipolar morphology indicative of outflows driven by accretion onto the central star(s). The spatial distribution of the newly discovered YSO candidates suggests that most of the recent massive star formation has occurred within the last few million years along the spiral arms of the Galaxy.", "paraphrased_abstract": "Several thousand new YSOs have been identified. Among these we have located some ten thousand young stars, of the class 0 to I. Most of them are located in the old-form regions, in the midst of the refractory disks and envelopes. The large portion of the candidates has a polarized form, indicative of the accretion of the central star. The spatial distribution of the newly discovered YSOs suggests that the greatest recent star-forming activity has taken place in the last few million years along the spiral arms of the galaxy. The Spitzer/IRAC data, obtained with Spitzer/IRAC, cover a third of the sky. In this study, we present the results of the IRAC observation of the Galactic plane, which covers the entire sky with a b = 1 degree. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.23, "title": "The anticenter old open cluster NGC 1883: radial velocity and metallicity", "abstract": "  Having already reported on the first photometric study of the intermediate-age open cluster NGC 1883 (Carraro et al. 2003), we present in this paper the first spectroscopic multi-epoch investigation of a sample of evolved stars in the same cluster. The aim is to derive the cluster membership, velocity and metallicity, and discuss recent claims in the literature (Tadross 2005) that NGC 1883 is as metal poor as globular clusters in the Halo. Besides, being one of the few outer Galactic disk intermediate-age open clusters known so far, it is an ideal target to improve our knowledge of the Galactic disk radial abundance gradient, that is a basic ingredient for any chemical evolution model of the Milky Way. The new data we obtained allow us to put NGC 1883's basic parameters more reliable. We find that the cluster has a mean metallicity of [Fe/H] = -0.20$\\pm$0.22, from which we infer an age (650$^{+70}_{-70}$ Myr) close to the Hyades one and a Galactocentric distance of 12.3$^{+0.4}_{-0.2}$ kpc. The metal abundance, kinematics, and position make NGC 1883 a genuine outer disk intermediate-age open cluster. We confirm that in the outer Galactic disk the abundance gradient is shallower than in the solar vicinity. ", "machine_abstract": "We present new spectroscopic observations for the open cluster NGC 1883, which is located at a distance of about 1 kpc in the constellation Cassiopeia (\u03b1 = 20 h 18 m , \u03b4 = +58\u00b0). The data were obtained with the 2-m telescope of the Observatorio Astron\u00f3mico Nacional de San Pedro M\u00e1rtir (OAN-SPM) on December 16-17, 2009 using the REOSC spectrograph equipped with grism #7 covering the wavelength range 3700-7000 \u00c5. We measured RVs for 23 stars by cross-correlating their spectra against those of template dwarfs observed under similar conditions. Our results show that most of these objects have heliocentric velocities between -40 to -50 km/sec, while only two stars are found outside this interval. These values agree well with previous determinations based on photometric methods.     In addition we derived metallicities [Fe/H] for 14 stars using the calibration of Alonso et al. (1999) . For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from -0.10 dex up to +0.20 dex. Only one object shows an iron abundance significantly higher than solar value (+0.30 dex).     Finally, we compared our results with previously published studies.", "paraphrased_abstract": "The results are compared with the previous results of other studies. In addition, we have measured metallicities for the stars in question by comparing their spectra with those of those in the trident. The tridents we compared with those of other stars have been compared to a set of calibrations by Alonso et al. (1999). This means that most of the objects we study have heliocentric velocities between -40 to -45 degrees, and the remaining two stars are located outside this range. In addition, we compared the metallicities of 14 stars with the tridental value of Alonso et al. (1998), which was calibrated by Alonso and his consort, respectively. The measurements of RVs were performed with the spectrometer of the observatory at San Pedro de Saint-Peir, from December 16 to 17, 2009, on the observatory telescope of the Observatory of Astronomy of San Pedro de Saint-Peir (OAN-SPM). This object was observed at a distance of 5\u00b0 in the constellation Cassiopeia ( = 21 h 18 m,  = 58\u00b0). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0158, "title": "Thermal noise informatics: Totally secure communication via a wire; Zero-power communication; and Thermal noise driven computing", "abstract": "  Very recently, it has been shown that thermal noise and its artificial versions (Johnson-like noises) can be utilized as an information carrier with peculiar properties therefore it may be proper to call this topic Thermal Noise Informatics. Zero Power (Stealth) Communication, Thermal Noise Driven Computing, and Totally Secure Classical Communication are relevant examples. In this paper, while we will briefly describe the first and the second subjects, we shall focus on the third subject, the secure classical communication via wire. This way of secure telecommunication utilizes the properties of Johnson(-like) noise and those of a simple Kirchhoff's loop. The communicator is unconditionally secure at the conceptual (circuit theoretical) level and this property is (so far) unique in communication systems based on classical physics. The communicator is superior to quantum alternatives in all known aspects, except the need of using a wire. In the idealized system, the eavesdropper can extract zero bit of information without getting uncovered. The scheme is naturally protected against the man-in-the-middle attack. The communication can take place also via currently used power lines or phone (wire) lines and it is not only a point-to-point communication like quantum channels but network-ready. Tests have been carried out on a model-line with ranges beyond the ranges of any known direct quantum communication channel and they indicate unrivalled signal fidelity and security performance. This simple device has single-wire secure key generation/sharing rates of 0.1, 1, 10, and 100 bit/second for copper wires with diameters/ranges of 21 mm / 2000 km, 7 mm / 200 km, 2.3 mm / 20 km, and 0.7 mm / 2 km, respectively and it performs with 0.02% raw-bit error rate (99.98 % fidelity). ", "machine_abstract": "We propose to use thermal noise as the information carrier for totally secure communications, zero power consumption devices, and computing systems. We show that by using an appropriate detection scheme, we can extract digital bits from the thermal noise with high fidelity at room temperature. The proposed system is based on a single-electron transistor (SET) which has been demonstrated in experiments recently. In addition, we demonstrate how this SET-based device can be used to perform logic operations such as AND gate and NOT gate. Finally, we discuss possible applications of our proposal including quantum key distribution, ultra-low power sensor networks, and energy efficient data centers. This work was supported by NSF under Grant No. ECCS-0926491. A new paradigm called \"thermal noise informatics\" is introduced where thermal noise is utilized as the information carrier for various tasks ranging from totally secure communication over wires to zero-power communication and computation. By employing an appropriate detection scheme, it is shown that one can extract digital bits from thermal noise with high fidelities even at room temperatures. It is also shown that these extracted digital bits can be used to perform logical operations such as AND gates and NOT gates. Possible applications are discussed along with their advantages compared to existing technologies.", "paraphrased_abstract": "Using an appropriate detection technique, it is shown that it is possible to extract digital bits from thermal noise at room temperature. Then, the extracted digital bits can be used to perform logics such as AND and NOT. This is followed by the discussion of possible advantages and disadvantages of the proposed method. This work is supported by NSF under the grant No. ECCS-0926491. We present a new, modern concept of thermal information processing, in which thermal noise is used for information exchange, from wired communication to computer communication and from computation. We present a system based on a single-electron transistor (SET), which has recently been shown in experiments. Besides, we demonstrate how the device can be used to perform logic, namely, if and when. The proposed system is based on a single-electron transistor (SET), which has recently been used in experiments. The proposed system is based on a single-electron transistor (SET), which has been tested recently by a certain number of people. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1418, "title": "Theory of Macroscopic Quantum Tunneling in High-T_c c-Axis Josephson Junctions", "abstract": "  We study macroscopic quantum tunneling (MQT) in c-axis twist Josephson junctions made of high-T_c superconductors in order to clarify the influence of the anisotropic order parameter symmetry (OPS) on MQT. The dependence of the MQT rate on the twist angle $\\gamma$ about the c-axis is calculated by using the functional integral and the bounce method. Due to the d-wave OPS, the $\\gamma$ dependence of standard deviation of the switching current distribution and the crossover temperature from thermal activation to MQT are found to be given by $\\cos2\\gamma$ and $\\sqrt{\\cos2\\gamma}$, respectively. We also show that a dissipative effect resulting from the nodal quasiparticle excitation on MQT is negligibly small, which is consistent with recent MQT experiments using Bi${}_2$Sr${}_2$CaCu${}_2$O${}_{8 + \\delta}$ intrinsic junctions. These results indicate that MQT in c-axis twist junctions becomes a useful experimental tool for testing the OPS of high-T_c materials at low temperature, and suggest high potential of such junctions for qubit applications. ", "machine_abstract": "We present the theory for macroscopic quantum tunneling (MQT) in high-temperature superconductors with an emphasis on its application to c-axis junctions. We show that MQT is suppressed by thermal fluctuations and magnetic field, but can be enhanced by applying a bias current or increasing the junction area. The results are compared with experiments performed at 4 K and 77 K. In particular we find good agreement between our theoretical predictions and experimental data obtained recently on YBa2Cu3O7-x single crystals.     Introduction     Superconductivity was discovered more than half a century ago [1] . Since then many new materials have been found which exhibit this fascinating phenomenon [2] , including some with very high transition temperatures T_c [3] . However, despite intensive research efforts there still remain several open questions about the nature of these novel compounds [4] . One such question concerns the mechanism responsible for their unusual properties [5] .     It has been suggested [6] that the pairing interaction may involve phonons [7 - 9] as well as spin excitations [10 - 12] . This leads to two possible scenarios for the formation of Cooper pairs [13] : either they form directly out of electrons via electron-phonon interactions [14] , or indirectly through spin-fluctuations [15] . These different mechanisms lead to distinct physical pictures [16] . For example, if one assumes that the pairing occurs only due to electron-phonon interactions [17] , it follows that the gap function should vanish along certain lines in momentum space [18] . On the other hand, if one considers the possibility of pair formation mediated by spin fluctuations [19] , the gap function vanishes over all momenta [20] .  The most important feature of both types of models is that they predict the existence of nodes [21] in the energy spectrum [22] . Nodes occur when the order parameter changes sign across the Fermi surface [23] . They were first predicted theoretically [24 - 26] and later observed experimentally [27] .", "paraphrased_abstract": "The study of superconductivity began half a century ago and has since been followed by a great deal of research. However, there remain a number of questions in the nature of these new materials, and it is particularly interesting to study how these substances work. One of the most important considerations is that they predict the existence of nodes in the energy spectrum. For example, if we assume that Cooperation only occurs by electron-phonon interactions, then the gap function disappears for certain points in the momentum space. If we consider the possibility of pairing with spin-fluctuations, then the gap function disappears for all momenta., whereas if we assume that the formation of the pair is mediated by spin-fluctuations, then the gap function vanishes over all the moments of the atom. This, however, remains to be studied in detail, despite all the efforts. We present a new theory for macroscopic quantum tunneling in high-temperature superconductors, and especially the application of MQT to c-axis junctions. We show that MQT is suppressed by thermal and magnetic fields, but can be enhanced by applying a bias current or by increasing the junction area. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3655, "title": "Evidence Of Dark Matter Annihilations In The WMAP Haze", "abstract": "  The WMAP experiment has revealed an excess of microwave emission from the region around the center of our Galaxy. It has been suggested that this signal, known as the ``WMAP Haze'', could be synchrotron emission from relativistic electrons and positrons generated in dark matter annihilations. In this letter, we revisit this possibility. We find that the angular distribution of the WMAP Haze matches the prediction for dark matter annihilations with a cusped density profile, $\\rho(r) \\propto r^{-1.2}$ in the inner kiloparsecs. Comparing the intensity in different WMAP frequency bands, we find that a wide range of possible WIMP annihilation modes are consistent with the spectrum of the haze for a WIMP with a mass in the 100 GeV to multi-TeV range. Most interestingly, we find that to generate the observed intensity of the haze, the dark matter annihilation cross section is required to be approximately equal to the value needed for a thermal relic, $\\sigma v \\sim 3 \\times 10^{-26}$ cm$^3$/s. No boost factors are required. If dark matter annihilations are in fact responsible for the WMAP Haze, and the slope of the halo profile continues into the inner Galaxy, GLAST is expected to detect gamma rays from the dark matter annihilations in the Galactic Center if the WIMP mass is less than several hundred GeV. ", "machine_abstract": "We present evidence for dark matter annihilation in the cosmic microwave background (CMB) haze, which is an excess emission at large angles with respect to the Galactic center that was first detected by Wilkinson Microwave Anisotropy Probe (WMAP). We use data from Planck and Fermi Large Area Telescope (LAT), as well as new measurements of the CMB temperature anisotropies made using the Atacama Cosmology Telescope (ACT). The observed spectrum of this signal can be explained if it originates from dark matter particles with masses between 1 GeV and 10 TeV, annihilating into pairs of photons or leptons. This interpretation requires a boost factor of about 100 relative to standard thermal relic expectations.     If confirmed, our results would provide strong support for models where dark matter self-annihilates into Standard Model particles. They also have important implications on the nature of dark matter itself, since they require either non-thermal production mechanisms or additional interactions beyond those predicted within the minimal supersymmetric extension of the Standard Model.", "paraphrased_abstract": "We argue that the cosmic microwave background (CMB) is a strong ion-exchange haze that is exaggerated by the rays of light at a large angle to the central Galactic centre and that was first detected by the Wilkinson Microwave Anisotropy Probe (WMAP). The results of this experiment show that dark matter particles are not only annihilated, but are also annihilated by the annihilation of particles of the mass 1 GeV to 10 TeV, annihilated by a pair of photons or leptons. This hypothesis, based on the results of the LAT and the ATCAT, is sufficient to support models for which dark matter is annihilated. The existence of dark matter is a very important question, because they may be the result of either an oxidizing process, or the existence of additional interactions that are outside the limits of the standard model. We present a spectrum of this signal, a signal that is obtained from the CMB haze, a layer of supersymmetric radiation in the vicinity of the Galactic center, which was first detected by the WMAP. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1689, "title": "Effective anisotropies and energy barriers of magnetic nanoparticles with Neel surface anisotropy", "abstract": "  Magnetic nanoparticles with Neel surface anisotropy, different internal structures, surface arrangements and elongation are modelled as many-spin systems. The results suggest that the energy of many-spin nanoparticles cut from cubic lattices can be represented by an effective one-spin potential containing uniaxial and cubic anisotropies. It is shown that the values and signs of the corresponding constants depend strongly on the particle's surface arrangement, internal structure and elongation. Particles cut from a simple cubic lattice have the opposite sign of the effective cubic term, as compared to particles cut from the face-centered cubic lattice. Furthermore, other remarkable phenomena are observed in nanoparticles with relatively strong surface effects: (i) In elongated particles surface effects can change the sign of the uniaxial anisotropy. (ii) In symmetric particles (spherical and truncated octahedral) with cubic core anisotropy surface effects can change its sign. We also show that the competition between the core and surface anisotropies leads to a new energy that contributes to both the 2nd- and 4th-order effective anisotropies. ", "machine_abstract": "We study the effective anisotropy constants, Keff , of an assembly of identical spherical particles in which each particle has uniaxial anisotropy along its easy axis (EA) that is tilted by an angle \u03b8 relative to the applied field direction. We show that for small values of \u03b8, Keff can be written as a sum of two terms: one term proportional to sin2\u03b8 arising due to dipolar interactions between the particles; another term independent of \u03b8 arising due to the shape anisotropy of individual particles. For large values of \u03b8, we find that Keff decreases rapidly with increasing \u03b8 because of the reduction in the number density of particles having their EA parallel to H. The dependence of Keff on \u03b8 is found to agree well with experimental results obtained recently for FePt nanoparticles.     In this work, we also calculate the activation energies associated with reversal processes involving rotation about different axes of the nanoparticle. It turns out that these activation energies are strongly dependent upon the value of \u03b8.", "paraphrased_abstract": "But it turned out that the activation of a reversal process takes place at the angle between the axes of the particle. This is also the case for the electrostatic effect of the nanoparticle, which is caused by the rotation of the axis. We have now developed a new method for measuring the effective anisotropy of an assembly of equal-size particles, where each particle has a radial anisotropy along its easy axis (EA), which is oriented by a radial angle, which is an angle, corresponding to the direction of the applied field. In this method, we calculate the activation energies of reversal, which are caused by rotation about different axes. We show that Keff is calculated in two terms, one proportional to sin2, which is the result of the dipolar interactions between the particles, and another proportional to the shape of individual particles. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2092, "title": "A Note on the Inapproximability of Correlation Clustering", "abstract": "  We consider inapproximability of the correlation clustering problem defined as follows: Given a graph $G = (V,E)$ where each edge is labeled either \"+\" (similar) or \"-\" (dissimilar), correlation clustering seeks to partition the vertices into clusters so that the number of pairs correctly (resp. incorrectly) classified with respect to the labels is maximized (resp. minimized). The two complementary problems are called MaxAgree and MinDisagree, respectively, and have been studied on complete graphs, where every edge is labeled, and general graphs, where some edge might not have been labeled. Natural edge-weighted versions of both problems have been studied as well. Let S-MaxAgree denote the weighted problem where all weights are taken from set S, we show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$ essentially belongs to the same hardness class in the following sense: if there is a polynomial time algorithm that approximates S-MaxAgree within a factor of $\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S', S'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda + \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high probability. A similar statement also holds for $S-MinDisagree. This result implies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree within a factor of $80/79-\\epsilon$, improving upon a previous known factor of $116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}. ", "machine_abstract": "We show that correlation clustering is NP-hard to approximate within any constant factor, even for graphs with maximum degree three and clusters of size at most four.  We also present an algorithm which solves this problem exactly in time O(n3). Our results are based on reductions from the exact cover by 3-sets (X3C) problem. The X3C problem asks whether there exists a collection of subsets of a set S such that each element of S belongs to exactly 3 sets in the collection; it has been shown to be NP-complete. For more information about our work see http://arxiv.org/abs/1206.0571 .  Correlation clustering is one of several problems studied under the umbrella of \"clustering\"; these include k-means clustering, spectral clustering, and graph partitioning. It was introduced independently by Bansal et al., Blum et al., and Dasgupta et al. as follows.   Given a weighted undirected graph G = (V, E), where V denotes the vertices and E denotes the edges, we say that two vertices u, v \u2208 V are adjacent if they share an edge e \u2208 E. A cluster C \u2286 V is defined as a subset of nodes whose pairwise distances satisfy some threshold t > 0. More formally, given a distance function d : V \u00d7 V \u2192 R+ , let dist(u,v) denote the shortest path between u and v; then, C is said to be a valid cluster if and only if for all pairs of nodes u, v \u2208 C:  dist(u,v)  \u2264 t", "paraphrased_abstract": "\u201cC  V is a cluster, defined by the distance t of the nodes that are shortest at each of the pairs of nodes which are u, v and C. It is defined by the distance t of the nodes u, v and C, and it is said to be a valid cluster if t  t  t  t. In our work we introduce two types of clustering, the k-means and the spectral clustering. In our study we introduce the X3C problem: a collection of subsets of a set S that the elements of S belong to exactly three sets; this problem is proved NP-complete. In general, we consider the X3C problem to be a collection of subsets of a set S such that each element of S belongs to exactly three sets of the set. This problem is presented in detail by Bansal et al., Blum et al., and Dasgupta et al. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2029, "title": "The Hopf Algebra Structure of the Character Rings of Classical Groups", "abstract": "  The character ring \\CGL of covariant irreducible tensor representations of the general linear group admits a Hopf algebra structure isomorphic to the Hopf algebra \\Sym$ of symmetric functions. Here we study the character rings \\CO and \\CSp of the orthogonal and symplectic subgroups of the general linear group within the same framework of symmetric functions. We show that \\CO and \\CSp also admit natural Hopf algebra structures that are isomorphic to that of \\CGL, and hence to \\Sym. The isomorphisms are determined explicitly, along with the specification of standard bases for \\CO and \\CSp analogous to those used for \\Sym. A major structural change arising from the adoption of these bases is the introduction of new orthogonal and symplectic Schur-Hall scalar products. Significantly, the adjoint with respect to multiplication no longer coincides, as it does in the \\CGL case, with a Foulkes derivative or skew operation. The adjoint and Foulkes derivative now require separate definitions, and their properties are explored here in the orthogonal and symplectic cases. Moreover, the Hopf algebras \\CO and \\CSp are not self-dual. The dual Hopf algebras \\CO^* and \\CSp^* are identified. Finally, the Hopf algebra of the universal rational character ring \\CGLrat of mixed irreducible tensor representations of the general linear group is introduced and its structure maps identified. ", "machine_abstract": "The character rings of classical groups are studied in this article, and their Hopf algebra structures are determined by using the results on the representation theory of finite groups obtained recently. The main result is that if G is an abelian group with order n{\\displaystyle n}, then the character ring K[G]K[G]$ has a Hopf algebra structure whose comultiplication map \u0394\u0394 satisfies (1) \u0394(g)(h)=\u03a3i=1n[g\u2212ih]/(n\u22121)! for all g\u2208G, h\u2208G, (2) \u0394(gh)=g\u0394(h)+h\u0394(g), and (3) \u0394(e)=e for e\u2208G. In particular, we have shown that the character ring of any cyclic group CcC_c$ has a Hopf algebra strucutre satisfying these conditions. We also give some examples to show how our method works.", "paraphrased_abstract": "The main result is that if G is an abelian group with order ndisplay style n, then the character ring K[G]K[G] has a Hopf algebra structure which is satisfying these conditions: (g)[H]i=i=1n[gi]/n1 for all gG, hG, [e][][2][3] for all gG, [4][5], [6], [7][8], [9],[9],[9],,[10][10],[12][13][13] [14]; [13][14], [14][16],[17],[17],[17][18],[19],[18],[19],[19],[19], and which is inscribed in the following sentence. The following example is shown. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3403, "title": "Sparticle Spectra and LHC Signatures for Large Volume String Compactifications", "abstract": "  We study the supersymmetric particle spectra and LHC collider observables for the large-volume string models with a fundamental scale of 10^{11} GeV that arise in moduli-fixed string compactifications with branes and fluxes. The presence of magnetic fluxes on the brane world volume, required for chirality, perturb the soft terms away from those previously computed in the dilute-flux limit. We use the difference in high-scale gauge couplings to estimate the magnitude of this perturbation and study the potential effects of the magnetic fluxes by generating many random spectra with the soft terms perturbed around the dilute flux limit. Even with a 40% variation in the high-scale soft terms the low-energy spectra take a clear and predictive form. The resulting spectra are broadly similar to those arising on the SPS1a slope, but more degenerate. In their minimal version the models predict the ratios of gaugino masses to be M_1 : M_2 : M_3=(1.5 - 2) : 2 : 6, different to both mSUGRA and mirage mediation. Among the scalars, the squarks tend to be lighter and the sleptons heavier than for comparable mSUGRA models. We generate 10 fb^{-1} of sample LHC data for the random spectra in order to study the range of collider phenomenology that can occur. We perform a detailed mass reconstruction on one example large-volume string model spectrum. 100 fb^{-1} of integrated luminosity is sufficient to discriminate the model from mSUGRA and aspects of the sparticle spectrum can be accurately reconstructed. ", "machine_abstract": "We study the phenomenology of string compactifications with large extra dimensions, focusing on supersymmetric particles in the mass range accessible to current experiments at the Large Hadron Collider (LHC). We consider two classes of models that are motivated by recent developments in string theory: weakly coupled heterotic orbifolds and strongly coupled Type IIB orientifold constructions. In both cases we find that there is an interesting interplay between the Kaluza-Klein excitations associated with the extra dimensions and the lightest Standard Model superpartners. For example, in some regions of parameter space it may be possible to produce gluinos or squarks directly via Drell-Yan processes; alternatively, these states can decay into lighter Standard Model superpartners which then cascade down to the LSP neutralino. The resulting collider signatures depend sensitively on the details of the underlying model parameters as well as the number of extra dimensions.", "paraphrased_abstract": "The phenomenology of the phenomenology of compactifications of great extra dimensions is analyzed in two different ways, as the result of recent developments in string theory. The weakly coupled heterotic orbifolds are a weakly coupled type IIB construction, and in the first place, it is interesting to observe that the Kaluza-Klein excitations of the extra dimensions are accompanied by the simplest superpartners of the standard model. The collider signature depends on the details of the model, the number of extra dimensions, and the number of extra dimensions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2399, "title": "Cosmological Constraints on f(R) Acceleration Models", "abstract": "  Models which accelerate the expansion of the universe through the addition of a function of the Ricci scalar f(R) leave a characteristic signature in the large-scale structure of the universe at the Compton wavelength scale of the extra scalar degree of freedom. We search for such a signature in current cosmological data sets: the WMAP cosmic microwave background (CMB) power spectrum, SNLS supernovae distance measures, the SDSS luminous red galaxy power spectrum, and galaxy-CMB angular correlations. Due to theoretical uncertainties in the nonlinear evolution of f(R) models, the galaxy power spectrum conservatively yields only weak constraints on the models despite the strong predicted signature in the linear matter power spectrum. Currently the tightest constraints involve the modification to the integrated Sachs-Wolfe effect from growth of gravitational potentials during the acceleration epoch. This effect is manifest for large Compton wavelengths in enhanced low multipole power in the CMB and anti-correlation between the CMB and tracers of the potential. They place a bound on the Compton wavelength of the field be less than of order the Hubble scale. ", "machine_abstract": "We study the cosmology of modified gravity models with an action that contains higher-order curvature terms, such as $R^2$ and $R_{\\mu \\nu \\rho \\sigma}$. We show how to derive constraints on these theories using observations of large-scale structure (LSS), cosmic microwave background radiation (CMB), supernovae Ia (SNeIa), baryon acoustic oscillations (BAO), Hubble constant measurements (HST), and gamma-ray bursts (GRBs). In particular we consider two classes of models: those in which the gravitational field equations are fourth order in derivatives; and those where they are second order but contain extra degrees of freedom beyond the usual massless graviton. For both cases we find that current data is consistent with general relativity at the level of one part in 10^6 or better. However, future experiments may be able to detect deviations from GR if they exist.     Keywords: Modified Gravity", "paraphrased_abstract": "In this way, the experiments are also able to detect deviations from the general relativity. The two classes of models we study are: those in which the gravity equations are fourth-order in derivatives, and those in which the extra degrees of freedom are more than ordinary. We find that the current data is consistent with the general relativity in one part in 106 or better. Moreover, we investigate the cosmology of the modified gravity models. We use a formula of higher order curvature, such as 224; and we show how to estimate the constraint with the cosmology of the cosmic microwave background, supernovae Ia (SNeIa), acoustic oscillations (BAO), Hubble constants (HST), and gamma-ray bursts (GRBs). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3156, "title": "How to clean a dirty floor: Probabilistic potential theory and the Dobrushin uniqueness theorem", "abstract": "  Motivated by the Dobrushin uniqueness theorem in statistical mechanics, we consider the following situation: Let \\alpha be a nonnegative matrix over a finite or countably infinite index set X, and define the \"cleaning operators\" \\beta_h = I_{1-h} + I_h \\alpha for h: X \\to [0,1] (here I_f denotes the diagonal matrix with entries f). We ask: For which \"cleaning sequences\" h_1, h_2, ... do we have c \\beta_{h_1} ... \\beta_{h_n} \\to 0 for a suitable class of \"dirt vectors\" c? We show, under a modest condition on \\alpha, that this occurs whenever \\sum_i h_i = \\infty everywhere on X. More generally, we analyze the cleaning of subsets \\Lambda \\subseteq X and the final distribution of dirt on the complement of \\Lambda. We show that when supp(h_i) \\subseteq \\Lambda with \\sum_i h_i = \\infty everywhere on \\Lambda, the operators \\beta_{h_1} ... \\beta_{h_n} converge as n \\to \\infty to the \"balayage operator\" \\Pi_\\Lambda = \\sum_{k=0}^\\infty (I_\\Lambda \\alpha)^k I_{\\Lambda^c). These results are obtained in two ways: by a fairly simple matrix formalism, and by a more powerful tree formalism that corresponds to working with formal power series in which the matrix elements of \\alpha are treated as noncommuting indeterminates. ", "machine_abstract": "We consider the problem of recovering an unknown signal on a domain $\\Omega$ by observing noisy measurements at some locations in $\\Omega$, where the noise is assumed to be additive white Gaussian with variance $\\sigma^2$.  We show that if the number of observations scales as $O(n^{1/3})$ then it is possible to recover the signal exactly using convex optimization techniques (where $n$ denotes the total number of pixels). This result improves upon previous results which required $O(n^{2/3})$ samples for exact recovery. The proof relies on recent developments in probabilistic potential theory and the so-called \"Dobrushin uniqueness theorems\". Our analysis also shows how one can use this framework to obtain guarantees under more general conditions than those considered previously.  The main idea behind our approach is to view the reconstruction process as a statistical estimation problem over a probability space equipped with a Gibbs measure. In particular, we introduce a new notion called \"local observability\" which quantifies the amount of information contained within each observation about the underlying signal.", "paraphrased_abstract": "We consider the problem of obtaining an unknown signal in the space of a maximum of a Gibbs measure, and present the results of our analysis as proofs against other conditions. The principal idea of our study is to make the reconstruction process a statistical estimation of a Gibbs-size function. In particular, we introduce a new concept, the local observation, which quantifies the amount of information contained in each observation of the signal. This approach is the result of recent developments in probabilistic potential theory and the so-called \u2018Dobrushin\u2019 uniqueness theorems\u2019. We introduce the notion of local observation as a measure of the information contained within an observation. Our analysis demonstrates that, if the number of observations is equal to O(n12/3) it is possible to recover the signal precisely by convex optimization techniques (where O represents the total number of pixels). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2394, "title": "Theory for superconductivity in a magnetic field: A local approximation approach", "abstract": "  We present a microscopic theory for superconductivity in a magnetic field based on a local approximation approach. We derive an expression for free energy density $F$ as a function of temperature $T$ and vector potential {\\bf a}, and two basic equations of the theory: the first is an implicit solution for energy gap parameter amplitude $|\\Delta_{\\bf k}|$ as a function of wave vector {\\bf k}, temperature $T$ and vector potential {\\bf a}; and the second is a London-like relation between electrical current density {\\bf j} and vector potential {\\bf a}, with an ``effective superconducting electron density'' $n_s$ that is both $T$- and {\\bf a}-dependent. The two equations allow determination of spatial variations of {\\bf a} and $|\\Delta_{\\bf k}|$ in a superconductor for given temperature $T$, applied magnetic field ${\\bf H}_a$ and sample geometry. The theory shows the existence of a ``partly-paired state,'' in which paired electrons (having $|\\Delta_{\\bf k}|>0$) and de-paired electrons (having $|\\Delta_{\\bf k}|=0$) co-exist. Such a ``partly-paired state'' exists even at T=0 when $H_a$ is above a threshold for a given sample, giving rise to a non-vanishing Knight shift at T=0 for $H_a$ above the threshold. We expect the theory to be valid for highly-local superconductors for all temperatures and magnetic fields below the superconducting transition. In the low-field limit, the theory reduces to the local-limit result of BCS. As examples, we apply the theory to the case of a semi-infinite superconductor in an applied magnetic field ${\\bf H}_a$ parallel to the surface of the superconductor and the case of an isolated vortex in an infinite superconductor, and determine, in each case, spatial variations of quantities such as {\\bf a} and $|\\Delta_{\\bf k}|$. We also calculate... ", "machine_abstract": "We present an approximate theory to study the effect of a magnetic field on the ground state properties and phase diagram of strongly correlated systems, such as high-Tc cuprates or heavy fermion compounds. The starting point is the exact diagonalization (ED) of small clusters with periodic boundary conditions at zero temperature. We use Lanczos ED method to calculate the energy spectrum of these clusters. Then we construct a variational wave function by projecting this cluster eigenstates onto a set of single-particle orbitals obtained within density functional theory (DFT). This allows us to obtain accurate results even when using only very few DFT states per orbital. Finally, we perform Monte Carlo simulations based on the Gutzwiller projected BCS wave functions to investigate the thermodynamic behavior of our system. Our main result shows that the application of a magnetic field leads to a suppression of the superfluid order parameter $\\psi$ which can be understood as a consequence of the Zeeman splitting between spin-up and -down electrons.", "paraphrased_abstract": "We start by examining the density of the earth, which is strongly correlated, with a high-frequency of energy, with a low temperature of zero, and then we calculate the energy spectrum of these clusters, by means of the ED method. We find the order of psi, which is due to the reconnection between spin-up and down electrons. In the first place, we have a mathematical model, based on the ED method, that studies the energy of small, irregular clusters at constant temperatures. We take the ED method to be precise, and ED is used to calculate the energy spectrum of these clusters, and then we compute the corresponding variational wave function for each cluster in a set of single-particle orbitals. We can obtain accurate results even when we have only a few DFT states per orbital. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.165, "title": "A new comprehensive set of elemental abundances in DLAs III. Star formation histories", "abstract": "  We obtained comprehensive sets of elemental abundances for eleven damped Ly-alpha systems (DLAs) at z_DLA=1.7-2.5. In Paper I of this series, we showed for three DLA galaxies that we can derive their star formation histories and ages from a detailed comparison of their intrinsic abundance patterns with chemical evolution models. We determine in this paper the star formation properties of six additional DLA galaxies. The derived results confirm that no single star formation history explains the diverse sets of abundance patterns in DLAs. We demonstrate that the various star formation histories reproducing the DLA abundance patterns are typical of local irregular, dwarf starburst and quiescent spiral galaxies. Independent of the star formation history, the DLAs have a common characteristic of being weak star forming galaxies; models with high star formation efficiencies are ruled out. All the derived DLA star formation rates per unit area are moderate or low, with values between -3.2 < log SFR < -1.1 M_sol yr^{-1} kpc^{-2}. The DLA abundance patterns require a large spread in ages ranging from 20 Myr up to 3 Gyr. The oldest DLA in our sample is observed at z_DLA=1.864 with an age estimated to more than 3 Gyr; it nicely indicates that galaxies were already forming at z_f>10. But, most of the DLAs show ages much younger than that of the Universe at the epoch of observation. Young galaxies thus seem to populate the high redshift Universe at z>2, suggesting relatively low redshifts of formation (z~3) for most high-redshift galaxies. The DLA star formation properties are compared with those of other high-redshift galaxies identified in deep imaging surveys with the aim of obtaining a global picture of high-redshift objects. ", "machine_abstract": "We present the results of our analysis of a sample of 25 high-resolution quasar absorption line systems with metallicities ranging between 1/100 and 1/10 solar, selected to have low dust content (i.e., < 0.1 mag extinction at 2200 A). We use these data together with those for another 20 DLAs presented by Pettini et al. (1999) to investigate the chemical enrichment history of DLA galaxies over cosmic time. The main conclusions are as follows:  The abundance patterns observed in this sample can be explained if we assume that most of the metals were produced during an early burst of star formation which occurred less than 10 Gyr ago.    This is consistent with previous studies based on smaller samples but it also shows that there may not always be evidence for recent star formation activity even when such activity has been inferred from other indicators.   In addition, we find no correlation between metallicity and dust content or neutral hydrogen column density.  Finally, we show that the mean value of [Fe/H] measured in DLAs agrees well with the predictions made using simple models of galactic chemical evolution.", "paraphrased_abstract": "I have presented here a series of 25 high-resolution absorption lines with metallicities between 1/100 and 1/100 solar, selected to have a low dust content (i.e.,  0.1 mag at 2200 A). We also demonstrate that the average value of Fe/H of the DLAs is more than agreeable with the predictions of simple models of the formation of galaxies. The main conclusions are as follows: The abundance of metals in this sample can be explained by the assumption that most of the metals are produced during a period of star formation, which happened less than 10 Gyrs ago. This is in keeping with studies based on small samples, but it is also possible that some of these metals were not present at all in the past, and it is possible that they were not present at all in the past. We use the results of our analysis to investigate the chemical enrichment history of a wide variety of quasars, which were found to be in the low-density range of a few hundred meters in diameter. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.113, "title": "A Communication Model for Adaptive Service Provisioning in Hybrid Wireless Networks", "abstract": "  Mobile entities with wireless links are able to form a mobile ad-hoc network. Such an infrastructureless network does not have to be administrated. However, self-organizing principles have to be applied to deal with upcoming problems, e.g. information dissemination. These kinds of problems are not easy to tackle, requiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks is arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could eliminate the need for any fixed infrastructure, has been damped. The goal is to overcome the limitations of pure ad-hoc networks by augmenting them with instant Internet access, e.g. via integration of UMTS respectively GSM links. However, this raises multiple questions at the technical as well as the organizational level. Motivated by characteristics of small-world networks that describe an efficient network even without central or organized design, this paper proposes to combine mobile ad-hoc networks and infrastructured networks to form hybrid wireless networks. One main objective is to investigate how this approach can reduce the costs of a permanent backbone link and providing in the same way the benefits of useful information from Internet connectivity or service providers. For the purpose of bridging between the different types of networks, an adequate middleware service is the focus of our investigation. This paper shows our first steps forward to this middleware by introducing the Injection Communication paradigm as principal concept. ", "machine_abstract": "In this work, we propose an adaptive service provisioning scheme to provide quality-of-service (QoS) guarantees and maximize the network utility by jointly optimizing resource allocation at both base stations (BSs) and mobile users (MUs). The proposed scheme is based on a communication model that incorporates user mobility into the QoS requirements. We formulate the problem as a joint optimization over BSs' power control variables, MUs' transmission rates, and their association with BSs. To solve it efficiently, we first decompose the original problem into two subproblems: one for each BS and another for all MUs. Then, we develop distributed algorithms to obtain solutions to these problems iteratively using dual decomposition techniques. Finally, simulation results show that our proposed algorithm can achieve better performance than existing schemes under various system settings. In recent years, wireless networks have been widely deployed around the world due to their low cost and easy deployment [1] . However, they are vulnerable to security attacks such as eavesdropping [2] , jamming [3] , and data tampering [4] . To enhance the security level of wireless communications, physical layer security has attracted much attention recently [5] - [8] . Physical layer security exploits the characteristics of the wireless channel to ensure secure transmissions without relying on any additional cryptographic keys or protocols [9] . It was shown in [10] that if the legitimate transmitter-receiver pair shares no common information about the statistical properties of the channels between them and other potential eavesdroppers, then perfect secrecy cannot be achieved even when there exists infinite number of antennas at the transmitter side. Therefore, practical approaches should consider imperfections in the estimation process [11] , limited transmit power [12] , and finite antenna numbers [13] .", "paraphrased_abstract": "And so it is necessary to have some semblance of asymmetrical information concerning the properties of the wireless network and of the number of antennas, because the number of antennas at the transmitter is infinite. To achieve the best security, the physical layer has attracted great attention. Increasingly, the wireless network has been popularized in the world for its simplicity and its convenience, and a number of security attacks have been committed. In this respect, it has been observed that if a reputable transmitter and receiver do not know each other's statistical properties, then it cannot be assured of complete secrecy, even if there are many antennas at the transmitter. It was shown in a study of the physical layer that if the transmitter and receiver have no common understanding of the statistical properties of the channel, it is impossible to achieve perfect secrecy even if there are infinite antennas at the transmitter. The problem of the proposed solution is an optimization of the power of the BSs, of the transmission rates of the MUs, of the connection with the BSs. In this study, we derived an adaptive system to provide QoS and maximize the network utility, by combining the QoS and the mobility of the mobile users. We formulate the problem as a joint optimization of the BSs' power, the MUs' transmission rates and the association with a ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0996, "title": "Non-Markovian entanglement dynamics of quantum continuous variable systems in thermal environments", "abstract": "  We study two continuous variable systems (or two harmonic oscillators) and investigate their entanglement evolution under the influence of non-Markovian thermal environments. The continuous variable systems could be two modes of electromagnetic fields or two nanomechanical oscillators in the quantum domain. We use quantum open system method to derive the non-Markovian master equations of the reduced density matrix for two different but related models of the continuous variable systems. The two models both consist of two interacting harmonic oscillators. In model A, each of the two oscillators is coupled to its own independent thermal reservoir, while in model B the two oscillators are coupled to a common reservoir. To quantify the degrees of entanglement for the bipartite continuous variable systems in Gaussian states, logarithmic negativity is used. We find that the dynamics of the quantum entanglement is sensitive to the initial states, the oscillator-oscillator interaction, the oscillator-environment interaction and the coupling to a common bath or to different, independent baths. ", "machine_abstract": "We study the non-Markovian evolution of bipartite Gaussian states under local dephasing noise and global squeezing interaction with an environment at finite temperature. We show that, for any initial state, there exists a critical time beyond which the system becomes separable. The critical time is determined by the initial state's purity and the environmental temperature. For pure initial states, we find that the critical time decreases monotonically as the temperature increases; while it first decreases then increases when the initial state has mixedness. In addition to this general result, we also present some specific examples where the critical times are analytically obtained. Our results provide useful insights into the effect of environmental temperature on the entanglement dynamics of open quantum systems. Introduction:-Entangled states play important roles in many fields such as quantum information processing [1] , condensed matter physics [2] , and quantum optics [3] . It is well known that the presence of external environments can lead to the loss of entanglement [4] . Therefore, understanding how the entanglement evolves in noisy environments is crucially important both theoretically and experimentally [5] . In recent years, much attention has been paid to studying the entanglement dynamics of quantum systems [6] - [8] . However, most previous works have focused only on Markovian processes [9] - [11] or special types of initial states [12] - [14] . Recently, several authors studied the entanglement dynamics of two-mode Gaussian states [15] - [17] . They found that the entanglement decays exponentially fast if one mode undergoes phase damping (or amplitude damping) [18] ; however, it may decay slowly even though the other mode experiences strong dissipation [19] . Moreover, they showed that the entanglement revival occurs periodically [20] . These studies were mainly based on numerical simulations [21] . Very recently, analytical solutions were presented [22] - [24] . Nevertheless, these investigations did not take into account the effects of environmental temperatures [25] - [27] .", "paraphrased_abstract": "In recent years, much attention has been paid to the study of the interaction of quantum systems with a noise-induced disturbance and the global squeezing of an external environment. The effects of this disturbance are known to the most part, but the main work in this area is focused only on the Markovian process, which is only used in special cases. The entanglement dynamics of quantum systems are very well known in all the fields, including the information processing, the condensed matter physics, and the optical sciences. We have, however, been primarily concerned with numerical simulations, although these studies did not take into account the effect of environmental conditions. Moreover, we have demonstrated that the entanglement in these states is constantly renewed, and that the entanglement is periodically revived. We have shown that for all initial states there is a certain critical time, the time ranging from the purity of the initial state to the temperature of the environment, depends on the purity of the initial state, and the temperature of the environment. We have shown that the critical time depends on the purity of the initial state and the temperature of the environment. In this respect, the critical time is determined by the degree of purity of the initial state and the degree of mixing with the environment. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1582, "title": "Measurement of the spin polarization of the magnetic semiconductor EuS with zero-field and Zeeman-split Andreev reflection spectroscopy", "abstract": "  We report measurements of the spin polarization (\\textbf{\\textit{P}}) of the concentrated magnetic semiconductor EuS using both zero-field and Zeeman-split Andreev reflection spectroscopy (ARS) with EuS/Al planar junctions. The zero-field ARS spectra are well described by the modified (spin-polarized) BTK model with expected superconducting energy gap and actual measurement temperature (no additional spectral broadening). The fittings consistently yield \\textbf{\\textit{P}} close to 80% regardless of the barrier strength. Moreover, we performed ARS in the presence of a Zeeman-splitting of the quasiparticle density of states in Al. To describe the Zeeman-split ARS spectra, we develop a theoretical model which incorporates the solution to the Maki-Fulde equations into the modified BTK analysis. The method enables the determination of the magnitude as well as the sign of \\textbf{\\textit{P}} with ARS, and the results are consistent with those from the zero-field ARS. The experiments extend the utility of field-split superconducting spectroscopy from tunnel junctions to Andreev junctions of arbitrary barrier strengths. ", "machine_abstract": "We report on measurements of the spin polarization in the ferromagnetic semiconductor EuS using two different spectroscopies, namely zero-field (ZF) and Zeeman split Andreev reflection (ZSAR). The ZF technique is based on measuring the conductance through an interface between a normal metal and a ferromagnet as function of the angle between magnetization directions of both materials. In contrast to this method we use the ZSAR technique which allows us to measure directly the spin polarization at the Fermi level by applying a small external field parallel to the sample surface. We find that for low temperatures below 10 K the results obtained by these techniques are consistent within experimental error bars. However, above T = 10 K there is a significant discrepancy between the values determined by the two methods. This may be explained by assuming that the temperature dependence of the spin polarization has a non-monotonic behavior. Our findings demonstrate that the combination of ZF and ZSAR experiments can provide valuable information about the electronic structure of ferromagnets.", "paraphrased_abstract": "The results of the two methods are consistent with the experimental errors. We compare the results of these two methods with the spectral method, ZF, and ZSAR. The spectral method is based on the eddy current of the normal metal and the ferromagnet, which depends on the angle between the magnetization directions. The spectral method is based on the analysis of the polarity of the ferromagnet and the polarity of the Fermi; in contrast, we apply the spectral method ZSAR, which directly measures the polarity of the Fermi. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0435, "title": "Type D Einstein spacetimes in higher dimensions", "abstract": "  We show that all static spacetimes in higher dimensions are of Weyl types G, I_i, D or O. This applies also to stationary spacetimes if additional conditions are fulfilled, as for most known black hole/ring solutions. (The conclusions change when the Killing generator becomes null, such as at Killing horizons.) Next we demonstrate that the same Weyl types characterize warped product spacetimes with a one-dimensional Lorentzian (timelike) factor, whereas warped spacetimes with a two-dimensional Lorentzian factor are restricted to the types D or O. By exploring the Bianchi identities, we then analyze the simplest non-trivial case from the above classes - type D vacuum spacetimes, possibly with a cosmological constant, dropping, however, the assumptions that the spacetime is static, stationary or warped. It is shown that for ``generic'' type D vacuum spacetimes the corresponding principal null directions are geodetic in any dimension (this applies also to type II spacetimes). For n>=5, however, there may exist particular cases of type D spacetimes which admit non-geodetic multiple principal null directions and we present such examples in any n>=7. Further studies are restricted to five dimensions, where the type D Weyl tensor is described by a 3x3 matrix \\Phi_{ij}. In the case with ``twistfree'' (A_{ij}=0) principal null geodesics we show that in a ``generic'' case \\Phi_{ij} is symmetric and eigenvectors of \\Phi_{ij} coincide with those of the expansion matrix S_{ij}, providing us with three preferred spacelike directions of the spacetime. Similar results are also obtained when relaxing the twistfree condition and assuming instead that \\Phi_{ij} is symmetric. The n=5 Myers-Perry black hole and Kerr-NUT-AdS metrics in arbitrary dimension are briefly studied as specific examples of type D vacuum spacetime. ", "machine_abstract": "We study the properties of type-D Einstein metrics on compact manifolds with boundary, and their relation to supersymmetric solutions of supergravity theories. We show that these spaces are characterized by an integrable complex structure J which is compatible with the metric g and satisfies certain additional conditions at the boundary. In particular we prove that if (M n ,g) is such a space then there exists a unique spinor field $\\psi$ satisfying $(D_\\mu\\psi)^\\alpha = 0$ for all $\\alpha\\in \\Gamma_n$ where $D_\\mu = \\partial_\\mu + iA^\\mu$ is the Dirac operator associated to the connection $A = \\mathrm{d}g - J\\wedge\\mathrm{d}$. Moreover this spinor field has constant norm and defines a Killing vector field $X = \\psi^{*}\\psi$ whose flow preserves both $g$ and $J$.     The existence of such a spinor field implies that the scalar curvature $R_g$ of the metric vanishes identically. Conversely, any solution of the vacuum Einstein equations with vanishing scalar curvature can be written as a warped product over a flat base manifold times a Riemannian Einstein solvmanifold. This result generalizes previous results obtained in dimension four by Bryant and Salamon [BS] .", "paraphrased_abstract": "We show that such spaces are characterized by an integrable complex J that is compatible with the metric g and meets certain additional conditions at the boundary. We show that these spaces are characterized by an integrable complex J, which is compatible with the metric g and fulfils certain additional conditions at the boundary. Moreover, this integrable complex J is a continuous, invariable, and has the effect of preserving the metric g and the metric g. Moreover, this integrable complex is constant and defining a killer vector J X = psi psi psi whose flow is preserved for both g and J. The axiom of this axiom converges with the axiom of the vacuum Einstein equations which disappear as a result of vanishing scalar curvature. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1088, "title": "G-convergence and homogenization of viscoelastic flows", "abstract": "  The paper is devoted to homogenization of two-phase incompressible viscoelastic flows with disordered microstructure. We study two cases. In the first case, both phases are modeled as Kelvin-Voight viscoelastic materials. In the second case, one phase is a Kelvin-Voight material, and the other is a viscous Newtonian fluid. The microscale system contains the conservation of mass and balance of momentum equations. The inertial terms in the momentum equation incorporate the actual interface advected by the flow. In the constitutive equations, a frozen interface is employed. The interface geometry is arbitrary: we do not assume periodicity, statistical homogeneity or scale separation. The problem is homogenized using G-convergence and oscillating test functions. Since the microscale system is not parabolic, previously known constructions of the test functions do not work here. The test functions developed in the paper are non-local in time and satisfy divergence-free constraint exactly. The latter feature enables us to avoid working with pressure directly. We show that the effective medium is a single phase viscoelastic material that is not necessarily of Kelvin-Voight type. The effective constitutive equation contains a long memory viscoelastic term, as well as instantaneous elastic and viscous terms. ", "machine_abstract": "We study the G-convergence for nonlinear viscoelastic flows in bounded domains with smooth boundaries, where the constitutive relation is given by an integral-type equation. We prove that if the sequence of solutions to the corresponding problems converges weakly* in $L^\\infty(0,T;L^2)$ then it also converges strongly in $L^1(0,T;H^{-1})$. As applications we consider two examples of such equations. The first one is the Oldroyd-B model which describes the flow behavior of polymeric fluids. In this case our results show that the limit problem can be formulated as a system of partial differential equations coupled through the stress tensor. The second example deals with the so-called Maxwell fluid whose relaxation time depends on the shear rate. Here we obtain a new formulation of the limit problem involving only ordinary differential equations. Finally, we present some numerical experiments illustrating these theoretical results. Keywords: Viscoelasticity, Homogenization, Limiting process", "paraphrased_abstract": ", we are able to study the kinetics of the viscoelastic fluids and we will examine the limits of the flow. The first one is the Oldroyd-B equation, which describes the behaviour of polymer fluids. In this case we find that the limit of the flow can be formulated by a system of partial differential equations coupled through the stress tensor. The second is the so-called Maxwell fluid, whose kinetics depend on the stress. In the second instance we consider the so-called Maxwell fluid, which is invariably induced by the tangential flow. We are trying to show that, if the sequence of solutions to the corresponding problems is weak in the sum of Linfty(0,T;L2), it is also strong in L1(0,T;H-1). We show that, if the sequence of solutions to the corresponding problems is weak in Linfty(0,T), then the sequence is strong in L1(0,T,H1)[18]. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2929, "title": "La controverse de 1874 entre Camille Jordan et Leopold Kronecker", "abstract": "  During the whole of 1874, Camille Jordan and Leopold Kronecker quar- relled vigorously over the organisation of the theory of bilinear forms. That theory promised a \"general\" and \"homogeneous\" treatment of numerous questions arising in various 19th-century theoretical contexts, and it hinged on two theorems, stated independently by Jordan and Weierstrass, that would today be considered equivalent. It was, however, the perceived difference between those two theorems that sparked the 1874 controversy. Focusing on this quarrel allows us to explore the algebraic identity of the polynomial practices of the manipulations of forms in use before the advent of structural approaches to linear algebra. The latter approaches identified these practices with methods for the classification of similar matrices. We show that the prac- tices -- Jordan's canonical reduction and Kronecker's invariant computation -- reflect identities inseparable from the social context of the time. Moreover, these practices reveal not only tacit knowledge, local ways of thinking, but also -- in light of a long history tracing back to the work of Lagrange, Laplace, Cau- chy, and Hermite -- two internal philosophies regarding the significance of generality which are inseparable from two disciplinary ideals opposing algebra and arithmetic. By interrogating the cultural identities of such practices, this study aims at a deeper understanding of the history of linear algebra without focusing on issues related to the origins of theories or structures. ", "machine_abstract": "Leopold Kronecker (1823-1901) est un math\u00e9maticien allemand qui s'est int\u00e9ress\u00e9 \u00e0 la th\u00e9orie des nombres, aux fonctions elliptiques ainsi qu'\u00e0 l'alg\u00e8bre lin\u00e9aire. Il fut l'un des principaux fondateurs du groupe Bourbaki en 1934 avec Andr\u00e9 Weil, Jean Delsarte, Ren\u00e9 Decartes, Nicolas Bourbaki, Jean Dieudonn\u00e9, Claude Chevalley, \u00c9lie Cartan, Jean-Louis Koszul, Samuel Eilenberg et Jacques-Louis Lions. En 1873, il publie dans le Journal f\u00fcr die reine und angewandte Mathematik une s\u00e9rie d'articles sur les alg\u00e8bres associatives et non associatives. Dans son article intitul\u00e9 \u00ab\u00a0Ueber die Theorie der endlichen Gruppen von Variabeln\u00a0\u00bb [Kronecker, L., Ueber die Theorie der Endlichen Gruppen von Varien, JFM 5.2 (1875), p.\u00a01\u201342], il introduit pour la premi\u00e8re fois la notion d'alg\u00e8bre associative finie.", "paraphrased_abstract": "In 1873 il publie dans le Journal f\u00fcr die Reine und angewandte Mathematik une s\u00e9rie de articles sur les alg\u00e9bres associatives et non associatives. Il fut l'un des principaux founders du groupe Bourbaki en 1934 avec Andr\u00e9 Weil, Jean Delsarte, Ren\u00e9 Decartes, Nicolas Bourbaki, Jean Dieudonn\u00e9, Claude Chevalley, \u00c9lie Cartan, Jean-Louis Koszul, Samuel Eilenberg, Jacques-Louis Lions. Leopold Kronecker, en 1873, est un math\u00e9maticien allemand qui s'est \u00e0 la th\u00e9orie des nombres, aux fonctions elliptiques, ainsi qu'\u00e0 l'alg\u00e8bre lin\u00e9aire. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3966, "title": "Determination of the Far-Infrared Cosmic Background Using COBE/DIRBE and WHAM Data", "abstract": "  Determination of the cosmic infrared background (CIB) at far infrared wavelengths using COBE/DIRBE data is limited by the accuracy to which foreground interplanetary and Galactic dust emission can be modeled and subtracted. Previous determinations of the far infrared CIB (e.g., Hauser et al. 1998) were based on the detection of residual isotropic emission in skymaps from which the emission from interplanetary dust and the neutral interstellar medium were removed. In this paper we use the Wisconsin H-alpha Mapper (WHAM) Northern Sky Survey as a tracer of the ionized medium to examine the effect of this foreground component on determination of the CIB. We decompose the DIRBE far infrared data for five high Galactic latitude regions into H I and H-alpha correlated components and a residual component. We find the H-alpha correlated component to be consistent with zero for each region, and we find that addition of an H-alpha correlated component in modeling the foreground emission has negligible effect on derived CIB results. Our CIB detections and 2 sigma upper limits are essentially the same as those derived by Hauser et al. and are given by nu I_nu (nW m-2 sr-1) < 75, < 32, 25 +- 8, and 13 +- 3 at 60, 100, 140, and 240 microns, respectively. Our residuals have not been subjected to a detailed anisotropy test, so our CIB results do not supersede those of Hauser et al. We derive upper limits on the 100 micron emissivity of the ionized medium that are typically about 40% of the 100 micron emissivity of the neutral atomic medium. This low value may be caused in part by a lower dust-to-gas mass ratio in the ionized medium than in the neutral medium, and in part by a shortcoming of using H-alpha intensity as a tracer of far infrared emission. ", "machine_abstract": "We have used data obtained by the DIRBE instrument on board the COBE satellite to determine the far-infrared cosmic background (CIB) at wavelengths between 1.25 mm and 240 m, which is dominated by emission lines from interstellar atomic hydrogen. We find that the CIB intensity in this wavelength range can be described as a power law with an index of -2.6 +/- 0.1. The total energy density of the CIB integrated over all wavelengths longer than 100 microns amounts to about 5 x 10(-3). This value agrees well with previous estimates based on IRAS observations but it is significantly higher than values derived from recent ISO measurements. Our results are consistent with theoretical predictions for the contribution of dusty galaxies to the CIB. They also provide new constraints on models describing the formation of structure in the universe.     Keywords: infrared radiation; cosmology; galaxy evolution; intergalactic medium", "paraphrased_abstract": ", the intergalactic medium., the cosmic background, which is composed of atoms, atoms, and molecules, in the range from 0 to 200 m, is dominated by atoms containing atomic hydrogen. We use DIRBE to study the intensity of the CIB in all wavelengths, from 1 to 240 m, in which the emission lines of the hydrogen atoms are dominated. We find that the intensity of the CIB is equal to a power law of -2.6 - 0.1; and in the area of a hundred microns it is about 5 x 10(-3), compared with the last one, derived from the ISO measurement. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1826, "title": "Dynamical non-axisymmetric instabilities in rotating relativistic stars", "abstract": "  We present new results on dynamical instabilities in rapidly rotating neutron-stars. In particular, using numerical simulations in full General Relativity, we analyse the effects that the stellar compactness has on the threshold for the onset of the dynamical bar-mode instability, as well as on the appearance of other dynamical instabilities. By using an extrapolation technique developed and tested in our previous study [1], we explicitly determine the threshold for a wide range of compactnesses using four sequences of models of constant baryonic mass comprising a total of 59 stellar models. Our calculation of the threshold is in good agreement with the Newtonian prediction and improves the previous post-Newtonian estimates. In addition, we find that for stars with sufficiently large mass and compactness, the m=3 deformation is the fastest growing one. For all of the models considered, the non-axisymmetric instability is suppressed on a dynamical timescale with an m=1 deformation dominating the final stages of the instability. These results, together with those presented in [1], suggest that an m=1 deformation represents a general and late-time feature of non-axisymmetric dynamical instabilities both in full General Relativity and in Newtonian gravity. ", "machine_abstract": "We study the dynamical stability properties of rapidly-rotating, axisymmetric neutron star models with different equations of state (EOS). We find that for all EOS considered here, there is an instability associated to the presence of toroidal magnetic fields which can be excited by differential rotation between core and crust regions. The growth timescale of this instability ranges from 10-100 ms depending on the strength of the field and the degree of differential rotation present initially.     This instability may have important consequences for magnetar flares as well as for the spin evolution of young pulsars. In particular we show how it could lead to rapid spindowns during the first few hundred milliseconds after birth if the initial conditions are such that the unstable mode has a large enough amplitude. Finally, we discuss possible observational signatures of these instabilities. Dynamical non-axisymmetric modes in rapidly-rotating relativistic stars", "paraphrased_abstract": "We investigate the stability of non-axisymmetric neutron stars in the form of a number of equations. Among these are a toroidal magnetic field which is emitted by the movement of the core and the crust, and which can be excited by a difference of rotation from the southeastern core to the northwest. We examine the dynamics of non-axisymmetric neutron stars with various equations of state. We find that the existence of this instability leads to a rapid rise of the axisymmetric field for the first few hundred milliseconds, depending on the strength of the field and the degree of rotation that is present. We show how it can lead to rapid spindowns in the first few hundred milliseconds, if the conditions are such that the unstable phase is large enough. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1127, "title": "Sensitivity of solar off-limb line profiles to electron density stratification and the velocity distribution anisotropy", "abstract": "  The effect of the electron density stratification on the intensity profiles of the H I Ly-$\\alpha$ line and the O VI and Mg X doublets formed in solar coronal holes is investigated. We employ an analytical 2-D model of the large scale coronal magnetic field that provides a good representation of the corona at the minimum of solar activity. We use the mass-flux conservation equation to determine the outflow speed of the solar wind at any location in the solar corona and take into account the integration along the line of sight (LOS). The main assumption we make is that no anisotropy in the kinetic temperature of the coronal species is considered. We find that at distances greater than 1 Rsun from the solar surface the widths of the emitted lines of O VI and Mg X are sensitive to the details of the adopted electron density stratification. However, Ly-$\\alpha$, which is a pure radiative line, is hardly affected. The calculated total intensities of Ly-$\\alpha$ and the O VI doublet depend to a lesser degree on the density stratification and are comparable to the observed ones for most of the considered density models. The widths of the observed profiles of Ly-$\\alpha$ and Mg X are well reproduced by most of the considered electron density stratifications, while for the O VI doublet only few stratifications give satisfying results. The densities deduced from SOHO data result in O VI profiles whose widths and intensity ratio are relatively close to the values observed by UVCS although only isotropic velocity distributions are employed. These density profiles also reproduce the other considered observables with good accuracy. Thus the need for a strong anisotropy of the velocity distribution (i.e. a temperature anisotropy) is not so clear cut as previous investigations of UVCS data suggested. ... ", "machine_abstract": "We have investigated how different assumptions about the velocity distribution function (VDF) affect the shape of the observed line profile in the solar corona, using an analytical model for the VDF that includes both isotropic thermal motions and anisotropic nonthermal motions. We find that the inclusion of nonthermal motions can significantly alter the shapes of the simulated line profiles compared with those obtained assuming purely Maxwellian distributions. The effects are more pronounced when the plasma temperature decreases and/or the degree of anisotropy increases.     In particular, we show that the presence of nonthermal motions leads to significant asymmetries between the red-and blueshifted wings of the line profiles. These results suggest that it may be possible to use observations of coronal lines to constrain the properties of the underlying VDFs. However, this requires accurate measurements of the Doppler shifts associated with each emission feature along the line-of-sight.", "paraphrased_abstract": "The measurement of Doppler shifts at the points of the line is necessary. Several experiments were performed on the VDF in the solar corona, and a numerical model of the VDF was presented. The results showed that the VDF induced the shape of the observed line profile, compared with the one observed in the Maxwellian distribution. The effects were more noticeable when the plasma temperature dipped and the anisotropic distribution increased. These results showed that it is possible to determine the geometry of the VDF from the observation of the heliospheric lines. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1625, "title": "Thermal Stability of Metallic Single-Walled Carbon Nanotubes: An O(N) Tight-Binding Molecular Dynamics Simulation Study", "abstract": "  Order(N) Tight-Binding Molecular Dynamics (TBMD) simulations are performed to investigate the thermal stability of (10,10) metallic Single-Walled Carbon Nanotubes (SWCNT). Periodic boundary conditions (PBC) are applied in axial direction. Velocity Verlet algorithm along with the canonical ensemble molecular dynamics (NVT) is used to simulate the tubes at the targeted temperatures. The effects of slow and rapid temperature increases on the physical characteristics, structural stability and the energetics of the tube are investigated and compared. Simulations are carried out starting from room temperature and the temperature is raised in steps of 300K. Stability of the simulated metallic SWCNT is examined at each step before it is heated to higher temperatures. First indication of structural deformation is observed at 600K. For higher heat treatments the deformations are more pronounced and the bond breaking temperature is reached around 2500K. Gradual (slow) heating and thermal equilibrium (fast heating) methods give the value of radial thermal expansion coefficient in the temperature range between 300K-600K as 0.31x10^{-5}(1/K) and 0.089x10^{-5}(1/K), respectively. After 600K, both methods give the same value of 0.089x10^{-5}(1/K). The ratio of the total energy per atom with respect to temperature is found to be 3x10^{-4} eV/K. ", "machine_abstract": "The thermal stability and melting behavior of metallic single-wall carbon nanotubes (SWCNTs) are investigated by using an efficient tight-binding molecular dynamics simulation method with the Tersoff potential function. The results show that SWCNTs can be melted at temperatures ranging from 2000 to 3000 K, depending on their diameters. It is found that the melting temperature increases as the diameter decreases for both zigzag- and armchair-type tubes. In addition, it is shown that the melting process involves two stages in which the tube wall first becomes disordered followed by the formation of liquid-like structures inside the tube. Finally, we find that the melting point of SWCNTs depends strongly on the chirality index n-m. For example, the melting points of zigzag-type SWCNTs increase rapidly when n-m changes from 0 to 1 or -1.     Keywords: Thermal stability, Melting behavior, Carbon nanotube", "paraphrased_abstract": "The temperature of the SWCNTs increases as the diameter of the tube increases. As a result, it is found that the melting temperature of SWCNTs increases rapidly with the increase of nm. This temperature increases for zigzag and chair-like tubes. The melting temperature is found to increase significantly with the decreasing size of the tube. In the end, it is shown that the melting temperature of SWCNTs increases rapidly with nm changing from 0 to 1 or -1. It is also shown that the melting temperature of the SWCNTs is influenced by the chirality index nm. For example, the melting temperature of the zigzag and chair-like tubes increases in rapid proportions with the change in nm. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0878, "title": "Structural relaxation around substitutional Cr3+ in MgAl2O4", "abstract": "  The structural environment of substitutional Cr3+ ion in MgAl2O4 spinel has been investigated by Cr K-edge Extended X-ray Absorption Fine Structure (EXAFS) and X-ray Absorption Near Edge Structure (XANES) spectroscopies. First-principles computations of the structural relaxation and of the XANES spectrum have been performed, with a good agreement to the experiment. The Cr-O distance is close to that in MgCr2O4, indicating a full relaxation of the first neighbors, and the second shell of Al atoms relaxes partially. These observations demonstrate that Vegard's law is not obeyed in the MgAl2O4-MgCr2O4 solid solution. Despite some angular site distortion, the local D3d symmetry of the B-site of the spinel structure is retained during the substitution of Cr for Al. Here, we show that the relaxation is accomodated by strain-induced bond buckling, with angular tilts of the Mg-centred tetrahedra around the Cr-centred octahedron. By contrast, there is no significant alteration of the angles between the edge-sharing octahedra, which build chains aligned along the three four-fold axes of the cubic structure. ", "machine_abstract": "The structural relaxation and the local vibrational modes (LVMs) are investigated by first-principles calculations for the substituted system Mg1-xCrxAl2-xO4 with x=0, 0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0 at%. The results show that the substitution of Al3+ ions by Cr3+ leads to an increase in volume as well as a decrease in the unit cell parameters. In addition, we find that there is no significant change in the bond lengths between nearest-neighboring atoms when the concentration of Cr3+ increases up to 5%. However, it can be seen clearly that the bond length between second-nearest neighboring atoms decreases gradually with increasing Cr3+ content. Furthermore, our calculated phonon dispersion curves indicate that all the LVMs are stable except those along the \u0393-X direction which become unstable above 1% Cr3+ doping.", "paraphrased_abstract": "Moreover, our measurements of the local phonon density and the local vibrations are shown to be stable, except for the -x direction, which is unstable below 1% Cr3+. The results show that the substitution of Al3+ with Cr3+ leads to an increase in volume, and to a decrease in the cell parameters. Moreover, we see that the bond lengths between the neighbouring atoms increase gradually with the concentration of Cr3+. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1834, "title": "Charge Ordering in Half-Doped Manganites: Weak Charge Disproportion and Leading Mechanisms", "abstract": "  The apparent contradiction between the recently observed weak charge disproportion and the traditional Mn$^{3+}$/Mn$^{4+}$ picture of the charge-orbital orders in half-doped manganites is resolved by a novel Wannier states analysis of the LDA$+U$ electronic structure. Strong electron itinerancy in this charge-transfer system significantly delocalizes the occupied low-energy \"Mn$^{3+}$\" Wannier states such that charge leaks into the \"Mn$^{4+}$\"-sites. Furthermore, the leading mechanisms of the charge order are quantified via our first-principles derivation of the low-energy effective Hamiltonian. The electron-electron interaction is found to play a role as important as the electron-lattice interaction. \\ignore{A general picture of doped holes in strongly correlated charge-transfer systems is presented and applied to the study of charge order in half-doped manganites, using a novel Wannier states analysis of the LDA$+U$ electronic structure. While residing primarily in the oxygen atoms, the doped holes form additional effective $e_g$ orbitals at the low-energy scale, leading to an effective Mn$^{3+}$/Mn$^{4+}$ valence picture that enables weak charge disproportion, resolving the current serious contradictions between the recent experimental observations of charge distribution and traditional models. Furthermore, the leading mechanisms of the observed charge order are quantified via our first-principles derivation of the low-energy effective Hamiltonian ", "machine_abstract": "Charge ordering (CO) is one of the most important phenomena in strongly correlated electron systems, which has been observed in many transition metal oxides such as manganese perovskite compounds. In this work we study charge disproportionation in half-doped manganites by using density functional theory with Hubbard U correction. We find that CO state can be stabilized at low temperatures due to strong Coulomb interaction between Mn3+ and Mn4+ ions. The energy gain for CO state over metallic state increases rapidly when temperature decreases below Tc. Our results show that CO state is more stable than other competing states including ferromagnetic insulator phase and antiferromagnetic insulating phase.     Keywords: Charge-ordering, Density-functional-theory, Correlated electrons, Transition-metal-oxides, Manganites, Energy-band-structure, Insulators, Ferromagnetism, Antiferromagnetism", "paraphrased_abstract": "The result is that the CO state is stable more than other competing states, including ferromagnetic insulator and antiferromagnetic insulator. This is an important phenomenon in strongly correlated electrons, as observed in many transition metals, such as manganese. In this study, we consider the reproportionation of the charged side by side of half-doped manganite by means of density functional theory. We found that the energy of the CO state is very high at low temperatures, because of the strong Coulomb interaction between Mn3 and Mn4 ions. We show that the energy of the CO state is more stable than the other competing states, such as ferromagnetic and antiferromagnetic. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0763, "title": "A new Generation of Spectrometer Calibration Techniques based on Optical Frequency Combs", "abstract": "  Typical astronomical spectrographs have a resolution ranging between a few hundred to 200.000. Deconvolution and correlation techniques are being employed with a significance down to 1/1000 th of a pixel. HeAr and ThAr lamps are usually used for calibration in low and high resolution spectroscopy, respectively. Unfortunately, the emitted lines typically cover only a small fraction of the spectrometer's spectral range. Furthermore, their exact position depends strongly on environmental conditions. A problem is the strong intensity variation between different (intensity ratios {>300). In addition, the brightness of the lamps is insufficient to illuminate a spectrograph via an integrating sphere, which in turn is important to calibrate a long-slit spectrograph, as this is the only way to assure a uniform illumination of the spectrograph pupil.   Laboratory precision laser spectroscopy has experienced a major advance with the development of optical frequency combs generated by pulsed femto-second lasers. These lasers emit a broad spectrum (several hundred nanometers in the visible and near infra-red) of equally-spaced \"comb\" lines with almost uniform intensity (intensity ratios typically <10). Self-referencing of the laser establishes a precise ruler in frequency space that can be stabilized to the 10e-18 uncertainty level, reaching absolute frequency inaccuracies at the 10e-12 level per day when using the Global Positioning System's (GPS) time signal as the reference. The exploration of the merits of this new technology holds the promise for broad-band, highly accurate and reproducible calibration required for reliable operation of current and next generation astronomic spectrometers. ", "machine_abstract": "We present the latest advances in optical frequency combs and their applications to precision metrology, including calibration techniques for high-resolution spectroscopy instruments such as Fourier transform spectrometers (FTS). We discuss how these techniques can be used to improve measurement accuracy by orders of magnitude over traditional methods.  In particular we describe two novel approaches that are being developed at NIST:  1) The use of an optical frequency comb locked to a high-accuracy atomic clock to calibrate FTS measurements with sub-Hz uncertainty. 2) A technique called \"self-calibration\" which allows one to measure absolute frequencies without requiring any external reference standards or other auxiliary equipment. These techniques have been demonstrated using both laboratory experiments and field tests. They represent important steps towards realizing the vision of a future where all spectroscopic measurements will be traceable back to fundamental physical constants. This is especially relevant today given the growing interest in developing portable, low-cost devices capable of performing accurate chemical analysis anywhere around the world.", "paraphrased_abstract": "It is also relevant today, with the growing interest in the field of portable devices for the analysis of chemical reactions. In this paper we will describe new technologies in optical frequency combs and discuss their application in the precision metrology, with particular emphasis on precision instruments for FTS, namely: 1) A method of calibration using an atomic clock to a higher accuracy, to calibrate the FTS at sub-Hz, b) a method of self-calibration, to measure absolute frequencies without the need of any external reference standards or other equipment. This method is particularly relevant now because of the rapid growth of scientific equipment and instrumentation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.094, "title": "The Intrinsically X-ray Weak Quasar PHL 1811. II. Optical and UV Spectra and Analysis", "abstract": "  This is the second of two papers reporting observations and analysis of the unusually bright (m_b=14.4), luminous (M_B=-25.5), nearby (z=0.192) narrow-line quasar PHL 1811. The first paper reported that PHL 1811 is intrinsically X-ray weak, and presented a spectral energy distribution (SED). Here we present HST STIS optical and UV spectra, and ground-based optical spectra. The optical and UV line emission is very unusual. There is no evidence for forbidden or semiforbidden lines. The near-UV spectrum is dominated by very strong FeII and FeIII, and unusual low-ionization lines such as NaID and CaII H&K are observed. High-ionization lines are very weak; CIV has an equivalent width of 6.6A, a factor of ~5 smaller than measured from quasar composite spectra. An unusual feature near 1200A can be deblended in terms of Ly\\alpha, NV, SiII, and CIII* using the blueshifted CIV profile as a template. Photoionization modeling shows that the unusual line emission can be explained qualitatively by the unusually soft SED. Principally, a low gas temperature results in inefficient emission of collisionally excited lines, including the semiforbidden lines generally used as density diagnostics. The emission resembles that of high-density gas; in both cases this is a consequence of inefficient cooling. PHL 1811 is very unusual, but we note that quasar surveys are generally biased against finding similar objects. ", "machine_abstract": "We present new optical and ultraviolet (UV) spectra for the intrinsically X-ray weak quasar PHL 1811, obtained with the Keck Observatory's HIRES spectrograph in 1998-99. The data cover wavelengths between 3200 A and 10400\u00c5 at resolution R = \u03bb/\u2206\u03bb \u2248 45000. We find that the continuum emission is well described by an accretion disk model with parameters similar to those found previously for other quasars. However, we detect no broad absorption lines or narrow absorption features associated with outflows. In addition, there are several unusual properties of the line profiles which suggest that this object may be different than most quasars studied so far.     Keywords: Quasars; Broad Absorption Lines; Accretion Disk Modeling. 1 Introduction     PHL 1811 was discovered as part of the Palomar-Green survey (Schmidt & Green 1983 ) and has been observed extensively since then. It is one of only two known examples of an X-ray weak quasar (Wilkes et al. 1994) , where the ratio of its soft X-ray flux density to its 2500 \u00c5 flux density is less than 0.1. Wilkes et al. (1994) suggested that it might have a high column density absorber along our line-of-sight, but subsequent observations failed to confirm this hypothesis (e.g., Mathur et al. 1995) . Instead, they concluded that the source must be intrinsically X-ray weak because of some unknown mechanism. Recent Chandra observations show that the spectrum below 2 keV can be fitted reasonably well using a power law plus Galactic absorption (Mathur et al. 2002 ) . This suggests that the intrinsic X-ray weakness could arise due to a steep spectral index rather than strong obscuration. Another possibility is that the X-rays are absorbed by ionized gas near the central black hole .     PHL 1811 also shows interesting variability on time scales ranging from hours to years. For example, Wilkes et al. (1995) reported rapid changes in both the hardness ratios and luminosity during their ASCA observation. They interpreted these variations as being caused by partial", "paraphrased_abstract": "The light was then collected and analysed at the Keck Observatory, in 1998, with the HIRES spectrograph of the Keck Observatory. In this study PHL 1811 was discovered as part of the Palomar-Green Survey (Schmidt and Green 1983). PHL 1811 was one of only two quasars to have a weak X-ray flux density, which is not more than 0.1; whereas Mathur and Sathur (2002) proposed a more plausible explanation: that the source was, it seems, an X-ray amplification by a steep spectral index, not a luminous flux, but a gas, absorbed in the central black hole. PHL 1811 demonstrates in the present work remarkable variation on time-scales, from hours to years. This is because of a partial entropy of the X-rays and its brightness, and by a slight decrease of the hardness and the luminosity. We consider that, however, the X-rays may have come from some unknown source, or from an incipient obscuration of the spectra. We propose that PHL 1811 be inherently X-ray weak, due to a peculiar, unknown mechanism, by means of which it is impossible to calculate its radius. PHL 1811 has recently been observed in the Keck Observatory", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0375, "title": "Rich methane premixed laminar flames doped by light unsaturated hydrocarbons - Part I : allene and propyne", "abstract": "  The structure of three laminar premixed rich flames has been investigated: a pure methane flame and two methane flames doped by allene and propyne, respectively. The gases of the three flames contain 20.9% (molar) of methane and 33.4% of oxygen, corresponding to an equivalence ratio of 1.25 for the pure methane flame. In both doped flames, 2.49% of C3H4 was added, corresponding to a ratio C3H4/CH4 of 12% and an equivalence ratio of 1.55. The three flames have been stabilized on a burner at a pressure of 6.7 kPa using argon as dilutant, with a gas velocity at the burner of 36 cm/s at 333 K. The concentration profiles of stable species were measured by gas chromatography after sampling with a quartz microprobe. Quantified species included carbon monoxide and dioxide, methane, oxygen, hydrogen, ethane, ethylene, acetylene, propyne, allene, propene, propane, 1,2-butadiene, 1,3-butadiene, 1-butene, isobutene, 1-butyne, vinylacetylene, and benzene. The temperature was measured using a PtRh (6%)-PtRh (30%) thermocouple settled inside the enclosure and ranged from 700 K close to the burner up to 1850 K. In order to model these new results, some improvements have been made to a mechanism previously developed in our laboratory for the reactions of C3-C4 unsaturated hydrocarbons. The main reaction pathways of consumption of allene and propyne and of formation of C6 aromatic species have been derived from flow rate analyses. ", "machine_abstract": "The effect of adding small amounts (0.5-2%) of the light unsaturated hydrocarbons, allene or propyne to rich methane/air mixtures is investigated experimentally in this study using counterflow diffusion flames at atmospheric pressure. The flame structure is examined with OH* chemiluminescence imaging and laser induced fluorescence measurements for CH2O and CH3O radicals. It was found that both additives have similar effects on the flame structure; they increase the flame temperature slightly but decrease significantly the maximum mole fraction of OH radical. This leads to an increased soot formation rate as evidenced by the higher soot volume fractions measured downstream of the flame front. In addition, it has been observed that the presence of these additives increases the flame speed.  These results are discussed in terms of chemical kinetic mechanisms involving reactions between hydrocarbon species and oxygen atoms. Finally, the influence of the additive concentration on the flame structure is analyzed.", "paraphrased_abstract": "This is explained by the chemical reactions between the hydrocarbons and oxygen atoms. The results of the experiments are compared with those of the chemical kinetics, which involve reactions between the hydrocarbons and oxygen atoms. The effect of the addition of small amounts (0.2-0.5%) of unsaturated hydrocarbons, allene, or propyne on the structure of the flames was investigated experimentally by counterflowing diffusion flames at atmospheric pressure. Among the additives, the flames were increased by an increase of about half a percent. The flame temperature was slightly increased, but there was a substantial decrease in the total mole fraction of OH radical. Besides, the increase in flame density was marked by the higher volume of the flame. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.169, "title": "Generalized Brjuno functions associated to $\\alpha$-continued fractions", "abstract": "  For \\alpha in the interval [0,1], we consider the one-parameter family of \\alpha-continued fraction maps, which include the Gauss map (\\alpha=1) and the nearest integer (\\alpha=1/2) and by-excess (\\alpha=0) continued fraction maps. To each of these expansions, and to each choice of a positive function u on the interval I_\\alpha=(0,max(\\alpha,1-\\alpha)) we associate a generalized Brjuno function B_(\\alpha,u)(x). For \\alpha=1/2 or \\alpha=1, and u(x)=-\\log(x), these functions were introduced by Yoccoz in his work on the linearization of holomorphic maps. Their regularity properties, including BMO regularity and their extension to the complex plane, have been thoroughly investigated. We compare the functions obtained with different values of \\alpha and we prove that the set of (\\alpha,u)-Brjuno numbers does not depend on the choice of \\alpha provided that \\alpha>0. We then consider the case \\alpha=0, u(x)=-\\log(x) and we prove that x is a Brjuno number (for \\alpha> 0) if and only if both x and -x are Brjuno numbers for \\alpha=0. ", "machine_abstract": "We introduce the notion of generalized Brjuno function and study its properties in connection with continued fractions. We show that if $(a_n)_n$ is an infinite sequence of positive integers such that for all $n$, $a_n+1>a_{n-1}+2$ then there exists a unique real number $x$ satisfying the following conditions: (i) $x\\in[0,1]$; (ii) the sequence $(a_nx^n)_n$ converges to $x$ as $n\\to\\infty$;  (iii) the sequence $(b_n)_n$ defined by $b_0=1$ and $b_{n+1}=\\prod\\limits_{i=0}^{n-1}\\frac1{a_i+1}-\\frac1{a_ix^{i+1}}$ satisfies the condition $\\displaystyle\\sum\\limits_{n\\geq 0}\\frac{|b_n|}{n^\\alpha}<\\infty$ for some $\\alpha>0$.  The main result of this article shows that any real number $x$ which can be written as above has a finite irrationality measure.", "paraphrased_abstract": "The main result of this study is that if the series of positive integers - a_n+1>a_n+1+2 - is infinite, then a unique real number - x is an irrational measure. Then, in the form of a graph, the irrational measure - b_n=0; b_n+1=prodlimits_, b_n=Prodlimits_, b_n=prodlimits_, b_n=alpha-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-infra-in-fra-in-fra", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1515, "title": "High Precision CTE-Measurement of SiC-100 for Cryogenic Space-Telescopes", "abstract": "  We present the results of high precision measurements of the thermal expansion of the sintered SiC, SiC-100, intended for use in cryogenic space-telescopes, in which minimization of thermal deformation of the mirror is critical and precise information of the thermal expansion is needed for the telescope design. The temperature range of the measurements extends from room temperature down to $\\sim$ 10 K. Three samples, #1, #2, and #3 were manufactured from blocks of SiC produced in different lots. The thermal expansion of the samples was measured with a cryogenic dilatometer, consisting of a laser interferometer, a cryostat, and a mechanical cooler. The typical thermal expansion curve is presented using the 8th order polynomial of the temperature. For the three samples, the coefficients of thermal expansion (CTE), $\\bar{\\alpha}_{#1}$, $\\bar{\\alpha}_{#2}$, and $\\bar{\\alpha}_{#3}$ were derived for temperatures between 293 K and 10 K. The average and the dispersion (1 $\\sigma$ rms) of these three CTEs are 0.816 and 0.002 ($\\times 10^{-6}$/K), respectively. No significant difference was detected in the CTE of the three samples from the different lots. Neither inhomogeneity nor anisotropy of the CTE was observed. Based on the obtained CTE dispersion, we performed an finite-element-method (FEM) analysis of the thermal deformation of a 3.5 m diameter cryogenic mirror made of six SiC-100 segments. It was shown that the present CTE measurement has a sufficient accuracy well enough for the design of the 3.5 m cryogenic infrared telescope mission, the Space Infrared telescope for Cosmology and Astrophysics (SPICA). ", "machine_abstract": "The development of large space telescopes requires the use of lightweight materials to reduce launch costs and increase telescope performance in orbit. Silicon Carbide (SiC) is an excellent candidate material due to its high strength, low density, and radiation hardness at cryogenic temperatures. However, it has been shown that SiC exhibits significant changes in thermal expansion with temperature which can lead to dimensional instability during cool-downs or warm-ups. This work presents results on the measurement of the coefficient of thermal expansion (CTE) of SiC-100 over a wide range of temperatures using a novel technique based on laser interferometry. The measured values are compared against literature data as well as theoretical predictions obtained by ab initio calculations. It was found that the experimental measurements agree very well with theory within the uncertainty limits. These results will be used to improve the design of future space missions such as JWST and WFIRST-AFTA.", "paraphrased_abstract": "It is shown that the CTE of SiC reaches a level of ten thousand ten thousand times the level of the literature and the predictions of ab initio calculations. In this paper, the effect is to compare the measured values with the theoretical predictions. This result, therefore, will be of use in future space missions, namely, JWST, WFIRST-AFTA. The development of large-scale telescopes requires the use of lightweight materials to reduce the cost of the launch, and increase the performance of the telescope in orbit. Silicon Carbide is a suitable material because of its low density and low density, and its hardness at cryogenic temperatures. However, the thermal expansion of this material increases with the temperature, which can cause it to be unstable at cool-down or at temperature rise. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0806, "title": "The Sloan Digital Sky Survey Quasar Catalog IV. Fifth Data Release", "abstract": "  We present the fourth edition of the Sloan Digital Sky Survey (SDSS) Quasar Catalog. The catalog contains 77,429 objects; this is an increase of over 30,000 entries since the previous edition. The catalog consists of the objects in the SDSS Fifth Data Release that have luminosities larger than M_i = -22.0 (in a cosmology with H_0 = 70 km/s/Mpc, Omega_M = 0.3, and Omega_Lambda = 0.7) have at least one emission line with FWHM larger than 1000 km/s, or have interesting/complex absorption features, are fainter than i=15.0, and have highly reliable redshifts. The area covered by the catalog is 5740 sq. deg. The quasar redshifts range from 0.08 to 5.41, with a median value of 1.48; the catalog includes 891 quasars at redshifts greater than four, of which 36 are at redshifts greater than five. Approximately half of the catalog quasars have i < 19; nearly all have i < 21. For each object the catalog presents positions accurate to better than 0.2 arcsec. rms per coordinate, five-band (ugriz) CCD-based photometry with typical accuracy of 0.03 mag, and information on the morphology and selection method. The catalog also contains basic radio, near-infrared, and X-ray emission properties of the quasars, when available, from other large-area surveys. The calibrated digital spectra cover the wavelength region 3800--9200A at a spectral resolution of ~2000. The spectra can be retrieved from the public database using the information provided in the catalog. The average SDSS colors of quasars as a function of redshift, derived from the catalog entries, are presented in tabular form. Approximately 96% of the objects in the catalog were discovered by the SDSS. ", "machine_abstract": "The Sloan Digital Sky Survey (SDSS) is an ongoing project to map the distribution and motion of galaxies, quasars, stars, and other celestial objects in space. The fourth data release was made public on September 30th 2003. This fifth data release contains more than 100,000 new quasar candidates selected by color criteria from the SDSS imaging survey. These are supplemented with about 20,000 previously known quasars that were not included in previous releases because they did not meet the selection criteria for inclusion at that time.     In addition to these newly discovered quasars, this catalog also includes all quasars found during the first four years of the survey as well as those found since then but which have yet to be released publicly. A total of over 250,000 quasars are now available through this catalog. All of them have been spectroscopically confirmed using observations obtained with the dedicated 2.5-meter telescope located at Apache Point Observatory near Sacramento, California.", "paraphrased_abstract": "This data set is made up of more than 250,000 quasars, which have been discovered since the first four years of the survey, as well as those which have been discovered since the first four years, but which have not yet been released. This fifth survey is of a new type, of a color, of more than 100,000 new quasars, selected from the survey by color, and which are augmented by some 20,000 previously known quasars which have not yet been confirmed. These new quasars, along with the others, are also announced. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3713, "title": "The Magnetar Nature and the Outburst Mechanism of a Transient Anomalous X-ray Pulsar", "abstract": "  Anomalous X-ray Pulsars (AXPs) belong to a class of neutron stars believed to harbor the strongest magnetic fields in the universe, as indicated by their energetic bursts and their rapid spindowns. However, an unambiguous measurement of their surface field strengths has not been made to date. It is also not known whether AXP outbursts result from changes in the neutron star magnetic field or crust properties. Here we report a spectroscopic measurement of the surface magnetic field strength of an AXP, XTE J1810$-$197, and solidify its magnetar nature. The field strength obtained from detailed spectral analysis and modeling, B = (2.72$\\pm$0.03)$\\times10^{14}$ G, is remarkably close to the value inferred from the rate of spindown of this source and remains nearly constant during numerous observations spanning over an order of magnitude in source flux. The surface temperature, on the other hand, declines steadily and dramatically following the 2003 outburst of this source. Our findings demonstrate that heating occurs in the upper neutron star crust during an outburst and sheds light on the transient behaviour of AXPs. ", "machine_abstract": "We report on our analysis of the outburst mechanism of SGR 1806-20, which is one of three known magnetars (neutron stars with superstrong magnetic fields). We have analyzed all available data obtained by Swift/BAT during its first year in orbit to search for periodicities associated with this source. The BAT light curve shows that there are two major flares lasting about 100 days each; these flares were also observed simultaneously at other wavelengths. In addition we find evidence for several smaller bursts occurring between the main flares. Using an improved version of the method developed by Israel et al. (2008) we searched for pulsations in the BAT data corresponding to the time intervals when the source was active. No significant signal could be found above background noise levels down to a limit of 1 mCrab (3\u03c3), but we did detect a weak signal below 0.5 mCrab. This signal has been confirmed using independent methods applied to different energy bands.", "paraphrased_abstract": "The pulsation rate of the source was only a minimum of a few microns (as a rule), but the signal was weaker than a micron, and therefore confirmed by several methods. This result was confirmed by several other methods of analyzing the data of Swift/BAT, in particular by searching for the periodicity of the pulsation from the BAT. We have analysed all available data collected by Swift in the first year of its orbit, in order to identify periodic periods corresponding to its presence. In the light curve of the BAT light, there are two major pulsations lasting a hundred days each, while simultaneously a few smaller pulsations occurred in other wavelengths. We describe the pulsation of SGR 1806-20, which is one of the three known magnetars (neutron stars with super-strong magnetic fields) of the three radiances. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0661, "title": "SQUID-based instrumentation for ultra-low-field MRI", "abstract": "  Magnetic resonance imaging at ultra-low fields (ULF MRI) is a promising new imaging method that uses SQUID sensors to measure the spatially encoded precession of pre-polarized nuclear spin populations at a microtesla-range measurement field. In this work, a seven-channel SQUID system designed for simultaneous 3D ULF MRI and magnetoencephalography (MEG) is described. The system includes seven second-order SQUID gradiometers, characterized by magnetic field resolutions of 1.2 - 2.8 fT/rtHz. It is also equipped with five sets of coils for 3D Fourier imaging with pre-polarization. Essential technical details of the design are discussed. The system's ULF MRI performance is demonstrated by multi-channel 3D images of a preserved sheep brain acquired at 46 microtesla measurement field with pre-polarization at 40 mT. The imaging resolution is 2.5 mm x 2.5 mm x 5 mm. The ULF MRI images are compared to images of the same brain acquired using conventional high-field MRI. Different ways to improve imaging SNR are discussed. ", "machine_abstract": "The development and application of superconducting quantum interference devices (SQUIDs) in magnetic resonance imaging has been reviewed, with emphasis on the recent advances made by our group at UCLA. The SQUID is an extremely sensitive magnetometer that can be used to detect very small changes in the local magnetic field produced by nuclear spins during NMR experiments. In this review we describe how these devices have been incorporated into several different types of low-field MRI systems including micro-SQUID arrays fabricated using standard semiconductor processing techniques as well as macroscopic SQUID gradiometers developed specifically for use in human brain studies. We also discuss some of the challenges associated with operating such instruments under conditions where the ambient magnetic fields are not completely shielded or controlled. Finally, we present results obtained recently using one of these new technologies to study the effects of anesthesia on functional connectivity within the human brain. Magnetic Resonance Imaging (MRI), which was first demonstrated more than 30 years ago, provides detailed information about the structure and function of living tissues through noninvasive measurements of their water content [1] . This technique relies upon the detection of tiny radio-frequency signals emitted by hydrogen nuclei when they precess around the main magnetic field B 0 , which is generated by a large electromagnet [2] . In conventional clinical scanners, the strength of B 0 ranges between 1 T and 3 T [3] . However, there are many applications where it would be desirable to perform MRI scans at much lower fields [4] . For example, the use of high-field magnets may cause significant heating problems due to dielectric losses in biological tissue [5] . Moreover, the cost of building and maintaining such large-scale facilities makes them inaccessible to most research laboratories [6] . Another disadvantage of working at higher fields is related to the fact that the relaxation times of protons become shorter [7, 8] . As a result, the signal-to-noise ratio decreases rapidly [9] making it difficult to obtain high-quality images [10] .", "paraphrased_abstract": "The MRI is used to determine the structure and function of living tissues through noninvasive measurements of water content. The magnetic resonance spectrometer has been used to investigate the nature and composition of living tissues by detecting the small resonant signals from hydrogen nuclei when they swell around the primary magnetic field B0, which is produced by a large electromagnet, and, if the spectra are sufficiently high, it is difficult to obtain sufficient data. Moreover, the magnetic fields in such a spectrometer are much lower than those at which the scientific laboratories are usually located. We have reviewed the latest developments in the field of superconducting quantum interference in magnetic resonance imaging, especially the recent advances made by our group at UCLA. We have also outlined the advantages and disadvantages of using high-frequency magnets in the spectrometer, such as micro-SQUIDs, fabricated from conventional semiconductor processing, and the micro-SQUIDs, specially developed for human brain spectrometers, are presented. However, there are many other applications that would be suited to using high-frequency magnets. The heating of protons decreases with the passage of time, and consequently the signal to noise ratio becomes weaker, and it is difficult to obtain satisfactory images. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.384, "title": "Produit d'entrelacement et action triangulaire d'alg\\`ebres de Lie", "abstract": "  Formal actions of Lie algebras over vector spaces are introduced in a purely algebraic way, as a mimic of infinitesimal operations of Banach Lie algebras over Banach analytic manifolds. In analogy with the case of abstract groups, complete wreath products and triangular actions are then defined for Lie algebras acting \"en cascade\" over vector spaces. Finally, a Kaloujnine-Krasner type theorem for Lie algebra extensions is proved.   -----   En mimant les lois d'op\\'erations infinit\\'esimales des alg\\`ebres de Lie sur les vari\\'et\\'e s analytiques banachiques, on introduit de mani\\`ere purement alg\\`ebrique la notion d'action formelle d'une alg\\`ebre de Lie sur un espace vectoriel. Ensuite, par analogie avec le cas des groupes abstraits, et en faisant op\\'erer les alg\\`ebres de Lie \"en cascade\", on d\\'efinit produit d'entrelacement (\"wreath product\") et action triangulaire pour les alg\\`ebres de Lie. On d\\'emontre enfin un th\\'eor\\`eme du type Kaloujnine-Krasner pour les extensions d'alg\\`ebres de Lie. ", "machine_abstract": "Nous donnons une nouvelle preuve du th\\'eor\\eme suivant, qui est un cas particulier d'un r\\'esultat plus g\\'en\\'eral obtenu par J. Bernstein (cf. [1]): Soit G une alg\\ebre de Lie nilpotente d'ordre n{\\displaystyle n} sur K{\\displaystyle \\mathbb {K} } . _SPLIT_ Alors il existe des polynomes Pk(X), k=0,...,n-1 tels que pour tout x\u2208G on ait\u00a0:  xn=x0P1+x1P2++xn\u22121Pn\u22121{\\displaystyle x^{n}=x_{0}P_{1}+x_{1}P_{2}+\\cdots +x_{n-1}P_{n-1}} o{\\displaystyle o} u les Pi{\\displaystyle {\\text{Pi}}} sont des polynomes homog\u00e8nes en X{\\displaystyle x} , degXi=Pk(i). Nous montrons aussi comment ce r\\'esultat peut \u00eatre utilis\u00e9 dans la construction d'une base de Malcev d'une alg\u00e8bre de Lie nilpotente quelconque.", "paraphrased_abstract": "... \u201d [5], [6] [7] [8] [10]] et [9], et [10], \u201c11] [11], [12], [14] [13] [14]..., [14] [14], [16].... Nous montrons aussi comment, qui est un cas particulier d'un rsultat plus general obtenu by J. Bernstein, et et te l'\u00e9sultat plus general et te rsultat ne d'ordre ne ne sur Ksultat o tu te te u les PitextPi sont des polynomies homog\u00e8nes en Xdisplaystyle o tu u les Pidisplaystyle o u les Pidisplaystyle o u les Pidisplaystyle o u les Pidisplaystyle o u les Pidisplaystyle o u les Pidisplaystyle o tu te u les Pidisplaystyle o u les Pi", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2741, "title": "Near-infrared polarimetric study of the bipolar nebula IRAS 19312+1950", "abstract": "  We obtained H-band polarimetric data of IRAS 19312+1950 using the near-infrared camera (CIAO) on the 8 m Subaru telescope. In order to investigate the physical properties of the central star and the nebula, we performed dust radiative transfer modeling and compared the model results with the observed spectral energy distributions (SEDs), the radial profiles of the total intensity image, and the fraction of linear polarization map. The total intensity image shows a nearly spherical core with ~3\" radius, an S-shaped arm extending ~10\" in the northwest to southeast direction, and an extended lobe towards the southwest. The polarization map shows a centro-symmetric vector alignment in almost the entire nebula and low polarizations along the S-shaped arm. These results suggest that the nebula is accompanied by a central star, and the S-shaped arm has a physically ring-like structure. From our radiative transfer modeling, we estimated the stellar temperature, the bolometric luminosity, and the current mass-loss rate to be 2800 K, 7000 L_sun, and 5.3x10^{-6} M_sun yr^{-1}, respectively. Taking into account previous observational results, such as the detection of SiO maser emissions and silicate absorption feature in the 10 $\\mu$m spectrum, our dust radiative transfer analysis based on our NIR imaging polarimetry suggests that (1) the central star of IRAS 19312+1950 is likely to be an oxygen-rich, dust-enshrouded AGB star and (2) most of the circumstellar material originates from other sources (e.g. ambient dark clouds) rather than as a result of mass loss from the central star. ", "machine_abstract": "We present near-infrared (NIR) polarimetry and spectroscopy for the bipolar reflection nebula IRAS 19312; 1950 . The NIR polarization vectors are aligned with those in optical images, indicating that they trace scattered light from an illuminating source located behind the dense molecular cloud core. We find evidence for two distinct scattering regions along our line-of-sight to this object; one is associated with the brightest part of the nebula, while another region shows lower polarization degrees but higher polarized fluxes at longer wavelengths. These results suggest that there may be multiple sources contributing to the observed emission. In addition, we detect several absorption features which can be attributed to water ice on dust grains near the central star(s). Our observations also reveal a possible outflow cavity traced by blueshifted CO emission lines. This work was supported by NASA through grant number HST-GO-11775.0-A from STScI, operated under NASA contract NAS5-26555.", "paraphrased_abstract": "This work was supported by NASA under the project HST-GO-11775.0 from STScI, under contract NAS-5-26555. The NIR polarization was determined from the optical spectra of the object, and the two lines of polarization were compared. One showed the brightest parts of the nebula, and the other showed the lower polarization, but higher polarization at the longer wavelengths. There are also two diffraction regions in our line of sight, one of which corresponds to the brightest part of the nebula, while another region is of lower polarization but has higher fluxes at longer wavelengths. Moreover, there is an absorption cavity in which CO emission lines have been traced. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.327, "title": "Hadronization in semi-inclusive deep-inelastic scattering on nuclei", "abstract": "  A series of semi-inclusive deep-inelastic scattering measurements on deuterium, helium, neon, krypton, and xenon targets has been performed in order to study hadronization. The data were collected with the HERMES detector at the DESY laboratory using a 27.6 GeV positron or electron beam. Hadron multiplicities on nucleus A relative to those on the deuteron, R_A^h, are presented for various hadrons (\\pi^+, \\pi^-, \\pi^0, K^+, K^-, p, and \\bar{p}) as a function of the virtual-photon energy \\nu, the fraction z of this energy transferred to the hadron, the photon virtuality Q^2, and the hadron transverse momentum squared p_t^2. The data reveal a systematic decrease of R_A^h with the mass number A for each hadron type h. Furthermore, R_A^h increases (decreases) with increasing values of \\nu (z), increases slightly with increasing Q^2, and is almost independent of p_t^2, except at large values of p_t^2. For pions two-dimensional distributions also are presented. These indicate that the dependences of R_A^{\\pi} on \\nu and z can largely be described as a dependence on a single variable L_c, which is a combination of \\nu and z. The dependence on L_c suggests in which kinematic conditions partonic and hadronic mechanisms may be dominant. The behaviour of R_A^{\\pi} at large p_t^2 constitutes tentative evidence for a partonic energy-loss mechanism. The A-dependence of R_A^h is investigated as a function of \\nu, z, and of L_c. It approximately follows an A^{\\alpha} form with \\alpha \\approx 0.5 - 0.6. ", "machine_abstract": "We present the results for hadron production in semiinclusive DIS off nuclei at large Bjorken x and low Q^2, obtained with the HERMES experiment using data taken between 1997 and 2002. The analysis is performed within the framework of collinear factorisation and the modified perturbative approach to nuclear shadowing developed by Frankfurt et al.. We find that the observed suppression of leading neutron production relative to deuterium can be explained by nuclear effects alone without invoking any additional mechanism such as intrinsic charm or gluon saturation. In addition we observe an enhancement of strange particle production which cannot be described by conventional partonic models but may be attributed to the presence of intrinsic strangeness in the proton wave function.     1 Introduction     Semi-inclusive deep-inelastic lepton-nucleus scattering (SIDIS) has been studied extensively over many years both experimentally [1]-[6] and theoretically [7][8][9] . This process provides information about the quark structure of the target nucleus through measurements of final state particles produced in association with the scattered lepton. At high values of Bjorken-x, where the struck quarks are highly virtual, SIDIS probes the transition region between the non-perturbative regime governed by confinement physics and the perturbative domain dominated by short-distance interactions [10] .  In this kinematic range it becomes possible to study the properties of bound-state systems directly via their interaction with hard probe photons [11] , thereby providing insight into the dynamics underlying the formation of composite states [12] - [14] . Theoretical studies have shown that the cross section for SIDIS depends strongly on the transverse momentum k_T of the outgoing hadrons [15] - [17] . It was found that the dependence of the cross sections on k_T could be used to discriminate among different theoretical approaches [18] - [20] . For example, calculations based on the standard DGLAP formalism [21] predict a strong increase of the cross section with increasing k_T [22] while those employing the CCFM evolution equations [23] lead to much weaker dependences [24] .     2 Experimentally measured quantities", "paraphrased_abstract": "A partial-incomplete scattering of the nucleus in semi-incomplete scattering (SIDIS) has been studied for many years, both experimentally and theoretically, and has been studied, at very high values, at high velocity points of the nucleus, at a rate of ten times higher than the normal kT. This study gives an indication of the structure of the target nucleus, which is, by the observations of final particles, the formation of the composite state. In this way, the study of the cross section of SIDIS depends strongly on the transverse momentum kT of the incoming hadrons, whereas the CCFM equations, which are more complicated, show a much weaker dependence... Then, it was shown that the cross section on kT was a function of the kT of the target hadrons, as well as that of the CCFM evolution equation, which was very weak. In this study, the semi-inclusive nature of the DIS and the corresponding swarming physics have been extensively studied in the last several years, both experimentally and theoretically, in the twofold kinematic range of the induced particles, as well as the characteristics of the entangled state of the particles, has been obtained, in the light of the reflected hadrons, and reveals that, in the latter respect,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2923, "title": "The Magnetic Fields of Classical T Tauri Stars", "abstract": "  We report new magnetic field measurements for 14 classical T Tauri stars (CTTSs). We combine these data with one previous field determination in order to compare our observed field strengths with the field strengths predicted by magnetospheric accretion models. We use literature data on the stellar mass, radius, rotation period, and disk accretion rate to predict the field strength that should be present on each of our stars according to these magnetospheric accretion models. We show that our measured field values do not correlate with the field strengths predicted by simple magnetospheric accretion theory. We also use our field strength measurements and literature X-ray luminosity data to test a recent relationship expressing X-ray luminosity as a function of surface magnetic flux derived from various solar feature and main sequence star measurements. We find that the T Tauri stars we have observed have weaker than expected X-ray emission by over an order of magnitude on average using this relationship. We suggest the cause for this is actually a result of the very strong fields on these stars which decreases the efficiency with which gas motions in the photosphere can tangle magnetic flux tubes in the corona. ", "machine_abstract": "We present the first detection of magnetic fields in classical T Tauri stars (CTTSs) using spectropolarimetric observations obtained with ESPaDOnS at CFHT and Narval at TBL, as well as archival data for some targets. We detect Zeeman signatures in Stokes V profiles of H$\\alpha$ lines in all observed CTTSs except TW Hya. The longitudinal field strengths range between -100 G to +300 G. In addition, we find that the mean unsigned circular polarization is higher than 0.1% in most cases. Our results show that the magnetic properties are similar among different types of CTTSs. However, there seems to be no correlation between the strength of the magnetic field and accretion rate or mass loss rates. This suggests that the origin of the magnetic field may not be related to disk accretion processes.  These findings will help us understand how stellar magnetic fields evolve during pre-main-sequence phase.", "paraphrased_abstract": "The spectroscopic observations of the ESPaDOnS instrument are compared with the ESPaDOnS instrument at the CFHT and Narval at the TBL, as well as with archived measurements. The magnetic properties of the swarming stars are similar. Generally, the magnetic properties of the swarming stars are similar, but they have no relationship with the mass of the disk, and therefore, they do not correspond. In general, the magnetic properties are similar in all swarming stars, except for TW Hya. The axis of the magnetic field, which is 100 G, is greater than -300 G, and the polarization is at least 0.1%, in most cases. Our findings will give an insight into the origin of the magnetic field in the central phase. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1615, "title": "Dynamical Coupled-Channel Model of $\\pi N$ Scattering in the W $\\leq$ 2 GeV Nucleon Resonance Region", "abstract": "  As a first step to analyze the electromagnetic meson production reactions in the nucleon resonance region, the parameters of the hadronic interactions of a dynamical coupled-channel model, developed in {\\it Physics Reports 439, 193 (2007)}, are determined by fitting the $\\pi N$ scattering data. The channels included in the calculations are $\\pi N$, $\\eta N$ and $\\pi\\pi N$ which has $\\pi\\Delta$, $\\rho N$, and $\\sigma N$ resonant components. The non-resonant meson-baryon interactions of the model are derived from a set of Lagrangians by using a unitary transformation method. One or two bare excited nucleon states in each of $S$, $P$, $D$, and $F$ partial waves are included to generate the resonant amplitudes in the fits. The parameters of the model are first determined by fitting as much as possible the empirical $\\pi N$ elastic scattering amplitudes of SAID up to 2 GeV. We then refine and confirm the resulting parameters by directly comparing the predicted differential cross section and target polarization asymmetry with the original data of the elastic $\\pi^{\\pm} p \\to \\pi^{\\pm} p$ and charge-exchange $\\pi^- p \\to \\pi^0 n$ processes. The predicted total cross sections of $\\pi N$ reactions and $\\pi N\\to \\eta N$ reactions are also in good agreement with the data. Applications of the constructed model in analyzing the electromagnetic meson production data as well as the future developments are discussed. ", "machine_abstract": "We present results for pion-nucleon scattering amplitudes obtained with an improved coupled-channel model that includes all known nucleon resonances up to $2.0 \\, \\mathrm{GeV}$ and is constrained by elastic-scattering data below $1.8 \\, \\mathrm{GeVa}_0$ . The model incorporates explicit contributions from the Delta resonance as well as from the S11(1535) and D13(1700) states which are generated dynamically through their coupling to the pion-nucleon continuum. We find good agreement between our predictions and experimental data on total cross sections, differential cross sections, polarization observables, and invariant-mass distributions at energies above $500 \\, \\mathrm{MeVa}_0$ . The resulting partial-wave amplitudes can be used to calculate nuclear reaction rates relevant for astrophysics applications such as the r-process or neutrino-induced reactions.    PACS numbers: 13.60.-r; 14.20.Gk; 12.39.Fe", "paraphrased_abstract": "PACS numbers 13.60.-r; 14.20.Gk; 12.39.Fe; 12.39.fe. We present the results of the partial-wave amplitudes obtained from the improved coupled-channel model. The model, which incorporates all known nucleon resonances up to 2.0 mathrmGeV, is constrained by elastic scattering at a scale of mathrmGeV, is restricted by elastics in the order of mathrmGeV0 , the model consists of explicit contributions from the Delta resonance and the D1315 resonance, which are dynamically generated through the pion-nucleon continuum. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1987, "title": "Pure inductive limit state and Kolmogorov's property", "abstract": "  Let $(\\clb,\\lambda_t,\\psi)$ be a $C^*$-dynamical system where $(\\lambda_t: t \\in \\IT_+)$ be a semigroup of injective endomorphism and $\\psi$ be an $(\\lambda_t)$ invariant state on the $C^*$ subalgebra $\\clb$ and $\\IT_+$ is either non-negative integers or real numbers. The central aim of this exposition is to find a useful criteria for the inductive limit state $\\clb \\raro^{\\lambda_t} \\clb$ canonically associated with $\\psi$ to be pure. We achieve this by exploring the minimal weak forward and backward Markov processes associated with the Markov semigroup on the corner von-Neumann algebra of the support projection of the state $\\psi$ to prove that Kolmogorov's property [Mo2] of the Markov semigroup is a sufficient condition for the inductive state to be pure. As an application of this criteria we find a sufficient condition for a translation invariant factor state on a one dimensional quantum spin chain to be pure. This criteria in a sense complements criteria obtained in [BJKW,Mo2] as we could go beyond lattice symmetric states. ", "machine_abstract": "The pure inductive limit state is introduced as the unique solution to an infinite system of linear equations with coefficients in the Banach algebra of bounded operators on Hilbert space, which are generated by a sequence of self-adjoint operators satisfying certain conditions.  The existence theorem for this state is proved under some mild assumptions. It is shown that if the spectrum of each operator in the sequence lies in a fixed compact set then the pure inductive limit state has all properties of Kolmogorov's state except positivity. In particular it satisfies the following relations:  $|\\langle \\phi | \\hat \\rho \\rangle| \\leq 1$; $\\langle \\phi | \\hat \\rho\\rangle = 0$ iff $\\phi$ belongs to the kernel of every operator in the sequence; $|\\langle \\phi_1 + \\phi_2 | \\hat \\rho \\rangle - \\langle \\phi_1 | \\hat \\rho \\rangle \\langle \\phi_2 | \\hat\\rho \\rangle| \\leq C(\\phi_1)C(\\phi_2)$ where $C(\\phi)$ depends only on the norm of $\\phi$.", "paraphrased_abstract": "A pure inductive state is introduced as the unique solution to an infinite system of linear equations with coefficients in the Banach algebra of bounded operators in Hilbert space. This is constituted by a group of self-adjoint operators expressing certain conditions. It is shown that the spectrum of all the operators of the sequence is composed of a compact set., where the phi12 hatrhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorhorho", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.178, "title": "HS1857+5144: A hot and young pre-cataclysmic variable", "abstract": "  We report the discovery of a new white dwarf/M dwarf binary, HS1857+5144, identified in the Hamburg Quasar Survey (HQS). Time-resolved optical spectroscopy and photometry were carried out to determine the properties of this new cataclysmic variable progenitor pre-CV). The light curves of HS1857+5144 display a sinusoidal variation with a period of Porb=383.52 min and peak-to-peak amplitudes of 0.7 mag and 1.1 mag in the B-band and R-band, respectively. The large amplitude of the brightness variation results from a reflection effect on the heated inner hemisphere of the companion star, suggesting a very high temperature of the white dwarf. Our radial velocity study confirms the photometric period as the orbital period of the system. A model atmosphere fit to the spectrum of the white dwarf obtained at minimum light provides limits to its mass and temperature of Mwd=~0.6-1.0 Msun and Twd=~70000-100000 K, respectively. The detection of HeII 4686 absorption classifies the primary star of HS1857+5144 as a DAO white dwarf. Combining the results from our spectroscopy and photometry, we estimate the mass of the companion star and the binary inclination to be Msec=~0.15-0.30 Msun and i=~45-55 deg, respectively. We classify HS1857+5144 as one of the youngest pre-CV known to date. The cooling age of the white dwarf suggests that the present system has just emerged from a common envelope phase ~10^5 yr ago. HS1857+5144 will start mass transfer within or below the 2-3h period gap. ", "machine_abstract": "We report the discovery of an eclipsing binary system HS1857+5144, which is composed of two white dwarfs with masses 0.6 M\u2299and 0.8M\u2299respectively. The orbital period of this system is 1.5 hours. We find that the mass ratio q = M2/M1=0.84 \u00b10.04 for this system by using the method of spectral disentangling. This value indicates that it may be in the stage before the final merger into one single degenerate star. In addition to its short orbital period, we also found that the temperature difference between these two components is very large (\u223c10000K). These results suggest that this system has just evolved out of common envelope phase.     Keywords: Hot subdwarf B stars; Eclipsing; Pre-Cataclysmic Variables; White Dwarfs; Spectral Disentangling", "paraphrased_abstract": "The hot subdwarf B stars, the hot subdwarf B stars, the hot subdwarf B stars, the hot subdwarf B stars, the hot subdwarf B stars, the hot subdwarf B stars, the hot subdwarf B stars, the hot subdwarf B stars, the hot subdwarf B stars, the hot subdwarf B stars, the spectral disentangling, the heat disentangling, the temperature difference, and the temperature difference, all these shows that this system has been created from common phase. It is found in a binary system HS1857+5144, composed of two white dwarfs with the masses of 0.6 M and 0.8 M, respectively. This system has a short orbit. We calculate the mass of this system by using the spectral disentangling. We find that the ratio of M2/M1 is 0.140.04, 0.04; this is the taint of the star, which is to be formed before the last molten synthesis of the two starlets. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0419, "title": "Critical Behaviour of Irreversible Reaction Systems", "abstract": "  An introductory review on the critical behaviour of some irreversible reaction systems is given. The study of these systems has attracted great attention during the last decades due to, on the one hand, the rich and complex underlying physics, and on the other hand, their relevance for numerous technological applications in heterogeneous catalysis, corrosion and coating, development of microelectronic devices, etc. The review is focuses on recent advances in the understanding of irreversible phase transitions (IPT's) providing a survey of the theoretical development of the field during the last decade, as well as a detailed discussion of relevant numerical simulations. The Langevin formulation for the treatment of second-order IPT's is discussed. Different Monte Carlo approaches are also presented in detail and the finite-size scaling analysis of second-order IPT's is described. Special attention is devoted to the description of recent progress in the study of first-order IPT's observed upon catalytic oxidation of carbon monoxide and the reduction of nitrogen monoxide, using lattice gas reaction models. Only brief comments are given on other reactions such as the oxidation of hydrogen, ammonia synthesis, etc. Also, a discussion of relevant experiments is presented and measurement are compared with the numerical results. Furthermore, promising areas for further research and open questions are also addressed. ", "machine_abstract": "We study the critical behaviour of irreversible reaction systems with mass-action kinetics in one dimension, using Monte Carlo simulations and mean-field theory. We find that for large system sizes there is no phase transition at all; instead we observe an abrupt change between two different dynamical regimes as a function of temperature T . For low temperatures (T < Tc) the dynamics are dominated by fluctuations which lead to slow relaxation times towards equilibrium. In contrast, for high temperatures (T > Tc), the dynamics become much faster since the system relaxes quickly into metastable states. The crossover temperature Tc depends on the number N of particles present in the system and increases logarithmically with N . The results presented here can be understood within the framework of the recently developed concept of \"active matter\". Active matter consists of self-propelled units such as living cells or artificial microswimmers. It has been shown that active matter exhibits similar properties as conventional condensed matter close to its critical point.", "paraphrased_abstract": "As soon as the critical point is reached, the crystalline state is reached. Then the critical phase of a reaction occurs, which in the present work is represented by the term active. The term active refers to a type of fluid containing the life of its components, the living cells or the artificial microswimmers. We find that the critical phase of the reaction is not at all interrupted; instead, we find that it is gradually altered, in the case of a large system, between two different phases. For the small system, the phase transition is only temporary, and the equilibrium is gradually restored, while for the large system, the change in phase is abrupt, because the refractory process has already started. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1422, "title": "A new, very massive modular Liquid Argon Imaging Chamber to detect low energy off-axis neutrinos from the CNGS beam. (Project MODULAr)", "abstract": "  The paper is considering an opportunity for the CERN/GranSasso (CNGS) neutrino complex, concurrent time-wise with T2K and NOvA, to search for theta_13 oscillations and CP violation. Compared with large water Cherenkov (T2K) and fine grained scintillators (NOvA), the LAr-TPC offers a higher detection efficiency and a lower backgrounds, since virtually all channels may be unambiguously recognized. The present proposal, called MODULAr, describes a 20 kt fiducial volume LAr-TPC, following very closely the technology developed for the ICARUS-T60o, and is focused on the following activities, for which we seek an extended international collaboration:   (1) the neutrino beam from the CERN 400 GeV proton beam and an optimised horn focussing, eventually with an increased intensity in the framework of the LHC accelerator improvement program;   (2) A new experimental area LNGS-B, of at least 50000 m3 at 10 km off-axis from the main Laboratory, eventually upgradable to larger sizes. A location is under consideration at about 1.2 km equivalent water depth;   (3) A new LAr Imaging detector of at least 20 kt fiducial mass. Such an increase in the volume over the current ICARUS T600 needs to be carefully considered. It is concluded that a very large mass is best realised with a set of many identical, independent units, each of 5 kt, \"cloning\" the technology of the T600. Further phases may foresee extensions of MODULAr to meet future physics goals.   The experiment might reasonably be operational in about 4/5 years, provided a new hall is excavated in the vicinity of the Gran Sasso Laboratory and adequate funding and participation are made available. ", "machine_abstract": "The Neutrino Factory and Muon Collider Collaboration (NFMCC), in collaboration with other European laboratories is proposing an innovative concept for a large liquid argon imaging detector that will be used as part of the future Neutrino Factory or Muon Collider experiments at CERN. The proposed project aims to build a very massive modular LArTPC using state-of-the-art technology. This would allow us to exploit the unique features offered by this type of detectors such as: excellent particle identification capabilities; high spatial resolution; good time resolution; hermetic detection volume; possibility to operate under intense magnetic fields etc., which are essential requirements for precision measurements on neutrino oscillations parameters. In addition, it could also provide important information about CP violation effects in the leptonic sector.     A detailed description of the physics case can be found here [1] .  A technical proposal has been submitted [2] , including a preliminary design study [3] .     In order to demonstrate the feasibility of our approach we have built a small prototype [4] consisting of: two TPCs filled with 1 tonne each of liquid argon; one central cathode made out of carbon fibre; four wire planes located above and below the cathode plane; three wire planes placed along the sides of the chamber; a set of scintillator paddles surrounding the active volume of the chambers.", "paraphrased_abstract": "It would therefore be possible to study the effect of CP violation of the CP in the field of chemistry. In order to show the feasibility of our approach we have built two TPCs each containing one tonne of liquid argon. One central coil made of carbon fibre; four wire beams extending above and below the coil; three wire beams running along the walls. A pair of oscilloscopes surrounding the active area of the TPC. A proposal for the construction of a large-scale argon TPC was submitted. The proposal provided a detailed description of the physics. The Neutrino-Filter and Muon-Crystallization Collaboration (NFMCC) in collaboration with other European laboratories, is proposing to develop a new type of high-resolution argon TPC to be used in the future Neutrino-Filtering and Muon-Crystallization experiments at CERN. The proposed project is to develop a large-scale argon TPC that could be used for neutrino-fracture experiments. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2074, "title": "Analysis on Heavy Quarkonia Transitions with Pion Emission in Terms of the QCD Multipole Expansion and Determination of Mass Spectra of Hybrids", "abstract": "  One of the most important tasks in high energy physics is search for the exotic states, such as glueball, hybrid and multi-quark states. The transitions $\\psi(ns)\\to \\psi(ms)+\\pi\\pi$ and $\\Upsilon(ns)\\to \\Upsilon(ms)+\\pi\\pi$ attract great attentions because they may reveal characteristics of hybrids. In this work, we analyze those transition modes in terms of the theoretical framework established by Yan and Kuang. It is interesting to notice that the intermediate states between the two gluon-emissions are hybrids, therefore by fitting the data, we are able to determine the mass spectra of hybrids. The ground hybrid states are predicted as 4.23 GeV (for charmonium) and 10.79 GeV (for bottonium) which do not correspond to any states measured in recent experiments, thus it may imply that very possibly, hybrids mix with regular quarkonia to constitute physical states. Comprehensive comparisons of the potentials for hybrids whose parameters are obtained in this scenario with the lattice results are presented. ", "machine_abstract": "We study heavy quarkonium transitions to hybrid mesons by using the multipole expansion method within the framework of nonrelativistic potential models. We calculate decay widths for various types of transitions, including E1, M1, E2, M2, E3, M3, E4, M4, and S-wave pion emission processes. The results are compared with available experimental data as well as theoretical predictions based on other approaches such as lattice gauge theory.     In addition we determine mass spectra of hybrids by solving Schr\u00f6dinger equations numerically. It is found that our calculated masses agree reasonably well with those obtained recently by lattice gauge theories. Finally, we discuss possible production mechanisms of hybrid states at future experiments. This work was supported by the National Natural Science Foundation of China under Grant No. 10675040. We thank Professors J. H. Chen and C. T. Chan for helpful discussions.", "paraphrased_abstract": "The results were compared with recent results, and the theories that are similar to the ones that are found in lattice gauge theory are also discussed. The researchers thank Mr. Chen and Mr. Chan for their assistance. The scientists praised the work of Jing Chen and Chan for their helpful discussions. Xiang Xiang, Xiang Zheng, Xiang Jing, Xiang Xiang, Xiang Zhen, Xiang Xiang, Xiang Xiang, Xiang Xiang, Xiang Zheng, Xiang Jing, Xiang Jing, Xiang Xiang Xiang. We study the strong quarks of the swarm and the hybrid mesons by using multipole expansion, and calculating the decay widths of various transitions, such as E1, M1, M2, M3, M3, M4, M4, M4 and S-waves. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0273, "title": "Flaming, bright galaxies along the filaments of A2744", "abstract": "  The existence of a clumpy, large-scale filamentary structure is at the basis of the current paradigm of cosmic formation and evolution of clusters. The star-formation history of galaxies falling into a cluster is altered as a result of the environmental stresses. We investigate the relation between substructure and properties of the galaxy population in a 30'x30' region centered on the massive merging cluster A2744 at z~0.3. Multi-object spectroscopy at low resolution and BVR photometry are used to detect the presence of substructure through a Dressler-Schectman analysis and the photometric redshift technique, respectively. Galaxies at the same photometric redshift of the cluster are divided into red and blue according to their distribution in the B-R vs. R colour-magnitude diagram. We identify two large-scale filaments associated with A2744. Along these filaments, the blue-to-red galaxy number ratio increases together with the cluster-centric distance but peaks slightly beyond the cluster virial radius. The filaments host a population of bright, large (i.e. more luminous than R* for the main body of the cluster and with angular sizes of 13-22 $h_{70}^{-1}$ kpc) blue galaxies that is hardly found among galaxies lying in a neighbouring low-density environment at the same redshift of the cluster. These results can be interpreted as a manifestation of galaxy harassment. ", "machine_abstract": "We present new observations with Hubble Space Telescope (HST) and Spitzer Space Telescope to study the galaxy population in the cluster Abell 2744 at z = 0.308. We find that most of the red sequence galaxies are located on the edges of the X-ray emission peaks, while blue cloud galaxies are found mostly inside these regions. The spatial distribution of star formation rate density is also consistent with this picture. These results suggest that ram pressure stripping may be responsible for quenching star formation activity in many central galaxies. In addition, we identify several flaring galaxies which show strong [O iii]\u03bb5007 line emissions in their spectra taken by HST/ACS grism. They have high SFRs ranging between 100 - 400 M\u2299 yr\u22121 , but they do not appear as AGNs based on their optical colors or mid-infrared properties. Their large velocity dispersions indicate that they might be undergoing mergers.", "paraphrased_abstract": "In addition, we found several flares, which showed an O-III5007 emission at the ACS, but whose spectra were not of a luminous colour or a mid-infrared color, which we classified as AGNs. These results suggested that ram pressure was responsible for reducing the activity of many central galaxies. We have also developed new observations with the Hubble Space Telescope and Spitzer Space Telescope in the vicinity of Abell 2744. Among these, the most red-blossom galaxies are located on the edges of the X-ray emission, and the most blue cloudy galaxies are located within these regions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0421, "title": "Mean-Field Magnetohydrodynamics of Accretion Disks", "abstract": "  We consider the accretion process in a disk with magnetic fields that are dragged in from the interstellar medium by gravitational collapse. Two diffusive processes are at work in the system: (1) \"viscous\" torques exerted by turbulent and magnetic stresses, and (2) \"resistive\" redistribution of mass with respect to the magnetic flux arising from the imperfect conduction of current. In steady state, self-consistency between the two rates of drift requires that a relationship exists between the coefficients of turbulent viscosity and turbulent resistivity. Ignoring any interactions with a stellar magnetosphere, we solve the steady-state equations for a magnetized disk under the gravitational attraction of a mass point and threaded by an amount of magnetic flux consistent with calculations of magnetized gravitational collapse in star formation. Our model mean-field equations have an exact analytical solution that corresponds to magnetically diluted Keplerian rotation about the central mass point. The solution yields the strength of the magnetic field and the surface density as functions of radial position in the disk and their connection with the departure from pure Keplerian rotation in representative cases. We compare the predictions of the theory with the available observations concerning T Tauri stars, FU Orionis stars, and low- and high-mass protostars. Finally, we speculate on the physical causes for high and low states of the accretion disks that surround young stellar objects. One of the more important results of this study is the physical derivation of analytic expressions for the turbulent viscosity and turbulent resistivity. ", "machine_abstract": "We present the results of our numerical simulations of magnetized accretion disks in which we solve the mean-field magnetohydrodynamic equations for an axisymmetric disk with a prescribed radial distribution of angular momentum and mass fluxes, using the shearing-box approximation.  We find that the magnetic field is amplified by differential rotation to produce large-scale poloidal fields whose strength increases outward as $(r^{-3/2})$ (where $r$ is the radius). The toroidal component of the magnetic field also grows rapidly due to winding up of the poloidal field lines by shear flows. As a result, the plasma beta parameter decreases inwardly toward the central object. In addition, we find that the Maxwell stress associated with the magnetic field causes significant redistribution of angular momentum within the disk. This leads to enhanced transport of angular momentum outwards across the disk surface compared to viscous stresses alone.", "paraphrased_abstract": "I find that the Maxwell strain of the magnetic field is of considerable importance, and it is this that the angular momentum is spread outwards across the disk, and that this is a major property of the angular momentum, which increases the angular momentum outwards. We show the results of our numerical experiments of magnetized accretion disks, where we calculate the mean field angular momentum and mass fluxes in a fixed radial direction, and then use the inverse approximation. We see that the magnetic field is multiplied by the radial rotation, which increases the poloidal field by about two-thirds (r), which in turn decreases the beta angular velocity towards the central object. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0354, "title": "Constraining dark energy via baryon acoustic oscillations in the (an)isotropic light-cone power spectrum", "abstract": "  The measurement of the scale of the baryon acoustic oscillations (BAO) in the galaxy power spectrum as a function of redshift is a promising method to constrain the equation-of-state parameter of the dark energy w. To measure the scale of the BAO precisely, a substantial volume of space must be surveyed. We test whether light-cone effects are important and whether the scaling relations used to compensate for an incorrect reference cosmology are in this case sufficiently accurate. We investigate the degeneracies in the cosmological parameters and the benefits of using the two-dimensional anisotropic power spectrum. Finally, we estimate the uncertainty with which w can be measured by proposed surveys at redshifts of about z=3 and z=1, respectively.   In the simulated survey we find that light-cone effects are small and that the simple scaling relations used to correct for the cosmological distortion work fairly well even for large survey volumes. The analysis of the two-dimensional anisotropic power spectra enables an independent determination to be made of the apparent scale of the BAO, perpendicular and parallel to the line of sight. This is essential for two-parameter w-models, such as the redshift-dependent dark energy model w=w_0+(1-a)w_a. Using Planck priors for the matter and baryon density and Delta(H_0)=5% for the Hubble constant, we estimate that the BAO measurements of future surveys around z=3 and z=1 will be able to constrain, independently of other cosmological probes, a constant w to ~ 12% and ~ 11% (68% c.l.), respectively. ", "machine_abstract": "We present an analysis of anisotropies in the angular correlation function and its Fourier transform, the angular power spectrum, using data from the Sloan Digital Sky Survey III Baryonic Oscillation Spectroscopic Survey (BOSS). We use this to constrain cosmological parameters including the equation-of-state parameter w{\\displaystyle w} for dark energy as well as the matter density $\\Omega_{m}$ and Hubble constant $H_{0}$. The results are consistent with previous measurements but have smaller statistical errors due to our larger sample size. In addition we find that the inclusion of redshift-space distortions improves constraints on all three parameters by about 10% compared to those obtained without them. This is because the effect of these distortions can be accurately modeled at low redshifts where they dominate over other effects such as gravitational lensing or non-linear evolution.  The full version of this article is available online at: http://arxiv.org/abs/1306.5995", "paraphrased_abstract": "I have found a way to model the effect of the redshift in the model, by introducing redshifts, which imply that at low redshifts the effect of these distortions is greatest. Then I will add to this the effect of a redshift distortion, a property which, for the sake of the numerical accuracy of the calculations, was the dominant of the other, such as gravity lensing or nonlinear evolution. The results are in accordance with previous measurements but have smaller statistical errors because of the larger sample. We use the method to constrain the angular correlation function and the Fourier transform of the angular power spectrum with data from the Sloan Digital Sky Survey (SDG3) and the BOSS AO. We apply the results to the calculation of the angular correlation function and the Fourier transform of the angular power spectrum in the light of the data obtained from the Sloan Digital Sky Survey, BOSS. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0574, "title": "The interplay between radio galaxies and cluster environment", "abstract": "  By combining the REFLEX and NORAS cluster datasets with the NVSS radio catalogue, we obtain a sample of 145, z<0.3, X-ray selected clusters brighter than 3 10^(-12) erg/s/cm^2 that show a central radio emission above 3 mJy. For virial masses M_vir<~10^(14.5) M_sun, 11 clusters out of 12 (corresponding to 92% of the systems) are inhabited by a central radio source. This fraction decreases with higher masses as M_vir^(-0.4) and suggests that the majority of X-ray selected clusters host in their centre a radio source brighter than about 10^(20) W/Hz/sr. A division of the sample into clusters harbouring either point-like or an extended radio-loud AGN reveals that the steepening of the L_X-T relation for low-temperature clusters is strongly associated with the presence of central radio objects with extended jets and/or lobe structures. In the latter case, L_X\\propto T^(4) while for point-like sources one recovers an approximately self-similar relation L_X\\propto T^(2.3). Monte Carlo simulations show that the steepening of the L_X-T relation is not caused by clusters being under-luminous in the X-ray band, but rather by overheating, most likely caused by the interplay between the extended radio structures and the intracluster medium. In the case of low-mass systems, we also find a tight correlation between radio luminosity and cluster temperature. The effects of the central radio source on the thermal state of a cluster become less important with increasing cluster mass. (Abridged) The luminosity distribution of the cluster radio population differs from that of all radio sources, as there is a deficit of low-luminosity (L_R<~10^(22) W/Hz/sr) objects, while the number of high-luminosity ones is boosted. The net effect on the radio luminosity function is of a flattening at all luminosities L_R<~ 10^(24) W/Hz/sr. ", "machine_abstract": "We present the results of an optical spectroscopic survey of radio galaxies in clusters at redshifts z=0.4-0.8, carried out with the William Herschel Telescope (WHT). We find that the fraction of AGN-powered radio galaxies decreases towards higher galaxy density environments within the clusters. This is consistent with previous studies which have found evidence for environmental quenching of star formation activity among massive galaxies. However we also find that there are many examples where powerful radio sources reside in dense regions without any obvious signs of being environmentally suppressed. These objects may be undergoing rapid evolution or they could represent a population of recently accreted field galaxies whose properties are still evolving to resemble those of their local counterparts. The sample consists of 20 radio galaxies selected from the VLA-COSMOS 3 GHz Large Project catalogue (Smol\u010di\u0107 et al., 2009) using the following criteria: 1) They lie in one of four X-ray luminous clusters at 0.4 < z < 0.8; 2) Their radio luminosity lies above L(3GHz) = 10 25 W Hz-1; 3) They do not show strong emission lines indicative of ongoing nuclear activity; 4) They were observed during our WHT run on 2010 May 24-25.", "paraphrased_abstract": "I am going to describe the results of a survey conducted on the WHT by the WHT instrument. It was a collection of X-ray galaxies selected from the VLA-COSMOS-3 GHz catalogue (Smolyet et al., 2009), which were collected from the catalogue of the VLA-COSMOS 3 GHz, and that is to say, in four clusters, 0.4  0.8  0.8; if they are found, they are not of the same type as a star; if they are, the radio emission does not show strong lines of active nuclear activity; if they are observed, they were observed on May 24, 2010. This sample consists of twenty radio galaxies, selected from the catalogue of the Vla-CoSMOS 3 GHz (Smoli et al., 2009) and which are in the 4 X-ray clusters 0.4  0.8  0.8  0.8; if they are luminous, their radiance is above the 3 GHz and, if they do not show strong spectra of activity, they are not luminous; if they are luminous, they are not strong in a strong way, which is a sign of a fast evolution, or, in other words, they are luminous, and", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3033, "title": "Two-photon transitions in hydrogen and cosmological recombination", "abstract": "  We study the two-photon process for the transitions ns --> 1s and nd --> 1s in hydrogen up to large n. For n<=20 we provide simple analytic fitting formulae to describe the non-resonant part of the two-photon emission profiles. Combining these with the analytic form of the cascade-term yields a simple and accurate description of the full two-photon decay spectrum, which only involves a sum over a few intermediate states. We demonstrate that the cascade term naturally leads to a nearly Lorentzian shape of the two-photon profiles in the vicinity of the resonances. However, due to quantum-electrodynamical corrections, the two-photon emission spectra deviate significantly from the Lorentzian shape in the very distant wings of the resonances. We investigate up to which distance the two-photon profiles are close to a Lorentzian and discuss the role of the interference term. We then analyze how the deviation of the two-photon profiles from the Lorentzian shape affects the dynamics of cosmological hydrogen recombination. Since in this context the escape of photons from the Lyman-alpha resonance plays a crucial role, we concentrate on the two-photon corrections in the vicinity of the Lyman-alpha line. Our computations show that the changes in the ionization history due to the additional two-photon process from high shell (n>2) likely do not reach the percent-level. For conservative assumptions we find a correction DN_e/N_e~-0.4% at redshift z~1160. This is numerically similar to the result of another recent study; however, the physics leading to this conclusion is rather different. In particular, our calculations of the effective two-photon decay rates yield significantly different values, where the destructive interference of the resonant and non-resonant terms plays a crucial role in this context (abridged) ", "machine_abstract": "We present the results of our calculation for two-photon transition rates between excited states of atomic hydrogen, including all relevant bound-bound and bound-free processes. We use these results to calculate the evolution of the number density of neutral hydrogen atoms during cosmic reionization. The main goal is to study how this process depends on the properties of the first stars that ionize the universe.     In particular we focus on the effect of different stellar initial mass functions (IMF) on the ionization history of the universe. For example, if massive stars are more numerous than low-mass ones then they will produce more photons per unit time which can lead to faster reionization. On the other hand, if most of the energy released by the first stars goes into winds or supernovae explosions rather than radiation then less UV photons would be produced leading to slower reionization. Our calculations show that the IMF has only minor effects on the overall duration of reionization but it does affect its detailed shape.", "paraphrased_abstract": "We subsequently investigate the influence of different initial mass-functions on the evolution of the ionization process. We examine the effects of the different types of initial mass-functions on the ionization history of the universe. For example, if the large, high-mass stars are more numerous than the small ones, then they will have more photons per unit time, which will make the ionization process faster. On the other hand, if the energy of the first stars is absorbed by wind or supernovae rather than radiation, then the UV photons will be produced less, which makes the ionization process slower. This analysis focuses on the properties of the first stars that ionize the universe, in particular on the effects of the IMF on the ionization process of the universe. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4412, "title": "Aid to Percutaneous Renal Access by Virtual Projection of the Ultrasound Puncture Tract onto Fluoroscopic Images", "abstract": "  Background and Purpose: Percutaneous renal access in the context of percutaneous nephrolithotomy (PCNL) is a difficult technique, requiring rapid and precise access to a particular calix. We present a computerized system designed to improve percutaneous renal access by projecting the ultrasound puncture tract onto fluoroscopic images. Materials and Methods: The system consists of a computer and a localizer allowing spatial localization of the position of the various instruments. Without any human intervention, the ultrasound nephrostomy tract is superimposed in real time onto fluoroscopic images acquired in various views. Results: We tested our approach by laboratory experiments on a phantom. Also, after approval by our institution's Ethics Committee, we validated this technique in the operating room during PCNL in one patient. Conclusion: Our system is reliable, and the absence of image-processing procedures makes it robust. We have initiated a prospective study to validate this technique both for PCNL specialists and as a learning tool. ", "machine_abstract": "The aim of this study was to evaluate whether virtual ultrasound (US) puncture tracts can improve percutaneous renal access in patients with complex anatomy and difficult-to-visualize calices on fluoroscopy.  In total, 50 consecutive patients underwent US-guided percutaneous nephrolithotomy using an in-room C-arm system for real-time image guidance. The procedure was performed under general anesthesia or conscious sedation. A pre-procedural CT scan was obtained without intravenous contrast medium injection. Using OsiriX MD software, two urologists delineated the kidney contour and identified all visible calices. Subsequently, they projected their findings onto the live fluoroscopic images during the procedure. They were asked to perform punctures into each calyx that could be visualized on fluoroscopy. After successful puncture, stone removal was attempted through the sheath inserted via the needle. Successful puncture was defined as reaching at least one calix. Overall success rate was 88%. No complications occurred related to the use of the US puncture tract projections. This technique may help urologists to achieve safe and efficient percutaneous renal access even if only few calices are clearly seen on fluoroscopy.", "paraphrased_abstract": "This method may help the urologists to achieve a safe and efficient percutaneous renal access, even in patients who have few calices. The results of this study were discussed. The two urologists characterized the kidney and found all the calices visible on the CT scan, and were instructed to cut into each calyx of the fluoroscopy image with a needle. Then, a hole was made with the needle and the stones were removed through a needle inserted through the needle. The success rate was 80%. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3354, "title": "J-class operators and hypercyclicity", "abstract": "  The purpose of the present work is to treat a new notion related to linear dynamics, which can be viewed as a \"localization\" of the notion of hypercyclicity. In particular, let $T$ be a bounded linear operator acting on a Banach space $X$ and let $x$ be a non-zero vector in $X$ such that for every open neighborhood $U\\subset X$ of $x$ and every non-empty open set $V\\subset X$ there exists a positive integer $n$ such that $T^{n}U\\cap V\\neq\\emptyset$. In this case $T$ will be called a $J$-class operator. We investigate the class of operators satisfying the above property and provide various examples. It is worthwhile to mention that many results from the theory of hypercyclic operators have their analogues in this setting. For example we establish results related to the Bourdon-Feldman theorem and we characterize the $J$-class weighted shifts. We would also like to stress that even non-separable Banach spaces which do not support topologically transitive operators, as for example $l^{\\infty}(\\mathbb{N})$, do admit $J$-class operators. ", "machine_abstract": "We study the connection between J-operators, which are defined as bounded linear operators on Banach spaces that satisfy the functional equation $T_J(x+y) = T_J(x) + T_J(y)$ for all $x, y \\in X$, and hypercyclic operators.  The main result is that if $X$ has an unconditional basis then every operator in the commutator ideal generated by any J-operator is hypercyclic (and even supercyclic). This generalizes earlier results due to Ansari, Bernal-Gonz\u00e1lez, Herrero-Burton, Johnson, Lancien, Salas, Shkarin, and Sourour.  We also show how this theorem can be used to prove that certain classes of operators are not hypercyclic or have only trivial hypercyclic vectors. Finally we give some examples illustrating our results. The research leading to these results was supported by EPSRC grant EP/D076446/1", "paraphrased_abstract": "The main result is that if X has an unconditional basis, then all operators in the ideal of any J operator are hypercyclic (and even supercyclic). We give examples of our results. We study the connection between J operators, which are linear operators on Banach spaces, whose functional equation is: 'T'J(x) - tj(x) - tj(y)' for all x, y, and hypercyclic operators. We show that, on the contrary, there are operators which are not hypercyclic, or are only trivial hypercyclic vectors. We study the relation between the J operators, which are bounded linear operators on Banach spaces, which satisfy the functional equation 'T'J(x) = T'J(x) + T'J(y)' for all x, y, and hypercyclic operators. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0493, "title": "Phase structure of a surface model on dynamically triangulated spheres with elastic skeletons", "abstract": "  We find three distinct phases; a tubular phase, a planar phase, and the spherical phase, in a triangulated fluid surface model. It is also found that these phases are separated by discontinuous transitions. The fluid surface model is investigated within the framework of the conventional curvature model by using the canonical Monte Carlo simulations with dynamical triangulations. The mechanical strength of the surface is given only by skeletons, and no two-dimensional bending energy is assumed in the Hamiltonian. The skeletons are composed of elastic linear-chains and rigid junctions and form a compartmentalized structure on the surface, and for this reason the vertices of triangles can diffuse freely only inside the compartments. As a consequence, an inhomogeneous structure is introduced in the model; the surface strength inside the compartments is different from the surface strength on the compartments. However, the rotational symmetry is not influenced by the elastic skeletons; there is no specific direction on the surface. In addition to the three phases mentioned above, a collapsed phase is expected to exist in the low bending rigidity regime that was not studied here. The inhomogeneous structure and the fluidity of vertices are considered to be the origin of such variety of phases. ", "machine_abstract": "We study the phase structure of a surface model defined by an energy functional that consists of two competing terms, one favoring smooth surfaces and another penalizing their curvature fluctuations. The latter is modeled as a harmonic term in the local mean curvature. We show that this model exhibits three phases depending on temperature T . At high temperatures (T > Tc), it behaves like a liquid; at low temperatures (T < Ts) it forms a solid-like state where all triangles are equilateral; for intermediate temperatures (Ts<T< Tc) we find a disordered glassy phase which can be characterized by its fractal dimension D = 2 \u2212 H \u2248 1.7 \u00b1 0.1. This value agrees well with numerical simulations performed recently by other authors.     In addition to these results, our analysis also provides evidence for a first-order transition between the ordered and disordered states. Finally, we discuss possible extensions of our approach towards more realistic models of biomembranes.", "paraphrased_abstract": "In this study, we propose an enzymatically modified surface model. We consider two competing terms, one favouring smooth surfaces and the other limiting their curvature to the local mean. We find that the two terms correspond to each other in the form of a harmonic term, corresponding to the local mean curvature. Our study demonstrates the existence of three different phases depending on the temperature T. At high temperatures T > Tc, it is liquid. At low temperatures T > Ts, it is solid, where all triangles are equilateral. The intermediate temperatures Ts  Tc, a disordered glassy phase is observed, whose dimension is D = 2  H  1.7  0.1. The numerical simulations performed by other authors have confirmed this value. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.245, "title": "Dirichlet or Potts ?", "abstract": "  When modeling the distribution of a set of data by a mixture of Gaussians, there are two possibilities: i) the classical one is using a set of parameters which are the proportions, the means and the variances; ii) the second is to consider the proportions as the probabilities of a discrete valued hidden variable. In the first case a usual prior distribution for the proportions is the Dirichlet which accounts for the fact that they have to sum up to one. In the second case, to each data is associated a hidden variable for which we consider two possibilities: a) assuming those variables to be i.i.d. We show then that this scheme is equivalent to the classical mixture model with Dirichlet prior; b) assuming a Markovian structure. Then we choose the simplest markovian model which is the Potts distribution. As we will see this model is more appropriate for the case where the data represents the pixels of an image for which the hidden variables represent a segmentation of that image. The main object of this paper is to give some details on these models and different algorithms used for their simulation and the estimation of their parameters.   Key Words: Mixture of Gaussians, Dirichlet, Potts, Classification, Segmentation. ", "machine_abstract": "We consider the problem of learning an unknown distribution over binary strings, where each string is generated by sampling independently at random with replacement according to some fixed probability vector . We show that if we are given samples drawn iid from , then it can be learned in polynomial time using standard statistical techniques (e.g., maximum likelihood). However, when the number of possible values for each bit grows large, this approach fails because there may not exist any sample which contains all possible values for every bit. In such cases, we propose a novel algorithm based on Gibbs sampling and prove its correctness under certain conditions. Finally, we present experimental results demonstrating our method's effectiveness. The main contribution of this work lies in showing how to learn distributions over combinatorial objects like graphs and trees. This is achieved by first reducing these problems into learning distributions over binary strings, and then applying our proposed algorithms. Learning distributions over combinatorial objects has many applications including data mining, computer vision, bioinformatics, etc. .", "paraphrased_abstract": "He proposed an inverse method for the inverse of the distribution of binary strings, and we formulated it in this way: if we assume the problem of learning the distribution of binary strings for the sake of convenience, it is possible to learn it in a single resemblance, using the standard statistical methods, for instance, maximum likelihood, and so on. This method, by reducing the problem to inverses, is in fact possible, and we demonstrate this through a number of experiments. We illustrate that the problem of learning the distribution of binary strings for each of them can be solved in polynomial time by applying a Gibbs sampling technique, which is to say, more precise than the traditional Gibbs sampling, but which is to say, insufficient, as there may not be a single sample that contains all possible values for each bit. It was shown that when a sample of binary strings was chosen from a hoop, it could be learned in polynomial time by the most ordinary methods of statistics, such as maximum likelihood. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3886, "title": "Two Energy Release Processes for CMEs: MHD Catastrophe and Magnetic Reconnection", "abstract": "  It remains an open question how magnetic energy is rapidly released in the solar corona so as to create solar explosions such as solar flares and coronal mass ejections (CMEs). Recent studies have confirmed that a system consisting of a flux rope embedded in a background field exhibits a catastrophic behavior, and the energy threshold at the catastrophic point may exceed the associated open field energy. The accumulated free energy in the corona is abruptly released when the catastrophe takes place, and it probably serves as the main means of energy release for CMEs at least in the initial phase. Such a release proceeds via an ideal MHD process in contrast with nonideal ones such as magnetic reconnection. The catastrophe results in a sudden formation of electric current sheets, which naturally provide proper sites for fast magnetic reconnection. The reconnection may be identified with a solar flare associated with the CME on one hand, and produces a further acceleration of the CME on the other. On this basis, several preliminary suggestions are made for future observational investigations, especially with the proposed KuaFu satellites, on the roles of the MHD catastrophe and magnetic reconnection in the magnetic energy release associated with CMEs and flares. ", "machine_abstract": "We propose that the energy release process in coronal mass ejections (CMEs) is due to two different physical mechanisms, magnetic reconnection and catastrophic loss of equilibrium driven by ideal magnetohydrodynamic (MHD) instabilities.  The first mechanism releases free magnetic energy stored in the corona into kinetic energy of plasma flows during solar eruptive events such as flares or CMEs. This mechanism has been studied extensively using numerical simulations based on resistive MHD models with various levels of complexity ranging from simple two-dimensional configurations to more realistic three-dimensional ones. In this work we focus our attention on the second mechanism which involves rapid conversion of magnetic field energy into thermal energy through an explosive reconfiguration of the magnetic topology. We show how this mechanism can be modeled analytically within the framework of ideal MHD theory. Our results suggest that both processes are likely to operate simultaneously but at different stages of evolution of the erupting flux rope.", "paraphrased_abstract": "The second one is the fast conversion of the magnetic energy from the field to the thermal energy, which is achieved by a rapid reconfiguration of the magnetic topology. The latter is well-known, but has been analyzed extensively by numerical simulations of resistive MHD, of varying complexity. It has been found that the release of magnetic energy during solar eruptions is based on two different physical processes: a magnetic reconnection and a catastrophic loss of equilibrium in the form of ideal MHD instabilities. This is a mechanism, which we have been able to understand in detail by numerical simulations based on the resistive MHD, from simple two-dimensional models to more realistic three-dimensional models. This process has been studied in detail by several numerical simulations, based on resistive MHD, ranging from the simple two-dimensional two-dimensional models to the realistic three-dimensional models. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1378, "title": "A Pair of Bootes: A New Milky Way Satellite", "abstract": "  As part of preparations for a southern sky search for faint Milky Way dwarf galaxy satellites, we report the discovery of a stellar overdensity in the Sloan Digital Sky Survey Data Release 5, lying at an angular distance of only 1.5 degrees from the recently discovered Bo{\\\"o}tes dwarf. The overdensity was detected well above statistical noise by employing a sophisticated data mining algorithm and does not correspond to any catalogued object. Overlaid isochrones using stellar population synthesis models show that the color-magnitude diagram of that region has the signature of an old (12 Gyr), metal-poor (${\\rm Fe/H}\\approx-2.0$) stellar population at a tentative distance of 60 kpc, evidently the same heliocentric distance as the Bo\\\"otes dwarf. We estimate the new object to have a total magnitude of $M_{V}\\sim-3.1\\pm1.1$ mag and a half-light radius of $r_{h}=4'.1\\pm1'.6$ ($72\\pm28$ pc) placing it in an apparent $40<r_{h}<100$ pc void between globular clusters and dwarf galaxies, occupied only by another recently discovered Milky Way Satellite, Coma Berenices. ", "machine_abstract": "We report the discovery of a new satellite galaxy, dubbed \"A pair of bootes\" (ApoBootes), orbiting around our Galaxy at a distance of about 300 kpc in projection and with an estimated mass of 1.5 x 10^10 M_sun . ApoBootes is located on the opposite side of the Galactic center to the Magellanic Clouds and has a very low surface brightness. We have used deep near-infrared images taken by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this object. The photometric properties are consistent with those expected for a dwarf spheroidal galaxy. This work was supported by the Australian Research Council Discovery Project funding scheme under grant DP130104011. We present evidence that ApoBootes may be associated with a previously known overdensity of stars discovered by Belokurov et al. (2007) using SDSS data.", "paraphrased_abstract": "In the X-ray spectra of the VISTA-VISTA spectra, we found a new satellite, named Apobotes, whose circumference was about 300 km and whose mass was about 1.5 x 1010 M., we have shown that the photometric properties are in accordance with that of a dwarf spherical galaxy. We show that the new satellite was found at a distance of about 300 km in projection, and whose mass was about 1.5 x 1010 M., based on the VISTA-VISTA spectra of the Via Lactea survey. We find that ApoBootes is a dwarf spherical galaxy, known to be overloaded by the researchers of Belokurov et al. (2007). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4242, "title": "Oscillation thresholds for \"strinking outwards\" reeds coupled to a resonator", "abstract": "  This paper considers a \"striking outwards\" reed coupled to a resonator. This expression, due to Helmholtz, is not discussed here : it corresponds to the most common model of a lip-type valve, when the valve is assumed to be a one degree of freedom oscillator. The presented work is an extension of the works done by Wilson and Beavers (1974), Tarnopolsky (2000). The range of the playing frequencies is investigated. The first results are analytical : when no losses are present in the resonator, it is proven that the ratio between the threshold frequency and the reed resonance frequency is found to be necessarily within the interval between unity and the square root of 3. This is a musical sixth. Actually the interval is largely smaller, and this is in accordance with e.g. the results by Cullen et al.. The smallest blowing pressure is found to be directly related to the quality factor of the reed. Numerical results confirm these statements, and are discussed in comparison with previous ones by Cullen et al (2000). ", "machine_abstract": "We study the dynamics of an ensemble of elastically-coupled reed oscillators driven by a common acoustic field, and show that they can exhibit striking phenomena such as synchronized chaos or phase-locked states with multiple incommensurate frequencies. We also demonstrate how these effects are modified when the system is connected to a passive resonator. The results presented here may be relevant to understanding the behavior of biological systems such as colonies of fireflies or neurons.     In recent years there has been growing interest in studying nonlinear dynamical systems composed of many interacting elements [1-3]. These studies have revealed new collective behaviors which cannot occur in single element systems [4] . Examples include synchronization [5-7] , clustering [8] , chimera states [9] , and spatiotemporal patterns [10-12] . In this work we consider a model consisting of N identical elastically-coupled reed-oscillators [13] driven by a common acoustic wavefield [14-16] (see Fig 1) . Each oscillator consists of two vibrating plates separated by a thin layer of air [17] . When the pressure difference across one plate exceeds its critical value, it begins to expand rapidly until the restoring force due to the other plate causes it to contract again. This process repeats periodically resulting in self-sustained oscillations at a frequency determined by the geometry of the device [18] .  The coupling between neighboring oscillators arises because each oscillator acts like a small loudspeaker radiating sound into the surrounding medium [19-21] . As a result, nearby oscillators experience similar driving forces and their natural frequencies become locked [22] . If all oscillators lock onto the same frequency then the entire system behaves coherently; if different groups of oscillators lock on to different frequencies then the system exhibits spatially-extended multistability [23] .", "paraphrased_abstract": "A common resonant wave is an oscillator, a pair of resonant plates, separated by a thin layer of air, and whose pressure is exceeded by the pressure of the other plate, the oscillator expands, and the resonant wave becomes a new one. Its elasticity is similar to that of a small speaker, which emits sound to the surrounding medium. In addition, if the pair of resonant wave is at the same frequency, then the whole system is coherent; if the groups of resonant wave are at different frequencies, then the system is spatially extended and resonant. In this study we are dealing with an ensemble of resonant waves driven by a common acoustic field, and show that they can exhibit remarkable effects, such as synchronized chaos, phase-locked states with multiple frequencies, and spatial patterns. This result is relevant to our understanding of the behavior of biological systems, such as colonies of fireflies and neurons. In the recent years, many researchers have been interested in the study of nonlinear dynamical systems composed of many elements, and many of them have become very interested. In the study of resonant waves, we have discovered new collective phenomena which are not possible in single-element systems. We consider a system composed of N identically coupled resonant re", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1315, "title": "Observations towards early-type stars in the ESO-POP survey: II -- searches for intermediate and high velocity clouds", "abstract": "  We present CaII K and TiII optical spectra of early-type stars taken mainly from the UVES Paranal Observatory Project, plus HI 21-cm spectra from the Vila-Elisa and Leiden-Dwingeloo surveys, which are employed to obtain distances to intermediate and high velocity clouds. HI emission at a velocity of -117 km/s towards the sightline HD 30677 with NHI=1.7x10^19 cm-2 has no corresponding CaII K absorption, which has a signal-to-noise (S/N) ratio of 610. The star has a spectroscopically determined distance of 2.7-kpc, and hence sets this as a firm lower distance limit towards Anti-Centre cloud ACII. Towards another sightline (HD 46185), HI at +122 km/s and NHI=1.2x10^19 cm-2 is seen. The CaII K spectrum has a S/N = 780, although no absorption is observed at the cloud velocity. This similarly places a firm lower distance limit of 2.9-kpc towards this parcel of gas that may be an intermediate velocity cloud. The lack of intermediate velocity (IV) CaII K absorption towards HD 196426 at a S/N = 500 reinforces a lower distance limit of 700-pc towards this part of Complex gp, where NHI is 1.1x10^19 cm-2 and velocity is +78 km/s. Additionally, no IV CaII K is seen in absorption in the spectrum of HD 19445, which is strong in HI with NHI=8x10^19 cm-2 at -42 km/s, placing a firm although uninteresting lower distance limit of 39-pc to this part of IV South. Finally, no HV CaII K absorption is seen towards HD 115363 at a S/N = 410, placing a lower distance of 3.2-kpc towards the HVC gas at velocity of +224 km/s and NHI=5.2x10^19 cm-2. This gas is in the same region of the sky as complex WE (Wakker 2001), but at higher velocities. The non-detection of CaII K absorption sets a lower distance of 3.2-kpc towards the HVC, which is unsurprising if this feature is indeed related to the Magellanic System. ", "machine_abstract": "We present new results on interstellar absorption lines toward early type stars observed with UVES at VLT, as part of the ESO-POP project (ESO program 085.D-0571). We have searched for high-velocity clouds (HVCs) by looking for blueshifted components in the MgII doublet line profiles. The sample consists of 16 OB-stars located within 1 kpc distance from Earth. In addition to previously known HVCs we find several new ones. Some of these are associated with nearby galaxies while others may be related to Galactic halo gas. A comparison between our data set and previous surveys shows that there is no significant difference in the number density distribution of HVCs along different sightlines. This suggests that most of them are small structures which do not cover much solid angle around their host galaxy or star.     Keywords: Interstellar medium", "paraphrased_abstract": "But there is no significant difference between the number of these HVCs on the different levels of the sky. Compared with previous observations, we have found several new ones. They are located in the vicinity of nearby galaxies and may be related to Galactic halo gas. We have also made a search for high-velocity clouds by focusing on the blue components of the MgIII doublets. This new group is composed of sixteen OB-stars, located only a kpc from Earth. We have found new clouds in the area of OB-stars, and there are many new ones in this area. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3379, "title": "Ray-tracing and physical-optics analysis of the aperture efficiency in a radio telescope", "abstract": "  The performance of telescope systems working at microwave or visible/IR wavelengths is typically described in terms of different parameters according to the wavelength range. Most commercial ray tracing packages have been specifically designed for use with visible/IR systems and thus, though very flexible and sophisticated, do not provide the appropriate parameters to fully describe microwave antennas, and thus to compare with specifications. In this work we demonstrate that the Strehl ratio is equal to the phase efficiency when the apodization factor is taken into account. The phase efficiency is the most critical contribution to the aperture efficiency of an antenna, and the most difficult parameter to optimize during the telescope design. The equivalence between the Strehl ratio and the phase efficiency gives the designer/user of the telescope the opportunity to use the faster commercial ray-tracing software to optimize the design. We also discuss the results of several tests performed to check the validity of this relationship that we carried out using a ray-tracing software, ZEMAX and a full Physical Optics software, GRASP9.3, applied to three different telescope designs that span a factor of $\\simeq 10 in terms of D/lambda. The maximum measured discrepancy between phase efficiency and Strehl ratio varies between $\\simeq 0.4 and 1.9 up to an offset angle of >40 beams, depending on the optical configuration, but it is always less than 0.5 where the Strehl ratio is >0.95. ", "machine_abstract": "We present an analytical model for calculating the aperture efficiency of a reflector antenna with circularly polarized feeds, based on ray tracing through the feed-horn optics and physical optics (PO) at the aperture plane. The PO method is used to calculate the electric field distribution over the aperture surface by solving Maxwell's equations using Green functions. We show that this approach can be applied to any type of feed horns, including corrugated conical horns or dual-polarized log-periodic dipole arrays. This work was motivated by our recent study of aperture efficiencies of two different types of antennas operating at 1.4 GHz: a single-dish radio telescope equipped with a corrugated conical feed horn and a multi-element interferometer consisting of eight log-periodic dipole array elements. In both cases we found good agreement between numerical results obtained with our new model and experimental data.     Keywords: Antenna design", "paraphrased_abstract": "I\u2019m addressing a new subject that is largely inspired by my recent study of two different antennas operating at 1.4 GHz, a single-dish radio telescope equipped with a corrugated conical feed horn and a multi-element interferometer consisting of eight dipole elements. The result of our research is that of a circular polarized feed horn with a circular polarized field. We show how this method can be applied to any kind of feed horn, either the corrugated conical feed horn or the dual-polarized dipole antenna. We demonstrate that the results of our study are in agreement with the experimental results. We present an analytical model for estimating the aperture efficiency of an antenna with circularly polarized feeds, based on a ray tracing of the feed horn optics and physical optics of the aperture plane, and the inverse function of Green function, which we show in the following. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2012, "title": "Spinons and triplons in spatially anisotropic frustrated antiferromagnets", "abstract": "  The search for elementary excitations with fractional quantum numbers is a central challenge in modern condensed matter physics. We explore the possibility in a realistic model for several materials, the spin-1/2 spatially anisotropic frustrated Heisenberg antiferromagnet in two dimensions. By restricting the Hilbert space to that expressed by exact eigenstates of the Heisenberg chain, we derive an effective Schr\\\"odinger equation valid in the weak interchain-coupling regime. The dynamical spin correlations from this approach agree quantitatively with inelastic neutron measurements on the triangular antiferromagnet Cs_2CuCl_4. The spectral features in such antiferromagnets can be attributed to two types of excitations: descendents of one-dimensional spinons of individual chains, and coherently propagating \"triplon\" bound states of spinon pairs. We argue that triplons are generic features of spatially anisotropic frustrated antiferromagnets, and arise because the bound spinon pair lowers its kinetic energy by propagating between chains. ", "machine_abstract": "We study the spin-1/2 Heisenberg model on the square lattice with nearest-neighbor (NN) exchange interactions J1, J2, and J3 along x-, y-, and z-directions respectively. We show that this system can be mapped onto an effective spin-1/2 XXZ chain by using the Holstein-Primakoff transformation for spins 1/2. The ground state phase diagram is obtained numerically by exact diagonalization method. In addition to the N\u00e9el ordered phase at small values of |J2/J1| < 0.5, we find two different types of quantum disordered phases depending upon the value of J3/J2. For large values of J3/J2 > 1.0, there exists a gapless singlet phase where all excitations are gapped out except one massless excitation corresponding to the Goldstone mode associated with the spontaneous breaking of translational symmetry. On the other hand, when J3/J2 < 0.5, there appears another type of disordered phase which has a finite energy gap between the ground state and first excited states. This phase corresponds to a valence bond solid phase where each site forms singlets with its NN sites only.  Finally, we also discuss how these results may be relevant to understand the magnetic properties of some quasi-two-dimensional organic compounds such as EtMe3Sb[Pd(dmit)2]2 and \u03ba-(BEDT-TTF)2Cu[N(CN)2]Cl.", "paraphrased_abstract": "As a result, the ground state phase is represented by the diagonalogram of the ground state. It is found that the N\u00e9el ordered phase exists at the small value of J2/J2 > 0.5, while a second, smaller type, which consists in a finite energy gap between the ground state and the first excited states, corresponds to the valence bond phase in which each site has only its NN atoms. The inverse geometry of the phase of a certain organic compound of the form of -(Bett-TTF)2 and -(BEDT-TTF)2Cu(CN)2Cl. Moreover, we consider the Heisenberg model on a square-toothed cylinder with NN-near-neighbor interactions, J1, J2 and J3, along the x-y-z direction, and to the effect of the Holstein-Primakoff transformation. In the vicinity of J2/J20, there is a quasi-duplex phase, and there is a gapless singlet, in which all excitations are filled, except for one massless excitation, which is the Goldstone trough, corresponding to the refractory phase, which is the epoch of the spontaneous breaking of the translational symmetry. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0089, "title": "A general approach to statistical modeling of physical laws: nonparametric regression", "abstract": "  Statistical modeling of experimental physical laws is based on the probability density function of measured variables. It is expressed by experimental data via a kernel estimator. The kernel is determined objectively by the scattering of data during calibration of experimental setup. A physical law, which relates measured variables, is optimally extracted from experimental data by the conditional average estimator. It is derived directly from the kernel estimator and corresponds to a general nonparametric regression. The proposed method is demonstrated by the modeling of a return map of noisy chaotic data. In this example, the nonparametric regression is used to predict a future value of chaotic time series from the present one. The mean predictor error is used in the definition of predictor quality, while the redundancy is expressed by the mean square distance between data points. Both statistics are used in a new definition of predictor cost function. From the minimum of the predictor cost function, a proper number of data in the model is estimated. ", "machine_abstract": "We present an overview of the theory and practice of nonparametric regression, with emphasis on its application in physics. We begin by introducing some basic concepts such as kernel density estimation (KDE) and spline smoothing. Then we discuss how these methods can be used for data analysis problems that arise frequently in physics experiments. Finally, we provide several examples illustrating our discussion. The goal is not only to introduce readers unfamiliar with this topic but also to give physicists who are familiar with it a new perspective on their work. This article serves as a companion piece to another one entitled \"Statistical Modeling of Physical Laws\" which appeared earlier in ArXiv.org. In that article, we introduced the concept of parametric models and discussed how they can be applied to solve various types of data analysis problems encountered in physics experiments. Parametric models assume that all unknown parameters have been determined exactly beforehand; however, in many cases there may exist uncertainty about those values due to limited experimental precision or lack of knowledge. Nonparametric regression provides a way around this problem by allowing us to estimate the functional form of the underlying model without making any assumptions about the exact value of each parameter.", "paraphrased_abstract": "This paper is a companion article to the article \"Statistical Modeling of Physical Laws\" which appeared earlier in the ArXiv, and which is similar in scope to the one cited above, which appeared earlier in ArXiv.org. The article introduces the concept of parametric models and discusses how they can be applied to the analysis of physics experiments. In this article, we introduce the concept of parametric models and discuss their application in various types of physics experiments. We begin by introducing the basic concept of kernel density estimation and the smoothing of spline. Next, we discuss how nonparametric regression can be applied to the analysis of physics experiments. Then we discuss several examples. This article will be helpful for readers who are unfamiliar with the subject, but also to give physics researchers familiar with it a new perspective. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.052, "title": "Constraints on the merging timescale of luminous red galaxies, or, where do all the halos go?", "abstract": "  In the LCDM cosmology dark matter halos grow primarily through the accretion of smaller halos. Much of the mass in a halo of 10^14Msun comes in through accretion of ~10^13Msun halos. If each such halo hosted one luminous red galaxy (LRG) then the accretion of so many halos is at odds with the observed number of LRGs in clusters unless these accreted LRGs merge or disrupt on relatively short timescales (~2Gyr). These timescales are consistent with classical dynamical friction arguments, and imply that 2-3 LRGs have merged or disrupted within each halo more massive than 10^14Msun by z=0. The total amount of stellar mass brought into these massive halos by z=0 is consistent with observations once the intracluster light (ICL) is included. If disrupted LRGs build up the ICL, then the hierarchical growth of massive halos implies that a substantial amount of ICL should also surround satellite LRGs, as suggested by recent observations of the Virgo cluster. Finally, we point out that these results are entirely consistent with a non-evolving clustering strength and halo occupation distribution, and note that observations of the latter in fact support the hypothesis that merging/disruption of massive galaxies does indeed take place at late times. ", "machine_abstract": "We present new constraints on the merger rate density and mass ratio distribution for luminous red galaxies (LRGs) using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use these results to place limits on the number of massive dark matter haloes that are not associated with LRG hosts in the local universe. The halo occupation model we employ is based upon an extension of the HOD formalism developed by Zheng et al. (2005) , which allows us to simultaneously constrain both the mean occupation function and its scatter at fixed luminosity as well as the fraction of satellite galaxies that have been recently accreted into their host haloes. Our analysis indicates that there exists a significant population of massive haloes that are not occupied by LRGs within our sample volume. This result suggests that either many of the most massive haloes were assembled very early during cosmic history and/or that they contain only low-mass galaxies.", "paraphrased_abstract": "The analysis shows that the number of gigantic dark matter haloes is enormous, and that they are not LRGs. Our study has revealed that there is a large number of dark matter haloes, and there are not LRGs in our sample. We present the density and density of the merger rate and the mass ratio of luminous red galaxies (LRGs) with the SLDS-7 data. We use the method to constrain the density and density of the haloes by the density and density of the haloes, and the halo population of the satellites in our universe. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2312, "title": "Carrier-carrier entanglement and transport resonances in semiconductor quantum dots", "abstract": "  We study theoretically the entanglement created in a scattering between an electron, incoming from a source lead, and another electron bound in the ground state of a quantum dot, connected to two leads. We analyze the role played by the different kinds of resonances in the transmission spectra and by the number of scattering channels, into the amount of quantum correlations between the two identical carriers. It is shown that the entanglement between their energy states is not sensitive to the presence of Breit-Wigner resonances, while it presents a peculiar behavior in correspondence of Fano peaks: two close maxima separated by a minimum, for a two-channel scattering, a single maximum for a multi-channel scattering. Such a behavior is ascribed to the different mechanisms characterizing the two types of resonances. Our results suggest that the production and detection of entanglement in quantum dot structures may be controlled by the manipulation of Fano resonances through external fields. ", "machine_abstract": "We study the dynamics of carrier-carrier interactions in semiconductor quantum dots (QDs) by solving numerically the time-dependent Schr\u00f6dinger equation for two interacting electrons or holes confined to an anisotropic QD potential well. We find that, depending on the initial state, there are three different regimes of interaction between carriers which can be classified as weak coupling regime with no significant energy exchange; strong coupling regime where one electron is excited into higher states while another remains in its ground state; and finally, intermediate regime where both carriers undergo transitions simultaneously but at slightly different frequencies. In addition we show how these results depend on the dot shape and size parameters. Finally, we discuss possible applications of our findings such as generation of entangled photon pairs via biexciton decay. Quantum dots have been studied extensively over past decade due to their unique optical properties [1] . The most important feature of QDs is the possibility of controlling their emission wavelength through variation of their size [2] , allowing them to operate within a wide range of wavelengths [3] . In this work we focus on studying the effects of carrier-carrier interactions [4] in semiconductor QDs using numerical solution of timedependent Schr\u00f6dinger equations [5] . Carriers interact strongly when they occupy neighboring single-particle levels [6] leading to formation of bound excitonic complexes [7, 8] . However, if carriers occupy distant single particle levels then their mutual Coulomb attraction leads to formation of virtual excitons [9] . These virtual excitons may either recombine radiatively [10] or non-radiatively [11] giving rise to Auger processes [12] . On the other hand, if carriers occupy adjacent single particle levels then their interaction becomes so strong that it cannot be treated perturbatively anymore [13] . This situation occurs e.g., during relaxation of photoexcited carriers [14] or in presence of external electric field [15] .", "paraphrased_abstract": "For instance, if the carriers are occupied by the same particle, the Coulombs attract each other, and the molecules are bound. Moreover, the atoms of these atoms can recombine, which can either be resolutely formed or non-resolved, so that the Auger processes can be prevented. In the last part of this article, we consider the effects of the interplay of the interplay of the interplay of the interplay between the interplaying electrons and the interplay between the two electrons. We find that in the first state, the interplay between the interplaying electrons is strong, and the interplay is resolutely induced, or in some cases resolutely suppressed, and at a very different frequency., we obtain, and analyze numerically the Schr\u00f6dinger equation of two electrons, or holes, confined to an anisotropic QD. This effect is the result of the combination of two opposite electrons in the same state, and is referred to as \"second order\" and \"third order\". ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3723, "title": "Dyson Indices and Hilbert-Schmidt Separability Functions and Probabilities", "abstract": "  A confluence of numerical and theoretical results leads us to conjecture that the Hilbert-Schmidt separability probabilities of the 15- and 9-dimensional convex sets of complex and real two-qubit states (representable by 4 x 4 density matrices rho) are 8/33 and 8/17, respectively. Central to our reasoning are the modifications of two ansatze, recently advanced (quant-ph/0609006), involving incomplete beta functions B_{nu}(a,b), where nu= (rho_{11} rho_{44})/(rho_{22} rho_{33}). We, now, set the separability function S_{real}(nu) propto B_{nu}(nu,1/2},2) =(2/3) (3-nu) sqrt{nu}. Then, in the complex case -- conforming to a pattern we find, manifesting the Dyson indices (1, 2, 4) of random matrix theory-- we take S_{complex}(nu) propto S_{real}^{2} (nu). We also investigate the real and complex qubit-qutrit cases. Now, there are two Bloore ratio variables, nu_{1}= (rho_{11} rho_{55})(rho_{22} rho_{44}), nu_{2}= (rho_{22} rho_{66})(rho_{33} rho_{55}), but they appear to remarkably coalesce into the product, eta = nu_1 nu_2 = rho_{11} \\rho_{66}}{\\rho_{33} \\rho_{44}}, so that the real and complex separability functions are again univariate in nature. ", "machine_abstract": "We introduce the concept of separability functions, which are generalizations of Dyson indices to arbitrary finite groups. We show that these functions can be used as indicators for quantum entanglement in mixed states. In particular we prove that separability probabilities vanish if and only if all separable pure state decompositions have vanishing Schmidt coefficients. This is an extension of the result by Peres-Horodecki on positive partial transpose matrices. Finally we present some explicit examples illustrating our results. The concept of separability functions was introduced recently [1] . These functions provide a generalization of Dyson indices [2] , which were originally defined for Lie algebras, to arbitrary finite groups. They play an important role in physics since they appear naturally when studying the properties of many-body systems with short-range interactions [3] . In this work we study how separability functions behave under tensor products. It turns out that their behavior depends crucially on whether or not the group has Abelian subgroups. For example it follows directly from the definition that separability functions do not change at all under tensor product over Abelian groups. On the other hand there exist non-Abelian groups where separability functions increase exponentially under tensor product [4] . As another consequence of our analysis we obtain new bounds on the number of linearly independent separability functions [5] .", "paraphrased_abstract": "The concept of separation functions has recently been introduced. It is a generalization of Dyson\u2019s index, originally given for Lie algebras, and it can be applied to arbitrary finite groups. We are able to demonstrate the significance of this in a particular sense. The tensor products we use are of the type derived from Lie algebras and the use of tensor products is based on a particular case of Abelian groups. We show how separation functions behave under tensor products. We show that in the case of Abelian groups the separation coefficients of tensor products do not change at all. Moreover, there are groups of non-Abelian groups where the separation coefficient is exponentially increased under tensor products. We also show that we can apply the concept of separation to the study of the properties of entanglement in mixed states. This result is an extension of the result of Peres-Horodecki\u2019s arithmetic of positive partial transpositions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1977, "title": "Challenging the Cosmological Constant", "abstract": "  We outline a dynamical dark energy scenario whose signatures may be simultaneously tested by astronomical observations and laboratory experiments. The dark energy is a field with slightly sub-gravitational couplings to matter, a logarithmic self-interaction potential with a scale tuned to $\\sim 10^{-3} {\\rm eV}$, as is usual in quintessence models, and an effective mass $m_\\phi$ influenced by the environmental energy density. Its forces may be suppressed just below the current bounds by the chameleon-like mimicry, whereby only outer layers of mass distributions, of thickness $1/m_\\phi$, give off appreciable long range forces. After inflation and reheating, the field is relativistic, and attains a Planckian expectation value before Hubble friction freezes it. This can make gravity in space slightly stronger than on Earth. During the matter era, interactions with nonrelativistic matter dig a minimum close to the Planck scale. However, due to its sub-gravitational matter couplings the field will linger away from this minimum until the matter energy density dips below $\\sim 10^{-12} {\\rm eV}^4$. Then it starts to roll to the minimum, driving a period of cosmic acceleration. Among the signatures of this scenario may be dark energy equation of state $w \\ne -1$, stronger gravity in dilute mediums, that may influence BBN and appear as an excess of dark matter, and sub-millimeter corrections to Newton's law, close to the present laboratory limits. ", "machine_abstract": "The cosmological constant is one of the most important parameters in modern physics, and its value has been determined by observations to be extremely small but nonzero.  In this article we will discuss how it can be explained as an effect of quantum gravity at very high energies. We will also show that if the universe underwent inflationary expansion after the Big Bang then there should exist primordial gravitational waves which could have observable effects on the cosmic microwave background radiation (CMBR). Finally, we will argue that these effects may provide us with new ways for testing the predictions of general relativity against those of alternative theories such as string theory or loop quantum gravity. The cosmological constant is one  of the most important parameters of modern physics. Its value was determined by observations to be incredibly small but non-zero. It plays a crucial role in our understanding of the evolution of the Universe since it determines whether the current accelerated expansion of space-time will continue forever or eventually slow down and stop. This question remains open despite many years of research into the nature of dark energy.", "paraphrased_abstract": "During the Big Bang the cosmic tidal waves were seen to act as a sort of cosmic microwave background radiation. Moreover, we have observed the effects of this phenomenon on the cosmic tidal waves, which can be observed in the cosmic microwave background radiation. This question has been a source of great interest for many years. The cosmological constant is a critical determinant of modern physics. It is defined by the observations to be very small but nonzero. This determinant is critical to our understanding of the evolution of the universe; it determines whether the current accelerated expansion of space-time will last forever or stop abruptly. This determinant is extremely important in our understanding of the universe, because it determines whether the accelerated expansion of space-time will last forever or pause forever. This determinant has been identified by the observations as extremely small, but nonzero. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1069, "title": "Low-temperature and high-temperature approximations for penetrable-sphere fluids. Comparison with Monte Carlo simulations and integral equation theories", "abstract": "  The two-body interaction in dilute solutions of polymer chains in good solvents can be modeled by means of effective bounded potentials, the simplest of which being that of penetrable spheres (PSs). In this paper we construct two simple analytical theories for the structural properties of PS fluids: a low-temperature (LT) approximation, that can be seen as an extension to PSs of the well-known solution of the Percus-Yevick (PY) equation for hard spheres, and a high-temperature (HT) approximation based on the exact asymptotic behavior in the limit of infinite temperature. Monte Carlo simulations for a wide range of temperatures and densities are performed to assess the validity of both theories. It is found that, despite their simplicity, the HT and LT approximations exhibit a fair agreement with the simulation data within their respective domains of applicability, so that they complement each other. A comparison with numerical solutions of the PY and the hypernetted-chain approximations is also carried out, the latter showing a very good performance, except inside the core at low temperatures. ", "machine_abstract": "We present low-temperature (LT) and high-temperature (HT) expansions for the excess internal energy, entropy, pressure, and chemical potential of hard-core penetrable-sphere (HCP-S) fluids in terms of the packing fraction $\\eta$. The LT expansion is obtained by using an exact expression for the HCP-S pair correlation function at contact to calculate the virial coefficients up to order $(\\eta^{10})$. We show that this approach yields results which are accurate over a wide range of temperatures and densities when compared against computer simulation data. In addition, we compare our results with those predicted by various integral equations including the Percus-Yevick approximation, the hypernetted chain closure, and the mean spherical approximation. Finally, we use the HT expansion to obtain analytical expressions for the thermodynamic properties of HCP-S fluids near their critical points.  I. INTRODUCTORY REMARkS The study of simple model systems has played an important role in understanding many aspects of condensed matter physics [1] . One such system consists of particles interacting via repulsive potentials whose cores can be penetrated [2] , i.e., they do not have any finite minimum separation distance between them. This class of models includes both soft spheres [3] as well as hard-core penetrable-spheres [4] . In recent years there has been renewed interest in studying these types of interactions due to their relevance in describing colloidal suspensions [5] , granular materials [6] , and protein solutions [7, 8] . For example, it was shown recently [9] that the effective interaction between two proteins immersed in water can be accurately described by a hard-core repulsion plus a short-range attraction. It should also be noted that the concept of \"penetrability\" may be used to describe other physical phenomena besides inter-particle interactions [10] .", "paraphrased_abstract": "The study of such simple models has played a significant role in the study of many aspects of condensed matter physics. One such example is that the interacting particles of a certain particle have their cores penetrated, i.e., there is no finite minimum separation between them. One of these is a soft sphere, and the other is a hard sphere. We consider the two spheres as soft and hard, and in general we consider the three spheres as hard and soft. Besides, we consider the relationship between the two spheres, the temperature and the density, the temperature and the pressure, the chemical potential, the pressure and the chemical potential of the HCP-S fluid, and the thermodynamic properties of these fluids at critical points., and, if we consider the elasticity of the sphere, it is a hard sphere. This type is called a hard sphere, and the hardness is infinite, so that the distance between them is not fixed. During the last years, much interest has been shown in this field of study, because they are of importance to the description of fluids, granular materials, and granular solutions, and they have been shown to be useful in the study of many physical phenomena. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1352, "title": "Energy dependence of jet transport parameter and parton saturation in quark-gluon plasma", "abstract": "  We study the evolution and saturation of the gluon distribution function in the quark-gluon plasma as probed by a propagating parton and its effect on the computation of jet quenching or transport parameter $\\hat{q}$. For thermal partons, the saturation scale $Q^2_s$ is found to be proportional to the Debye screening mass $\\mu_D^2$. For hard probes, evolution at small $x=Q^2_s/6ET$ leads to jet energy dependence of $\\hat{q}$. We study this dependence for both a conformal gauge theory in weak and strong coupling limit and for (pure gluon) QCD. The energy dependence can be used to extract the shear viscosity $\\eta$ of the medium since $\\eta$ can be related to the transport parameter for thermal partons in a transport description. We also derive upper bounds on the transport parameter for both energetic and thermal partons. The later leads to a lower bound on shear viscosity-to-entropy density ratio which is consistent with the conjectured lower bound $\\eta/s\\geq 1/4\\pi$. We also discuss the implications on the study of jet quenching at the BNL Relativistic Heavy Ion Collider and the CERN Large Hadron Collider and the bulk properties of the dense matter. ", "machine_abstract": "We study the energy dependence of jet transport coefficient $\\kappa$ and parton saturation scale $Q_s$ in strongly coupled N = 4 supersymmetric Yang-Mills theory (SYM) at finite temperature using AdS/CFT correspondence. We find that both quantities exhibit power law behavior with respect to the temperature, i.e., $\\kappa \\sim T^{-2/3}$ and $Q_s\\sim T^{-1/2}$. The results are consistent with those obtained by lattice QCD simulations for hot nuclear matter produced in heavy ion collisions. In addition, we show that the ratio between these two scales is independent of temperature as expected from the dimensional analysis. Finally, we discuss possible implications on the phenomenology of relativistic heavy-ion collisions. PACS: 05.45.Mt, 11.10.Wx, 12.38.Gc, 14.20.Dh, 25 .75.-q", "paraphrased_abstract": "q.75. . . - q. - q. - q. .          .         -  - ;        -. The results are compared with the results of the QCD in a supersymmetric Yang-Mills theory, and their relation to the temperature, which is i.e.       -, i.e.   -         - ;     - 2;    - 2;    - 2 ;           -  - 1; ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0668, "title": "J/psi Production in an Equilibrating Partonic System", "abstract": "  Any color singlet or octet ccbar pair is created at short distances and then expands to a full size of J/psi. Such a dynamical evolution process is included here in calculations for the J/psi number distribution as a function of transverse momentum and rapidity in central Au-Au collisions at both RHIC and LHC energies. The ccbar pairs are produced in the initial collision and in the partonic system during the prethermal and thermal stages through the partonic channels ab to ccbar [{2S+1}L_J] and ab to ccbar [{2S+1}L_J]x, and then they dissociate in the latter two stages. Dissociation of ccbar in the medium occurs via two reactions: (a) color singlet ccbar plus a gluon turns to color octet ccbar, (b) color octet ccbar plus a gluon persists as color octet. There are modest yields of ccbar in the prethermal stage at RHIC energy and through the reactions ab to ccbar [{2S+1}L_J] at LHC energy for partons with large average momentum in the prethermal stage at both collider energies and in the thermal stage at LHC energy. Production from the partonic system competes with the suppression of the initial yield in the deconfined medium. Consequently, a bulge within -1.5<y<1.5 has been found for the J/psi number distribution and the ratio of J/psi number distributions for Au-Au collisions to nucleon-nucleon collisions. This bulge is caused by the partonic system and is thus an indicator of a deconfined partonic medium. Based on this result we suggest the rapidity region worth measuring in future experiments at RHIC and LHC to be -3<y<3. ", "machine_abstract": "We present results on J/psi production at midrapidity (y = 0) and forward rapidity (y = 3.2), using the PHENIX detector, for Au+Au collisions at sqrt(s_NN) = 130 GeV. The data are compared to theoretical calculations based on perturbative QCD with different sets of parton distribution functions. We find that none of these models can describe both datasets simultaneously.  These results suggest that there is significant medium-induced gluon radiation which affects the fragmentation function of quarks into hadrons containing charm or bottom quarks. This effect may be related to the observed suppression of high-pT particles produced by heavy flavor quarks. PACS numbers: 25.75.-q, 11.10.Kk, 12.38.Mh  I . Introduction   Heavy quarkonia such as J/ psi , \\Psi_c$, and $\\Psi_b$ have been proposed as probes of the hot dense matter created in relativistic nuclear collisions [1] . In particular, it has been suggested that their yields could be suppressed due to color screening effects [2] and/or Debye mass effects [3] in this medium. However, recent measurements [4] show no evidence for such a suppression. Instead, they indicate that the yield increases slightly with centrality [5] . In order to understand how heavy quarks fragment in the presence of a thermalized medium [6] , we study J/psi production in Au+Au collisions at sNN= 130 GeV. Measurements were performed with the PHENIX experiment [7] at RHIC [8] .  II. Experiment  A. Detector Description The PHENIX detector consists of four main subsystems: two spectrometers covering the pseudorapidity range |\u03b7| < 0.35 [9] ; one electromagnetic calorimeter covering -1< \u03b7 < 1 [10] ; and one muon identifier covering -2.2< \u03b7 < 2.4 [11] . A detailed description of each subdetector system can be found elsewhere [12] .  Figure 1 shows a schematic view of the PHENIX detector.  B. Data Analysis J/psi mes", "paraphrased_abstract": "The experiment was performed at RHIC. There is a schematic representation of the instrument: X. The PHENIX detector was divided into four subsystems: two spectrometers with the pseudorapidity 0.30 [5], one electrochronometer 0.30 [6], and one muon identification system 2.4 [7]. There is also a description of the detectors in the other sections of the instrument: X. PHENIX detector; Figure 1. The PHENIX detector; the PHENIX detector; and a series of experiments on the PHENIX detector, in RHIC, for which PHENIX  a 100 100  100105 GeV was analyzed. In our experiments we investigate the gluon-radiated synthesis of the heavy particles in the presence of the thermal medium, as in the case of the heat-stressed, cosmopolitan and relativistic irradiation of the gluons, and - if there are any color-screened, and - for debye mass in the medium - - it may be due to the suppression of the crystalline ions in the irradiated irradiated particles by the synthesis of the atoms with the top or bottom qu", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0057, "title": "Fifteen Years of High-Resolution Radio Imaging of Supernova 1987A", "abstract": "  Supernova 1987A in the Large Magellanic Cloud provides a spectacularly detailed view of the aftermath of a core-collapse explosion. The supernova ejecta initially coasted outward at more than 10% of the speed of light, but in 1990 were observed to decelerate rapidly as they began to encounter dense circumstellar material expelled by the progenitor star. The resulting shock has subsequently produced steadily brightening radio synchrotron emission, which is resolved by the Australia Telescope Compact Array (ATCA) into an expanding limb-brightened shell. Here we present 15 years of ATCA imaging of Supernova 1987A, at an effective angular resolution of 0.4 arcsec. We find that the radio remnant has accelerated in its expansion over this period, from approx 3600 km/s in 1992 to approx 5200 km/s at the end of 2006. The published diameters of the evolving X-ray shell have been ~15% smaller than the corresponding radio values, but a simultaneous Fourier analysis of both radio and X-ray data eliminates this discrepancy, and yields a current diameter for the shell in both wave-bands of approx 1.7 arcsec. An asymmetric brightness distribution is seen in radio images at all ATCA epochs: the eastern and western rims have higher fluxes than the northern and southern regions, indicating that most of the radio emission comes from the equatorial plane of the system, where the progenitor star's circumstellar wind is thought to be densest. The eastern lobe is brighter than and further from the supernova site than the western lobe, suggesting an additional asymmetry in the initial distribution of supernova ejecta. ", "machine_abstract": "Supernova remnants are the most energetic phenomena in our universe, and their study is important for understanding how stars die.  The supernova remnant Cassiopeia A (CasA) was discovered by radio astronomers in 1974.   In this talk I will describe recent results on CasA obtained with the Very Large Array (VLA), which has been observing it since its first light in 1980.  These observations have revealed many new details about the explosion that created CasA more than 30 years ago.  They also provide an opportunity to test models of core-collapse supernovae against data taken at very early times after the explosion. This work was supported by NSF grant AST-0907860. Keywords: Supernova Remnant; Core-Collapse Supernovae; VLA; Cassiopeia A. URL: http://arxiv.org/abs/astro-ph/0504070", "paraphrased_abstract": "It is an important research topic for the study of the explosion of CasA, which was observed in the VLA at first light in 1980. It is an observation with which the study of the supernovae has been carried out and is called the \u2018Recently Discovered Supernova\u2019. Its origin was found in CasA in 1974. In this article I will present recent observations of CasA, made with the VLA, which has been studying it since its first light in 1980. These observations provide a great deal of information about the explosion that produced CasA more than thirty years ago. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0682, "title": "Mapping the circumstellar SiO maser emission in R Leo", "abstract": "  The study of the innermost circumstellar layers around AGB stars is crucial to understand how these envelopes are formed and evolve. The SiO maser emission occurs at a few stellar radii from the central star, providing direct information on the stellar pulsation and on the chemical and physical properties of these regions. Our data also shed light on several aspects of the SiO maser pumping theory that are not well understood yet. We aim to determine} the relative spatial distribution of the 43 GHz and 86 GHz SiO maser lines in the oxygen-rich evolved star R Leo. We have imaged with milliarcsecond resolution, by means of Very Long Baseline Interferometry, the 43 GHz (28SiO v=1, 2 J=1-0 and 29SiO v=0 J=1-0) and 86 GHz (28SiO v=1 J=2-1 and 29SiO v=0 J=2-1) masing regions. We confirm previous results obtained in other oxygen-rich envelopes. In particular, when comparing the 43 GHz emitting regions, the 28SiO v=2 transition is produced in an inner layer, closer to the central star. On the other hand, the 86 GHz line arises in a clearly farther shell. We have also mapped for the first time the 29SiO v=0 J=1-0 emission in R Leo. The already reported discrepancy between the observed distributions of the different maser lines and the theoretical predictions is also found in R Leo. ", "machine_abstract": "We present new maps of the circumstellar SiO (v=1, v=2) masers around the Mira variable R Leo obtained with the Very Long Baseline Array at 22 GHz and 43 GHz. The observations were carried out on September 24th 2004 using all ten antennas available for VLBA operation during that time period. We detect two distinct groups of masers: one group is located near the star's position as determined by optical astrometry; another group lies about 0.5 arcsec to the south-west of this location. Both groups are associated with an extended bipolar structure seen in previous single-dish observations. This structure has been interpreted as a shell-like envelope surrounding the central star. Our results show that both groups of masers trace different parts of this shell-like structure. In addition we find evidence for a third component which may be related to the presence of a companion object.     Keywords: Masers", "paraphrased_abstract": ", we found a second maser. We reconstructed the structure of the circumstellar SiO masers around the Mira variable R Leo. The observations took place on the 24th of September 2004 on all the antennas used by VLBA. We show the two masers that have a bipolar pattern, which we have described earlier as a shell-like envelope around the central star. In addition, we show evidence of a third part of the shell structure that may be related to the presence of a companion object. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1544, "title": "Pseudogap and charge density waves in two dimensions", "abstract": "  An interaction between electrons and lattice vibrations (phonons) results in two fundamental quantum phenomena in solids: in three dimensions it can turn a metal into a superconductor whereas in one dimension it can turn a metal into an insulator. In two dimensions (2D) both superconductivity and charge-density waves (CDW) are believed to be anomalous. In superconducting cuprates, critical transition temperatures are unusually high and the energy gap may stay unclosed even above these temperatures (pseudogap). In CDW-bearing dichalcogenides the resistivity below the transition can decrease with temperature even faster than in the normal phase and a basic prerequisite for the CDW, the favourable nesting conditions (when some sections of the Fermi surface appear shifted by the same vector), seems to be absent. Notwithstanding the existence of alternatives to conventional theories, both phenomena in 2D still remain the most fascinating puzzles in condensed matter physics. Using the latest developments in high-resolution angle-resolved photoemission spectroscopy (ARPES) here we show that the normal-state pseudogap also exists in one of the most studied 2D examples, dichalcogenide 2H-TaSe2, and the formation of CDW is driven by a conventional nesting instability, which is masked by the pseudogap. Our findings reconcile and explain a number of unusual, as previously believed, experimental responses as well as disprove many alternative theoretical approaches. The magnitude, character and anisotropy of the 2D-CDW pseudogap are intriguingly similar to those seen in superconducting cuprates. ", "machine_abstract": "We study the competition between pseudogap formation, CDW ordering, and superconductivity within an extended Hubbard model on square lattice using DMFT+ED method. We find that at low temperatures there is no phase separation but instead a coexistence region where both phases are present with different weights depending on doping concentration. The results show that the pseudogap opens up around half-filling for all values of U studied here (U = 2 \u2212 8t). At higher dopings we observe a dome-like structure which becomes more pronounced as U increases. In addition to this feature, we also see another peak in DOS near the Fermi level which corresponds to the onset of CDW order. This second peak disappears when temperature is increased or doping decreases. Finally, we have calculated the superfluid weight $\\Delta_{s}^{2}$ by solving linearized Eliashberg equations self-consistently. Our results indicate that the maximum value of $\\Delta_{s}^{2}$ occurs close to optimal doping.", "paraphrased_abstract": "The results showed that the pseudogap was shaped by a slight overhanging at all the values of U, i.e. 2  8. Then the diltiometry was calculated and the concentration determined by linearization of Eliashberg equations. In addition, we found a second peak of DOS, near the Fermi level, which indicates the beginning of CDW order, and disappears with increasing temperatures. In conclusion, the superfluid value Delta  2  8, which corresponds to the height of the CDW order, was calculated by the linearized equations of Eliashberg. This result showed that the pseudogap was half full for all values of U, which was 28t. We also studied the competition between pseudogap and superconductivity in an extended Hubbard model, a symmetrical ring, whose square axis was in constant rotation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0632, "title": "Detecting and Characterizing Planetary Systems with Transit Timing", "abstract": "  In the coming decades, research in extrasolar planets aims to advance two goals: 1) detecting and characterizing low-mass planets increasingly similar to the Earth, and 2) improving our understanding of planet formation. We present a new planet detection method that is capable of making large advances towards both of these objectives and describe a modest network of telescopes that is able to make the requisite observations. In a system where a known planet transits its host star, a second planet in that system will cause the time between transits to vary. These transit timing variations can be used to infer the orbital elements and mass of the perturbing planet even if it has a mass that is smaller than the mass of the Earth. This detection technique complements other techniques because it is most sensitive in mean-motion resonances where, due to degeneracies, other techniques have reduced sensitivity. Small ground-based observatories have already exceeded the photometric precision necessary to detect sub-Earth mass planets. However, TTV planet searches are currently limited by the relatively small number of high-precision transit data and insufficient observing time on existing telescopes. These issues will be compounded as the number of known transiting planets suitable for TTV study will increase substantially in the near future. A relatively modest investment in a ground-based network of small ($\\sim 0.5 {\\rm m}$ telescopes could provide the needed coverage and so dramatically increase the effectiveness of transit timing observations. ", "machine_abstract": "Transit timing is an important tool for detecting planets in multi-planet systems, as well as characterizing their orbits. In this work we present the first transit timing analysis of the Kepler data using the method developed by Agol et al. (2005) to detect additional bodies in planetary systems. We find that there are two new candidate planet pairs around KOI-284 (Kepler Object of Interest 284), which has been previously identified as having three transiting planets. The best-fitting model consists of four planets orbiting one star with periods ranging between 1.5 days and 12 years. This system shows strong evidence for orbital eccentricity among its planets. We also report on another system containing five planets, KOI-377, where our results show that it contains at least six planets. Finally, we discuss how these newly discovered systems can be used to test theories about formation and evolution of planetary systems. Transits provide information not only about the radius but also the mass of exoplanets. However, most known transiting planets have been found through photometric surveys such as Kepler or CoRoT, which do not allow precise measurements of the masses due to limited precision of the light curves. On the other hand, radial velocity surveys usually measure the mass more precisely than photometry does, but they cannot determine whether a given object transits because they lack phase coverage near inferior conjunction. Therefore, combining both techniques allows us to obtain accurate values of the radii and masses of transiting planets.     Here we present the discovery of two new multi-planet systems based on combined photometric and spectroscopic observations made with the HARPS spectrograph attached to the 3.6 m telescope at La Silla Observatory. Both systems contain several super-Earths with short orbital periods. One of them, KOI-284, was already known to host three transiting planets; however, here we demonstrate that it actually harbors four planets. Our results suggest that all four planets have significant orbital eccentricities. Another system, KOI-377, hosts at least six planets.", "paraphrased_abstract": "The first we propose is a model of the four planetary systems in which the Kepler data are taken, which has only two stars and a period of 1.5 days, 12 years, and is capable of detecting the orbital eccentricity of the planetary masses. The radii and masses of the planetary masses are generally more precise than radii, but they do not give a clear idea of the distance between two planetary systems, and they have not a clear view of the phase in which the objects are placed., whereas radii and masses are more exact than radii, and they can only be calculated from the density of the radii and the masses of the planetary masses. The transit time is very useful for detecting the orbital radii and the masses of exoplanets. However, most of the known transits are only known by the spectroscopic studies of Kepler and CoRoT, which do not give accurate measurements of the masses because they are not able to measure the transit because their phase coverage is incongruous with the direction of the light curves. We present the first orbital metric analysis of the Kepler data, based on the method of Agol et al. (2005), to measure the size of the planets. We show that there are two new transiting planets in KOI", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0913, "title": "Photon Dominated Region Modeling of Barnard 68", "abstract": "  We use the Barnard 68 dark globule as a test case for a spherically symmetric PDR model exposed to low-UV radiation fields. With a roughly spherical morphology and an accurately determined density profile, Barnard 68 is ideal for this purpose. The processes governing the energy balance in the cloud surface are studied in detail. We compare the spherically symmetric PDR model by Stoerzer, Stutzki & Sternberg (1996) to observations of the three lowest rotational transitions of 12CO, 13CO J = 2-1 and J = 3-2 as well as the [CI] 3P_1-3P_0 fine structure transition. We study the role of Polycyclic Aromatic Hydrocarbons (PAHs) in the chemical network of the PDR model and consider the impact of depletion as well as of a variation of the external FUV field. We find it difficult to simultaneously model the observed 12CO and 13CO emission. The 12CO and [CI] emission can be explained by a PDR model with a external FUV field of 1-0.75 chi_0, but this model fails to reproduce the observed 13CO by a factor of ~2. Adding PAHs to the chemical network increases the [CI] emission by 50% in our model but makes [CII] very faint. CO depletion only slightly reduces the 12CO and 13CO line intensity (by <10% and <20%, respectively). Predictions for the [CII] 2P_3/2-2P_1/2, [CI] 3P_2-3P_1 and 12CO J= 5-4 and 4-3 transitions are presented. This allows a test of our model with future observations (APEX, NANTEN2, HERSCHEL, SOFIA). ", "machine_abstract": "We present new near-infrared (NIR) observations and modeling results for the young stellar object, Barnard 68 (B68). The NIR data were obtained with the Gemini Near-Infrared Spectrograph on the 8-meter Gemini North telescope in Hawaii during two nights in December 2005. We used these data to construct an SED model that includes both photospheric emission and dust continuum emission. Our best-fit model suggests that B68 is surrounded by a dense core with a mass of 0.1 M_solar_(M_solar = 1.99 x 10^30 kg), which has been heated up to about 100 K due to the central star's radiation. This temperature corresponds to a luminosity of 3 L_sun_(L_sun = 3.84 x 10^26 W). In addition, we found that there are at least three other sources within the field-of-view of our observation whose fluxes contribute significantly to the total observed flux density.", "paraphrased_abstract": "He is a young star, the star Barnard 68, a young object. The new data and the modeling results of the Near-Infrared Spectrometer were taken from the 8-meter Gemini North telescope in Hawaii, and a single ray of light from it was taken. Our model was based on a partial differential equation, and found that the central star had a radiance of 1 molars, a mass of 0.1 Molars, or 0.9 x 10 g. This temperature corresponds to a luminosity of three li. (Li = 3.84 x 10 w). Our model proved that B68 had a dense core, about 0.1 Molar (0.3 x 10 g), surrounded by a mass of 0.1 Molar (0.3 x 10 g), and which was heated to a temperature of a hundred K, due to the central star's radiation, to a temperature of about 100 K, which corresponds to a luminosity of three L_suns. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1676, "title": "Personalizing Image Search Results on Flickr", "abstract": "  The social media site Flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. Flickr offers multiple ways of browsing or searching it. One option is tag search, which returns all images tagged with a specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. We show how to exploit this metadata to personalize search results for the user, thereby improving search performance. First, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. Secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. The users' interests can similarly be described by the tags they used for annotating their images. The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user. ", "machine_abstract": "We present an approach to personalize image search results in the context of social networks, by exploiting user-generated tags and ratings. We propose a novel method for ranking images based on their relevance with respect to users' interests as well as their popularity among other users. Our experiments show that our proposed algorithm outperforms state-of-the-art approaches both quantitatively (in terms of precision) and qualitatively (by showing more relevant images). In addition, we demonstrate how this new ranking can be used to improve the performance of existing applications such as tag recommendation systems or photo sharing services. Social media has become one of the most popular ways people share information online. One of its main advantages is that it allows users to easily connect with each other through shared content. For example, Facebook currently hosts over 1 billion photos uploaded by its members [1] . Similarly, Flickr [2] , which was launched in 2002, now contains more than 100 million public photos [3] . In these platforms, users are able to upload pictures they have taken themselves or find interesting ones from others. They also provide mechanisms to annotate them using free-form text labels called \"tags\" [4] . These tags allow users to describe what is depicted in the picture, but may not always accurately reflect the user's true interest [5] . As a result, when searching for specific topics, users often get irrelevant results [6] . To address this problem, researchers have recently started exploring techniques to rank images according to their relevance to users' interests [7, 8] . However, there still exists significant room for improvement [9] .", "paraphrased_abstract": "And in this way, when people are looking for something, they find it difficult to find it. For example, Facebook has a social media network, which contains more than one billion photos, and Flickr has over 100 million pictures. They are also able to add tags and comments to the pictures to describe the object of the pictures, which, however, do not always express the true interest of the user. This is the reason why, in the course of a search, it is often impossible to find something of the subject. In order to overcome this problem, research has recently been conducted to determine what kinds of images are relevant to the interest of the users. We present an approach to personalizing the search results of social networks by using the tags and ratings of users, which are automatically created by the users, in order to improve the quality of the results. We have developed a method to rank images by their relevancy, according to the interest of the users, and to calculate the popularity of the images. We have shown that our method outperforms the previous one in both quantitative accuracy and qualitative accuracy, which showed that the images were more relevant. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0162, "title": "Estimation of experimental data redundancy and related statistics", "abstract": "  Redundancy of experimental data is the basic statistic from which the complexity of a natural phenomenon and the proper number of experiments needed for its exploration can be estimated. The redundancy is expressed by the entropy of information pertaining to the probability density function of experimental variables. Since the calculation of entropy is inconvenient due to integration over a range of variables, an approximate expression for redundancy is derived that includes only a sum over the set of experimental data about these variables. The approximation makes feasible an efficient estimation of the redundancy of data along with the related experimental information and information cost function. From the experimental information the complexity of the phenomenon can be simply estimated, while the proper number of experiments needed for its exploration can be determined from the minimum of the cost function. The performance of the approximate estimation of these statistics is demonstrated on two-dimensional normally distributed random data. ", "machine_abstract": "The problem is to estimate the number of independent measurements in an experiment, given that some statistical properties are known for each measurement. The method proposed here uses only information about the mean value and variance of the distribution of results obtained by repeated measurements on one sample (or several samples). It can be used as a tool for planning experiments with minimal error or for estimating the accuracy of existing experimental data. This article presents a new approach to this problem based on the concept of entropy. In particular, it shows how to calculate the mutual information between two random variables using their probability density functions. A numerical example illustrates the application of these methods.     Keywords: Redundancy estimation, Entropy, Mutual Information, Experiment Planning     1 Introduction     When designing an experiment, we need to know what kind of precision our measuring instrument will give us. If we want to measure something precisely enough, then we should make sure that there is no correlation between successive measurements made on the same object [1] . For example, if we have a device which measures the temperature of water at room temperature T = 20 \u00b0C , then we would like to obtain values close to 20 \u00b1 0.1\u00b0C when repeating the measurement many times [2] .   In practice, however, such repeatability cannot always be achieved because of various factors affecting the measurement process [3] . Therefore, before starting any research work, you must determine whether your measuring equipment meets all requirements [4] .     2 Problem statement     Let X be a continuous random variable describing the result of a single measurement performed under certain conditions [5] . We assume that the distribution function F(x) of X has been determined experimentally [6] . Then the question arises - how many independent measurements do we need to perform so that the average deviation of the measured values does not exceed a specified threshold ?", "paraphrased_abstract": "So it\u2019s a question of how many independent measurements are needed in an experiment. Then we\u2019ll need to know whether the number of measurements is enough, or how many there are. We can use it to estimate the amount of error, or how accurate the experiment is. The article presents a new approach to this problem, based on the concept of entropy, and shows how to calculate the number of independent measurements, as a whole, by analyzing their relative value. To begin with, we need to know whether the instrument we are using is the correct one. In other words, if we are measuring the temperature of water at a certain temperature, we would like to measure it to be twenty degrees, and that the measurement should not exceed 20 0.1\u00b0C. This, however, cannot be accomplished without some factors which may affect the measurement. This is a problem for the experimentalist, since it must be able to determine how accurate it is. This is the main problem, and we are dealing with the calculation of the number of independent measurements in an experiment, for there is no general rule for this, and it is very important to know the ratio of the mean and the variance of the two measurements. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.031, "title": "VLBI observations of nineteen GHz-Peaked-Spectrum radio sources at 1.6 GHz", "abstract": "  Aims and Methods: We present the results of VLBI observations of nineteen GHz-Peaked-Spectrum (GPS) radio sources at 1.6 GHz. Of them, 15 sources are selected from the Parkes Half Jansky (PHJ) sample (Snellen 2002), 4 others are from our previous observation list. We aimed at imaging the structure of GPS sources, searching for Compact Symmetric Objects (CSOs) and studying the absorption for the convex radio spectra of GPS sources.   Results: We obtained total intensity 1.6 GHz VLBI images of 17 sources for the first time. Of them, 80% show mini-double-lobe radio structure, indicating that they are CSOs or candidates, and their host AGNs could be edge-on to us. This result suggests that there is a high incidence of mini double-lobe sources (or CSOs) in the PHJ sample. The sources J0323+0534, J1135-0021, J1352+0232, J2058+0540, J2123-0112 and J2325-0344 with measured redshift, showing double-lobe structure with sizes of <1 kpc, are classified as CSOs. Three sources J1057+0012, J1600-0037 and J1753+2750 are considered as core-jet sources according to their morphologies and flux variability. ", "machine_abstract": "We present the results of Very Long Baseline Interferometry (VLBI) observations made with the European VLBI Network (EVN) and the NRAO Very Large Array (VLA). The sample consists of 19 GPS radio sources selected on the basis that they have flat or inverted spectra between 5 and 8GHz, but steep spectra above 10GHz. We find that all these objects are compact in our EVN images; their sizes range from 0.1 to 2 mas. In addition we detect one source which is resolved into two components separated by about 3mas. This object has been previously detected as an extended source using VLA data alone. Our results show that most GPS sources appear unresolved even when observed with baselines up to 6000 km. However there appears to be some evidence for structural evolution within this class of objects. These results will help us understand how GPS sources evolve over time.     Keywords: Radio astronomy", "paraphrased_abstract": "This consists of 19 GPS radio sources, chosen by us, on the basis of their flat or inverted spectra, but above 10 spectra, which are quite steep. All of them are small in the EVN image; their diameters range from 0.1 to 2 meters; and, above all, there is a source, which has a radiance of less than 3 meters, which has been found previously as an extended source, using the EVN data alone. We present the results of a survey of the very long baseline of radiographers, of which all are resolvable, but the last, up to 6000 km, has not been resolvable, although it seems to show some kind of evolution in the shape of the source. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0706, "title": "Intervening Metal Systems in GRB and QSO sight-lines: The Mgii and Civ Question", "abstract": "  Prochter et al. 2006 recently found that the number density of strong intervening 0.5<z<2 MgII absorbers detected in gamma-ray burst (GRB) afterglow spectra is nearly 4 times larger than in QSO spectra. We have conducted a similar study using CIV absorbers. Our CIV sample, consisting of a total of 20 systems, is drawn from 3 high resolution and high to moderate S/N VLT/UVES spectra of 3 long-duration GRB afterglows, covering the redshift interval 1.6< z<3.1. The column density distribution and number density of this sample do not show any statistical difference with the same quantities measured in QSO spectra. We discuss several possibilities for the discrepancy between CIV and MgII absorbers and conclude that a higher dust extinction in the MgII QSO samples studied up to now would give the most straightforward solution. However, this effect is only important for the strong MgII absorbers. Regardless of the reasons for this discrepancy, this result confirms once more that GRBs can be used to detect a side of the universe that was unknown before, not necessarily connected with GRBs themselves, providing an alternative and fundamental investigative tool of the cosmic evolution of the universe. ", "machine_abstract": "We present new observations of intervening metal systems at z ~ 1.5-2.0 towards the gamma-ray burst (GRB) 050525A, which is one of only two GRBs with spectroscopic redshifts known to date. We detect absorption lines due to Mg ii \u03bb\u03bb2796, 2803, Feii \u03bb2382, 2344+2600, Ciii \u03bb977, Siiv \u03bb1394, 1403, Ovi \u03bb1032, 1038, Nv \u03bb1239, 1243, Ly\u03b1, and Ly\u03b2 associated with an absorber at z = 2.01 \u00b1 0.02. This system has log NHI/cm\u22122 = 19.6 +0.2 \u22120.1 , corresponding to a total hydrogen column density of 5 \u00d7 1020 cm\u22122 . It also shows strong low-ionization transitions such as Al iii \u03bb1854, 1854 + 1862, and S iv \u03bb1063, 1073 that are not seen in typical high-redshift absorbers. These features suggest that this absorber may be similar to those found along quasar sightlines.", "paraphrased_abstract": "\u201cSuch a system exhibits high-ionization transitions, such as Al ii 1854, 1854, 1862, and S ii 1063, 1073, which are not present in other high-resistance absorbing systems. In this system we have also observed low-ionization transitions like Al iii 1854, 1854, 1862, and S ii 1063, 1073, which are not present in many high-resistance absorbing systems. We present two new observations of crystalline metals at z = 1.5-2.0 toward GRB 050525A, one of the only two crystalline structures known to date. The absorption lines of Mg ii 2796, 2803, Feii ii 2382, 2344, 2600, Ciii ii 977, Sii iii iii iii iii iii, ovi iii, Ovi iii, iii, iii, iii, iii, iii, iii, iii, iii, iii, iii, iii, iii, iii, iii", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2279, "title": "Quantum spin liquid states in the two dimensional kagome antiferromagnets, ZnxCu4-x(OD)6Cl2", "abstract": "  A three-dimensional system of interacting spins typically develops static long-range order when it is cooled. If the spins are quantum (S = 1/2), however, novel quantum paramagnetic states may appear. The most highly sought state among them is the resonating valence bond (RVB) state in which every pair of neighboring quantum spins form entangled spin singlets (valence bonds) and the singlets are quantum mechanically resonating amongst all the possible highly degenerate pairing states. Here we provide experimental evidence for such quantum paramagnetic states existing in frustrated antiferromagnets, ZnxCu4-x(OD)6Cl2, where the S = 1/2 magnetic Cu2+ moments form layers of a two-dimensional kagome lattice. We find that in Cu4(OD)6Cl2, where distorted kagome planes are weakly coupled to each other, a dispersionless excitation mode appears in the magnetic excitation spectrum below ~ 20 K, whose characteristics resemble those of quantum spin singlets in a solid state, known as a valence bond solid (VBS), that breaks translational symmetry. Doping nonmagnetic Zn2+ ions reduces the distortion of the kagome lattice, and weakens the interplane coupling but also dilutes the magnetic occupancy of the kagome lattice. The VBS state is suppressed and for ZnCu3(OD)6Cl2 where the kagome planes are undistorted and 90% occupied by the Cu2+ ions, the low energy spin fluctuations in the spin liquid phase become featureless. ", "machine_abstract": "We report on neutron scattering experiments performed to investigate magnetic excitations and their dispersion relations in the two-dimensional (2D) frustrated quantum magnets Zn0.8Cu3.2(OD)6Cl2 and Zn1.5Cu2.5(OD)6Cl2 with different Cu2+/Cu3+ ratio. The results show that both compounds are characterized by an energy gap at the zone center which is consistent with theoretical predictions for the 2D Heisenberg model on the kagome lattice. In addition we find evidence for a broad continuum of low-energy excitations extending up to about 5 meV. These findings suggest that these materials may be close to a quantum spin liquid state. We also present high-resolution powder x-ray diffraction data showing that the crystal structure remains unchanged upon deuteration.         Quantum spin liquids are exotic phases of matter where spins remain disordered down to zero temperature due to strong quantum fluctuations. They have been predicted theoretically as ground states of many strongly correlated systems such as doped Mott insulators or geometrically frustrated magnets but so far only few experimental realizations exist. Here we report on neutron scattering investigations of two new candidate materials for this fascinating phase - the deuterated copper-oxalate based salts Zn0.8Cu2.9(OD)6Cl2 (Zn0.8Cu3O2Cl2\u00b72H2O) and Zn1.5-Cu1.5(OD) 6Cl2 (Zn1.5Cu2O3Cl2\u00b72H2O). Both compounds crystallize in the same monoclinic space group C2/c as previously reported for the non-deuterated parent compound ZnCu3(OH)6Cl2. However, they differ significantly in the amount of Cu2+ ions replaced by Cu3+ leading to changes in the local environment around each Cu ion. This has important consequences for the exchange interactions between neighboring spins resulting in significant differences in the observed magnetic properties. While Zn0.8Cu 3.2(OD) 6Cl2 shows no sign of long-range order down to 1.7 K it exhibits", "paraphrased_abstract": "The two new phosphorus salts are reconstituted in the same monoclinic space, c2/c, as have been shown previously for the non-deuterated parent compound ZnCu3(OH)6Cl2. We now report on the results of the investigation of two new candidates for this fascinating phase: the deuterated copper-oxalate salts Zn0.8Cu2.9(OD)6Cl2 (Zn0.8Cu3O2Cl22H2O) and Zn1.5Cu2.5(OD)6Cl2 with a different Cu2+/Cu3+ ratio. We also present the high-resolution powder x-ray diffraction data, showing that the crystal structure remains unchanged after deuteration. We show here two new candidates for this intriguing phase: the deuterated copper-oxalate salts Zn0.8Cu2.9(OD)6Cl2 (Zn0.8Cu3O22H2O2) and the new zinc-oxalate salts Zn0.8Cu2.9(OD)6Cl2 (Zn0.8Cu3O22H2O2), both of which crystallize in the same monoclinic space group C2/c, as previously reported, and Zn1.5(OD)6Cl2, Zn1.5(OD)6Cl2,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2571, "title": "Extraction of Freshwater and Energy from Atmosphere", "abstract": "  Author offers and researches a new, cheap method for the extraction of freshwater from the Earth atmosphere. The suggected method is fundamentally dictinct from all existing methods that extract freshwater from air. All other industrial methods extract water from a saline water source (in most cases from seawater). This new method may be used at any point in the Earth except Polar Zones. It does not require long-distance freshwater transportation. If seawater is not utilized for increasing its productivity, this inexpensive new method is very environment-friendly. The author method has two working versions: (1) the first variant the warm (hot) atmospheric air is lifted by the inflatable tube in a high altitude and atmospheric steam is condenced into freswater: (2) in the second version, the warm air is pumped 20-30 meters under the sea-surface. In the first version, wind and solar heating of air are used for causing air flow. In version (2) wind and propeller are used for causing air movment.   The first method does not need energy, the second needs a small amount. Moreover, in variant (1) the freshwater has a high pressure (>30 or more atm.) and can be used for production of energy such as electricity and in that way the freshwater cost is lower. For increasing the productivity the seawater is injected into air and solar air heater may be used. The solar air heater produces a huge amount of electricity as a very powerful electricity generation plant. The offered electricity installation in 100 - 200 times cheaper than any common electric plant of equivalent output.   Key words: Extraction freshwater, method of getting freshwater, receiving energy from atmosphere, powerful renewal electric plant. ", "machine_abstract": "The extraction of fresh water and energy from the atmosphere is proposed as an alternative to conventional sources, which are limited in supply or environmentally damaging.  The process involves condensing atmospheric moisture into liquid water using solar power and then collecting this water on a surface coated with hydrophobic materials that allow it to be easily transported by air currents.   This technology could provide clean drinking water for remote communities without requiring large amounts of land area or infrastructure investment. It also has potential applications in agriculture where irrigation can be provided at low cost through the use of wind-driven sprayers. In addition, the collected water may be used directly as fuel if combined with electrolysis cells powered by renewable electricity. The process requires minimal maintenance once installed and would operate continuously over many years. A pilot-scale demonstration system was constructed near Tucson Arizona (USA) during 2011-2013. The results show that the system produces up to 1 gallon per day of potable water under favorable conditions.", "paraphrased_abstract": "The experiment was conducted in a remote region of Arizona, in the year 2011 and 2013 in the year 2013. It proved to be capable of producing at least a gallon per day of potable water. The system, which could be installed on a small scale in the desert, was a pilot project in the USA. The water from the atmosphere is to be distilled into liquid water using solar energy, and it is collected on a surface covered with hydrophobic material. It is easy to carry by the air. It is also useful for irrigation in remote areas, without requiring large land and infrastructure. It is a way of obtaining clean drinking water without requiring extensive excavation or capital. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3257, "title": "Millimeter dust continuum emission unveiling the true mass of giant molecular clouds in the Small Magellanic Cloud", "abstract": "  CO observations have been so far the best way to trace molecular gas in external galaxies, but at low metallicity the gas mass deduced could be largely underestimated. At present, the kinematic information of CO data cubes are used to estimate virial masses and trace the total mass of the molecular clouds. Millimeter dust emission can also be used as a dense gas tracer and could unveil H2 envelopes lacking CO. These different tracers must be compared in different environments. This study compares virial masses to masses deduced from millimeter emission, in two GMC samples: the local molecular clouds in our Galaxy and their equivalents in the Small Magellanic Cloud (SMC), one of the nearest low metallicity dwarf galaxy. In our Galaxy, mass estimates deduced from millimeter emission are consistent with masses deduced from gamma ray analysis and trace the total mass of the clouds. Virial masses are systematically larger (twice on average) than mass estimates from millimeter dust emission. This difference decreases toward high masses and has already been reported in previous studies. In the SMC however, molecular cloud masses deduced from SIMBA millimeter observations are systematically higher (twice on average for conservative values of the dust to gas ratio and dust emissivity) than the virial masses from SEST CO observations. The observed excess can not be accounted for by any plausible change of dust properties. Taking a general form for the virial theorem, we show that a magnetic field strength of ~15 micro Gauss in SMC clouds could provide additional support to the clouds and explain the difference observed. Masses of SMC molecular clouds have therefore been underestimated so far. Magnetic pressure may contribute significantly to their support. ", "machine_abstract": "We present new observations at 1 mm and 3 mm wavelengths toward two Giant Molecular Clouds (GMC) in the Small Magellanic: Cloud, LMC N11B and 30 Doradus. We use these data to derive accurate masses for both GMCs by fitting their spectral energy distributions with modified blackbody functions. The derived masses are M(LMC N11B) = 2.1 x 10^6 M_sol and M(30 Doradus) = 5.2 x 10^7 M_sol . These values are significantly higher than those obtained using previous methods based on CO line measurements alone. This discrepancy is likely due to an underestimation of the gas temperature when only one or few lines are used as tracers. Our results show that the total cloud masses inferred from millimeter dust continuum emission can be up to three times larger than previously estimated. Millimeter dust continuum emission has been shown to provide more reliable estimates of the total cloud masses compared to other techniques such as single-dish radio observations of carbon monoxide (CO). However, most studies have focused on Galactic Giant Molecular Clouds (GMGs), while little work has been done towards extragalactic GMGs. In this study we present new observations at 1mm and 3mm wavelengths toward two Giant Molecular Clusters (GMCs) in the Small Magellan: Cloud, LMC N 11B and 30 Doradus; we also include archival data taken with the IRAM-30m telescope. Using these data sets we fit the observed SEDs with modified blackbody functions; our analysis shows that the derived masses are M(N11B)=2.1x10^6M_sol and M(Doradus)=5.2x10^7M_sol , which are significantly higher than those reported in previous works.", "paraphrased_abstract": "The results of this study are presented as a new experiment at the wavelength of a millimeter. It is shown that the total mass can be computed by millimeter-scale emission of the gas. The most recent studies have been performed on the ethereal atoms, but they have not been studied on the extragalactic giant atoms. Among the present study we have examined the atoms of two giant atoms of the small sphere, N.11B and 30. The atoms of the small sphere, N.11B, and 30.9 Doradus, were derived. These atoms were much higher than the average, and they were also much higher than the average of the CO lines, which are used for the estimation of temperature. These atoms were calculated by using modified blackbody functions, and the resulting atoms were: N.11B = 2.1 x 106 M., and D.10  5.2 x 107 M. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1024, "title": "Determination of Low-Energy Parameters of Neutron--Proton Scattering on the Basis of Modern Experimental Data from Partial-Wave Analyses", "abstract": "  The triplet and singlet low-energy parameters in the effective-range expansion for neutron--proton scattering are determined by using the latest experimental data on respective phase shifts from the SAID nucleon--nucleon database. The results differ markedly from the analogous parameters obtained on the basis of the phase shifts of the Nijmegen group and contradict the parameter values that are presently used as experimental ones. The values found with the aid of the phase shifts from the SAID nucleon--nucleon database for the total cross section for the scattering of zero-energy neutrons by protons, $\\sigma_{0}=20.426 $b, and the neutron--proton coherent scattering length, $f=-3.755 $fm, agree perfectly with the experimental cross-section values obtained by Houk, $\\sigma_{0}=20.436\\pm 0.023 $b, and experimental scattering-length values obtained by Houk and Wilson, $f=-3.756\\pm 0.009 $fm, but they contradict cross-section values of $\\sigma_{0}=20.491\\pm 0.014 $b according to Dilg and coherent-scattering-length values of $f=-3.7409\\pm 0.0011 $fm according to Koester and Nistler. ", "machine_abstract": "The low-energy parameters of neutron-proton scattering are determined by using modern experimental data obtained in partial-wave analyses (PWAs). The results for the S-wave phase shifts and mixing angles, as well as for the P-wave amplitudes at zero energy, are presented here. It is shown that these values agree with those extracted previously from other experiments within their uncertainties. In addition to this, we present new results for the D-wave amplitude at zero energy which were not available before.  I. INTRODUCTIO N Neutron-proton elastic scattering has been studied extensively over many decades [1] . This process plays an important role in nuclear physics since it provides information about the nucleon-nucleon interaction potential [2] , which can be used to calculate properties of nuclei [3] . In recent years there have been significant advances in our understanding of the structure of the nucleon-nucleus system [4] . These developments include precise measurements of cross sections [5] , polarization observables [6] , spin-correlation coefficients [7] , etc., performed mainly at intermediate energies [8] . However, despite all efforts made so far, some questions remain open [9] . For example, one still needs more accurate determinations of the lowenergy parameters of the nucleonnucleon interaction [10] .", "paraphrased_abstract": "And so far, there have been considerable advances in our understanding of the structure of the nucleus-nucleus system. These measurements, in particular, have been made of the cross-sections, the polarizations, the spin correlation coefficients, etc., in particular at the intermediate energies. There have been many such measurements, and these include the measurement of the S-wave phase and the mixing angle, and of the P-wave amplitudes at zero energy. Moreover, we present new results on the D-wave amplitude, which were not previously known. The low-energy parameters of neutron-proton scattering have been formulated in recent years by means of experimental measurements, and are based on experimental measurements derived from a certain combination of experimental and experimental measurements. This is a major field of physics in the nuclear system, because it provides information about the potential of the nucleon and the nucleon, which can be used to calculate the properties of the nucleon. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.036, "title": "Towards an Explicit Model of D-brane Inflation", "abstract": "  We present a detailed analysis of an explicit model of warped D-brane inflation, incorporating the effects of moduli stabilization. We consider the potential for D3-brane motion in a warped conifold background that includes fluxes and holomorphically-embedded D7-branes involved in moduli stabilization. Although the D7-branes significantly modify the inflaton potential, they do not correct the quadratic term in the potential, and hence do not cause a uniform change in the slow-roll parameter eta. Nevertheless, we present a simple example based on the Kuperstein embedding of D7-branes, z_1=constant, in which the potential can be fine-tuned to be sufficiently flat for inflation. To derive this result, it is essential to incorporate the fact that the compactification volume changes slightly as the D3-brane moves. We stress that the compactification geometry dictates certain relationships among the parameters in the inflaton Lagrangian, and these microscopic constraints impose severe restrictions on the space of possible models. We note that the shape of the final inflaton potential differs from projections given in earlier studies: in configurations where inflation occurs, it does so near an inflection point. Finally, we comment on the difficulty of making precise cosmological predictions in this scenario. This is the companion paper to arXiv:0705.3837. ", "machine_abstract": "We propose to use the open string field theory (OSFT) as a model for inflation in type IIB superstring theory, which is based on the tachyon condensation and its decay into closed strings. We show that this OSFT can be described by a noncommutative geometry with a fuzzy sphere background. The resulting effective action contains higher derivative terms such as R4 term, but it does not contain any ghosts or tachyons. This suggests that our proposal may provide a consistent description of inflationary universe within the framework of string theory. In addition we find that there are two possible ways to realize inflation depending on whether the inflaton is identified with the radius of the fuzzy sphere or the radial component of the gauge fields living on the branes. Finally we discuss some phenomenological consequences of these models. Introduction:-In recent years many attempts have been made towards understanding cosmology using string/M-theory [1] . One of the most interesting proposals has been the idea of D-brane inflation [2] , where one considers a stack of N coincident D3-branes moving slowly through the extra dimensions. It was shown that if the distance between the branes becomes small enough then they start to interact strongly via exchange of massless closed string modes [3] . As a result the potential energy density stored in the form of open strings stretched between them starts to decrease rapidly leading to spontaneous symmetry breaking [4] . The main problem associated with this scenario is how to stabilize the size of the extra dimensions so that the system remains at the minimum of the potential during inflation [5] ? A simple way out would be to consider compactification of the extra dimensions [6] . However, this leads to problems like moduli stabilization [7, 8] and also introduces unwanted degrees of freedom [9] . Another possibility is to introduce anti-D3-branes [10] which lead to additional contributions to the scalar potential [11] . But again this requires fine tuning [12] . Recently Sen [13] proposed another mechanism called \"tachyon condensation\" [14] , where he showed that the unstable vacuum state corresponding to the maximum of the potential decays into stable states corresponding to minima", "paraphrased_abstract": "This is a great difficulty in the mathematical method of cosmology, especially since the development of string and M-theory has made many attempts to understand the cosmology of string and M-theory. One of these attempts was to find a way to limit the size of the extra dimensions to a minimum, and to limit the possibility of a certain amount of expansion, which arose by means of the tachyon condensation of the extra dimensions. However, this is a problem of moduli, and again it has its drawbacks, such as a loss of scalar potential. In the last experiment we show that the tachyon condensation of the extra dimensions is an example of a method of scalar expansion. The idea of the tachyon condensation of the extra dimensions is an interesting one, and it is proposed that, in the absence of the extra dimensions, the potential of the open strings is lowered, and the potential of the open strings is reduced rapidly, which will inevitably lead to symmetry-break. Among the many possible solutions is the theory of tachyon condensation. The theory of this theory is of a noncommutative geometry, which is based on tachyon condensation, and decaying into closed strings. The resulting effect is an act of tachyon condensation, which is then ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2532, "title": "3D Radiative Hydrodynamics for Disk Stability Simulations: A Proposed Testing Standard and New Results", "abstract": "  Recent three-dimensional radiative hydrodynamics simulations of protoplanetary disks report disparate disk behaviors, and these differences involve the importance of convection to disk cooling, the dependence of disk cooling on metallicity, and the stability of disks against fragmentation and clump formation. To guarantee trustworthy results, a radiative physics algorithm must demonstrate the capability to handle both the high and low optical depth regimes. We develop a test suite that can be used to demonstrate an algorithm's ability to relax to known analytic flux and temperature distributions, to follow a contracting slab, and to inhibit or permit convection appropriately. We then show that the radiative algorithm employed by Meji\\'a (2004) and Boley et al. (2006) and the algorithm employed by Cai et al. (2006) and Cai et al. (2007, in prep.) pass these tests with reasonable accuracy. In addition, we discuss a new algorithm that couples flux-limited diffusion with vertical rays, we apply the test suite, and we discuss the results of evolving the Boley et al. (2006) disk with this new routine. Although the outcome is significantly different in detail with the new algorithm, we obtain the same qualitative answers. Our disk does not cool fast due to convection, and it is stable to fragmentation. We find an effective $\\alpha\\approx 10^{-2}$. In addition, transport is dominated by low-order modes. ", "machine_abstract": "We present the results of 3D radiation hydrodynamic simulations of accretion disks around black holes, performed with our new code RHD3DPHOTON. We show that this code is able to reproduce previous results obtained by other authors using different codes (e.g., JETSPEC), as well as some new results which have not been previously reported in the literature. In particular we find that:  1) The disk becomes unstable when its luminosity exceeds a critical value Lcrit = 0.1LEdd.  2) For super-Eddington luminosities there are two types of instability modes: one associated with thermal convection and another related to photon bubbles.  3) There exists an upper limit on the mass flux through the disk, above which no steady state solution can be found. This result has important implications for models of AGN feedback. 4) When the luminosity approaches or exceeds LEdd, the disk develops strong outflows along the equatorial plane.", "paraphrased_abstract": "The calculations are performed with our new numerical code RHD3DPHOTON and demonstrate that this code can reproduce the results obtained by other authors using different codes, such as JETSPEC. The results indicate that the disk is unstable when the luminosity exceeds a critical value, Lcrit = 0.1 Ldd. 3) The disk becomes unstable when the luminosity exceeds Lcrit = 0.1 Ldd. 4) There is a upper limit to the mass flow through the disk, above which no steady state solution can be obtained. 5) When the luminosity reaches Lcrit, the disk begins to leak out strongly along the equatorial plane. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3645, "title": "Dynamics of interacting Brownian particles: a diagrammatic formulation", "abstract": "  We present a diagrammatic formulation of a theory for the time dependence of density fluctuations in equilibrium systems of interacting Brownian particles. To facilitate derivation of the diagrammatic expansion we introduce a basis that consists of orthogonalized many-particle density fluctuations. We obtain an exact hierarchy of equations of motion for time-dependent correlations of orthogonalized density fluctuations. To simplify this hierarchy we neglect contributions to the vertices from higher-order cluster expansion terms. An iterative solution of the resulting equations can be represented by diagrams with three and four-leg vertices. We analyze the structure of the diagrammatic series for the time-dependent density correlation function and obtain a diagrammatic interpretation of reducible and irreducible memory functions. The one-loop self-consistent approximation for the latter function coincides with mode-coupling approximation for Brownian systems that was derived previously using a projection operator approach. ", "machine_abstract": "We present an alternative approach to the dynamics of interacting Brownian particles, based on Feynman diagrams and path integrals in phase space. The method is illustrated by considering two examples: (i) the overdamped motion of N identical particles diffusing in one dimension under the influence of pairwise additive interactions; (ii) the diffusion of a single particle in three dimensions with arbitrary time-dependent external forces acting upon it. In both cases we obtain exact results for the mean-square displacement as well as higher-order correlation functions. We also discuss how our formalism can be used to study more complicated systems such as those involving hydrodynamic interactions between particles or active particles that perform directed motions. Interacting Brownian particles are ubiquitous in nature, ranging from colloidal suspensions [1] , granular matter [2] , and biological cells [3] to molecular motors [4] . Their collective behavior often exhibits non-trivial features which cannot be captured within standard statistical mechanics approaches [5] . In this work we develop a new theoretical framework to describe the stochastic dynamics of many-body systems composed of interacting Brownian particles. Our starting point is Feynman's path integral representation [6] of the probability distribution function P(rN;tN|r1t1;r2t2\u2026rntn\u22121), where rN denotes the positions of all particles at times tN=t1,\u2026,tn=N\u0394t, \u0394t being the discretization step size [7, 8] . This expression involves a sum over all possible trajectories connecting pairs of points {r1,\u2026,rn} and {t1,\u2026,tn}; each trajectory is weighted by its corresponding action S[rN;tN]=\u222bdtN[\u2212kB TlnP(rN;tN=rNt;rN\u22121tN\u22121\u2026)+Pext(rN;tN)], where kB is Boltzmann's constant and T is temperature [9] .", "paraphrased_abstract": "In general, Brownian particles are present everywhere, from colloidal suspensions, to the granular matter, to the cell-organs and to the motors, whose behavior is often described by non-trivial features, which, however, cannot be captured by the most elementary models. In our work, we formulate a new theoretical framework for the analysis of many-body systems composed of Brownian particles, based on Feynman diagrams and path integrals in phase space. We use this equation to describe the stochastic dynamics of many-body systems composed of Brownian particles. We begin with a probability distribution function P(rN, tN, t2t2... tn...), in which rN is the number of particles at times tN=t1, tn=tn, tn=t, tn=ttn, tn=tt, tn=t, tn=t, tn =t, t = temperature.... In this way, we demonstrate two examples: a dripping motion of N identical particles in one dimension under the influence of pairwise additive forces, a dripping of a single particle in three dimensions, a single particle in three dimensions. In both cases, we have the exact mean square displacement, and the higher", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3576, "title": "Relaxation of a dewetting contact line Part 1: A full-scale hydrodynamic calculation", "abstract": "  The relaxation of a dewetting contact line is investigated theoretically in the so-called \"Landau-Levich\" geometry in which a vertical solid plate is withdrawn from a bath of partially wetting liquid. The study is performed in the framework of lubrication theory, in which the hydrodynamics is resolved at all length scales (from molecular to macroscopic). We investigate the bifurcation diagram for unperturbed contact lines, which turns out to be more complex than expected from simplified 'quasi-static' theories based upon an apparent contact angle. Linear stability analysis reveals that below the critical capillary number of entrainment, Ca_c, the contact line is linearly stable at all wavenumbers. Away from the critical point the dispersion relation has an asymptotic behaviour sigma~|q| and compares well to a quasi-static approach. Approaching Ca_c, however, a different mechanism takes over and the dispersion evolves from |q| to the more common q^2. These findings imply that contact lines can not be treated as universal objects governed by some effective law for the macroscopic contact angle, but viscous effects have to be treated explicitly. ", "machine_abstract": "We present the results of a numerical study on the relaxation dynamics of an initially straight contact line in a two-dimensional geometry, which is driven by surface tension and viscous dissipation at the moving interface between liquid and gas phases.  We solve the Navier-Stokes equations for incompressible fluids with free-slip boundary conditions using a spectral element method to simulate the flow field around the evolving droplet shape. The initial condition consists of a circular droplet sitting on top of a flat substrate that has been perturbed slightly away from its equilibrium position. As time evolves, we observe the formation of capillary waves along the contact line as well as the development of small satellite drops near the main droplet due to pinching off events. In addition, we find that the contact angle decreases continuously during this process until it reaches zero degrees when the entire droplet detaches from the substrate. Finally, we compare our simulation results against experimental data obtained from high-speed video microscopy measurements performed by other researchers.", "paraphrased_abstract": "The result is that the elasticity of the contact is continually decreasing and decreasing with the time, until it reaches zero degrees when the whole droplet disappears from the substrate. The simulation results are compared with those obtained from microscopy studies performed by others. We show the kinetics of a straight contact line with an initially circular shape, seated on a flat substrate, with a slight declination. Then, the evaporation and the dissipation of the liquid and the gas are in contact, and the contact of the droplet is affected by the evaporation of the liquid and the dissipation of the gas. The first contact is a circular droplet, placed on a flat surface, slightly perturbed, which is deviated from its equilibrium position. Then, we observe that the angle of the droplet decreases continually until it reaches zero degrees, when the droplet has detached from the substrate. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0947, "title": "Jet-disturbed molecular gas near the Seyfert 2 nucleus in M51", "abstract": "  Previous molecular gas observations at arcsecond-scale resolution of the Seyfert 2 galaxy M51 suggest the presence of a dense circumnuclear rotating disk, which may be the reservoir for fueling the active nucleus and obscures it from direct view in the optical. However, our recent interferometric CO(3-2) observations show a hint of a velocity gradient perpendicular to the rotating disk, which suggests a more complex structure than previously thought. To image the putative circumnuclear molecular gas disk at sub-arcsecond resolution to better understand both the spatial distribution and kinematics of the molecular gas. We carried out CO(2-1) and CO(1-0) line observations of the nuclear region of M51 with the new A configuration of the IRAM Plateau de Bure Interferometer, yielding a spatial resolution lower than 15 pc. The high resolution images show no clear evidence of a disk, aligned nearly east-west and perpendicular to the radio jet axis, as suggested by previous observations, but show two separate features located on the eastern and western sides of the nucleus. The western feature shows an elongated structure along the jet and a good velocity correspondence with optical emission lines associated with the jet, suggesting that this feature is a jet-entrained gas. The eastern feature is elongated nearly east-west ending around the nucleus. A velocity gradient appears in the same direction with increasingly blueshifted velocities near the nucleus. This velocity gradient is in the opposite sense of that previously inferred for the putative circumnuclear disk. Possible explanations for the observed molecular gas distribution and kinematics are that a rotating gas disk disturbed by the jet, gas streaming toward the nucleus, or a ring with another smaller counter- or Keplarian-rotating gas disk inside. ", "machine_abstract": "We present new observations with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal an extended region of disturbed molecular gas surrounding the active galactic nucleus (AGN) of NGC 5194, also known as M51a or Whirlpool Galaxy. The AGN is located at the center of this interacting galaxy pair and has been classified as a Seyfert 2 based on optical spectroscopy. We detect two prominent dust lanes extending to the north-east and south-west of the AGN along its minor axis. These are likely caused by tidal forces between the galaxies during their interaction. In addition, we find evidence for a third dust lane oriented perpendicularly to these two features which may be associated with a nuclear bar. Our ALMA data show that the distribution of dense molecular gas traced by HCN(1-0), HCO+(1-0), and CS(2-1) emission lines exhibits a ring-like structure around the AGN. This feature appears to have been shaped by powerful outflows driven by the AGN.", "paraphrased_abstract": "This is the first X-ray ray of a high-pressure, high-pressure ALMA ray, which was analyzed in detail with the Atacama. Our ALMA data showed that the distribution of dense gas, characterized by HCN, HCO+, CS-2-1, was a ring-shaped one around the AGN. This ring was likely shaped by the great influxes of the AGN. During this interaction, the AGN was a Seyfert2 star. The AGN was located in the center of this interacting galaxy. We call it Seyfert 2 in spectra. The AGN was located at the centre of this interacting pair and is classified as a Seyfert 2. We also observed two dense, disturbed regions in the AGN near the North and South Poles, which were formed by the tidal force of the galaxies, which may be the result of the tidal tidal forces of the AGN. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1053, "title": "Glimpsing through the high redshift neutral hydrogen fog", "abstract": "  We analyze the transmitted flux in a sample of 17 QSOs spectra at 5.74<zem<6.42 to obtain tighter constraints on the volume-averaged neutral hydrogen fraction, xHI, at z~6. We study separately the narrow transmission windows (peaks) and the wide dark portions (gaps) in the observed absorption spectra. By comparing the statistics of these spectral features with Lyalpha forest simulations, we conclude that xHI evolves smoothly from 10^{-4.4} at z=5.3 to 10^{-4.2} at z=5.6, with a robust upper limit xHI<0.36 at z=6.3. The frequency and physical sizes of the peaks imply an origin in cosmic underdense regions and/or in HII regions around faint quasars or galaxies. In one case (the intervening HII region of the faint quasar RD J1148+5253 at z=5.70 along the LOS of SDSS J1148+5251 at z=6.42) the increase of the peak spectral density is explained by the first-ever detected transverse proximity effect in the HI Lyalpha forest; this indicates that at least some peaks result from a locally enhanced radiation field. We then obtain a strong lower limit on the foreground QSO lifetime of tQ>11 Myr. The observed widths of the peaks are found to be systematically larger than the simulated ones. Reasons for such discrepancy might reside either in the photoionization equilibrium assumption or in radiative transfer effects. ", "machine_abstract": "We present new results on the evolution of galaxy clustering in the range 0 < z < 5, based on an analysis of data obtained with the VIMOS spectrograph at the Very Large Telescope (VLT). The sample consists of about 2000 galaxies selected by their Lyman-alpha emission line fluxes and covers a wide range of redshifts between 2<z<5. We measure the two-point correlation function for this sample using both direct counts-in-cells methods as well as Fourier space techniques. Our main result is that we find no evidence for any significant change in the amplitude or slope of the correlation function over this large redshift interval. This suggests that there has been little evolution in the typical mass scale of dark matter halos hosting these galaxies since z=5. In addition to measuring the overall shape of the correlation function, we also examine how it depends upon various physical properties such as luminosity, color, and spectral type.", "paraphrased_abstract": "Moreover, we examine how this correlation function is shaped by various physical properties, such as luminosity, color and spectral type. We present new data on the evolution of clusters in the range 0 - z 5 for which we have been able to obtain a spectroscopic image of the VIMOS spectrograph from the Very Large Telescope. The sample consisted of about 2000 galaxies of Lyman alpha emission, which covered a wide range of redshifts between z 5 and z 5. We measured the correlation between these galaxies in a direct way, and we applied the Fourier transform. The main result was that there was no significant change in the correlation between amplitude and slope over the wide redshift interval. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0859, "title": "Profinite complexes of curves, their automorphisms and anabelian properties of moduli stacks of curves", "abstract": "  Let ${\\cal M}_{g,[n]}$, for $2g-2+n>0$, be the D-M moduli stack of smooth curves of genus $g$ labeled by $n$ unordered distinct points. The main result of the paper is that a finite, connected \\'etale cover ${\\cal M}^\\l$ of ${\\cal M}_{g,[n]}$, defined over a sub-$p$-adic field $k$, is \"almost\" anabelian in the sense conjectured by Grothendieck for curves and their moduli spaces.   The precise result is the following. Let $\\pi_1({\\cal M}^\\l_{\\ol{k}})$ be the geometric algebraic fundamental group of ${\\cal M}^\\l$ and let ${Out}^*(\\pi_1({\\cal M}^\\l_{\\ol{k}}))$ be the group of its exterior automorphisms which preserve the conjugacy classes of elements corresponding to simple loops around the Deligne-Mumford boundary of ${\\cal M}^\\l$ (this is the \"$\\ast$-condition\" motivating the \"almost\" above). Let us denote by ${Out}^*_{G_k}(\\pi_1({\\cal M}^\\l_{\\ol{k}}))$ the subgroup consisting of elements which commute with the natural action of the absolute Galois group $G_k$ of $k$. Let us assume, moreover, that the generic point of the D-M stack ${\\cal M}^\\l$ has a trivial automorphisms group. Then, there is a natural isomorphism: $${Aut}_k({\\cal M}^\\l)\\cong{Out}^*_{G_k}(\\pi_1({\\cal M}^\\l_{\\ol{k}})).$$ This partially extends to moduli spaces of curves the anabelian properties proved by Mochizuki for hyperbolic curves over sub-$p$-adic fields. ", "machine_abstract": "We study the profinite complex associated to a family of smooth projective curves over a field k with fixed genus g>0. We show that this is homotopy equivalent to the classifying space of the group Aut(C) of automorphisms of one curve C in the family (Theorem 1). This result generalizes previous results by Deligne on families of elliptic curves. In particular we obtain new information about the cohomology groups H^1(Aut(C),k*) for all such families (Corollary 2). In section 3 we use our results to prove some basic facts about the anabelian geometry of the moduli stack M_g of stable curves of genus g. For example we give a simple proof of the fact that the Picard scheme Pic0M_g has no torsion points if char(k)=0 or p>3g-2 (Proposition 4). The main tool used here are the results obtained in sections 1-2 combined with Grothendieck's comparison theorem between \u00e9tale fundamental groups and Galois groups.", "paraphrased_abstract": "And in section 3 we test the anaesthetics of the equilateral stack M_g of a stable curve of a genus g. We test the theory of Picard's Picard hexagram with a torsion-point 0g, for instance. The result is an apropos of the previous results of Deligne on curves of elliptic geometry. The results are summarized by the comparison of Grothendieck's two-sided groups. The result is similar to the homoty of Aut(C) of the automorphisms of one axis in the family (Theorem 1). This result is generalized by the results of Deligne's researches on curves of elliptic geometry. We show that this is homoty equivalent to the classification of the group Aut(C) of automorphisms of one axis in the family (Theorem 1). In Section 3, we demonstrate some basic facts of the anaesthetic geometry of the moduli stack M_g, a stable curve of genus g. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0302, "title": "Neutrino Decays and Neutrino Electron Elastic Scattering in Unparticle Physics", "abstract": "  Following Georgi's unparticle scheme, we examine the effective couplings between neutrinos and unparticle operators. As an immediate consequence, neutrinos become unstable and can decay into the unparticle stuff. Assuming the dimension transmutation scale is around $\\Lambda^{}_{\\cal U} \\sim 1 ~{\\rm TeV}$, we implement the cosmological limit on the neutrino lifetime to constrain the neutrino-unparticle couplings for different scaling dimensions $d$. In addition, provided that the electron-unparticle coupling is restricted due to the precise measurement of the anomalous magnetic moment of electron, we calculate the unparticle contribution to the neutrino-electron elastic scattering. It is more important to jointly deal with the couplings of the unparticle to the standard model particles rather than separately. Taking into account both electron- and neutrino-unparticle couplings, we find that the scaling dimension of the scalar unparticle should lie in the narrow range $1 < d < 2$ by requiring the observables to be physically meaningful. However, there is no consistent range of $d$ for the vector unparticle operator. ", "machine_abstract": "We study the decay modes of neutrinos into unparticles, as well as their elastic scattering off electrons mediated by virtual unparticles. We find that these processes are strongly suppressed for small values of the coupling constant between ordinary particles and unparticles. For larger values of this coupling we obtain branching ratios which can be tested at future experiments such as T2K or NOVA. The results presented here may also have implications on other models with extra dimensions beyond those considered so far.     Introduction     In recent years there has been an increasing interest in theories where new physics is described by fields whose excitations do not carry standard model (SM) charges [1] . These so-called \"unparticles\" [2] , if they exist, could manifest themselves through various experimental signatures [3] .   In particular, it was shown [4] that decays of SM particles to pairs of unparticles would lead to deviations from the expected exponential behavior of the corresponding lifetimes. This effect should be observable experimentally [5] . Furthermore, it was suggested [6] that unparticles might play a role in explaining some puzzling features observed recently in cosmic ray data [7, 8] .     Another interesting possibility is that unparticles couple directly to SM fermions [9] . If this were true then one would expect to see effects similar to those predicted in Ref. [10] for Kaluza-Klein gravitons coupled to leptons. Namely, the cross sections for certain processes involving SM fermions and unparticles would grow logarithmically with energy [11] . Such logarithmic growths have indeed been found [12] - [14] in several cases including e+e-\u2192e+e-U, U\u2192e\u03bd, and U\u2192\u03bc\u03c4. However, in all these studies only the case of scalar unparticles was considered. It turns out [15] that vector-like couplings give rise to additional contributions to the amplitudes which modify significantly the predictions obtained previously [16] .", "paraphrased_abstract": "It was found that, for example, the ratio of E e-e-U, e-e-U, e-e-U, e-e-ee-U, e-e-U, e-e-e and e-e-u were based on logarithmic growth. These were the results obtained at T2K and NOVA, and, for more extreme cases, the inverse is possible. In addition, it has been shown that the decays of SM particles to the pairs of SM particles were strongly suppressed for the small coupling constant between ordinary particles and SM particles, and that this effect could be observed in a particular experiment. Besides, it was proposed that these SM particles \u2013 so-called \u201cunparticles\u201d \u2013 may also be explained by certain experimental signs. The experimental signs of these unparticles are observed in some cases. Then we find that these effects, if they exist, would be quite different from those predicted by Kaluza-Klein gravitons and Leptons. In the recent years, interest in new physics has increased in the field of the fields which do not contain SM charges. The so-called \u201cunparticles\u201d, if they exist, could", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.197, "title": "Energy Density-Flux Correlations in an Unusual Quantum State and in the Vacuum", "abstract": "  In this paper we consider the question of the degree to which negative and positive energy are intertwined. We examine in more detail a previously studied quantum state of the massless minimally coupled scalar field, which we call a ``Helfer state''. This is a state in which the energy density can be made arbitrarily negative over an arbitrarily large region of space, but only at one instant in time. In the Helfer state, the negative energy density is accompanied by rapidly time-varying energy fluxes. It is the latter feature which allows the quantum inequalities, bounds which restrict the magnitude and duration of negative energy, to hold for this class of states. An observer who initially passes through the negative energy region will quickly encounter fluxes of positive energy which subsequently enter the region. We examine in detail the correlation between the energy density and flux in the Helfer state in terms of their expectation values. We then study the correlation function between energy density and flux in the Minkowski vacuum state, for a massless minimally coupled scalar field in both two and four dimensions. In this latter analysis we examine correlation functions rather than expectation values. Remarkably, we see qualitatively similar behavior to that in the Helfer state. More specifically, an initial negative energy vacuum fluctuation in some region of space is correlated with a subsequent flux fluctuation of positive energy into the region. We speculate that the mechanism which ensures that the quantum inequalities hold in the Helfer state, as well as in other quantum states associated with negative energy, is, at least in some sense, already ``encoded'' in the fluctuations of the vacuum. ", "machine_abstract": "We study energy density-flux correlations for a free scalar field in 1+1 dimensions, both at finite temperature T and in vacuum (T=0). We show that these correlation functions are nontrivial even when evaluated on the light cone x+x=t+t0=2ct. In particular we find that they exhibit power law behavior with exponents which depend continuously on c. This is in contrast to what happens in ordinary quantum mechanics where such correlators vanish identically outside the light cone. The results presented here can be obtained by using standard techniques developed within the framework of thermofield dynamics. They provide further evidence that this formalism provides a useful description of thermal states also beyond equilibrium situations. Energy density-flux correlations play an important role in various physical phenomena ranging from hydrodynamics to particle production processes in heavy ion collisions. However their calculation has been hampered so far by the fact that it requires knowledge about off-diagonal elements of the two-point function of the corresponding operator. Here we present explicit expressions for these quantities for a free massless scalar field theory in one spatial dimension.", "paraphrased_abstract": "They are a well-known function of the field of massless scalars. Its calculation is a complex matter, because of the fact that it requires knowledge of the off-diagonal elements of the two-point function of the operator. The results we present are based on techniques developed within the framework of thermodynamics. The analysis of these results can be compared with the standard thermodynamics methods. We demonstrate that these correlation functions are non-nil, even when applied to the light cone x = x t = t0 2 ct. These relations show the properties of the law of constant exponents. These are in contrast to the phenomenon of quantum mechanics, where such correlation functions vanish exactly outside the light cone. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0367, "title": "Is there Evidence for a Hubble bubble? The Nature of Type Ia Supernova Colors and Dust in External Galaxies", "abstract": "  We examine recent evidence from the luminosity-redshift relation of Type Ia Supernovae (SNe Ia) for the $\\sim 3 \\sigma$ detection of a ``Hubble bubble'' -- a departure of the local value of the Hubble constant from its globally averaged value \\citep{Jha:07}. By comparing the MLCS2k2 fits used in that study to the results from other light-curve fitters applied to the same data, we demonstrate that this is related to the interpretation of SN color excesses (after correction for a light-curve shape-color relation) and the presence of a color gradient across the local sample. If the slope of the linear relation ($\\beta$) between SN color excess and luminosity is fit empirically, then the bubble disappears. If, on the other hand, the color excess arises purely from Milky Way-like dust, then SN data clearly favors a Hubble bubble. We demonstrate that SN data give $\\beta \\simeq 2$, instead of the $\\beta \\simeq 4$ one would expect from purely Milky-Way-like dust. This suggests that either SN intrinsic colors are more complicated than can be described with a single light-curve shape parameter, or that dust around SN is unusual. Disentangling these possibilities is both a challenge and an opportunity for large-survey SN Ia cosmology. ", "machine_abstract": "We present new results on the nature of dust extinction in external galaxies, based on observations with HST/ACS of type Ia supernovae (SNe) at redshifts z=0.1-0.7. We find that SNe Ia are systematically bluer than expected if they were standard candles, but this effect is consistent with being due to dust extinction by interstellar material along their line-of-sight. In addition we find evidence for an additional component of dust extinction which increases rapidly towards higher redshift. This extra extinction may be associated with the intergalactic medium surrounding galaxy clusters, or it could arise within individual galaxies as a result of recent star formation activity. These findings have important implications for cosmological studies using distant SNe Ia as distance indicators.     Keywords: Galaxy cluster, Interstellar matter, Intergalactic medium, Redshift evolution, Supernova, Supernova Ia", "paraphrased_abstract": "This finding, together with some recent stars formation, has important implications for the study of cosmological phenomena, which can be used to distinguish distant SNe Ia as distant slits. We have derived new data on the nature of dust extinction in galaxies and demonstrate that SNe Ia are systematically bluer than expected if they were standard candles, but that this is consistent with the dust extinction by the interstellar medium, or perhaps due to the recent formation of star-forming regions. This new data is based on observations with HST and ACS of SNe Ia, at redshifts z=0.1 to 0.7. We have found that SNe Ia are systematically bluer than expected, but this is a result of the dust extinction by the interstellar medium at the poles of their orbits. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3697, "title": "Quantification of discreteness effects in cosmological N-body simulations: II. Evolution up to shell crossing", "abstract": "  We apply a recently developed perturbative formalism which describes the evolution under their self-gravity of particles displaced from a perfect lattice to quantify precisely, up to shell crossing, the effects of discreteness in dissipationless cosmological N-body simulations. We give simple expressions, explicitly dependent on the particle density, for the evolution of power in each mode as a function of red-shift. For typical starting red-shifts the effect of finite particle number is to {\\it slow down} slightly the growth of power compared to that in the fluid limit (e.g. by about ten percent at half the Nyquist frequency), and to induce also dispersion in the growth as a function of direction at a comparable level. In the limit that the initial red-shift tends to infinity, at fixed particle density, the evolution in fact diverges from that in the fluid limit (described by the Zeldovich approximation). Contrary to widely held belief, this means that a simulation started at a red-shift much higher than the red-shift of shell crossing actually gives a worse, rather than a better, result. We also study how these effects are modified when there is a small-scale regularization of the gravitational force. We show that such a smoothing may reduce the anisotropy of the discreteness effects, but it then {\\it increases} their average effect. This behaviour illustrates the fact that the discreteness effects described here are distinct from those usually considered in this context, due to two-body collisions. Indeed the characteristic time for divergence from the collisionless limit is proportional to $N^{2/3}$, rather than $N/ \\log N$ in the latter case. ", "machine_abstract": "We present the results of an investigation into the evolution of dark matter halos and their substructure, using high-resolution cosmological N-body simulations with different mass resolutions. We find that the number density profiles of subhalos are well described by a power law at all redshifts z < 5 for both low-mass (10^9 Msun/h) and high-mass (10^12 Msun/h) halos. The slope of this profile is independent of halo mass but depends on redshift; it steepens as time progresses. This behavior can be understood if we assume that the subhalo population consists of two components: one which follows the host's potential closely and another whose orbits have been significantly affected by dynamical friction. In addition, we show that the fraction of subhalos within r200c decreases rapidly towards higher masses. Finally, we demonstrate how our findings can be used to quantify the effect of numerical resolution on the abundance of subhalos.", "paraphrased_abstract": "Moreover, we show that the proportion of subhalo at r200c decreases rapidly with the increase in mass. Then, we conclude that the abundance of subhalo decreases rapidly with the increase in mass. The development of dark matter and its substructure is shown by a power law of z-5 for the low density (9 Msun/h) and the high density (12 Msun/h) halos, and for the high density (12 Msun/h) halos. We show that the number density is characterized by a power law of z-5 for the low density (109 Msun/h) and the high density (12 Msun/h) halos. This slope is independent of the mass, but it rises with the rise of time. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0339, "title": "Product Formulae for Ozsvath-Szabo 4-manifold Invariants", "abstract": "  We give formulae for the Ozsvath-Szabo invariants of 4-manifolds X obtained by fiber sum of two manifolds M_1, M_2 along surfaces S_1, S_2 having trivial normal bundle and genus g>0. The formulae follow from a general theorem on the Ozsvath-Szabo invariants of the result of gluing two 4-manifolds along a common boundary, which is phrased in terms of relative invariants of the pieces. These relative invariants take values in a version of Heegaard Floer homology with coefficients in modules over certain Novikov rings; the fiber sum formula follows from the theorem that this \"perturbed\" version of Heegaard Floer theory recovers the usual Ozsvath-Szabo invariants, when the 4-manifold in question has b^+>1. The construction allows an extension of the definition of the Ozsvath-Szabo invariants to 4-manifolds having b^+ = 1 depending on certain choices, in close analogy with Seiberg-Witten theory. The product formulae lead quickly to calculations of the Ozsvath-Szabo invariants of various 4-manifolds; in all cases the results are in accord with the conjectured equivalence between the Ozsvath-Szabo and Seiberg-Witten invariants. ", "machine_abstract": "We give explicit formulae for the product of two Ozsvath-Szab o 4-dimensional manifold invariants in terms of their associated modular forms and theta functions.  We also show that these products are not always positive, which is an obstruction to using them as intersection numbers on moduli spaces of curves. The results presented here were obtained by applying techniques developed by Vafa-Witten and Witten-Taubes to the case of genus 2 Riemann surfaces with four punctures. These techniques involve studying certain periods of meromorphic differentials on hyperelliptic Riemann surfaces. As such they can be used to study other problems involving hyperelliptic Riemann surface invariants. This article was published in Journal of Physics A: Mathematical and General 37 (2004) L55-L62. URL: http://arxiv.org/abs/0406.0366v1 .  Authors: David J. Jeffrey1 , Michael A. McMullen2 . Affiliation: 1University of California Santa Cruz, Santa Cruz, CA 95064, USA; 2University", "paraphrased_abstract": "The author was David J. Jeffrey, Michael A. McMullen.1 The authors are David Jeffrey, Michael A. McMullen, David Jeffrey, Michael A. McMullen, The article was published in the Journal of Physics A: Mathematical and General 37 (2004) L55-L62. We present the results of the morphology of two Ozsvath-Szab o 4-dimensional manifolds in terms of their modular forms and theta functions, and we demonstrate that the results are not always positive, which is an obstruction to using them for intersections in moduli space. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0547, "title": "Mid-Infrared Fine Structure Line Ratios in Active Galactic Nuclei Observed with Spitzer IRS: Evidence for Extinction by the Torus", "abstract": "  We present the first systematic investigation of the [NeV] (14um/24um) and [SIII] (18um/33um) infrared line flux ratios, traditionally used to estimate the density of the ionized gas, in a sample of 41 Type 1 and Type 2 active galactic nuclei (AGNs) observed with the Infrared Spectrograph on board Spitzer. The majority of galaxies with both [NeV] lines detected have observed [NeV] line flux ratios consistent with or below the theoretical low density limit, based on calculations using currently available collision strengths and ignoring absorption and stimulated emission. We find that Type 2 AGNs have lower line flux ratios than Type 1 AGNs and that all of the galaxies with line flux ratios below the low density limit are Type 2 AGNs. We argue that differential infrared extinction to the [NeV] emitting region due to dust in the obscuring torus is responsible for the ratios below the low density limit and we suggest that the ratio may be a tracer of the inclination angle of the torus to our line of sight. Because the temperature of the gas, the amount of extinction, and the effect of absorption and stimulated emission on the line ratios are all unknown, we are not able to determine the electron densities associated with the [NeV] line flux ratios for the objects in our sample. We also find that the [SIII] emission from the galaxies in our sample is extended and originates primarily in star forming regions. Since the emission from low-ionization species is extended, any analysis using line flux ratios from such species obtained from slits of different sizes is invalid for most nearby galaxies. ", "machine_abstract": "We present mid-infrared (MIR) spectroscopic observations of six active galactic nuclei (AGNs), obtained using the Infrared Spectrograph on board the Spitzer Space Telescope, and compare them to predictions based on radiative transfer models. We find that all objects show strong silicate absorption features at 9.7{\\mu}m and 18.2{\\mu}m, which are well reproduced by our models if we assume an optical depth of $\\sim$10-20 along the line-of-sight through each torus component. The observed MIR fine structure lines can be explained by photoionization modeling only when taking into account extinction due to dust inside the torus. This is consistent with previous results showing that AGN tori have large covering factors. Our analysis shows that the obscuring material has a temperature distribution peaking around $300\\,000\\,K$. Finally, we discuss how these findings may affect current ideas about the geometry of AGN tori.", "paraphrased_abstract": "The observed MIR properties can be explained by the photoionization of the light by the dust obscuring the torus. In our analysis, we show that the material has a temperature of 300,000 K. Moreover, the observed fine structure can be explained by photoionization only when the extinction of the dust is included in the equation. The mid-infrared spectroscopy of six active galactic nuclei (AGNs) was performed on board the telescope of Spitzer Space Telescope and compared with a simulated radiative transfer model. We show that all AGNs have strong silicate absorption at 9.7 mum and at 18.2 mum, which can be easily reproduced by our models when the optical depth of the object lies in sim10-20\u00b0 through each torus. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2273, "title": "On the Information Rate of MIMO Systems with Finite Rate Channel State Feedback and Power On/Off Strategy", "abstract": "  This paper quantifies the information rate of multiple-input multiple-output (MIMO) systems with finite rate channel state feedback and power on/off strategy. In power on/off strategy, a beamforming vector (beam) is either turned on (denoted by on-beam) with a constant power or turned off. We prove that the ratio of the optimal number of on-beams and the number of antennas converges to a constant for a given signal-to-noise ratio (SNR) when the number of transmit and receive antennas approaches infinity simultaneously and when beamforming is perfect. Based on this result, a near optimal strategy, i.e., power on/off strategy with a constant number of on-beams, is discussed. For such a strategy, we propose the power efficiency factor to quantify the effect of imperfect beamforming. A formula is proposed to compute the maximum power efficiency factor achievable given a feedback rate. The information rate of the overall MIMO system can be approximated by combining the asymptotic results and the formula for power efficiency factor. Simulations show that this approximation is accurate for all SNR regimes. ", "machine_abstract": "In this work, we consider a multiple-input-multiple-output (MIMO) system where each transmitter has limited feedback information about its channel state to the receiver. We assume that there is no cooperation between transmitters in terms of power allocation or transmission strategies. Each transmitter can only adjust its own transmit power level based on local CSI knowledge at the transmitter side. In addition, each transmitter may turn off its radio completely when it does not have any data to send. The objective is to maximize the sum rate by optimizing both the power control policy as well as the transmission strategy for all users simultaneously under these constraints.     First, we derive an upper bound on the achievable sum-rate using finite-rate feedback assuming Gaussian codebooks. Then, we propose two algorithms to solve the optimization problem numerically. Finally, simulation results are presented to show the performance gain achieved by our proposed algorithm over existing schemes.", "paraphrased_abstract": "I first derive the maximum value of the power-supply system from the Gaussian code. Then I propose two algorithms for solving this problem numerically. In addition, I present an example of how to achieve the maximum sum by combining the power-supply system with the transmission system of the receiver. I consider a multidirectional system with no cooperation between transmitters. We assume that there is no exchange of power among the transmitters and only the receiver can be controlled by the local CSI. Therefore, we think that there is no communication between transmitters. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4659, "title": "On the nature of two superconducting transitions in the specific heat of PrOs$_4$Sb$_{12}$: Effects of crystal grinding", "abstract": "  Specific heat, dc- and ac-magnetic susceptibility are reported for a large single crystal of PrOs$_4$Sb$_{12}$ and, after grinding, its powder. The room temperature effective paramagnetic moment of the crystal was consistent with the Pr$^{3+}$ ionic configuration and full occupancy of the Pr-sublattice. The crystal showed two distinct anomalies in the specific heat and an overall discontinuity in $C/T$ of more than 1000 mJ/K$^2$mol. The upper transition (at $T_{c1}$) was rounded, in an agreement with previous reports. The anomaly at $T_{c2}$ was very sharp, consistent with a good quality of the crystal. We observed a shoulder in $\\chi$' and two peaks in $\\chi$'' below $T_{c1}$. However, there were no signatures in $\\chi$' of the lower temperature transition. PrOs$_4$Sb$_{12}$ is extremely sensitive to grinding, which suppresses the upper superconducting transition in both the specific heat and magnetic susceptibility. $\\Delta C/T_{c}$ was reduced to 140 mJ/K$^2$ mol in the powdered sample. Existing data on ground, polished, and sliced crystals suggests the existence of a length scale of order 100 $\\mu$, characterizing the higher temperature superconducting phase. ", "machine_abstract": "We report on measurements of the specific heat and magnetic susceptibility for single crystals of PrOs$_{4}$Sb$_{12}$$ The results show that there are at least three different phases, including an antiferromagnetic phase below $T_{1/2} = 1.5 K$ (the N\u00e9el temperature), a paramagnetic phase between $1.5 K < T < 5.8 K$, and another phase above $5.8 K$ which is not yet understood.  We also find that the specific heat data can be fitted with two separate power laws near the transition temperatures, suggesting that there may be more than one type of order parameter involved in this system. Finally we discuss how our results compare to previous studies of this material. PACS: 75.10.Pq, 74.20.-q, 76.30.+z  I. INTRODUCTORY REMARk The heavy fermion compound PrOs$_{4}\\sb{_2}$Sb$_{12} $ has been studied extensively since its discovery by de Haas-van Alphen effect [1] . It was found to have several interesting properties such as non-Fermi liquid behavior [2] , unconventional superconductivity [3] , and quantum criticality [4] . In particular, it shows two distinct superconducting transitions in both resistivity [5] and specific heat [6] experiments. However, these features were observed only after the sample had undergone severe mechanical stress during the measurement process [7, 8] . This suggests that the physical properties of PrOs$_{4}$$\\sb{_2}$$ Sb$_{12} $ might depend strongly on the microscopic structure of the samples [9] .  In fact, recent theoretical calculations suggest that the ground state of PrOs$_{4}}$$\\sb{_2}$ Sb$_{12} $ should contain multiple competing orders [10] . Therefore, it would be very important to study the effects of external perturbations on the physical properties of this material [11] .", "paraphrased_abstract": "He found that it was the most complex liquid ever to be studied, having been studied in the light of the Haas-van Alphen effect. The latter had found that it had a remarkable non-Fermi liquid behavior, unusual superconductivity and quantum criticality. This, however, was only observed after the material had been subjected to severe mechanical stresses. Moreover, it showed that the two superconducting transitions were able to be measured with different power-laws near the transition temperatures, and that they could be associated with more than one order. Moreover, in the specific heat measurements, two separate power laws could be obtained at the transition temperatures, implying that there might be more than one order in the material. Moreover, the measurements show that the specific heat and magnetic susceptibility are found to be at least three different temperatures, an antiferromagnetic phase at 2/3  1.5 K (the N\u00e9el temperature) and a paramagnetic phase at 2.5 K (the K = 5.8 K) and another phase at 5 K (still not understood). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0107, "title": "Strongly modulated transmission of a spin-split quantum wire with local Rashba interaction", "abstract": "  We investigate the transport properties of ballistic quantum wires in the presence of Zeeman spin splittings and a spatially inhomogeneous Rashba interaction. The Zeeman interaction is extended along the wire and produces gaps in the energy spectrum which allow electron propagation only for spinors lying along a certain direction. For spins in the opposite direction the waves are evanescent far away from the Rashba region, which plays the role of the scattering center. The most interesting case occurs when the magnetic field is perpendicular to the Rashba field. Then, the spins of the asymptotic wavefunctions are not eigenfunctions of the Rashba Hamiltonian and the resulting coupling between spins in the Rashba region gives rise to sudden changes of the transmission probability when the Fermi energy is swept along the gap. After briefly examining the energy spectrum and eigenfunctions of a wire with extended Rashba coupling, we analyze the transmission through a region of localized Rashba interaction, in which a double interface separates a region of constant Rashba interaction from wire leads free from spin-orbit coupling. For energies slightly above the propagation threshold, we find the ubiquitous occurrence of transmission zeros (antiresonances) which are analyzed by matching methods in the one-dimensional limit. We find that a a minimal tight-binding model yields analytical transmission lineshapes of Fano antiresonance type. More general angular dependences of the external magnetic field is treated within projected Schroedinger equations with Hamiltonian matrix elements mixing wavefunction components. Finally, we consider a realistic quantum wire where the energy subbands are coupled via the Rashba intersubband coupling term and discuss its effect on the transmission zeros. ", "machine_abstract": "We study the transport properties in a one-dimensional (1D) spin-orbit coupled system, where the electron-electron interactions are treated within the Hartree-Fock approximation. We find that for strong enough spin-orbit coupling and repulsive interactions there is an insulating phase at half-filling which can be understood as a Mott insulator due to the formation of bound states between electrons on neighboring sites. The transition into this state occurs when the Fermi energy crosses the lowest bound state. In addition we show how the presence of disorder changes these results. Finally, we discuss possible experimental realizations of our model using semiconductor nanowires or carbon nanotubes. Introduction:-In recent years it has been realized that many interesting phenomena observed in condensed matter physics such as high-Tc superconductivity [1] , fractional quantum Hall effect [2] etc., have their origin in strongly correlated electronic systems. One of the simplest models describing interacting fermions is the Hubbard model [3] . However, even though much progress has been made over the past few decades [4] , exact solutions of the Hubbard model are still lacking [5] . Recently, several authors [6] - [8] studied the effects of spin-orbit coupling on the ground-state properties of 1D Hubbard chains by employing various numerical techniques like density matrix renormalization group [9] , exact diagonalization [10] , DMRG [11] , Bethe ansatz [12] , variational Monte Carlo [13] , Quantum Monte Carlo [14] etc.. It was found that depending upon the strength of spin-orbit coupling and the value of Coulomb repulsion U , different phases appear in the ground state. For example, if the spin-orbit coupling is weak compared to the hopping amplitude t then the ground state is either metallic or insulating depending on whether U/t < 2 or U/t > 2 respectively [15] . On the other hand, if the spin-orbit", "paraphrased_abstract": "We are now going to show how it can be applied to one-dimensional chains, with the aid of various numerical methods, such as density matrix renormalization, exact diagonalization, Bethe ansatz, variational Monte Carlo, Quantum Monte Carlo, and Quantum Monte Carlo. We will present an example of the effects of spin-orbit coupling on the properties of a one-dimensional Hubbard chain by means of the density matrix renormalization group, density matrix renormalization group, DMRG group, Bethe ansatz, variational Monte Carlo, Quantum Monte Carlo, and Quantum Monte Carlo. This result shows how the kinetics of the kinetics of the Hubbard chain is influenced by the strength of the spin-orbit coupling and by the degree of Coulomb repulsion, and the number of Coulomb repulsions. It is shown that depending on the strength of the spin-orbit coupling and the coefficient of Coulomb repulsion, different phases of the ground state appear. For instance, if the spin-orbit coupling is weak compared with the hopping amplitude t, the ground state is either metallic or insulating, depending on the value of U/t and the degree of U/t  2. On the other hand, if the spin-orbit coupling is weak ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0297, "title": "Search for a kaonic nuclear state via $^4$He$(K^-, N)$", "abstract": "  Very recently, we have performed a couple of experiments, {\\it{KEK PS-E549/E570}}, for the detailed study of the strange tribaryon $S^0(3115)$ obtained in {\\it{KEK PS-E471}}. These experiments were performed to accumulate much higher statistics with improved experimental apparatusespecially for the better proton spectroscopy of the $^4$He({\\it{stopped K}}$^-$, {\\it{N}}) reaction. In contrast to the previous proton spectrum, no narrow ($\\sim$ 20 MeV) peak structure was found either in the inclusive $^4$He({\\it{stopped K}}$^-$, {\\it{p}}) or in the semi-inclusive $^4$He({\\it{stopped K}}$^-$, {\\it{p}}$X^\\pm$) reaction channel, which is equivalent to the previous $E471$ event trigger condition. Detailed analysis of the present data and simulation shows that the peak, corresponding to $S^0(3115)$, has been an experimental artifact. Present analysis does not exclude the possible existence of a much wider structure. To be sensitive to such structure and for better understanding of the non-mesonic $K^-$ absorption reaction channel, detailed analysis of the data is in progress. ", "machine_abstract": "We report on the search for a K-nuclear bound state in 4 He using the reaction $^4$He (K - , n). The experiment was performed at TRIUMF with an incident beam energy of 1 GeV and a target thickness of 0.5 cm. A total number of 2.1 x 10 9 events were recorded by two large area silicon strip detectors placed downstream of the target. No evidence is found for such a state within the kinematic limits set by the experimental resolution. Upper limits are determined as a function of the binding energy B and the width \u0393 of the hypothetical state. For a narrow resonance with B = 50 MeV/c2 we find that the upper limit to its production cross section is 3 nb/sr at 90% confidence level. This corresponds to a lower limit on the coupling constant gNN of the order of 5 x 10 -4 .  The results presented here represent one of the most stringent constraints yet obtained on this type of exotic nuclear structure.     Keywords: Kaon nucleus interaction", "paraphrased_abstract": "This is the most rigorous kinematical constraint yet imposed on this kind of nuclear structure. We report the discovery of a state of K+N ligand in the n-atom (K+n+). We examine the relation between the binding energy of B and the size of the hypothetical state. For a small resonance at B=50 MeV/c2 we find that the minimum cross-section of the n-atom was three nb/sr at 90% confidence. This corresponds to a lower limit on the coupling constant gNN to the order of five x 10-4. The lower limit is determined by the binding energy B and the width  of the n-atom. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3707, "title": "Characterization of the Soluble Nanoparticles Formed through Coulombic Interaction of Bovine Serum Albumin with Anionic Graft Copolymers at Low pH", "abstract": "  A static light scattering (SLS) study of bovine serum albumin (BSA) mixtures with two anionic graft copolymers of poly (sodium acrylate-co-sodium 2-acrylamido-2-methyl-1-propanesulphonate)-graft-poly (N, N-dimethylacrylamide), with a high composition in poly (N, N-dimethylacrylamide) (PDMAM) side chains, revealed the formation of oppositely charged complexes, at pH lower than 4.9, the isoelectric point of BSA. The core-corona nanoparticles formed at pH = 3.00, were characterized. Their molecular weight and radius of gyration were determined by SLS, while their hydrodynamic radius was determined by dynamic light scattering. Small angle neutron scattering measurements were used to determine the radius of the insoluble complexes, comprising the core of the particles. The values obtained indicated that their size and aggregation number of the nanoparticles, were smaller when the content of the graft copolymers in neutral PDMAM side chains was higher. Such particles should be interesting drug delivery candidates, if the gastrointestinal tract was to be used. ", "machine_abstract": "The formation and characterization of bovine serum albumin (BSA) nanoparticles formed by electrostatic interaction between anionic graft copolymer poly(acrylic acid-co-methacrylamide) (PAM-g-PAA) and protein in aqueous solution are reported here. The size, shape, surface charge density distribution, zeta potential, and stability of these particles were investigated as functions of polymer concentration, ionic strength, temperature, and pH value. It was found that PAM-g-PAA/BSA nanoparticles could be prepared over wide ranges of conditions. At low pH values, the particle sizes increased slightly with increasing polymer concentrations due to the increase in intermolecular interactions among proteins. However, when the pH value reached 7.0, the particle sizes decreased significantly because of the decrease in net charges on both polymers and proteins. In addition, it is shown that the shapes of PAM-g-PAA/protein nanoparticles changed from spherical to ellipsoidal or rod-like structures depending upon the pH values.", "paraphrased_abstract": "The size, composition, zeta potential, stability of these particles were analyzed for various conditions. At low pH, the particle size increased slightly with increasing polymer content, because of the intermolecular interactions among proteins. At 7.0 pH, the particle size increased substantially, because the total charge of both polymers and proteins decreased. The structure of PAM-g-PAA and the protein particles was shown to be changed from spherical to ellipsoidal or rodlike in response to pH. The synthesis and character of bovine serum albumin (BSA) were described. In the aqueous phase, the synthesis of BSA, an electrostatic interaction between an anionic graft copolymer and the protein, was studied. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.021, "title": "On the morphologies, gas fractions, and star formation rates of small galaxies", "abstract": "  We use a series of N-body/smoothed particle hydrodynamics simulations and analytic arguments to show that the presence of an effective temperature floor in the interstellar medium at T_F ~ 10^4 K naturally explains the tendency for low-mass galaxies to be more spheroidal, more gas rich, and less efficient in converting baryons into stars than larger galaxies. The trend arises because gas pressure support becomes important compared to angular momentum support in small dark matter haloes. We suggest that dwarf galaxies with rotational velocities ~ 40 km/s do not originate as thin discs, but rather are born as thick, puffy systems. If accreted on to larger haloes, tenuous dwarfs of this kind will be more susceptible to gas loss or tidal transformation than scaled-down versions of larger spirals. For a constant temperature floor, pressure support becomes less important in large haloes, and this produces a tendency for massive isolated galaxies to have thinner discs and more efficient star formation than their less massive counterparts, as observed. ", "machine_abstract": "We present new results on the morphological properties, gas content, and star-formation activity in a sample of low-mass (M* < 1010 M\u2299) galaxies at z = 0.5 \u2212 1.0 drawn from the GOODS-North field. We use deep HST/ACS imaging to measure galaxy sizes, S\u00e9rsic indices, and bulge-to-disk ratios for these objects. In addition we obtain Spitzer/MIPS 24\u00b5m photometry which allows us to estimate their total infrared luminosities as well as dust-corrected SFRs using the prescription of Kennicutt et al. (2009) . Our main findings are summarized below:  -The majority of our sample is composed by disk-dominated systems with n < 2. The median value of B/T is 0.3 \u00b1 0.1.  -There exists an anti-correlation between stellar mass and size that can be explained if most of this population has experienced recent episodes of intense star formation. This would have caused them to expand significantly while still being dominated by young stars.  -Most of the galaxies in our sample show high specific star-formation rates (SSFR > 10^-10 yr-1). These values are comparable or even higher than those found among local dwarf irregular galaxies. -Our data suggest that there may exist two different populations within our sample. One group shows relatively large sizes and low SSFRs indicative of quiescent disks. A second class displays smaller sizes but much higher SSFRs suggesting they could be undergoing rapid growth through mergers and/or accretion events.", "paraphrased_abstract": "Is the mean of a swarm of young stars as small as a crow? - In our study there are two classes of stars, one with a relatively large size and low swarm, indicating a quiescent disk, and the other with a much smaller size, suggesting a rapid growth due to mergers and/or accretion. We describe in detail the morphology, gas content and formation activity of a small sphere of the type G-S-D, whose average swarm of stars is z = 0.5 - 1.0. We are interested in the morphological properties of the spheres, gas content and the formation of stars, for which we have a special method of measuring the thickness of the sphere, the volume, the thickness of the disk, and the density of the disk. We have therefore obtained, by means of the swarming method, the measurement of the total infrared luminosity, the S-F of the dust-corrected S-F of the sphere, which is according to the prescription of Kennicutt et al. (2005). The first of these results is that, in our sample, there are a number of systems, with n = 2. The n=2 numbers are the highest. The second is small, but the SSFR is much", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0492, "title": "Refuting the Pseudo Attack on the REESSE1+ Cryptosystem", "abstract": "  We illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition Z/M - L/Ak < 1/(2 Ak^2) is not sufficient for f(i) + f(j) = f(k). Illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. Demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * D at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. Further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. We explain why Cx = Ax * W^f(x) (% M) is changed to Cx = (Ax * W^f(x))^d (% M) in REESSE1+ v2.1. To the signature fraud, we point out that [8] misunderstands the existence of T^-1 and Q^-1 % (M-1), and forging of Q can be easily avoided through moving H. Therefore, the conclusion of [8] that REESSE1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in REESSE1+) is fully incorrect, and as long as the parameter Omega is fitly selected, REESSE1+ with Cx = Ax * W^f(x) (% M) is secure. ", "machine_abstract": "The authors present an attack against the REESSE1+ cryptosystem, which is based on the hardness of solving certain lattice problems.  They show that this system can be broken in polynomial time using their algorithm and they provide experimental results to demonstrate its efficiency.   The authors conclude by discussing possible countermeasures for the system. This work was supported by the National Science Foundation under Grant No. CCF-0430018. Authors:  Yi-Cheng Chang (Chang), Chih-Chin Chen (Chen), Hsinchu, Taiwan; Ming-Hsuan Wu (Wu), Taipei, Taiwan; Yu-Ting Lin (Lin), Taichung, Taiwan; Shih-Fang Tsai (Tsai), Taipei, Taiwan; Wen-Jen Lee (Lee), Taipei, Taiwan; Kuo-Yi Huang (Huang), Taipei, Taiwan; Tzi-Chao Chan (Chan), Taipei, Taiwan; Chin-Long Yeh (Yeh), Taipei, Taiwan; Chung-Ming Liu (Liu), Taipei, Taiwan; Yuan-Youh Chen (Chen), Taipei, Taiwan; Jyh-Shing Wang (Wang), Taipei, Taiwan; Cheng-Wei Lai (Lai), Taipei, Taiwan", "paraphrased_abstract": "Yi Chu Chang, Chi Chi Chen, Hun Chen, Hsinchu, Taiwan; Ming Huan Wu, Taipei, Taiwan; Liu Bing (Hsinchu), Taipei, Taiwan; Wen Jen Liu, Taipei, Taiwan; Jia Zhong (Hsinchu), Taiwan; Yeh Chang, Taipei, Taiwan; Wen Jue, Taipei, Taiwan; Yuan You Chen, Taipei, Taiwan; Yuan You Chen, Taipei, Taiwan; Liu Jing (Wang), Taipei, Taiwan; Wen Jen, Taipei, Taiwan; Jia Zhong (Chan), Taipei, Taiwan; Wen Jen, Taipei, Taiwan; Xuanzang: Xuanzang; Xuanzang: Xuanzang, Xuanzang, Xuanzang: the system is hard, and it is used in the calculation of the inverse. The author has received a grant from the National Science Foundation, which is a project of Xuanzang: the problem is the inverse. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2146, "title": "Efficient Multidimensional Data Redistribution for Resizable Parallel Computations", "abstract": "  Traditional parallel schedulers running on cluster supercomputers support only static scheduling, where the number of processors allocated to an application remains fixed throughout the execution of the job. This results in under-utilization of idle system resources thereby decreasing overall system throughput. In our research, we have developed a prototype framework called ReSHAPE, which supports dynamic resizing of parallel MPI applications executing on distributed memory platforms. The resizing library in ReSHAPE includes support for releasing and acquiring processors and efficiently redistributing application state to a new set of processors. In this paper, we derive an algorithm for redistributing two-dimensional block-cyclic arrays from $P$ to $Q$ processors, organized as 2-D processor grids. The algorithm ensures a contention-free communication schedule for data redistribution if $P_r \\leq Q_r$ and $P_c \\leq Q_c$. In other cases, the algorithm implements circular row and column shifts on the communication schedule to minimize node contention. ", "machine_abstract": "We present an efficient data redistribution scheme that allows the parallel execution of computations on dynamically resized computational domains in distributed memory environments. The proposed approach is based on multidimensional block-cyclic distributions and exploits locality by using space-filling curves to map blocks onto processors. We show how this technique can be used to efficiently redistribute data between different processor configurations, while minimizing communication overheads. Our experimental results demonstrate significant performance improvements over existing approaches when executing applications with dynamic load balancing requirements. In particular, we achieve speedups up to 3\u00c2 compared to state-of-the-art techniques such as the one presented in [1] .  Introduction  Parallel computing has become increasingly important due to its ability to solve large problems faster than serial computers [2] , but it also presents new challenges related to the distribution of work among multiple processing units [3] . In order to take advantage of parallelism, many algorithms are designed so that they can run simultaneously on several processors [4] . However, these algorithms often require some form of data redistribution during their execution [5] . For example, consider a computation where each process stores part of a dataset (e.g., matrix) [6] . If the number of processes changes at runtime, then all processes need to exchange information about which parts of the dataset they store before continuing [7] . This problem becomes even more challenging if the size of the datasets stored by individual processes varies [8] or if there are dependencies between them [9] .", "paraphrased_abstract": "The advantage of parallel computing is that it can solve large problems faster than serial machines. However, parallel machines have some additional challenges, and the data redistribution has been observed to be a main feature of the solution. To make use of parallelism, many algorithms have been developed, which can run simultaneously on several processors, and thus there is a need for some special synchronization of the data in various ramifications. For example, if the number of processors changes during the computation, then all the processors must exchange the information about which parts of the dataset they store before they can continue. This problem is even more complicated if the number of processors changes at runtime, or if the number of processes is synchronized. The present method uses a cyclic distribution, which exploits locality, based on a space-filling curve, to map blocks to processors, which reduces the overhead of communication. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3003, "title": "Deformation principle and further geometrization of physics", "abstract": "  The space-time geometry is considered to be a physical geometry, i.e. a geometry described completely by the world function. All geometrical concepts and geometric objects are taken from the proper Euclidean geometry. They are expressed via the Euclidean world function \\sigma_E and declared to be concepts and objects of any physical geometry, provided the Euclidean world function \\sigma_E is replaced by the world function \\sigma of the physical geometry in question. The set of physical geometries is more powerful, than the set of Riemannian geometries, and one needs to choose a true space-time geometry. In general, the physical geometry is multivariant (there are many vectors which are equivalent to a given vector, but are not equivalent between themselves). The multivariance admits one to describe quantum effects as geometric effects and to consider existence of elementary particles as a geometrical problem, when the possibility of the physical existence of an elementary geometric object in the form of a physical body is determined by the space-time geometry. Multivariance admits one to describe discrete and continuous geometries, using the same technique. A use of physical geometry admits one to realize the geometrical approach to the quantum theory and to the theory of elementary particles. ", "machine_abstract": "The deformation principle is the main tool in this article for constructing new geometric structures on physical spaces, which are not necessarily Riemannian manifolds but can be more general objects such as Finsler or pseudo-Finsler ones.  The basic idea behind it consists in deforming an initial space into another one by means of some suitable transformation group acting transitively on both spaces. This method allows to obtain many interesting results concerning various aspects of geometry and physics (e.g., geodesic flows,...). In particular, we show that any homogeneous Finsler manifold admits a canonical connection with totally skew-symmetric torsion whose curvature tensor satisfies certain properties similar to those satisfied by the Weyl conformal curvature tensor. We also prove that every locally Minkowski spacetime has a natural generalization of the Poincar\u00e9 metric, called the generalized Poincar\u00e9 metric, which turns out to be invariant under all local Lorentz transformations. Finally, we present several examples illustrating our approach.", "paraphrased_abstract": "The article shows how the deformation principle is used to form new geometric structures on physical space, not necessarily Riemannian ones, but general ones such as Finsler or pseudo-Finsler ones. We also show that any local Minkowski space is a metric of Poincar\u00e9, a metric of Poincar\u00e9 which is invariant with all local Lorentz transformations. In this way we have several important results in the area of geometry and physics (i.e., flows of geodesic fluids, etc.). We have also proved that every Minkowski space, including a local Minkowski space, has a natural generalization of the Poincar\u00e9 metric called the generalized Poincar\u00e9 metric, which, according to the local Lorentz transformation, is invariant with all local Lorentz transformations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2517, "title": "Midlatitude Cirrus Clouds and Multiple Tropopauses from a 2002-2006 Climatology over the SIRTA Observatory", "abstract": "  This study present a comparison of lidar observations of midlatitude cirrus clouds over the SIRTA observatory between 2002 and 2006 with multiple tropopauses (MT) retrieved from radiosounding temperature profiles. The temporal variability of MT properties (frequency, thickness) are discussed. Results show a marked annual cycle, with MT frequency reaching its lowest point in May (~18% occurrence of MT) and slowly rising to more than 40% in DJF. The average thickness of the MT also follows an annual cycle, going from less than 1 km in spring to 1.5 km in late autumn. Comparison with lidar observations show that cirrus clouds show a preference for being located close below the 1st tropopause. When the cloud top is above the 1st tropopause (7% of observations), in 20% of cases the cloud base is above it as well, resulting in a cirrus cloud \"sandwiched\" between the two tropopauses. Compared to the general distribution of cirrus, cross-tropopause cirrus show a higher frequency of large optical depths, while inter-tropopause cirrus show almost exclusively low optical depths (Tau < 0.03 in 90% of cases) typical of subvisible clouds. Results suggest the occurrence of inter-tropopause cirrus clouds is correlated with the frequency of multiple tropopauses. ", "machine_abstract": "Cirrus clouds are an important component in Earth's radiation budget, but their radiative properties remain poorly understood because they occur at high altitude where direct measurements are difficult to make. In this study we use data collected by the Scripps Institution of Oceanography (SIO) lidar system located on the SIRTA observatory near Paris during [2002] [2003] [2004] [2005] [2006] . The observations show that cirrus clouds can be found between 5 km and 12 km above sea level with a mean optical depth of 0.2 \u00b1 0.1. We find that mid-latitude cirrus clouds have multiple tropopause features such as double or triple tropopauses which may affect their microphysical structure. These results suggest that cirrus clouds play an important role in determining the vertical distribution of water vapor in the atmosphere. Citation:  Introduction  Cirrus clouds cover about 10%-20% of the globe's surface area [Sassen et al., 2008] , yet little is known about how these clouds form and evolve. They are particularly challenging to observe since they occur at high altitudes (5-12km), where temperatures are low enough for ice particles to exist, but too cold for liquid droplets to condense. As a result, most information about cirrus clouds comes indirectly through remote sensing techniques using instruments like lidars, radars, satellites, and aircrafts. However, each instrument has its own strengths and weaknesses depending upon the measurement technique used. For example, ground-based lidars provide accurate profiles of cloud height and extinction coefficients, while satellite retrieval algorithms suffer from poor spatial resolution and uncertainties associated with aerosol contamination.", "paraphrased_abstract": "They cover a large area of about ten percent of the world\u2019s surface, and yet little is known about how they form and evolve. The cirrus clouds form and grow at a distance of between five and twelve km, and are more difficult to detect than the lower ones. They are particularly difficult to study because they are located at high altitudes, about ten kilometers from the ground, where the temperatures are low enough for ice to form, and where the water drops are too cold for liquid to disperse. Consequently, the main method of determining cirrus clouds is to use a lidar, radar, and aircraft. Various methods are available to obtain this information, but all are of different strengths and weaknesses, and they are very different from each other, and each method has its own advantages and disadvantages. For instance, ground-based lidars yield accurate measurements of the cloud height and extinction coefficients, whereas the satellites provide inadequate resolution and uncertainties from aerosols. These results suggest that cirrus clouds play an important role in determining the vertical distribution of water vapour in the atmosphere. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2139, "title": "Spectral type dependent rotational braking and strong magnetic flux in three components of the late-M multiple system LHS 1070", "abstract": "  We show individual high resolution spectra of components A, B, and C of the nearby late-M type multiple system LHS 1070. Component A is a mid-M star, B and C are known to have masses at the threshold to brown dwarfs. From our spectra we measure rotation velocities and the mean magnetic field for all three components individually. We find magnetic flux on the order of several kilo-Gauss in all components. The rotation velocities of the two late-M objects B and C are similar (vsini = 16km/s), the earlier A component is spinning only at about half that rate. This suggests weakening of net rotational braking at late-M spectral type, and that the lack of slowly rotating late-M and L dwarfs is real. Furthermore, we found that magnetic flux in the B component is about twice as strong as in component C at similar rotation rate. This indicates that rotational braking is not proportional to magnetic field strength in fully convective objects, and that a different field topology is the reason for the weak braking in low mass objects. ", "machine_abstract": "We report on spectropolarimetric observations of the M8+M9 binary star LHS 1070A,B (GJ 436) with ESPaDOnS at CFHT. The two stars are separated by only 0. \u2032\u2032 1 and have been known to be magnetically active for many years. We find that both stars show significant circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields. In addition we detect Stokes V signatures indicating net linear polarization across all observed spectral lines. This is likely caused by scattering processes within the stellar atmosphere. Using our new data set together with previously published photometric measurements we derive rotation periods of P A = 3.6 \u00b1 0.1 days and P B = 4.2 \u00b1 0.3 days for the primary and secondary component respectively. These values are significantly longer than those derived from previous studies which were based solely on photometry. Our results suggest that the rotation period of each individual component depends strongly on its effective temperature as well as its surface gravity.", "paraphrased_abstract": "At the same time, we also show that our observations of the EM constellation LHS 1070 A and B in the CFHT are of much greater length than the previous ones, which merely made use of photometry. These measurements indicate that the rotational period of the individual components is strongly influenced by its effective temperature and the gravity of the atmosphere. We show that both of them exhibit a very circular polarized emission line, which indicates the reversal of Zeeman from one to another by the influence of magnetic forces. This polarization is probably caused by scattering processes in the atmosphere. We report a study of the spectropolarimetric measurements of the M8-M9 binary star, LHS 1070 A-B (GJ 436) with ESPaDOnS, a sphere located at CFHT. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.2381, "title": "Koszul duality in deformation quantization, I", "abstract": "  Let $\\alpha$ be a polynomial Poisson bivector on a finite-dimensional vector space $V$ over $\\mathbb{C}$. Then Kontsevich [K97] gives a formula for a quantization $f\\star g$ of the algebra $S(V)^*$. We give a construction of an algebra with the PBW property defined from $\\alpha$ by generators and relations. Namely, we define an algebra as the quotient of the free tensor algebra $T(V^*)$ by relations $x_i\\otimes x_j-x_j\\otimes x_i=R_{ij}(\\hbar)$ where $R_{ij}(\\hbar)\\in T(V^*)\\otimes\\hbar \\mathbb{C}[[\\hbar]]$, $R_{ij}=\\hbar \\Sym(\\alpha_{ij})+\\mathcal{O}(\\hbar^2)$, with one relation for each pair of $i,j=1...\\dim V$. We prove that the constructed algebra obeys the PBW property, and this is a generalization of the Poincar\\'{e}-Birkhoff-Witt theorem. In the case of a linear Poisson structure we get the PBW theorem itself, and for a quadratic Poisson structure we get an object closely related to a quantum $R$-matrix on $V$. At the same time we get a free resolution of the deformed algebra (for an arbitrary $\\alpha$).   The construction of this PBW algebra is rather simple, as well as the proof of the PBW property. The major efforts should be undertaken to prove the conjecture that in this way we get an algebra isomorphic to the Kontsevich star-algebra. ", "machine_abstract": "We give an explicit formula for the star product on the cotangent bundle $T^*M$ over a Poisson manifold $(M,\\pi)$ with symplectic leaves $L_i$ and corresponding moment maps $\\mu_i : T^*M \\to L_i$.  We show that this star product is given by the formula  $$ (f \\star g)(x) = \\sum_{i=0}^n f(x)g(\\mu_i^{-1}(x)) \\pi^{ij}(x)  $$ where $\\pi^{ij}$ are the components of the inverse matrix to the Poisson tensor $\\pi_{ij}$.  This result generalizes earlier results obtained by Kontsevich [K] , Fedosov [F]  and others.   It also gives a new proof of the fact that the star product is uniquely determined up to isomorphism by its properties of associativity and commutativity.    The main idea behind our approach is to use the formalism of Koszul complexes developed by Getzler [G1] .", "paraphrased_abstract": "He added: \u201cThe main idea of the research was to take advantage of the formalism of Koszul complexes by Getzler, who has developed them. The main idea of this study was to employ the formalism of Koszul complexes developed by Getzler, the formalism of which he adopted. Moreover, this result is a new proof of the fact that the product of a bundle is unique up to isomorphism by the property of being associative and commutative. Hence, we have here a formula for the product of a bundle of T*M, which, over a Poisson-Manifold, with symplectic leaves of Li, and corresponding moment maps, mui: mui - T*M to Li. We show that this product is given by the formula  (fstar g) = = sumi=0, sumi=0, pii1(x)pii0 ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.286, "title": "Luminosity function of binary X-ray sources calculated using the Scenario Machine", "abstract": "  Using the ``Scenario Machine'' we have carried out a population synthesis of X-ray binaries for the purpose of modelling of X-ray luminosity functions (XLFs) in different types of galaxies: star burst, spiral, and elliptical. This computer code allows to calculate, by using Monte Carlo simulations, the evolution of a large ensemble of binary systems, with proper accounting for the spin evolution of magnetized neutron stars.   We show that the XLF has no universal type. It depends on the star formation rate in the galaxy. Also it is of importance to take into account the evolution of binary systems and life times of X-ray stages in theoretical models of such functions. We have calculated cumulative and differential XLFs for the galaxy with the constant star formation rate. Also we have calculated cumulative luminosity functions for different intervals of time after the star formation burst in the galaxy and curves depicting the evolution of the X-ray luminosity after the star formation burst in the galaxy. ", "machine_abstract": "We present here an application of the Scenario Machine (SM) to calculate the luminosity functions for different types of binaries in our Galaxy, including black hole and neutron star systems as well as white dwarf - main sequence stars. The SM is used to generate synthetic populations of these objects by assuming that they are formed according to some initial mass distribution and evolve through various stages following evolutionary tracks obtained from stellar evolution calculations. We find good agreement between the results of this method with those derived from observations. This work was supported by NASA grant NAG5-10842. Keywords: Binary X-rays Sources; Luminosity Function; Stellar Evolutionary Tracks. 1 Introduction X-ray binaries are composed of either two neutron stars or one neutron star plus another object such as a black hole or a white dwarf. They can be divided into three categories based on their orbital periods; short-period (P orb < 3 hrs), intermediate-period (3 hrs < P orb < 100 days), and long-period (P orb > 100 days). In addition there exist several classes of X-ray transients which have been observed at all periods but whose nature has not yet been determined conclusively [1] . These include soft X-ray transients, supersoft X-ray transients, classical novae, symbiotic stars, recurrent Novae, and microquasars [2] . The number density of X-ray binaries per unit volume depends upon both the formation rate of binaries and how many survive until they become detectable [3] . Since most of them are located within 10 kpc of Earth [4] , it is possible to estimate the total number of X-ray binaries in our galaxy if we know the space density of each type of system [5] . However, since only about 10% of known Galactic X-ray binaries have measured distances [6] , it is difficult to determine the true space densities accurately. Therefore, it becomes necessary to use other methods to obtain estimates of the space density of X-ray binaries [7, 8] .", "paraphrased_abstract": "For this purpose, we have developed a technique for the calculation of the luminosity of various stars in our galaxy, namely the black hole and the neutron star, the main sequence stars, and the white dwarf. Besides, there are many X-ray transients, but their nature is not yet clear, and there are some classes of transients, but which do not have a conclusive definition, such as the soft X-ray transients, the super-soft X-ray transients, the novae, the symbiotic stars, the recurrent novae, and the microquasars. The X-ray transients, however, are composed of either two neutron stars or one neutron star, whereas they are divided into three groups, in which the distance between them is taken into account, and the number of binaries of each class is estimated, and therefore we propose to use the SM to compute the radiance of these objects in order to make them appear in different stages. We are interested in the calculation of the luminosity of different types of X-rays in the galaxy, and we are based on the fact that X-rays are formed by the same mass distribution and have developed in various stages, following the epoch of the stellar evolution. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3128, "title": "Variation of the Amati Relation with the Cosmological Redshift: a Selection Effect or an Evolution Effect?", "abstract": "  Because of the limit in the number of gamma-ray bursts (GRBs) with available redshifts and spectra, all current investigations on the correlation among GRB variables use burst samples with redshifts that span a very large range. The evolution and selection effects have thus been ignored, which might have important influence on the results. In this Letter, we divide the 48 long-duration GRBs in Amati (2006, 2007) into four groups with redshift from low to high, each group contains 12 GRBs. Then we fit each group with the Amati relation $\\log E_\\iso = a + b \\log E_\\p$, and check if the parameters $a$ and $b$ evolve with the GRB redshift. We find that $a$ and $b$ vary with the mean redshift of the GRBs in each group systematically and significantly. Monte-Carlo simulations show that there is only $\\sim 4$ percent of chance that the variation is caused by the selection effect arising from the fluence limit. Hence, our results may indicate that GRBs evolve strongly with the cosmological redshift. ", "machine_abstract": "We have studied the variation of the Amati relation (Amati et al., 2002) in a sample of about 100 GRBs detected by Swift satellite and we found that it is not constant, but varies as a function of redshift z. We also find that this effect cannot be explained only by selection effects due to detector thresholds.     The observed evolution can be described by a power law dependence on cosmological time t~(1+z)3.5. This result suggests that there may exist some physical mechanism responsible for such behaviour. In particular, we suggest that the luminosity distance dL could play a role in determining the shape of the spectrum at high energies. If so, then the spectral properties of GRB prompt emission are related to its intrinsic luminosity rather than being determined solely by the geometry of the emitting region.  The results presented here support previous claims that the Amati relation should be used cautiously when applied to distant bursts.", "paraphrased_abstract": "This is consistent with the previous conclusion that the Amati relation should be cautiously applied to distant bursts. We have studied the radiance of the Amati relation (Amati et al., 2002) in a sample of about 100 of the enigmatic swarms detected by Swift, and we have found that this effect is not constant, but varies according to redshift z. The radiance of the enigmatic swarms is not fixed, but is determined by the distance between the enigmatic regions. This is not true, and can be explained by the selection of the detectors. We also find that the radiance of a swarm is a function of the redshift z, and that this is not a general phenomenon, but is a function of the distance between the enigmatic regions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1919, "title": "Interpreting and predicting the yield of transit surveys: Giant planets in the OGLE fields", "abstract": "  Transiting extrasolar planets are now discovered jointly by photometric surveys and by radial velocimetry. We want to determine whether the different data sets are compatible between themselves and with models of the evolution of extrasolar planets. We simulate directly a population of stars corresponding to the OGLE transit survey and assign them planetary companions based on radial velocimetry discoveries. We use a model of the evolution and structure of giant planets assuming a variable fraction of heavy elements. The output list of detectable planets of the simulations is compared to the real detections. We confirm that the radial velocimetry and photometric survey data sets are compatible within the statistical errors, assuming that planets with periods between 1 and 2 days are approximately 5 times less frequent than planets with periods between 2 and 5 days. We show that evolution models fitting present observational constraints predict a lack of small giant planets with large masses. We also identify distinct populations of planets: those with short periods (P < 10d) are only found in orbit around metal-rich stars with [Fe/H] > -0.07. We further confirm the relative absence of low-mass giant planets at small orbital distances. ", "machine_abstract": "We present an analysis of the properties of giant planet candidates detected by the Optical Gravitational Lensing Experiment (OGLE) survey, which has been monitoring stars towards the Galactic bulge for more than ten years. We find that most of these systems are likely to be composed of two low-mass objects with masses below 0.5 MJup orbiting each other on periods longer than 100 days. The majority of them have orbital inclinations close to 90 degrees. These results suggest that many of the planetary mass companions found around nearby solar-type stars may also exist around distant red giants. This is important because it allows us to study their atmospheric composition using high-resolution spectroscopy.     Keywords: Planetary systems; Planets; Transiting exoplanet searches; Red-giant stars; Bulge; Massive star formation; Stellar evolution; Extrasolar planets; Astrophysics", "paraphrased_abstract": "The authors of this work were able to make a systematic analysis of the composition of the planetary masses of the star-traveling stars. The authors found that most of the planetary masses found around the Sun were also found on the Red-giant stars. The swarming stars of the Red-giant stars were found to be at their heights, and with orbital inclinations to 90 degrees. Moreover, the atmospheric composition of the stars in this region is quite precise. In this paper, we investigate the physics of the giant planets in the Observation of the Optical Gravitational Lensing Experiment, which has been looking for stars in the Galactic Galactic Belt for more than ten years, we have obtained the results of the OGLE Observation. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4089, "title": "Local purity distillation with bounded classical communication", "abstract": "  Local pure states are an important resource for quantum computing. The problem of distilling local pure states from mixed ones can be cast in an information theoretic paradigm. The bipartite version of this problem where local purity must be distilled from an arbitrary quantum state shared between two parties, Alice and Bob, is closely related to the problem of separating quantum and classical correlations in the state and in particular, to a measure of classical correlations called the one-way distillable common randomness. In Phys. Rev. A 71, 062303 (2005), the optimal rate of local purity distillation is derived when many copies of a bipartite quantum state are shared between Alice and Bob, and the parties are allowed unlimited use of a unidirectional dephasing channel. In the present paper, we extend this result to the setting in which the use of the channel is bounded. We demonstrate that in the case of a classical-quantum system, the expression for the local purity distilled is efficiently computable and provide examples with their tradeoff curves. ", "machine_abstract": "We consider the problem of local purification in quantum networks, where each node is allowed to perform only local operations and communicate classically with its neighbors. We show that for any graph G = (V, E) there exists an algorithm which transforms any pure state on V into another pure state such that:  1. The number of qubits communicated between neighboring nodes does not exceed O(log n), where n is the size of the network.  2. Each node performs at most polylogarithmic many local operations. Our result implies that if we allow unlimited amount of entanglement then it suffices to use only logarithmic amount of classical communication per edge. This improves upon previous results by Aaronson et al., who showed that \u2126(n log n) bits are necessary even when all states are restricted to be product states. In addition our result shows how to constructively convert this bound into an efficient protocol.", "paraphrased_abstract": "The following graph, G, X, is an example of a procedure that transforms any pure state on V into another pure state, such that, when the numbers of qubits are multiplied by O(log n), O is the size of the network. The result shows that, when the entanglement is unlimited, there is still only logarithmic number of bits per edge. We present the result of our work, and it is improved by Aaronson et al., who showed that there is still a requirement for (n log n) bits, even if all the states are product. We present a method for removing the entanglement from a network where each node is permitted to perform only local tasks and communicate with its neighbors. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.158, "title": "Optical implementation and entanglement distribution in Gaussian valence bond states", "abstract": "  We study Gaussian valence bond states of continuous variable systems, obtained as the outputs of projection operations from an ancillary space of M infinitely entangled bonds connecting neighboring sites, applied at each of $N$ sites of an harmonic chain. The entanglement distribution in Gaussian valence bond states can be controlled by varying the input amount of entanglement engineered in a (2M+1)-mode Gaussian state known as the building block, which is isomorphic to the projector applied at a given site. We show how this mechanism can be interpreted in terms of multiple entanglement swapping from the chain of ancillary bonds, through the building blocks. We provide optical schemes to produce bisymmetric three-mode Gaussian building blocks (which correspond to a single bond, M=1), and study the entanglement structure in the output Gaussian valence bond states. The usefulness of such states for quantum communication protocols with continuous variables, like telecloning and teleportation networks, is finally discussed. ", "machine_abstract": "We propose an optical scheme for implementing the Gaussian valence bond (GVB) state, which is one of the most important classes of quantum states in condensed matter physics. The GVB state can be used to describe many-body systems with strong correlations such as spin liquids or Mott insulators. We show that our proposed scheme allows us to distribute entanglement between two distant parties by using only linear optics elements and single-photon sources. Our results may have potential applications in quantum information processing.     Introduction     Quantum entanglement plays a crucial role in various fields ranging from quantum communication [1] , quantum metrology [2] , quantum sensing [3] , and quantum computing [4] . In particular, it has been shown that quantum entangled states are useful resources for quantum teleportation [5] , superdense coding [6] , remote state preparation [7] , and quantum key distribution [8] .   In recent years, there has been growing interest in studying quantum entanglement in many-body systems [9] - [11] . For example, the ground-state wavefunction of strongly correlated fermions on lattices can be written as a product of local singlet pairs known as valence bonds [12] . This class of states is called valence-bond solid (VBS) states [13] . It was later found that VBS states can also be represented by so-called valence bond basis [14] . These states include the famous N\u00e9el state [15] describing antiferromagnetic order [16] , the Haldane phase [17] corresponding to integer-spin chains [18] , and the Affleck-Kennedy-Lieb-Tasaki (AKLT) model [19] representing gapped spin-1/2 chain [20] .     Recently, several schemes [21]- [23] were proposed to generate these types of quantum states experimentally. However, all existing proposals require nonlinear interactions among photons [24] and/or complicated setups [25] . Therefore, they cannot be implemented easily in practice. On the other hand, some experimental demonstrations [26] - [28] have been performed recently to produce photonic qubits [29] . Thus, it would be interesting if we could find ways to implement these quantum states without requiring any nonlinear interaction [30] .", "paraphrased_abstract": "In recent years, a great deal of interest has sprung up on the study of quantum entanglement in many-body systems. These states are called valence-bond solids. These are called valence-bond solids. They are also called valence-bond solids. They are named valence-bond solids. They are of a general nature and are used in the most complex fields such as quantum communication, metrology, sensing, and computer-aided computation. These are called valence-bond solids. Various approaches have been made to study these states, but all of them are not ingenious. Some experiments have been carried out to produce photonic crystals. They are presented here. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1392, "title": "Two-dimensional N=(2,2) super Yang-Mills theory on computer", "abstract": "  We carry out preliminary numerical study of Sugino's lattice formulation \\cite{Sugino:2004qd,Sugino:2004qdf} of the two-dimensional $\\mathcal{N}=(2,2)$ super Yang-Mills theory (2d $\\mathcal{N}=(2,2)$ SYM) with the gauge group $\\SU(2)$. The effect of dynamical fermions is included by re-weighting a quenched ensemble by the pfaffian factor. It appears that the complex phase of the pfaffian due to lattice artifacts and flat directions of the classical potential are not problematic in Monte Carlo simulation. Various one-point supersymmetric Ward-Takahashi (WT) identities are examined for lattice spacings up to $a=0.5/g$ with the fixed physical lattice size $L=4.0/g$, where $g$ denotes the gauge coupling constant in two dimensions. WT identities implied by an exact fermionic symmetry of the formulation are confirmed in fair accuracy and, for most of these identities, the quantum effect of dynamical fermions is clearly observed. For WT identities expected only in the continuum limit, the results seem to be consistent with the behavior expected from supersymmetry, although we do not see clear distintion from the quenched simulation. We measure also the expectation values of renormalized gauge-invariant bi-linear operators of scalar fields. ", "machine_abstract": "We present the results of numerical simulations of two-dimensional N = (2, 2) super-Yang-Mills theory with gauge group SU(N). We use an improved action and perform calculations at several values of the coupling constant g in the range 0.1 < g < 1.0. The lattice size is 16 x 32 for all our runs except one where we used 24 x 48 sites. In order to study finite-size effects we have also performed some runs using 12 x 24 and 20 x 40 lattices.  The main goal of this work was to check whether the phase transition between confinement and deconfinement phases observed previously by other authors persists when the continuum limit is approached.     Our data show that there are no significant differences between the results obtained on different sizes of lattices within statistical errors. This indicates that the system does not undergo any phase transitions as it approaches the continuum limit.", "paraphrased_abstract": "The main purpose of the present work was to test whether the transition between confinement and deconstitution was present at the boundary of the line of the continuum. In this regard we derived the results of a numerical simulation of two-dimensional N = Yang-Mills with gauge group SU(N) in a gauge group of gauges of about 0.1. To examine the effects of these results, we carried out numerical simulations of the three-dimensional N. between two-dimensional N and two-dimensional N. With a modified action, we derived the following results for each of the two-dimensional N. and N.. The numerical analyses took place on a fixed, unscaled, and symmetrical basis, and we gave them some numerical results for each of the three-dimensional SU(N) parameters. We therefore presented the results of our calculations for the two-dimensional SU(N) parameters, averaging the results of the numerical simulations to a value of 0.1 g  1.0. We compared these results with the results of the other authors, and show that the stipulation of the transition from the confinement phase to the separation of the two phases, as we have been able to observe in previous papers, has not been observed. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2296, "title": "Phase resolved measurements of stimulated emission", "abstract": "  The development of the semiconductor quantum cascade laser (QCL) [1] has enabled bright coherent sources operating at frequencies between the optical (>100 THz) and electronic (<0.5 THz) ranges opening this frequency region for fundamental science investigations [2-5] as well as for applications [6]. However, detailed information about the internal processes in QCLs and their ultrafast dynamics are needed for their further development towards high output power, longer wavelengths and stable pulsed operation. We introduce a novel approach to study the gain and dynamics of a terahertz (THz) QCL [7] using phase resolved measurements of the stimulated emission. This is enabled by direct recording of the emitted electric field with <100 fs time resolution. For the case of the THz QCL we demonstrate the validity of the quantum mechanical description of the laser. We show for the first time the real bandwidth of the terahertz gain medium, the current dependence of this gain, and the losses associated with the wave propagation in the laser waveguide. ", "machine_abstract": "We report on the measurement and analysis of phase-resolved spontaneous emission in an optically pumped semiconductor laser with external cavity feedback (ECF). The ECF is formed by two mirrors, one of which has a variable distance to the active region of the laser diode. We show that this allows for tuning the delay between the optical pulses emitted at each round trip through the external cavity. By measuring the time dependent intensity autocorrelation function we are able to determine the pulse duration as well as the repetition rate of the pulsed output. In addition, we demonstrate how these parameters can be used to extract information about the carrier dynamics inside the gain medium.     A detailed description of the experimental setup together with results obtained under different operating conditions will be presented. Finally, we discuss possible applications of our method including its use for studying relaxation oscillations or mode locking phenomena in lasers. Stimulated emission occurs when electrons in a material absorb energy from photons and then re-emit them into free space. This process is responsible for generating light in many common devices such as LEDs and lasers. However, it remains challenging to directly measure the properties of stimulated emission because the signal-to-noise ratio is often very low due to the small number of spontaneously emitted photons compared to those generated via stimulated emission. Herein, we present a new approach based on using a tunable external cavity to enhance the signal strength while simultaneously allowing us to control the timing of the emitted photons. Using this technique, we have been able to perform high-resolution measurements of the temporal characteristics of stimulated emission in a semiconductor laser.", "paraphrased_abstract": "The stimulus-induced photons are absorbed in the material by the electrons and released into space. The light produced by these devices is very common in many devices, such as the LED and the laser. This stimulus-induced emitted photons have a low signal-to-noise ratio and the number of emitted photons is low. To this end, a new approach has been developed which enables us to measure the temporal characteristics of the emitted photons in a semiconductor laser. In this way, we have obtained a precise measurement of the temporal properties of the emitted photons. We will present the results of this method and the possible applications: to study the relaxation of the emitted beam, to observe the reversal of the beam, to examine the relaxation of the emitted beams in the light. In our work, we demonstrate the analysis of phase resolved spontaneous emission in a silicon laser with a feedback circuit. The feedback circuit consists of two mirrors, each one located at a certain distance from the active region of the laser diode. The mirror has two distances, each with a different distance to the active region of the laser. The external circuit, in the form of a cavity, is a mirror of an inert chamber, with a tunable distance to the active region of the laser di", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2205, "title": "Vector mesons from AdS/TC to the LHC", "abstract": "  With the use of the AdS/CFT dictionary, a five-dimensional effective description of dynamical electro-weak symmetry breaking with walking behavior is constructed. The minimal model contains only two new parameters, the confinement scale and the effective coupling of the new strong sector. This parameter space is restricted by the precision electro-weak constraints and by the requirement that the five-dimensional coupling be perturbative (corresponding to the large-N regime in four-dimensional language). The lightest observable new states are a set of four nearly degenerate spin-1 states with the same quantum numbers as the standard-model electro-weak gauge bosons, and masses in the few TeV range. Their decay rate is dominated by two-fermion final states. The number of pp -> mu mu and pp -> mu nu events is studied as a function of the LHC integrated luminosity and of the two free parameters. Discovery at the LHC is possible over a significant part of the allowed parameter space up to masses of 4 TeV already with 10/fm of integrated luminosity. ", "machine_abstract": "We present an overview of our recent work on vector meson production in heavy ion collisions at RHIC and LHC energies, based on holographic QCD models with chiral symmetry breaking (AdS/QCD). We discuss how these models can be used to calculate hadronic observables such as transverse momentum spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear reactions. In particular we focus on the role played by the coupling between the bulk fields and the gauge field fluctuations dual to vector mesons. The results are compared with experimental data obtained at RHIC and LHC: they show good agreement both qualitatively and quantitatively.     Keywords: Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality     1 Introduction     One of the most exciting discoveries made recently at RHIC is that strongly interacting matter behaves like a nearly perfect fluid [1] . This observation has led many theorists to propose new ways of describing this state of matter using effective theories which incorporate hydrodynamics [2] , or even more exotic descriptions involving quark-gluon plasma droplets [3] .   In order to understand better what happens during the early stages of heavy-ion collisions it would be very useful if one could study experimentally the properties of the hot dense medium created in those collisions. However, due to its extremely short lifetime, this medium cannot be directly probed through standard scattering experiments. Instead, information about the initial conditions of the collision process must be inferred indirectly from final-state measurements [4] . For example, the collective expansion of the system leads to anisotropic particle emission patterns known as azimuthal asymmetries [5] . These anisotropies have been measured [6] and found to agree well with theoretical predictions [7, 8] .     Another important observable characterizing the dynamics of the expanding fireball is the spectrum of emitted particles [9] . It was shown [10] that the shape of this spectrum depends sensitively on the equation-of-state of the medium [11] . Moreover, the observed suppression [12] of high-pT hadrons", "paraphrased_abstract": "Then, we present our latest results on the production of vector mesons in heavy ion collisions at RHIC and LHC, based on holographic QCD models with chiral symmetry breaking (AdS/QCD) and how they can be used to calculate the transverse momentum and the elliptic flow coefficients of light quarks and gluons in nuclear reactions. This discovery was particularly exciting in RHIC, and led many theorists to present new theories of describing this state of matter in the most effective manner, by means of hydrodynamics and in the more unusual case of gluon-rich plasmas. One of the most remarkable discoveries made recently in RHIC was that the highly interacting matter behaves like a liquid, nearly solid. This observation has led many theorists to propose new theories for describing the state of matter and of the medium that is forming it, such as the radial symmetry of the plasma, and the equivalence of the particles. Hence, the density of the particles, the angle of the fluxes, the distribution of the radii, the ellipticity of the flow. We will also present the results of our recent work on vector mesons in the collision of heavy ions and of holographic QCD models with chiral symmetry (AdS", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3408, "title": "The Trade-off between Processing Gains of an Impulse Radio UWB System in the Presence of Timing Jitter", "abstract": "  In time hopping impulse radio, $N_f$ pulses of duration $T_c$ are transmitted for each information symbol. This gives rise to two types of processing gain: (i) pulse combining gain, which is a factor $N_f$, and (ii) pulse spreading gain, which is $N_c=T_f/T_c$, where $T_f$ is the mean interval between two subsequent pulses. This paper investigates the trade-off between these two types of processing gain in the presence of timing jitter. First, an additive white Gaussian noise (AWGN) channel is considered and approximate closed form expressions for bit error probability are derived for impulse radio systems with and without pulse-based polarity randomization. Both symbol-synchronous and chip-synchronous scenarios are considered. The effects of multiple-access interference and timing jitter on the selection of optimal system parameters are explained through theoretical analysis. Finally, a multipath scenario is considered and the trade-off between processing gains of a synchronous impulse radio system with pulse-based polarity randomization is analyzed. The effects of the timing jitter, multiple-access interference and inter-frame interference are investigated. Simulation studies support the theoretical results. ", "machine_abstract": "In this work, we investigate the trade-offs among processing gains and timing jitters for impulse radio ultra-wideband (IR-UWB) systems with different pulse shapes. We first derive closed-form expressions to calculate the bit error rate (BER), which is used as performance measure. Then, by using these results, we analyze how the BER changes according to various system parameters such as signal-to-noise ratio (SNR), number of pulses per symbol, and timing jitter variance. Finally, numerical examples are provided to verify our analysis. The main contributions of this study can be summarized as:  1 . Derivation of closed-form expression for calculating the BER.  2 . Analysis on the effect of timing jitter on IR-UWB systems.  3 . Numerical verification of analytical results. In recent years, there has been growing interest in developing high data-rate wireless communication systems that operate over unlicensed bands [1] , especially those based on impulse radio ultrawideband (IR-UWB: [2] ). Compared to conventional narrowband systems, IR-UWB systems have several advantages including low power consumption [3] , immunity against multipath fading [4] , and high security [5] . However, one major drawback of IR-UWB systems is their vulnerability to timing jitter [6] - [8] . This problem arises because the received signals may experience time delays due to channel dispersion or clock imperfections at both transmitter and receiver sides [9] . As a result, the transmitted symbols cannot be recovered correctly if they arrive out of phase [10] . Therefore, it is important to understand the effects of timing jitter on the performance of IR-UWB systems [11] . To address this issue, many studies have investigated the impact of timing jitter on various aspects of IR-UWB systems [12]- [16] . For example, in [13] , the authors analyzed the effect of timing jitters on the energy efficiency of IR-UWB systems. They showed that the energy efficiency decreases when the timing jitter increases. Also, in [14] , the authors studied the relationship between timing jitter and bit error probability (BEP). However, most existing works only", "paraphrased_abstract": "But he is only a small one, and the transmission rate can only be regulated by the channel of the transmitter and the receiver, which is very noisy and the clock may be irregular. The receiver and the transmitter can not be synchronized, and the signal will be lost. This is the reason why the receiver has to have the jittery timing. So the work is primarily conducted on the subject of timing jitter. Several studies have already been conducted, and a large number of studies are on the effect of timing jitter on the performance of IR UWB systems. The study was conducted on the basis of the closed-form BER, which is the measurement of the performance of the IR UWB system. This result is shown in the following: 1. The study of the timing jitter and the energy efficiency of IR UWB systems. 2, In the IR UWB system, the power consumption is reduced, the security is improved, and there are many advantages, such as low power consumption, immunity from multipath fading, and the security is high. In this study, we will examine the effects of jitter on the performance of IR UWB systems. In the past, many studies have examined the effect of jitter on the energy efficiency of IR UWB systems. The results show that the energy efficiency is lowered when jit", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1669, "title": "The Elongated Structure of the Hercules dSph from Deep LBT Imaging", "abstract": "  We present a deep, wide-field photometric survey of the newly-discovered Hercules dwarf spheroidal galaxy, based on data from the Large Binocular Telescope. Images in B, V and r were obtained with the Large Binocular Camera covering a 23' times 23' field of view to a magnitude of ~25.5 (5 sigma). This permitted the construction of colour-magnitude diagrams that reach approximately 1.5 magnitudes below the Hercules main sequence turnoff. Three-filter photometry allowed us to preferentially select probable Hercules member stars, and examine the structure of this system at a previously unattained level. We find that the Hercules dwarf is highly elongated (3:1), considerably more so than any other dSph satellite of the Milky Way except the disrupting Sagittarius dwarf. While we cannot rule out that the unusual structure is intrinsic to Hercules as an equilibrium system, our results suggest tidal disruption as a likely cause of this highly elliptical structure. Given the relatively large Galactocentric distance of this system (132 +/- 12 kpc), signs of tidal disruption would require the Hercules dwarf to be on a highly eccentric orbit around the Milky Way. ", "machine_abstract": "We present deep imaging data for the nearby dwarf spheroidal galaxy, Hercules (dSph), obtained with the Large Binocular Telescope (LBT). The new observations are used to study the structure and stellar populations in this system. We find that the surface brightness profile is well described by an exponential function over most of its extent but shows evidence for a break at about 30 arcsec radius. This feature may be associated with tidal disruption or stripping due to interactions between Hercules and other galaxies. Using colour-magnitude diagrams we show that there exists two distinct components within Hercules; one which has been stripped off and another which appears to have remained intact. These results suggest that Hercules was once more extended than it currently is today. Finally, using our photometric catalogue we measure the line-of-sight velocity dispersion as a function of projected distance from the centre of Hercules. Our measurements indicate that the central region of Hercules exhibits higher values compared to those measured further out.", "paraphrased_abstract": "At the end of the experiment, we examined the brightness of Hercules, which is in the dSph region, obtained with the Large Binocular Telescope. The measurement indicated that Hercules' central region had higher values than that at the far end. We found that Hercules was expanded, which means that it was again expanded than it is now. Using the color-magnitude spectrum, we find that Hercules has two components: one that has been removed, and another that has remained intact. These observations indicate that Hercules has been expanded again. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0819, "title": "Position-Velocity Diagrams for the Maser Emission coming from a Keplerian Ring", "abstract": "  We have studied the maser emission from a thin, planar, gaseous ring in Keplerian rotation around a central mass observed edge-on. The absorption coefficient within the ring is assumed to follow a power law dependence with the distance from the central mass as, k=k0r^{-q}. We have calculated position-velocity diagrams for the most intense maser features, for different values of the exponent q. We have found that, depending on the value of q, these diagrams can be qualitatively different. The most intense maser emission at a given velocity can either come mainly from regions close to the inner or outer edges of the amplifying ring or from the line perpendicular to the line of sight and passing through the central mass (as is commonly assumed). Particularly, when q>1 the position-velocity diagram is qualitatively similar to the one observed for the water maser emission in the nucleus of the galaxy NGC 4258. In the context of this simple model, we conclude that in this object the absorption coefficient depends on the radius of the amplifying ring as a decreasing function, in order to have significant emission coming from the inner edge of the ring. ", "machine_abstract": "We present position-velocity diagrams (PVD) of masers in an accretion disk around a young stellar object, obtained with VLBI observations at 22 GHz and 43 GHz. The PVD are compared to those expected by models that include both rotation and radial motions. We find that our data can be explained if we assume that the emission comes from a ring-like structure located between 0.1 AU and 1 AU from the central star. This is consistent with previous results based on single-dish observations. Our analysis also shows that there must exist some kind of mechanism able to produce inward motions within this region. These could be due either to infalling material or to outflows produced by the protostar itself. Finally, we show how these results can be used as diagnostics tools to study the physical conditions inside circumstellar disks. Keywords: Accretion Disk, Circumstellar Disks, Infrared", "paraphrased_abstract": "Accretion Disk, Circumstellar Disks, Infrared Disks, Ionic Disks. We find that the emission originates from a ring-like structure, located between 0.1 AU and 1 AU from the central star, which is consistent with the observations made with single-dish observation. Our calculations prove that it is possible to make an inward movement in this region, and that it is able to produce it either by inward movement or by the excitation of the protostar itself. Then we show that our findings can be used as a diagnostic tool to determine the physical conditions of the circumstellar disk. We present a position-velocity-total diagram of the masers of a young star in the vicinity of a young star, calculated by VLBI observations of the 22 GHz and 43 GHz bands. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2941, "title": "The dust temperatures of the prestellar cores in the rho Oph main cloud and in other star forming regions: consequences for the core mass function", "abstract": "  We estimate the dust temperatures of the clumps in the rho Oph main cloud taking into account the 3D geometry of the region, and external heating from the interstellar radiation field and from HD147879,a nearby luminous B2V star, which is believed to dominate the radiation field in the region. We find that the regions where prestellar cores are observed (i.e. at optical visual extinctions >7 mag) are colder than ~10-11K. These dust temperatures are smaller than those which previous studies of the same region have assumed. We use the new dust temperatures to estimate the masses of the prestellar cores in the rho Oph main cloud from mm observations, and we find core masses that are larger than previous estimates by a factor of ~2-3. This affects the core mass function (CMF) of the region; we find that the mass at which the core mass spectrum steepens from a slope alpha~1.5 to a slope alpha~2.5 has moved from ~0.5 Msun to ~1Msun. In contrast with the CMF in other star forming regions (e.g. Orion), there is no indication for a turnover down to the completeness limit (~0.2Msun), but the CMF may flatten at around ~0.4Msun. We generalize our results to the prestellar cores in Taurus and in Orion. In Taurus the ambient radiation field heating the prestellar cores is believed to be weaker than than that in rho Oph. Hence, the dust temperatures of the cores in Taurus are expected to be below ~ 10-11 K.In Orion the radiation field is believed to be 10^3 times stronger than the standard interstellar radiation field. Based on this assumption we estimate that the dust temperatures of the prestellar cores in Orion are around ~20-30K. ", "machine_abstract": "We present Herschel Space Observatory observations at 70, 160, 250, 350, and 500 microns toward two fields centered on the densest parts of the Rho Ophiuchi (RO) molecular cloud complex. The data are used to derive the temperature distribution within dense cores identified by their infrared emission using the method developed by John Myers & Sean Carey. We find that most of these cores have temperatures between 10 K and 20 K with only one colder than 8 K. This is consistent with previous studies showing that cold cores are rare in star-forming clouds. Using our derived temperatures we calculate masses assuming optically thin greybody emission. These masses range from 0.1 Msun to more than 100 Msun. In addition, we use the same dataset to study the properties of protostars embedded in the RO region. We identify 16 Class I sources based on their spectral energy distributions and compare them to those found in other nearby star-forming regions such as Serpens South or Orion B North.", "paraphrased_abstract": "A large horizon is formed. The horizon is occupied by a swarm of stars, called Ophiuchi. The data are used to study the properties of the sphere in the Rho Ophiuchi. We find the radiance of most of the radiances at the wavelength of infrared light and find that most of the radiances at the wavelength of infrared light are between 10 and 20 K. It is true that cold radiances are rare in the formation of clouds. We present the results of the Herschel Space Observatory in a field of about 100, 160, 250, 350, and 500 mm. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0185, "title": "Excluded volume, local structural cooperativity,and the polymer physics of protein folding rates", "abstract": "  A coarse-grained variational model is used to investigate the polymer dynamics of barrier crossing for a diverse set of two-state folding proteins. The model gives reliable folding rate predictions provided excluded volume terms that induce minor structural cooperativity are included in the interaction potential. In general, the cooperative folding routes have sharper interfaces between folded and unfolded regions of the folding nucleus and higher free energy barriers. The calculated free energy barriers are strongly correlated with native topology as characterized by contact order. Increasing the rigidity of the folding nucleus changes the local structure of the transition state ensemble non-uniformly across the set of protein studied. Neverthless, the calculated prefactors k0 are found to be relatively uniform across the protein set, with variation in 1/k0 less than a factor of five. This direct calculation justifies the common assumption that the prefactor is roughly the same for all small two-state folding proteins. Using the barrier heights obtained from the model and the best fit monomer relaxation time 30ns, we find that 1/k0 (1-5)us (with average 1/k0 4us). This model can be extended to study subtle aspects of folding such as the variation of the folding rate with stability or solvent viscosity, and the onset of downhill folding. ", "machine_abstract": "We present an analysis of the relationship between excluded volume and cooperative effects in proteins with respect to their folding rates. We show that these two factors are not independent but rather have opposite relationships with rate constants for different classes of proteins. The results suggest that both excluded volume and cooperativity play important roles in determining the folding rates of globular proteins. In particular, we find that the effect of excluded volume on folding rates is strongly dependent upon the degree of cooperativity exhibited by each protein class. This dependence arises because the presence of large amounts of nonlocal structure can compensate for the loss of local interactions due to excluded volume. Our results also indicate that there may be a fundamental difference between how fast-folding and slow-folding proteins fold. Finally, our findings provide insight into why some proteins fold faster than others despite having similar structures. Proteins fold through a series of conformational changes driven by thermal fluctuations or other external forces such as ligand binding. These processes occur over time scales ranging from microseconds to seconds depending on the size and complexity of the folded state1 . Despite this wide range of timescales, all known proteins share common features including compact tertiary folds2 , hydrophobic cores3 , and secondary structure elements4 . Theoretical studies5-7 have shown that the free energy landscape governing protein folding contains many metastable states separated by barriers whose heights depend on the strength of native contacts8-10 . Because the height of these barriers determines the overall folding rate11-13 , it has been suggested14-16 that the folding process occurs via a rugged funnel-like free energy surface17-20 . However, recent experiments21-24 have challenged this viewpoint25-27 by showing that the folding mechanism of several small single-domain proteins does not involve significant kinetic intermediates28-30 . Instead, these proteins appear to fold directly from unfolded states to folded ones without passing through any partially folded states31-33 .", "paraphrased_abstract": "This is an example of how the protein folds, in spite of its large size, but also by a large amount of subunits. We show that the exclusion of a portion of the protein's volume has an important effect on the folding rate. The exclusion of a portion of the protein's volume has a strong influence on the folding rate, mainly because the strength of the contact with the protein is determined by the height of the barrier. The height of the barrier, in general, determines the folding rate, and the higher the barrier, the higher the folding rate, the higher the rate. Thus, it has been proposed that the folding of the globular proteins is strongly dependent on the level of cooperation of each class. Among the scalability of the fold, a certain amount of nonlocality compensates for the absence of local interaction with the protein, which has been excluded from the equation. In particular, the effects of the exclusion of volume and the degree of cohesion are very different from each other, and therefore are not independent, but have opposite values for the different classes of proteins. We present an analysis of the relation between the exclusion of volume and the cooperative effect of proteins in relation to the folding rates. We find that the exclusion of volume and the cooperation of the proteins are not independent but are opposed in opposite directions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3675, "title": "First proper motions of thin dust filaments at the Galactic Center", "abstract": "  Context: L'-band (3.8 micron) images of the Galactic Center show a large number of thin filaments in the mini-spiral, located west of the mini-cavity and along the inner edge of the Northern Arm. One possible mechanism that could produce such structures is the interaction of a central wind with the mini-spiral. Additionally, we identify similar features that appear to be associated with stars. Aims: We present the first proper motion measurements of the thin dust filaments observed in the central parsec around SgrA* and investigate possible mechanisms that could be responsible for the observed motions. Methods: The observations have been carried out using the NACO adaptive optics system at the ESO VLT. The images have been transformed to a common coordinate system and features of interest were extracted. Then a cross-correlation technique could be performed in order to determine the offsets between the features with respect to their position in the reference epoch. Results: We derive the proper motions of a number of filaments and 2 cometary shaped dusty sources close (in projection) to SgrA*. We show that the shape and the motion of the filaments does not agree with a purely Keplerian motion of the gas in the potential of the supermassive black hole at the position of SgrA*. Therefore, additional mechanisms must be responsible for their formation and motion. We argue that the properties of the filaments are probably related to an outflow from the disk of young mass-losing stars around SgrA*. In part, the outflow may originate from the black hole itself. We also present some evidence and theoretical considerations that the outflow may be collimated. ", "machine_abstract": "We report on the first detection of proper motion in two thin dust filaments located near Sgr A*, the supermassive black hole (SMBH) at the center of our Galaxy. The filaments are detected as dark features against bright infrared background emission and have been observed with the Submillimeter Array for more than four years. We find that both filaments show significant proper motions toward us, which we interpret to be due to their orbital motion around Sgr A*. Using simple models, we estimate the masses of these filaments to be between 10^6 M_sol and 10^7 M_sol . These results provide new insights into the formation process of such structures close to galactic nuclei. Dust filaments are ubiquitous in galaxies across cosmic time. They can form by gravitational instabilities or through outflows driven by active galactic nuclei (AGN). However, it is still unclear how they evolve over time and what physical processes drive them. In this Letter, we present the first measurement of proper motions of two thin dust filaments at projected distances of less than 100 pc from Sagittarius A* (Sgr A*), the central supermassive black hole of the Milky Way. Our observations were carried out using the Submillimeter Array (SMA; Ho et al., 2004) , an interferometer operating at wavelengths ranging from 0.8 mm to 3.5 mm. Both filaments appear as dark features against the bright infrared background emission produced by warm dust surrounding Sgr A*. Their apparent velocities range from 50 km/s to 150 km/s , consistent with previous measurements based on single-dish data (Molinari et al., 2011) . By modeling the orbits of the filaments under Newtonian gravity, we derive mass estimates of about 106-107 solar masses (Msolar) for each filament. This result suggests that the filaments may represent gravitationally bound objects orbiting Sgr A*.", "paraphrased_abstract": "It is possible that such filaments may have a gravitational relationship and be emitted from Sgr A., the central supermassive black hole of the galaxy, which is near our galaxy, and that the emitted radiation has reached a value of between 100 and 150 kilometers per hour, and, as was confirmed by the last measurements made with a single dipper, the velocity is less than 100 kHz, in keeping with the recent results from the measurements of the submillimeter. Here we present the first measurement of two thin filaments at the distance of less than 100 pc from Sgr A, the central supermassive black hole of the galaxy. These filaments are dark in their infrared reflections, as compared to the radiance of warm dust surrounding Sgr A. They are characterized by a striking appearance in the infrared, compared to the infrared, asymmetry, and temperature, respectively, which are attributed to their orbital motion around Sgr A. The radiance of the two filaments is about 50 km/s, in the same way as those of single-dish experiments (Molinari et al., 2011). These experiments show that these filaments are a result of gravity on their orbits, and a mass of about 106-107 solar masses (M", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1054, "title": "The Compact X-ray Source 1E 1547.0-5408 and the Radio Shell G327.24-0.13: A New Proposed Association between a Candidate Magnetar and a Candidate Supernova Remnant", "abstract": "  We present X-ray, infrared and radio observations of the field centered on X-ray source 1E 1547.0-5408 in the Galactic Plane. A new Chandra observation of this source shows it is unresolved at arc-second resolution, and a new XMM observation shows that its X-ray spectrum is best described by an absorbed power-law and blackbody model. A comparison of the X-ray flux observed from this source between 1980 and 2006 reveals that its absorbed 0.5-10 keV X-ray flux decreased from ~2x10^-12 ergs cm-2 s-1 to ~3x10^-13 ergs cm-2 during this period. The most recent XMM observation allows us to put a 5 sigma confidence upper limit of 14% for the 0.5-10 keV peak-to-peak pulsed fraction. A near-infrared observation of this field shows a source with magnitude Ks = 15.9+/-0.2 near the position of 1E 1547.0-5408, but the implied X-ray to infrared flux ratio indicates the infrared emission is from an unrelated field source, allowing us to limit the IR magnitude of 1E 1547.0-5408 to >17.5. Archival radio observations reveal that 1E 1547.0-5408 sits at the center of a faint, small (4' diameter) radio shell, G327.24-0.13, which is possibly a previously unidentified supernova remnant. The X-ray properties of 1E 1547.0-5408 suggest that this source is a magnetar - a young neutron star whose X-ray emission is powered by the decay of its extremely strong magnetic field. The spatial coincidence between this source and G327.24-0.13 suggests that 1E 1547.0-5408 is associated with a young supernova remnant, supporting a neutron star interpretation. Additional observations are needed to confirm the nature of both 1E 1547.0-5408 and G327.24-0.13, and to determine if these sources are associated. If so, this pair will be an important addition to the small number of known associations between magnetars and supernova remnants. ", "machine_abstract": "We report on new observations made with Chandra, XMM-Newton, and VLA that provide evidence for an association between the compact X-ray source 1E 1547. 0-5408  and the radio shell G327. 24-0.13 . The X-ray spectrum is consistent with emission from a magnetar; however, we find no pulsations in our data set. We also present optical spectroscopy of two stars near the center of the remnant which show strong Balmer absorption lines characteristic of Wolf-Rayet (WR) stars; these are likely to be associated with the supernova event that created the remnant.     Keywords: Supernova remnants, Pulsar wind nebulae, Wolf Rayets, Chandra, XMM-NEWTON, VLA, Optical Spectroscopy     Introduction     In this Letter, we report on new observational results concerning the possible association between the candidate magnetar 1E 1547.0+ 5408 , located at the center of the supernova remnant G327.24 -0.13 , and its surrounding environment. This object was discovered by the Einstein Observatory as part of the first systematic survey of the Galactic plane (Hertz & Grindlay 1984) . It has been observed several times since then using different instruments including ASCA (Sugizaki et al. , 1997) , BeppoSAX (Giacani et al. , 2001 ) , RXTE (Israel et al. , 2002 ) and Chandra (Pavlov et al. , 2004 ) . Its position coincides within errors with the brightest peak of the radio shell detected by the VLA (Kothes et al. , 2006 ; Gaensler et al. , 2008 ) . However, there have been conflicting reports about whether or not it shows any periodic behavior. While Israel et al. (2002) reported detection of a periodicity of 6 s during their observation campaign, Sugizaki et al. (1997) found only marginal evidence for such a signal when they analyzed archival ASCA data. More recently, Pavlov et al. (2004) did not detect any significant pulsation down to a", "paraphrased_abstract": "\u201cIn the present letter, we report the observations made by the investigators of the magnetar 1E1547.0-5408, which is at the centre of the supernova remnant G327.24-0.13; the star which is in the centre of the remnant, is coincidentally within the band of the radio-shell of G327.24-0.13; the latter was found to have an apex of ten thousand radii, of which there was no sign. The apex was calculated from the standard ASCA; Israel and Giacani (2000), and RXTE (Israel et al., 2002); and Chandra (Pavlov et al., 2004). This object was discovered by the Einstein Observatory as part of the first systematic survey of the Galactic plane (Hertz & Grindlay, 1984). The object was identified several times using various instruments, including the ASCA, BeppoSAX, Giacani et al.,, RXTE, Israel et al., 2002; XMM-Newton and VLA, the two of which show the strong, enigmatic rays characteristic of the WR stars, probably caused by the supernova. The X-rays are consistent with the emission of a magnetar; however, we find no pulsations", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.336, "title": "A double main sequence turn-off in the rich star cluster NGC 1846 in the Large Magellanic Cloud", "abstract": "  We report on HST/ACS photometry of the rich intermediate-age star cluster NGC 1846 in the Large Magellanic Cloud, which clearly reveals the presence of a double main sequence turn-off in this object. Despite this, the main sequence, sub-giant branch, and red giant branch are all narrow and well-defined, and the red clump is compact. We examine the spatial distribution of turn-off stars and demonstrate that all belong to NGC 1846 rather than to any field star population. In addition, the spatial distributions of the two sets of turn-off stars may exhibit different central concentrations and some asymmetries. By fitting isochrones, we show that the properties of the colour-magnitude diagram can be explained if there are two stellar populations of equivalent metal abundance in NGC 1846, differing in age by approximately 300 Myr. The absolute ages of the two populations are ~1.9 and ~2.2 Gyr, although there may be a systematic error of up to +/-0.4 Gyr in these values. The metal abundance inferred from isochrone fitting is [M/H] ~ -0.40, consistent with spectroscopic measurements of [Fe/H]. We propose that the observed properties of NGC 1846 can be explained if this object originated via the tidal capture of two star clusters formed separately in a star cluster group in a single giant molecular cloud. This scenario accounts naturally for the age difference and uniform metallicity of the two member populations, as well as the differences in their spatial distributions. ", "machine_abstract": "We present new photometry for the open cluster NGC 1846, located at the edge of the Small Magellanic Cloud (SMC). The data were obtained with the Wide Field Imager on the MPG/ESO 2.2 m telescope and cover an area of about 1 square degree around the cluster center. We find that there is no evidence for any significant differential reddening across this field. Using these observations we derive a distance modulus to the SMC of 18.9 mag, which agrees well with previous determinations based on other methods.     In addition, we identify two distinct sequences in our color-magnitude diagram (CMD) along the lines expected if the cluster contains both old and intermediate-age populations. This finding supports earlier suggestions by several authors who found similar features in CMDs of other clusters in the SMC. However, while most of those studies suggested ages between 10 and 100 Myr for their second population, we find here that it must be significantly older than 300 Myr.", "paraphrased_abstract": "The astronomy of the open cluster NGC 1846 is observed by the Wide Field Imager of the 2.2 m telescope of the Observatory, which covers a square kilometer around the centre of the cluster. The spectral index is 18.9 mag, which agrees well with the previous estimations of the other clusters in the SMC. Besides, the two populations, as they are grouped together, form two distinct lines, which must be expected, in the case of an old and a middle population, in the case of an old population. This new measurement confirms some previous theories of a similar nature, and they are very similar to the one that has been described in the SMC by several authors. In this paper we have obtained new spectral data on the open cluster of NGC 1846 in the SMC. The data have been collected with the Wide Field Imaging device, and they cover an area of a square degree around the center of the cluster. The results are consistent with the previous ones. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0732, "title": "Hot Jupiters in binary star systems", "abstract": "  Radial velocity surveys find Jupiter mass planets with semi-major axes a less than 0.1 AU around ~1% of solar-type stars; counting planets with $a$ as large as 5 AU, the fraction of stars having planets reaches ~ 10% {Marcy,Butler}. An examination of the distribution of semi-major axes shows that there is a clear excess of planets with orbital periods around 3 or 4 days, corresponding to a~0.03$ AU, with a sharp cutoff at shorter periods (see Figure 1). It is believed that Jupiter mass planets form at large distances from their parent stars; some fraction then migrate in to produce the short period objects. We argue that a significant fraction of the `hot Jupiters' (a<0.1 AU) may arise in binary star systems in which the orbit of the binary is highly inclined to the orbit of the planet. Mutual torques between the two orbits drive down the minimum separation or periapse r_p between the planet and its host star (the Kozai mechanism). This periapse collapse is halted when tidal friction on the planet circularizes the orbit faster than Kozai torque can excite it. The same friction then circularizes the planet orbit, producing hot Jupiters with the peak of the semimajor axis distribution lying around 3 days. For the observed distributions of binary separation, eccentricity and mass ratio, roughly 2.5% of planets with initial semimajor axis a_p ~ 5au will migrate to within 0.1au of their parent star. Kozai migration could account for 10% or more of the observed hot Jupiters. ", "machine_abstract": "We present the discovery and characterization of two \"hot Jupiter\" planets orbiting stars that are members of wide binaries, HD 196885AB (a = 1.8 AU) and HD 208598AB (a = 3.6 AU). The planet around HD 196885A is an inflated gas giant with M sin i = 0.88 MJup and P = 4.3 days; it orbits its primary at a distance of only 0.04 AU. We find no evidence for additional companions to either host star down to masses as low as 5 MJup within separations of 10 AU. Both systems have orbital eccentricities consistent with zero. These results suggest that hot Jupiters can survive close encounters with other stars during their formation or early evolution.  - Introduction     Hot Jupiters are massive gaseous planets on short-period orbits about solar-type stars. They represent one of the most extreme environments in our Solar System, but they may be common among nearby Sun-like stars. In fact, recent surveys indicate that roughly 20% of sun-like stars harbor such planets . However, these planets are thought to form beyond several AU before migrating inward through interactions with the protoplanetary disk and/or gravitational scattering by other bodies. This raises questions regarding how these planets manage to avoid being ejected into interstellar space after undergoing strong dynamical interactions with other objects while still retaining sufficient angular momentum to reach their current locations near their parent stars .  In this Letter we report the detection of two new \"hot Jupiter\" planets using high-precision radial velocity measurements obtained over more than eight years with the High Accuracy Radial Velocity Planet Searcher instrument (HARPS), which is installed on the European Southern Observatory's 3.6-m telescope located at La Silla Observatory in Chile. One of these planets has an extremely small semi-major axis of just 0.04 AU, making it one of the closest known exoplanets to its parent star.", "paraphrased_abstract": "I will introduce two new \u201chot Jupiters,\u201d namely, two abysses, one of which has an extremely small axis of about 0.04 AU, making it one of the closest known exoplanets. The other abyss, the abyss of the axis of the star, is of very low magnitude, and it is known that it is one of the smallest exoplanets in the Solar System. The abyss of the abyss is a giant gas-stiff with a mass of about 0.88 MJ, a period of 4.3 days. Moreover, it has no other companions, as high as five MJ, in the smallest distances. These observations indicate that the hot Jupiters can survive close encounters with other stars during their formation or early evolution. The abyss of the abyss is extremely small, with a minor axis of only 0.04 AU. The abyss is very small, with a height of only 0.04 AU. One of these abysses has an extremely small axis, just 0.04 AU, and is the smallest known exoplanets. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0643, "title": "Mapping the Youngest Galaxies to Redshift One", "abstract": "  We describe results of a narrow band search for ultra-strong emission line galaxies (USELs) with EW(H beta) > 30 A. 542 candidate galaxies are found in a half square degree survey using two ~100 Angstrom 8150 A and 9140 A filters with Subaru/SuprimeCam. Followup spectroscopy for randomly selected objects in the sample with KeckII/DEIMOS shows they consist of [OIII] 5007, [OII] 3727, and H alpha selected strong-emission line galaxies at intermediate redshifts (z < 1), and Ly alpha emitting galaxies at high-redshift (z >> 5). We determine the H beta luminosity functions and the star formation density of the USELs, which is 5-10% of the value found from ultraviolet continuum objects at z=0-1, suggesting they correspond to a major epoch in galaxy formation at these redshifts. Many USELs show the temperature-sensitive [OIII] 4363 auroral lines and about a dozen have oxygen abundances characteristic of eXtremely Metal Poor Galaxies (XMPGs). These XMPGs are the most distant known today. Our high yield rate of XMPGs suggests this is a powerful method for finding such populations. The lowest metallicity measured in our sample is 12+log(O/H) = 7.06 (6.78-7.44), close to the minimum metallicity found in local galaxies. The luminosities, metallicities and star formation rates of USELs are consistent with the strong emitters being start-up intermediate mass galaxies and suggest that galaxies are still forming in relatively chemically pristine sites at z < 1. ", "machine_abstract": "We present deep near-infrared imaging and spectroscopy for two fields in the Hubble Deep Field South (HDF-S) covering an area of 0.5 square degrees each, centered on RA = 03:32:47.6 Dec = -27:48:30 (J2000). The data were obtained with ISAAC at VLT as part of our ESO Large Programme \"First Stars First Galaxies\". We have detected more than 1000 galaxies down to KsAB = 24 mag over this field using SExtractor. In addition we have spectroscopically confirmed about 100 objects selected by their photometric redshifts between z=0.7-1.3. This is one of the largest samples of high-z galaxies ever assembled. Our sample includes both Lyman-break galaxies and sub-millimeter sources which are likely to be dusty starbursting systems. These results will allow us to study galaxy formation and evolution up to redshift 1.", "paraphrased_abstract": "In addition, we have spectroscopically confirmed about 100 objects selected by redshift between 0.7 and 1.3, which is quite sufficient for the study of the galaxy formation and evolution up to redshift 1. This is one of the largest collections of high-z galaxies ever assembled. The sample includes both Lyman-breaks and sub-millimetre sources, which are refractory to dusty star-bursting systems. This is the largest collection of high-z galaxies ever assembled. We present data on two fields in the Hubble Deep Field South (HDF-S), covering an area of 0.03 square degrees each, centered at RA = 03:32:47.6 Dec = 27:48:30 (J2000). ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2067, "title": "Complex marginal deformations of D3-brane geometries, their Penrose limits and giant gravitons", "abstract": "  We apply the Lunin--Maldacena construction of gravity duals to beta-deformed gauge theories to a class of Type IIB backgrounds with U(1)^3 global symmetry, which include the multicenter D3-brane backgrounds dual to the Coulomb branch of N=4 super Yang-Mills and the rotating D3-brane backgrounds dual to the theory at finite temperature and chemical potential. After a general discussion, we present the full form of the deformed metrics for three special cases, which can be used for the study of various aspects of the marginally-deformed gauge theories. We also construct the Penrose limits of the solutions dual to the Coulomb branch along a certain set of geodesics and, for the resulting PP--wave metrics, we examine the effect of beta-deformations on the giant graviton states. We find that giant gravitons exist only up to a critical value of the sigma-deformation parameter, are not degenerate in energy with the point graviton, and remain perturbatively stable. Finally, we probe the sigma-deformed multicenter solutions by examining the static heavy-quark potential by means of Wilson loops. We find situations that give rise to complete screening as well as linear confinement, with the latter arising is an intriguing way reminiscent of phase transitions in statistical systems. ", "machine_abstract": "We study the Penrose limit of certain complex marginal deformations of AdS5\u00d7S 5 . We show that these are dual to supergravity solutions describing wrapped M2-and M5-branes on S 3 \u00d7 R 4 , which can be interpreted as giant gravitons in AdS5\u00d7S5 . The corresponding string theory backgrounds have been constructed by Lunin and Maldacena using T-duality transformations along Hopf fibers of Sasaki-Einstein spaces . The Penrose limit is taken with respect to an appropriate null geodesic lying inside the horizon of the black hole solution . In this way we obtain new supersymmetric pp-wave solutions of type IIB supergravity whose near-horizon geometry is given by AdS3\u00d7S 3 \u00d7R 2 or AdS2\u00d7S 2 \u00d7R 2 depending on whether one takes the Penrose limit for the deformation parameter equal to zero or not respectively .", "paraphrased_abstract": ", AdS3 R2 R2, and AdS2 R 2, depending on whether the Penrose limit is given for the deformation parameter equal to zero or not. In this way, we obtain new supersymmetric solutions of type IIB supergravity, whose nearest horizon is given by AdS3 R 2 or AdS2 R 2, depending on whether you take the Penrose limit for the deformation parameter equal to zero. Then, if you take the Penrose limit, the null geodesic lies at the edge of the black hole solution. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3107, "title": "Shelling-type orderings of regular CW-complexes and acyclic matchings of the Salvetti complex", "abstract": "  Motivated by the work of Salvetti and Settepanella we introduce certain total orderings of the faces of any shellable regular CW-complex (called `shelling-type orderings') that can be used to explicitly construct maximum acyclic matchings of the poset of cells of the given complex. Building on an application of this method to the classical zonotope shellings we describe a class of maximum acyclic matchings for the Salvetti complex of a linear complexified arrangement. To do this, we introduce and study a new combinatorial stratification of the Salvetti complex. For the obtained acyclic matchings we give an explicit description of the critical cells that depends only on the chosen linear extension of the poset of regions. It is always possible to choose the linear extension so that the critical cells can be explicitly constructed from the chambers of the arrangement via the bijection to no-broken-circuit sets defined by Jewell and Orlik. Our method can be generalized to arbitraty oriented matroids. ", "machine_abstract": "We study shelling-type orderings on regular cell complexes, which are defined by an ordering of their cells such that each new face is attached to exactly one old face in its boundary. We show how these can be used to construct acyclic matchings for the Salvetti complex associated with any finite Coxeter group W . This yields a combinatorial proof of the fact that the Poincar\u00e9 polynomial of this complex satisfies P(t) = (1 \u2212 t)(1 + w\u2208W t l(w) ), where l denotes the length function on W . The results presented here were obtained during my PhD thesis at ETH Zurich under the supervision of Prof. Dr. J\u00fcrg Stadler. I would like to thank him as well as Prof. Dr. Peter Schmitt for many helpful discussions. The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP/2007-2013 Grant Agreement n o 325928).", "paraphrased_abstract": "The results presented here were obtained during my thesis at the University of Zurich under the supervision of Prof. Dr. J\u00fcrg Stadler. I thank him and Prof. Dr. Peter Schmitt for all his help. The results were derived from the experiments conducted during my PhD at ETH Zurich under the supervision of Prof. Dr. J\u00fcrg Stadler. I thank him for his kindly comments and for his good discussions with Prof. Dr. Peter Schmitt. We study shelling-type orderings of complexes, whose cells are ordered so that every new face of them is surrounded by the same old face. This is a combinatorial proof of the fact that the Poincar\u00e9 polynomial of this complex is satisfied by P = (1  t)(1 + w), where l is the length function of W. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2942, "title": "The Star-forming Region NGC 346 in the Small Magellanic Cloud with Hubble Space Telescope ACS Observations. II. Photometric Study of the Intermediate-Age Star Cluster BS 90", "abstract": "  We present the results of our investigation of the intermediate-age star cluster BS 90, located in the vicinity of the HII region N 66 in the SMC, observed with HST/ACS. The high-resolution data provide a unique opportunity for a very detailed photometric study performed on one of the rare intermediate-age rich SMC clusters. The complete set of observations is centered on the association NGC 346 and contains almost 100,000 stars down to V ~28 mag. In this study we focus on the northern part of the region, which covers almost the whole stellar content of BS 90. We construct its stellar surface density profile and derive structural parameters. Isochrone fits on the CMD of the cluster results in an age of about 4.5 Gyr. The luminosity function is constructed and the present-day mass function of BS 90 has been obtained using the mass-luminosity relation, derived from the isochrone models. We found a slope between -1.30 and -0.95, comparable or somewhat shallower than a typical Salpeter IMF. Examination of the radial dependence of the mass function shows a steeper slope at larger radial distances, indicating mass segregation in the cluster. The derived half-mass relaxation time of 0.95 Gyr suggests that the cluster is mass segregated due to its dynamical evolution. From the isochrone model fits we derive a metallicity for BS 90 of [Fe/H]=-0.72, which adds an important point to the age-metallicity relation of the SMC. We discuss our findings on this relation in comparison to other SMC clusters. ", "machine_abstract": "We present photometry for stars in an intermediate-age open cluster, BS90 (Sharpless 1959), located at the center of the star-forming region NGC346 in the Small Magellan Cloud (SMC). The data were obtained using the Advanced Camera for Surveys on board the Hubble Space Telescope and cover a field-of-view of about 1 arcmin2 centered on the cluster. We have detected more than 1000 candidate members down to V = 25 mag within a radius of 3 arcmin around the cluster center. Using these new observations we derived the age, distance modulus, reddening, metallicity, mass function slope, and total luminosity of this open cluster. Our results are consistent with previous studies but provide higher precision values due to our larger sample size.     Keywords: Open clusters; SMC; HST/ACS; photometry; star formation history; intermediate-age", "paraphrased_abstract": "The metric is: \u201cAcclimatization of the Open Clouds,\u201d the \u201cNew Age,\u201d the \u201cIndividual Magellan Cloud,\u201d the \u201cOpen Cloud.\u201d The \u201cOpen Cloud\u201d is the small Magellan Cloud, a region in the middle of the NGC 346 galaxy. The data were collected with the Advanced Camera for Surveying and cover a field of view of about 1 arcmin. We have detected more than 1000 candidates, down to a diameter of one arcmin, in the vicinity of the cluster. The data we obtained were taken with the Advanced Camera for Surveying and cover a field of view of about 1 arcmin. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1196, "title": "Single Transverse-Spin Asymmetry in Hadronic Dijet Production", "abstract": "  We study the single transverse-spin asymmetry for dijet production in hadronic collisions in both the collinear QCD factorization approach and the Brodsky-Hwang-Schmidt model. We show that a nonvanishing asymmetry is generated by both initial-state and final-state interactions, and that the final-state interactions dominate. We find that in the leading kinematic region where the transverse momentum imbalance of the two jets, q_\\perp = P_{1\\perp}+P_{2\\perp}, is much less than the momentum of either jet, the contribution from the lowest non-trivial perturbative order to both the spin-averaged and the spin-dependent dijet cross sections can be factorized into a hard part that is a function only of the averaged jet momentum P_\\perp = (P_{1\\perp}-P_{2\\perp})/2, and perturbatively generated transverse momentum dependent (TMD) parton distributions. We show that the spin asymmetry at this non-trivial perturbative order can be described by the TMD parton distributions defined in either semi-inclusive DIS or the Drell-Yan process. We derive the same hard parts from both the collinear factorization approach and in the context of the Brodsky-Hwang-Schmidt model, verifying that they are not sensitive to details of the factorized long distance physics. ", "machine_abstract": "We present the first measurement of single-transverse-spin asymmetries (SSA) for hadronic dijets produced at midrapidity in p+p collisions at sqrt(sNN) = 5.02 TeV using data collected by the CMS experiment during 2012 corresponding to an integrated luminosity of 2.3 fb-1 . The SSAs are extracted as functions of jet transverse momentum and rapidity, azimuthal angle between jets, and event centrality. We observe no significant dependence on any kinematic variable except that the magnitude of the asymmetry decreases with increasing jet rapidity. Our results are compared to theoretical predictions based on perturbative QCD calculations including higher-order corrections and parton distribution function uncertainties.  The measured values agree well within experimental and theoretical uncertainties. This is the most precise measurement of this observable performed so far.     Introduction     Single transverse-spin asymmetries have been observed in several processes involving polarized protons or neutrons [1] , such as inclusive pion production [2] , semi-inclusive deep-inelastic scattering [3] , Drell-Yan lepton pair production [4] , prompt photon production [5] , and direct photons [6] . These measurements provide important information about the spin structure of nucleons [7, 8] .   In particular, they can be used to test the validity of factorization theorems [9] which relate hard-scattering cross sections to partonic distributions inside the proton [10] . In addition, these observables may also shed light on new physics beyond the Standard Model [11] .     For example, it has recently been suggested [12] that large single-spin asymmetries could arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high-energy pp collisions. Such effects would violate parity conservation and thus constitute evidence for new physics [13] . However, there exists only one previous measurement [14] of single-spin asymmeties in hadronic dijet production at high energies. That study was carried out at RHIC [15] where the center-of-mass energy per nucleon-nucleon collision \u221asNN=200 GeV is much lower", "paraphrased_abstract": "In a case of high energy, there is only one measurement that is known of the existence of a single spherical spherical dijet, i.e., that is, at the center of mass, the center of mass is a few hundred teV, and it is only at RHIC, where the centre of mass is only a few teV. In a case like that, we can use it to verify the validity of the factorization theorems which relate the spherical spherical part of the proton to the spherical part. We calculate these spherical spherical spherical spherical parts by means of transverse and transverse spins, relative to the jet velocity, azimuthal angle between jets, and in a manner of general importance that is asymmetric with respect to the kinematics of the spherical parton distribution. We calculate the spherical spherical parton function. We find that the SSAs are not proportional to any kinematic variables, but the degree of asymmetry diminishes with the rapidity of the jets, and in turn the intensity of the asymmetry decreases with the rapidity of the jets, and in", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3438, "title": "Black-body components in Gamma-Ray Bursts spectra?", "abstract": "  We study 7 Gamma Ray Bursts (GRBs), detected both by the BATSE instrument, on-board the Compton Gamma Ray Observatory, and by the Wide Field Camera (WFC), on-board BeppoSAX. These bursts have measured spectroscopic redshifts and are a sizeable fraction of the bursts defining the correlation between the peak energy E_peak (i.e. the peak of the vFv spectrum) and the total prompt isotropic energy E_iso (the so called \"Amati\" relation). Recent theoretical interpretations of this correlation assume that black-body emission dominates the time resolved spectra of GRBs, even if, in the time integrated spectrum, its presence may be hidden by the change of its temperature and by the dilution of a possible non-thermal power law component. We perform a time resolved spectral analysis, and show that the sum of a power-law and a black-body gives acceptable fits to the time dependent spectra within the BATSE energy range, but overpredicts the flux in the WFC X-ray range. Moreover, a fit with a cutoff power-law plus a black-body is consistent with the WFC data, but the black-body component contributes a negligible fraction of the total flux. On the contrary, we find that fitting the spectra with a Band model or a simple cutoff power-law model yields an X-ray flux and spectral slope which well matches the WFC spectra. ", "machine_abstract": "We present the results of our analysis on the time-averaged spectrum and light curve of GRB 090902B, which is one of the brightest bursts ever detected by Fermi/GBM (Gamma-ray Burst Monitor). We find that its average spectrum can be well fitted with two blackbody functions plus an additional power-law component at high energies. The temperature of each blackbody function decreases as the emission radius increases. This behavior is consistent with theoretical predictions for photospheric emission from relativistic outflows. In addition to this thermal component, we also detect non-thermal emission above 100 MeV. Our spectral fitting suggests that there are three distinct physical regions contributing to the observed gamma-rays: 1) A photosphere-like region producing thermal radiation; 2) An internal shock region generating nonthermal radiation; 3) A magnetized wind region responsible for high-energy photons. These findings provide new insights into the physics of GRBs.     Keywords: Black body, Photon index, Power law", "paraphrased_abstract": "The average spectrum of the GRB-090902 is well matched to the blackbody functions, a corresponding power law function at high temperatures, and the temperature of each blackbody function increases as the radius of the emission increases. Our analysis suggests that there are three different physical regions in which gamma rays are observed: 1) the photosphere, whose temperature is thermal, 2) the internal shock, which is thermal; 3) the magnetized wind, which produces high energy photons. The results of this analysis provide a new insight into the physics of gamma rays. Our study reveals that the gamma rays are emitted in three different physical regions: 1) the photosphere, whose temperature is thermal; 2) the internal shock, whose temperature is non-thermal; and 4) the magnetized wind, which is the source of high-energy photons. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1915, "title": "Thermal inertia of near-Earth asteroids and implications for the magnitude of the Yarkovsky effect", "abstract": "  Thermal inertia determines the temperature distribution over the surface of an asteroid and therefore governs the magnitude the Yarkovsky effect. The latter causes gradual drifting of the orbits of km-sized asteroids and plays an important role in the delivery of near-Earth asteroids (NEAs) from the main belt and in the dynamical spreading of asteroid families. At present, very little is known about the thermal inertia of asteroids in the km size range. Here we show that the average thermal inertia of a sample of NEAs in the km-size range is 200 $\\pm$ 40 J m&#8722;2 s&#8722;0.5 K&#8722;1. Furthermore, we identify a trend of increasing thermal inertia with decreasing asteroid diameter, D. This indicates that the dependence of the drift rate of the orbital semimajor axis on the size of asteroids due to the Yarkovsky effect is a more complex function than the generally adopted D^(&#8722;1) dependence, and that the size distribution of objects injected by Yarkovsky-driven orbital mobility into the NEA source regions is less skewed to smaller sizes than generally assumed. We discuss how this fact may help to explain the small difference in the slope of the size distribution of km-sized NEAs and main-belt asteroids. ", "machine_abstract": "We present new results on thermal inertia measurements of NEAs based on infrared observations with Spitzer Space Telescope (SST). We use these data to derive an improved estimate of the mean value of the surface thermal inertia, I = 100 \u00b1 50 J m-2 s-1/2 K-1, which is in good agreement with previous estimates obtained by other authors using different methods. The derived values are also consistent with laboratory experiments performed at high temperatures that show how the thermal conductivity decreases as temperature increases. Using our measured range of thermal inertias we calculate the expected range of magnitudes of the Yarkovsky force acting upon NEAs. Our calculations suggest that this force may be responsible for driving some NEAs into orbits crossing Earth's orbit. This would have important consequences for future space missions aimed at deflecting potentially hazardous objects away from Earth. Near-Earth Asteroids (NEAs) represent a significant threat to human civilization because they can impact the Earth within one million years. In order to mitigate such threats it will be necessary to develop technologies capable of deflecting or redirecting NEAs out of their current orbits before they hit the Earth. One possible method involves applying a small impulse to the asteroid's trajectory through the action of the Yarkovsky-O'Keefe-Radzievskii-Paddack (YORP) effect. However, the effectiveness of this approach depends critically on the ability to predict accurately the strength of the YORP effect.", "paraphrased_abstract": "The near-earth asteroids are a serious threat to human civilization, and the earliest astroturfs are capable of falling into the Earth\u2019s orbit within a million years. The first step is to introduce a small impulse to the trajectory of the asteroids, as the Yorps are. However, the effectiveness of this technique depends on the ability to predict accurately the strength of the Yorps. The Yorps are a force of some kind and can be used to drive some asteroids out of their orbit before they reach Earth. We have developed new thermal inertia estimates, using the Spitzer Space Telescope. The Yorps are an energy that is capable of shunting and driving some asteroids into orbits which are not straying away from Earth. We present a new analysis of the thermal inertia of asteroids, based on the infrared measurements of Spitzer Space Telescope, and our observations indicate that the surface thermal inertia has a range of magnitudes which is attributed to the Yarkovsky force on a NEA. The calculated values are also consistent with laboratory experiments that show how the conductivity of a substance increases as temperature increases. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1044, "title": "Particle-vibration coupling within covariant density functional theory", "abstract": "  Covariant density functional theory, which has so far been applied only within the framework of static and time dependent mean field theory is extended to include Particle-Vibration Coupling (PVC) in a consistent way. Starting from a conventional energy functional we calculate the low-lying collective vibrations in Relativistic Random Phase Approximation (RRPA) and construct an energy dependent self-energy for the Dyson equation. The resulting Bethe-Salpeter equation in the particle-hole ($ph$) channel is solved in the Time Blocking Approximation (TBA). No additional parameters are used and double counting is avoided by a proper subtraction method. The same energy functional, i.e. the same set of coupling constants, generates the Dirac-Hartree single-particle spectrum, the static part of the residual $ph$-interaction and the particle-phonon coupling vertices. Therefore a fully consistent description of nuclear excited states is developed. This method is applied for an investigation of damping phenomena in the spherical nuclei with closed shells $^{208}$Pb and $^{132}$Sn. Since the phonon coupling terms enrich the RRPA spectrum with a multitude of $ph\\otimes$phonon components a noticeable fragmentation of the giant resonances is found, which is in full agreement with experimental data and with results of the semi-phenomenological non-relativistic approach. ", "machine_abstract": "We present an ab initio method for the calculation of phonon dispersion relations in solids, which is based on the direct solution of the Bethe-Salpeter equation (BSE) and includes electron-phonon interaction effects beyond the adiabatic approximation. The BSE describes the scattering between pairs of valence electrons mediated by screened Coulomb interactions. We solve this equation using a recently developed scheme that allows us to treat large supercells with high accuracy. In order to account for nonadiabatic corrections we introduce a self-consistent treatment of electronic screening into our approach. This enables us to calculate accurate phonon dispersions at arbitrary points in reciprocal space without any additional computational effort compared to standard DFT calculations. As a first application of our new method we study the influence of electron-phonon interaction on the band gap renormalization in silicon. Our results show good agreement with experimental data and previous theoretical studies.", "paraphrased_abstract": "The BSE is a direct solution of the Bethe-Salpeter equation, which includes a number of other induced effects that are not in the adiabatic version, and which allows us to resolve the equation with high accuracy. We present a new method of calculating phonon dispersion in solids, which is based on a direct solution of the BSE, with a new algorithm, which enables us to compute accurately phonon dispersion in large supercells. We present an ab initio method for the determination of the phonon dispersion of solids, based on the direct solution of the BSE, and incorporates electrochemical interactions of the electron-phonon type without adiabatic approximations. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1797, "title": "National Scientific Facilities and Their Science Impact on Non-Biomedical Research", "abstract": "  H-index, proposed by Hirsch is a good indicator of the impact of a scientist's research. When evaluating departments, institutions or labs, the importance of h-index can be further enhanced when properly calibrated for size. Particularly acute is the issue of federally funded facilities whose number of actively publishing scientists frequently dwarfs that of academic departments. Recently Molinari and Molinari developed a methodology that shows the h-index has a universal growth rate for large numbers of papers, allowing for meaningful comparisons between institutions.   An additional challenge when comparing large institutions is that fields have distinct internal cultures, with different typical rates of publication and citation; biology is more highly cited than physics, which is more highly cited than engineering. For this reason, this study has focused on the physical sciences, engineering, and technology, and has excluded bio-medical research. Comparisons between individual disciplines are reported here to provide contextual framework. Generally, it was found that the universal growth rate of Molinari and Molinari holds well across all the categories considered, testifying to the robustness of both their growth law and our results.   The overall goal here is to set the highest standard of comparison for federal investment in science; comparisons are made with the nations preeminent private and public institutions. We find that many among the national facilities compare favorably in research impact with the nations leading universities. ", "machine_abstract": "The National Institutes of Health (NIH) is the largest funder of biomedical research in the United States, but it also supports non-biomedical research through its extramural program.  The NIH has funded thousands of scientists at hundreds of institutions across the country to conduct basic science research that may have important applications outside of medicine.   This study examines how these researchers are using their NIH funding for non-biomedical projects by analyzing data collected during interviews with them conducted as part of an ongoing longitudinal survey of NIH-funded investigators.  We find that many of these scientists use their NIH funds primarily or exclusively for non-biomedically related research activities such as teaching, administration, and service work.  However, we also find that some scientists who receive NIH support for non-biomedics-related research still spend most of their time conducting biomedically focused research.  In addition, our results show that scientists' perceptions about whether they are spending more time doing biomedically versus non-biomedically focused research do not always match up with actual behavior.", "paraphrased_abstract": "It turns out that a large number of the scientists in the NIH \u2014 who have received their grants for other activities \u2014 are often mainly engaged in the non-biomedical research. However, there are those who, in addition to their research in the field of science, are most engaged in non-biomedical research. The NIH is the most important funder of medical research in the United States, but it also provides funding for non-medical research, primarily in teaching and service. These researchers, however, spend most of their time in non-biomedical research. Moreover, our findings show that NIH scientists are not always able to decide whether to spend their time more in biomedical or less in the field of medical research. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0979, "title": "Cosmogenic neutrinos as a probe of the transition from Galactic to extragalactic cosmic rays", "abstract": "  There are two promising scenarios that explain the ankle, which is a dip in the spectrum of cosmic rays at $\\sim 10^{19}$ eV. A scenario interprets the ankle as the transition from Galactic to extragalactic cosmic rays ({\\it ankle-transition scenario}), while the other is that the dip caused by pair production on the cosmic microwave background radiation ({\\it proton-dip scenario}). In this paper, we consider whether cosmogenic neutrinos can be a clue to judge which scenario is favored. We calculated the fluxes of cosmogenic neutrinos following these scenarios with plausible physical parameter sets, and found several important features as follows. First of all, the neutrino flux at $\\sim 10^{20}$ eV becomes much higher in the ankle-transition scenario as long as the maximum energy of the cosmic rays at sources is sufficiently high. On the other hand, the neutrino spectrum has a characteristic peak at $\\sim 10^{16}$ eV in the proton-dip scenario on the condition that extragalactic protons significantly contribute to the observed cosmic rays down to $10^{17}$ eV. Thus, we conclude cosmogenic neutrinos should give us a clue to judge which scenario is favored, unless these features are masked by the neutrino background coming from possible, powerful neutrino sources such as AGNs and GRBs. We also found an interesting feature that the neutrino flux at $\\sim 10^{18}$ eV depends only on the cosmological evolution of the cosmic ray sources. That means cosmogenic neutrinos with the energy bring us information on the cosmological evolution of the sources of ultra-high energy cosmic rays. Finally, we compare the fluxes of cosmogenic neutrinos with the expected sensitivity curves of several neutrino detectors, and conclude the detection of cosmogenic neutrinos in the near future is promising. ", "machine_abstract": "We present an analysis of cosmogenic neutrino fluxes expected in IceCube for different models of cosmic ray sources and propagation, including recent results on anisotropy measurements by PAMELA and AMS-02 experiments. We show that these data can be used to discriminate between various scenarios of cosmic ray origin and propagation. In particular we find that:  - The observed anisotropies are consistent with predictions based on conventional astrophysical acceleration mechanisms only if one assumes very hard injection spectra at source (with spectral index below -2). This is inconsistent with observations of other messengers such as gamma-rays or radio waves.  - If one allows for more exotic particle physics processes like decaying dark matter particles then it becomes possible to explain both the observed anisotropies and the spectrum of cosmic rays without violating any observational constraints. However this scenario requires fine-tuning of parameters which makes it less appealing than conventional astrophysics explanations.", "paraphrased_abstract": "The observed anisotropies are consistent with predictions of conventional accelerated mechanism for the most part, if we assume that the source is a very hard particle, with a spectral index of minus -2. However, this is a complicated situation, which is more attractive than the usual physics. We analyze the cosmogenic neutrino flux expected in IceCube for different models of accelerated cosmic rays, and we present recent results from PAMELA and AMS-02 experiments. We show that this data can be used to distinguish among various scenarios the accelerated and the absorbed cosmic rays. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3331, "title": "Light nuclei in galactic globular clusters : constraints on the self-enrichment scenario from nucleosynthesis", "abstract": "  Hydrogen-burning is the root cause of the star-to-star abundance variations of light nuclei in Galactic globular clusters (GC). In the present work we constrain the physical conditions that gave rise to the observed abundance patterns of Li, C, N, O, Na, Mg, Al, as well as Mg isotopes in the typical case of NGC6752. We perform nucleosynthesis calculations at constant temperature, adopting realistic initial abundances for the proto-cluster gas. We use a detailed nuclear reaction network and state-of-the-art nuclear reaction rates. Although simplistic, our analysis provides original results and new constraints on the self-enrichment scenario for GCs. Our parametrized calculations allow us to determine a narrow range of temperature where the observed extreme abundances of all light elements and isotopes in NGC6752 are nicely reproduced simultaneously. This agreement is obtained after mixing of the H-processed material with 30 % of unprocessed gas. The observed C-N, O-Na, Mg-Al, Li-Na and F-Na anticorrelations, as well as the behaviour of the Mg isotopes can be recovered by assuming mixing with even larger dilution factors. Li production by the stars that build up the other abundance anomalies is not mandatory in the case of NGC 6752. Observations of O, Na, Mg and Al constrain the temperature range for H-burning; such temperatures are encountered in the two main candidate ``polluters'' proposed for GCs, namely massive AGBs and the most massive main-sequence stars. (Abridged). ", "machine_abstract": "We present new results for light element abundances (Li, Be and B) in two metal-poor stars belonging to the Galactic Globular Cluster M4. The observed abundance patterns are compared with predictions based on standard Big Bang Nucleosynthesis theory as well as those obtained by assuming that these elements were produced during hydrostatic burning phases in massive AGB stars. We find that both scenarios fail to reproduce simultaneously all three measured elemental ratios at metallicities below [Fe/H] = -2.0 dex. This suggests that additional processes must be responsible for producing LiBeB in this cluster.  These findings provide important clues about the origin of light elements in low-metallicity environments such as dwarf galaxies or primordial gas clouds. In particular, they suggest that the production mechanisms may have been different than previously thought. Finally, we discuss possible implications of our results for the formation history of globular clusters.", "paraphrased_abstract": "The result is that the abundances of the light elements Li, Be and B are in two metal-poor stars belonging to the Galactic Globular Cluster M4 are not at all in accordance with the previous predictions. We suggest that there are additional mechanisms that synthesis may be different from the previous. In addition, we discuss the implications of the findings for the formation of globular clusters. Among other things, we present new findings for the abundance of the three elements Li, Be and B, which are in two metal-poor stars belonging to the Galactic Globular Cluster M4 and M4: the latter compared with the prediction based on the Big Bang theory, and on the assumption that they were formed in the hydrostatic phases of large AGB stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2278, "title": "Unequal dimensional small balls and quantization on Grassmann Manifolds", "abstract": "  The Grassmann manifold G_{n,p}(L) is the set of all p-dimensional planes (through the origin) in the n-dimensional Euclidean space L^{n}, where L is either R or C. This paper considers an unequal dimensional quantization in which a source in G_{n,p}(L) is quantized through a code in G_{n,q}(L), where p and q are not necessarily the same. It is different from most works in literature where p\\equiv q. The analysis for unequal dimensional quantization is based on the volume of a metric ball in G_{n,p}(L) whose center is in G_{n,q}(L). Our chief result is a closed-form formula for the volume of a metric ball when the radius is sufficiently small. This volume formula holds for Grassmann manifolds with arbitrary n, p, q and L, while previous results pertained only to some special cases. Based on this volume formula, several bounds are derived for the rate distortion tradeoff assuming the quantization rate is sufficiently high. The lower and upper bounds on the distortion rate function are asymptotically identical, and so precisely quantify the asymptotic rate distortion tradeoff. We also show that random codes are asymptotically optimal in the sense that they achieve the minimum achievable distortion with probability one as n and the code rate approach infinity linearly. Finally, we discuss some applications of the derived results to communication theory. A geometric interpretation in the Grassmann manifold is developed for capacity calculation of additive white Gaussian noise channel. Further, the derived distortion rate function is beneficial to characterizing the effect of beamforming matrix selection in multi-antenna communications. ", "machine_abstract": "In this work, we study the problem of finding an optimal set of points in a Grassmann manifold that are equidistant to each other with respect to some distance measure. We propose two algorithms for solving this problem: one based on random sampling and another based on gradient descent. The first algorithm is shown to be efficient when the number of dimensions is large compared to the number of points required; however it does not scale well as the dimension increases. On the other hand, our second approach scales better but requires more iterations than the first method. Finally, numerical experiments show that both methods perform similarly in practice. This research was supported by NSERC Discovery Grants RGPIN-2014-04567 (A.S.) and RGPIN-2016-05189 (M.D.).  Introduction  Let $Gr(k,n)$ denote the Grassmannian manifold consisting of all $k$-dimensional subspaces of $\\mathbb{R}^n$. In many applications such as signal processing or machine learning, one needs to find a subset of points in $Gr(k, n)$ which are close together according to some metric.  For example, consider the following scenario where one wants to transmit data over multiple channels simultaneously using space-time coding techniques. Here, the transmitter encodes information into a vector of symbols drawn from a finite alphabet $\\mathcal{A}$. Then, these vectors can be transmitted across different antennas at the same time. However, if there are too few antennas available then the receiver may experience poor performance due to interference between signals received through adjacent antennas. To overcome this issue, one could use multi-antenna transmission schemes whereby the transmitter sends several independent streams of information across different antennas. These streams would correspond to different points in $Gr(1,N)$, where $N$ denotes the total number of antennas used. Another application involves compressive sensing problems where one seeks to recover a sparse vector $x$ from its noisy measurements $y$ given by: $$y = Ax + w$$", "paraphrased_abstract": "A common example is that of transmitting information over several channels at the same time. Here, as in the following example, a transmission is made over several channels by the transmitter of an infinite alphabet, the symbol of which is represented by the letters \"A\" in a form of mathematicscal A. For this purpose, a transmission is made in several directions, wherein the resulting signals are sent to the different antennas, wherein the antennas belong to the same antenna. Then, we present a method of solving this problem, one based on random sampling and the other on gradient descent. In the first case, we show that the number of dimensions is very large, and that it is not suitable for the increase in the number of antennas; on the other hand, our second method scales better but needs more iterations than the first method. This work was supported by the NSERC Discovery Grants RGPIN-2014-04567 (A.S. and RGPIN-2016-05189): ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3055, "title": "Disclinations, dislocations and continuous defects: a reappraisal", "abstract": "  Disclinations, first observed in mesomorphic phases, are relevant to a number of ill-ordered condensed matter media, with continuous symmetries or frustrated order. They also appear in polycrystals at the edges of grain boundaries. They are of limited interest in solid single crystals, where, owing to their large elastic stresses, they mostly appear in close pairs of opposite signs. The relaxation mechanisms associated with a disclination in its creation, motion, change of shape, involve an interplay with continuous or quantized dislocations and/or continuous disclinations. These are attached to the disclinations or are akin to Nye's dislocation densities, well suited here. The notion of 'extended Volterra process' takes these relaxation processes into account and covers different situations where this interplay takes place. These concepts are illustrated by applications in amorphous solids, mesomorphic phases and frustrated media in their curved habit space. The powerful topological theory of line defects only considers defects stable against relaxation processes compatible with the structure considered. It can be seen as a simplified case of the approach considered here, well suited for media of high plasticity or/and complex structures. Topological stability cannot guarantee energetic stability and sometimes cannot distinguish finer details of structure of defects. ", "machine_abstract": "The concept of defects in crystals has been developed by the Russian school since the 1930s. The main idea is that any crystal can be considered as an elastic continuum with some local deviations from its ideal structure which are called defects. In this work we present a brief review on the history of the development of the theory of defects in solids. We also discuss the modern concepts of point-like defects (dislocations), line-like defects (disclinations) and continuous defects. Finally, we give examples of how these ideas have been applied to different physical systems such as liquid crystals or magnetic materials. Defects play an important role in many areas of physics ranging from solid state physics to condensed matter physics and even biology. They appear naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures. For example, they may lead to plastic deformations in metals or glassy materials. On the other hand, defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization.", "paraphrased_abstract": "Usually, defects occur during phase transitions between ordered states, such as those which occur at melting points or at critical temperatures. They may be the cause of plastic deformation in metals or glassy materials. Then, they are the cause of macroscopic properties of solids, such as electrical conductivity and magnetism. This study is an overview of the history of the theory of defects in solids. We also present the concepts of point-like defects, of lines, of irregularities, and of continuous defects. This theory of defect in crystals has been formulated by the Russian school since the 1930s. The main idea of this theory is that every crystal is a elastic continuum, with local differences from its ideal structure, called defects. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3424, "title": "A New Proof of Pappus's Theorem", "abstract": "  Any stretching of Ringel's non-Pappus pseudoline arrangement when projected into the Euclidean plane, implicitly contains a particular arrangement of nine triangles. This arrangement has a complex constraint involving the sines of its angles. These constraints cannot be satisfied by any projection of the initial arrangement. This is sufficient to prove Pappus's theorem. The derivation of the constraint is via systems of inequalities arising from the polar coordinates of the lines. These systems are linear in r for any given theta, and their solubility can be analysed in terms of the signs of determinants. The evaluation of the determinants is via a normal form for sums of products of sines, giving a powerful system of trigonometric identities. The particular result is generalized to arrangements derived from three edge connected totally cyclic directed graphs, conjectured to be sufficient for a complete analysis of angle constraining arrangements of lines, and thus a full response to Ringel's slope conjecture. These methods are generally applicable to the realizability problem for rank 3 oriented matroids. ", "machine_abstract": "In this note, we present an elementary proof for the following theorem due to Pappus: Theorem 1 (Pappus' theorem). Let $ABC$ be any triangle and let $D$ be its incenter. Then the three medians $AD$, $BE$, and $CF$ intersect at one point $K$ inside the triangle.  Proof. We first prove that the line segments $AB$, $BC$, and $CA$ are concurrent in one point $K$ by using only basic geometry. The idea is as follows. Consider the circle $C_1$ centered at $A$ with radius $AC$. Similarly consider circles $C_2$ and $C_3$ centered at $B$ and $C$ respectively with radii $BD$ and $CD$. Note that all these three circles have their centers on the same side of the triangle $ABC$. Now if we draw lines through $A'$, $B'$, and $C'$ which are intersection points between the circumcircles of triangles $ABC$ and $A_1B_1C_1$, $A_2B_2C_2$, and $A_3B_3C_3$ then it can easily be seen that they will meet at one common point $K$.", "paraphrased_abstract": "But here we show you the principle. If you trace through the intersections of the two circumcircles of the triangles\u2014A\u201d and B\u201d\u2014and A2 and B\u201d\u2014and A3 and B\u201d\u2014across the circumcircles of the triangles\u2014A\u201d and B\u201d\u2014and A\u201d and B\u201d\u2014then you can easily see that they will meet at a common point\u2014K.\u201d So if you think of the circle C_1\u2014centered at A, and radius AC\u2014, consider the circle C_2 and C_3\u2014centered at B, and at C\u2014\u2014and radii BD and CD. We will show the simple proof that the line segments AB, BC, and C are concurrently connected at one point K. The basic principle is shown below. Let the triangle be a triangle, and let the incenter be a square, and let the incenter be a square. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1837, "title": "A rigidity theorem for the mapping class group action on the space of unmeasured foliations on a surface", "abstract": "  Let $S$ be a surface of finite type which is not a sphere with at most four punctures, a torus with at most two punctures, or a closed surface of genus two. Let $\\mathcal{MF}$ be the space of equivalence classes of measured foliations of compact support on $S$ and let $\\mathcal{UMF}$ be the quotient space of $\\mathcal{MF}$ obtained by identifying two equivalence classes whenever they can be represented by topologically equivalent foliations, that is, forgetting the transverse measure. The extended mapping class group $\\Gamma^*$ of $S$ acts as by homeomorphisms of $\\mathcal{UMF}$. We show that the restriction of the action of the whole homeomorphism group of $\\mathcal{UMF}$ on some dense subset of $\\mathcal{UMF}$ coincides with the action of $\\Gamma^*$ on that subset. More precisely, let $\\mathcal{D}$ be the natural image in $\\mathcal{UMF}$ of the set of homotopy classes of not necessarily connected essential disjoint and pairwise nonhomotopic simple closed curves on $S$. The set $\\mathcal{D}$ is dense in $\\mathcal{UMF}$, it is invariant by the action of $\\Gamma^*$ on $\\mathcal{UMF}$ and the restriction of the action of $\\Gamma^*$ on $\\mathcal{D}$ is faithful. We prove that the restriction of the action on $\\mathcal{D}$ of the group $\\mathrm{Homeo}(\\mathcal{UMF})$ coincides with the action of $\\Gamma^*(S)$ on that subspace. ", "machine_abstract": "We prove that if a pseudo-Anosov homeomorphism is homotopic to an automorphism, then it has no periodic points in its Julia set.  This result implies that any such map can be approximated by a linear Anosov diffeomorphism with respect to the Teichm\u00fcller metric and hence gives another proof of Thurston's classification of surfaces.   We also show how this result leads to a new proof of the fact that the mapping class group acts properly discontinuously on the space of measured laminations (and thus on the space of all measured foliations) when restricted to those which are not supported on multicurves.    The first author would like to thank his advisor Professor Richard S. Hamilton for suggesting this problem as well as many helpful discussions throughout the course of this work. The second author would like to thank Professor David Gabai for several useful conversations during the preparation of this manuscript.", "paraphrased_abstract": "The first author would like to thank his advisor Professor Richard Hamilton for suggesting this problem, and for the help he had in making the manuscript. The second author would like to thank Professor David Gabai for his comments on the preparation of this manuscript. The second author would like to thank Professor David Gabai for his helpful discussions during the preparation of this manuscript. The second author would like to thank Professor David Gabai for several helpful discussions during the preparation of the manuscript. The first author would like to thank Professor Richard Hamilton for suggesting the problem and for the helpful discussions during the preparation of this manuscript. This result implies that any such map is perfectly linear to the Teichm\u00fcller metric, and thus is a new proof of Thurston\u2019s classification of surfaces. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.242, "title": "Higher K-theory via universal invariants", "abstract": "  Using the formalism of Grothendieck's derivators, we construct `the universal localizing invariant of dg categories'. By this, we mean a morphism U_l from the pointed derivator associated with the Morita homotopy theory of dg categories to a triangulated strong derivator M^loc such that U_l commutes with filtered homotopy colimits, preserves the point, sends each exact sequence of dg categories to a triangle and is universal for these properties. Similary, we construct the `the universal additive invariant of dg categories', i.e. the universal morphism of derivators U_a to a strong triangulated derivator M^add which satisfies the first two properties but the third one only for split exact sequences. We prove that Waldhausen K-theory appears as a mapping space in the target of the universal additive invariant. This is the first conceptual characterization of Quillen-Waldhausen's K-theory since its definition in the early 70's. As an application we obtain for free the higher Chern characters from K-theory to cyclic homology. ", "machine_abstract": "We give an explicit description of the higher algebraic K-groups in terms of certain universal cohomology classes, which are defined by using only the ring structure and the unit element of the underlying commutative ring. This is done for any commutative ring with unity R (not necessarily Noetherian). The main result can be formulated as follows: Let M be a module over R. Then there exists a natural isomorphism between the higher algebraic K-groups:  K_n(R) = Ext^n_R(M, R) and the group of all n-fold Massey products on M modulo those that vanish under some suitable finiteness condition. We also show how this theorem leads to a new proof of Quillen's localization theorem. Finally we discuss applications to the study of equivariant K-theory. In particular, we prove that if G is a compact Lie group acting freely on a smooth manifold X then the equivariant K-theory groups of X are isomorphic to the ordinary K-theory groups of the fixed point set X^G.", "paraphrased_abstract": "In particular, we prove that if G is a compact Lie group acting freely on a smooth manifold X, then the equivariant K-theory groups of X are equivariant to the K-theory groups of the fixed point set XG. This result is also the proof of the localization theorem of Quillen. We also show that this theorem has a new proof of Quillen's localization theorem. We describe the higher algebraic K-groups with an explicit unified cohomology, based on a single ring and a single unit element in the ring. The main result is as follows: Let M be a module over R. Then a natural isomorphism takes place between the higher algebraic K-groups: Kn(R) = ExtnR(M, R) and the group of all the n-fold massey products on M, modulo those which vanish under a certain conditions. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4467, "title": "SN 2005hj: Evidence for Two Classes of Normal-Bright SNe Ia and Implications for Cosmology", "abstract": "  HET Optical spectra covering the evolution from about 6 days before to about 5 weeks after maximum light and the ROTSE-IIIb unfiltered light curve of the \"Branch-normal\" Type Ia Supernova SN 2005hj are presented. The host galaxy shows HII region lines at redshift of z=0.0574, which puts the peak unfiltered absolute magnitude at a somewhat over-luminous -19.6. The spectra show weak and narrow SiII lines, and for a period of at least 10 days beginning around maximum light these profiles do not change in width or depth and they indicate a constant expansion velocity of ~10,600 km/s. We analyzed the observations based on detailed radiation dynamical models in the literature. Whereas delayed detonation and deflagration models have been used to explain the majority of SNe Ia, they do not predict a long velocity plateau in the SiII minimum with an unvarying line profile. Pulsating delayed detonations and merger scenarios form shell-like density structures with properties mostly related to the mass of the shell, M_shell, and we discuss how these models may explain the observed SiII line evolution; however, these models are based on spherical calculations and other possibilities may exist. SN 2005hj is consistent with respect to the onset, duration, and velocity of the plateau, the peak luminosity and, within the uncertainties, with the intrinsic colors for models with M_shell=0.2 M_sun. Our analysis suggests a distinct class of events hidden within the Branch-normal SNe Ia. If the predicted relations between observables are confirmed, they may provide a way to separate these two groups. We discuss the implications of two distinct progenitor classes on cosmological studies employing SNe Ia, including possible differences in the peak luminosity to light curve width relation. ", "machine_abstract": "We present the discovery, photometric and spectroscopic observations of SN 2005hj (PTF10ygu), an apparently normal-bright Type Ia supernova discovered by PTF in late March 2005 at z = 0.084. The light curve shows two distinct peaks separated by about one month with no evidence for interaction between ejecta and circumstellar material. We find that this object is consistent with being a member of the class of \"normal-bright\" SNe Ia defined by Phillips et al. (1999) but has a higher peak luminosity than most members of this class. Using our own data as well as published results we estimate the distance to SN 2005hj using three different methods. All three give distances which are inconsistent with each other within their uncertainties. This may be due to systematic errors or it could indicate that there exists more than one subclass of \"normal-bright\" objects. If confirmed, these findings have important implications for cosmological studies based on SNe Ia.     Keywords: Supernovae", "paraphrased_abstract": "\u2013 we have found and analyzed the distance to SN 2005hi (PTF10ygu) (the object is a type Ia supernova discovered in March 2005 at z = 0.084). This object is the result of the classification of the normal-coloured SNe Ia as defined by Phillips et al. (1999) but it has a higher luminosity than most SNe Ia. If confirmed, this result may have important implications for the study of SNe Ia. We present here the identification, the spectroscopic observations and the spectroscopic observations of SNe 2005 (PTF10ygu) at z = 0.084, a type Ia supernova discovered by PTF in late March 2005 at z = 0.084. This object was of the type of \"normal\" Type Ia, which Phillips and colleagues described in 1999. It was a supernova of Type Ia. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2923, "title": "Coriolis force corrections to g-mode spectrum in 1D MHD model", "abstract": "  The corrections to g-mode frequencies caused by the presence of a central magnetic field and rotation of the Sun are calculated. The calculations are carried out in the simple one dimensional magnetohydrodynamical model using the approximations which allow one to find the purely analytical spectra of magneto-gravity waves beyond the scope of the JWKB approximation and avoid in a small background magnetic field the appearance of the cusp resonance which locks a wave within the radiative zone. These analytic results are compared with the satellite observations of the g-mode frequency shifts which are of the order one per cent as given in the GOLF experiment at the SoHO board. The main contribution turns out to be the magnetic frequency shift in the strong magnetic field which obeys the used approximations. In particular, the fixed magnetic field strength 700 KG results in the mentioned value of the frequency shift for the g-mode of the radial order n=-10. The rotational shift due to the Coriolis force appears to be small and does not exceed a fracton of per cent, \\alpha_\\Omega < 0.003. ", "machine_abstract": "We present the results of our study on coriolis effects on the global oscillation modes (g-modes) in magnetized white dwarfs using one-dimensional, fully compressible numerical simulations with realistic equation-of-state and opacities. We find that the effect is significant for low-order g-modes but negligible for high-order ones. The frequency shifts are found to be proportional to the mode inertia which can be estimated by fitting the eigenfunctions obtained numerically into those calculated analytically under the thin-shell approximation. For typical parameters relevant to ZZ Ceti stars, we estimate the frequency shift due to coriolis forces to be about 0.1 % at most. This result suggests that it may not be necessary to take account of such effects when interpreting observed frequencies of pulsating white dwarf stars. However, this conclusion should be checked against more sophisticated models including rotation as well as magnetic fields.", "paraphrased_abstract": "Hence, the assumption is that the observed frequency of white dwarf stars should not be regarded as an observation of coriolis, and a certain careful consideration should be taken with the other factors, especially the rotation of the stars and the magnetic fields. We present the results of our work on the coriolis effect on the global oscillation modes of the white dwarf stars in the form of a numerical simulation with a realistic eigenvalue and an analytical assumption of the inertia of the g-modes. The effect of this coriolis in the form of the g-modes is significant for low-order g-modes, while the effect of the high-order g-modes is negligible for the high-order ones. In order to be sure of this result, it is necessary to verify the accuracy of the models of rotation and of magnetic fields. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0856, "title": "Is the Concentration of Dark Matter Halos at Virialization Universal ?", "abstract": "  Several recent studies suggest a correlation between dark matter halo mass and the shape of the density profile. We re-analyze simulations from Ricotti (2003) in which such a correlation was proposed. We use a standard analysis of the halo density profiles and compare the old simulations to new ones performed with Gadget2, including higher resolution runs. We confirm Ricotti's result that, at virialization, the central log slopes alpha, at 5%-10% of the virial radius are correlated with the halo mass and that the halo concentration is a universal constant. Our results do not contradict the majority of published papers: when using a split power law to fit the density profiles, due to the alpha-concentration degeneracy, the fits are consistent with halos having a universal shape with alpha=1 or 1.5 and concentrations that depend on the mass, in agreement with results published elsewhere.   Recently, several groups have found no evidence for convergence of the inner halo profile to a constant power law. The choice of a split power law parameterization used in this letter is motivated by the need to compare our results to previous ones and is formally valid because we are not able to resolve regions where the slope of the fitting function reaches its asymptotic constant value. Using a non-parameterized technique, we also show that the density profiles of dwarf galaxies at z ~ 10 have a log slope shallower than 0.5 within 5% of the virial radius. ", "machine_abstract": "We study the concentration-mass relation for dark matter haloes in cosmological N-body simulations with different initial conditions and resolutions, focusing on the dependence on halo mass and redshift. We find that the concentrations are well described by an empirical formula proposed recently by Navarro et al. (2004) : c = c0(M/M0)^a(z), where M is the virial mass of the halo, z its formation time (defined as the epoch when half of the final mass was assembled into progenitors), c0 ,a and M0 are free parameters to be determined numerically. The best-fit values of these parameters depend only weakly on the simulation resolution or the initial power spectrum index n. In particular, we show that the value of a0 is independent of both n and the numerical resolution. This result suggests that the concentration of dark matter haloes may not be universal but depends on their formation history.", "paraphrased_abstract": "The best possible value of the concentrations depends largely on the resolution of the simulations and on the initial power spectrum index n. The most suitable values of the concentrations depend not only on the resolution of the simulations, but also on the initial resolution of n. The most favorable values of a0 are independent of n and n. The result shows that the concentration of dark matter haloes is not universal but depends on the formation history. We analyze the concentration and mass relation of dark matter haloes in N-body simulations, with different resolutions, and examine their dependence on the mass and redshift of haloes. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3405, "title": "Point-contact spectroscopy of the antiferromagnetic superconductor HoNi2B2C in the normal and superconducting state", "abstract": "  Point-contact (PC) spectroscopy measurements on antiferromagnetic (AF) (T_N=5.2K) HoNi2B2C single crystals in the normal and two different superconducting (SC) states (T_c=8.5K and $T_c^*=5.6K) are reported. The PC study of the electron-boson(phonon) interaction (EB(P)I) spectral function reveals pronounced phonon maxima at 16, 22 and 34meV. For the first time the high energy maxima at about 50meV and 100meV are resolved. Additionally, an admixture of a crystalline-electric-field (CEF) excitations with a maximum near 10meV and a `magnetic` peak near 3meV are observed. The contribution of the 10-meV peak in PC EPI constant \\lambda_PC is evaluated as 20-30%, while contribution of the high energy modes at 50 and 100meV amounts about 10% for each maxima, so the superconductivity might be affected by CEF excitations. The SC gap in HoNi2B2C exhibits a standard single-band BCS-like dependence, but vanishes at $T_c^*=5.6K<T_c, with 2\\Delta/kT_c^*=3.9. The strong coupling Eliashberg analysis of the low-temperature SC phase with T_c^*=5.6K =T_N, coexisting with the commensurate AF structure, suggests a sizable value of the EPI constant \\lambda_s=0.93. We also provide strong support for the recently proposed by us ''Fermi surface (FS) separation'' scenario for the coexistence of magnetism and superconductivity in magnetic borocarbides, namely, that the superconductivity in the commensurate AF phase survives at a special (nearly isotropic) FS sheet without an admixture of Ho 5d states. Above T_c^* the SC features in the PC characteristics are strongly suppressed pointing to a specific weakened SC state between T_c* and T_c. ", "machine_abstract": "We report on point contact Andreev reflection (PCAR) measurements performed on single crystals of the heavy fermion compound HoNi2B2C, which is an antiferromagnet with T N = 1.5 K that becomes a type-II superconductor below Tc = 0.8 K. The PCAR spectra show clear evidence for multiple gaps at low temperatures. We find two distinct gap values, one of them being close to twice the value of the other. This observation suggests that there are two different bands crossing the Fermi level. In addition we observe a temperature dependence of both gaps indicating their nodal character. Our results provide further insight into the electronic structure of this material. Heavy-fermion compounds have attracted considerable interest over recent years because they often exhibit unconventional physical properties such as non-Fermi liquid behavior or even quantum criticality [1] . These materials can be described by the periodic Anderson model [2] , where conduction electrons hybridize strongly with localized f -electrons leading to the formation of narrow bands near the Fermi energy E F [3] . HoNi 2 B 2 C belongs to the family of so-called borocarbides [4] . It crystallizes in the tetragonal ThCr 2 Si 2 structure [5] and has been shown to become a type-II superconductor [6] below T c \u2248 0.8 K [7, 8] . At ambient pressure it orders magnetically around T N = 1.6 K [9] . Recent studies suggest that the magnetic order is driven by strong spin-orbit coupling [10] . A number of experiments indicate that the ground-state wave function consists of singlet pairs [11, 12] . However, the exact nature of the pairing mechanism remains unclear [13] .", "paraphrased_abstract": "On the contrary, they have a weak magnetic force and are therefore able to direct the magnetic field around a predetermined magnetic field at T N=1.6 K. Then, we have recently found evidence that this magnetic field is the result of a strong coupling between spin-orbits. The periodic Anderson model is employed, in which conduction electrons, interacting with local fem-electrons, form narrow bands at the Fermi energy E F. This material is the borocarbide of the group of so-called borocarbides. These materials are of the category of the material which is called borocarbides. The crystallization of this compound is of the Th-Cr-Si 2 structure, which is a type-II superconductor of borocarbides below a temperature of 1.6 K. It is a heavy borocarbide of the borocarbides family. They are made of Th-Cr-Si2 stichonite and have shown to be a type-II superconductor below a temperature of 0.8 K. The results of the PCAR measurements prove that the two gap sizes of the heavy borocarbides are in two different ranges at low temperatures, and one of them is almost twice the value of the other. This shows that there are", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3919, "title": "On over-reflection and generation of Gravito-Alfven waves in solar-type stars", "abstract": "  The dynamics of linear perturbations is studied in magnetized plasma shear flows with a constant shearing rate and with gravity-induced stratification. The general set of linearized equations is derived and the two-dimensional case is considered in detail. The Boussinesq approximation is used in order to examine relatively small-scale perturbations of low-frequency modes: Gravito-Alfven waves (GAW) and Entropy Mode (EM) perturbations. It is shown that for flows with arbitrary shearing rate there exists a finite time interval of non-adiabatic evolution of the perturbations. The non-adiabatic behavior manifests itself in a twofold way, viz. by the over-reflection of the GAWs and by the generation of GAWs from EM perturbations. It is shown that these phenomena act as efficient transformers of the equilibrium flow energy into the energy of the perturbations for moderate and high shearing rate solar plasma flows. Efficient generation of GAW by EM takes place for shearing rates about an order of magnitude smaller than necessary for development of a shear instability. The latter fact could have important consequences for the problem of angular momentum redistribution within the Sun and solar-type stars. ", "machine_abstract": "We study the reflection of Alfven waves at the surface of a rotating star with an inclined magnetic field, which is relevant to the problem of coronal heating by MHD turbulence generated near the stellar surface. We show that if the angle between the wave vector k and the normal n to the reflecting surface satisfies cos(k\u00b7n) > 1 (over-reflection), then there exists a critical frequency \u03c9cr = \u221a 2\u2126 above which no Alfven waves can propagate into the interior region of the star. This leads to the formation of standing waves on the surface of the star whose amplitude grows exponentially as time goes on. The growth rate depends only weakly on the inclination angle \u03b8B of the magnetic field lines but strongly on the density contrast across the surface layer. For typical parameters of the Sun we find that the exponential growth starts after about one day for \u03b8B = 60 degrees and after about ten days for \u03b8B = 90 degrees.", "paraphrased_abstract": "The Sun's inclination B, the angle between the lines of the radial magnetic field, is a weak point, but its strength is strong. As the Sun rotates, it has an inclined magnetic field, and the inclination is a large inclination. The Alfven waves are reflected at the surface of a star with an inclined magnetic field. We show that if the angle  B is 1 a significant inclination, and if the angle  B n is 1 (to over-reflect), then there is a critical frequency cr =  2, and the Alfven waves cannot enter the interior of the star. The amplitude of these waves increases with time. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0617, "title": "Effects of the anomaly on the two-flavor QCD chiral phase transition", "abstract": "  We use strongly coupled lattice QED with two flavors of massless staggered fermions to model the physics of pions in two-flavor massless QCD. Our model has the right chiral symmetries and can be studied efficiently with cluster algorithms. In particular we can tune the strength of the QCD anomaly and thus study its effects on the chiral phase transition. Our study confirms the widely accepted view point that the chiral phase transition is first order in the absence of the anomaly. Turning on the anomaly weakens the transition and turns it second order at a critical anomaly strength. The anomaly strength at the tricritical point is characterized using $r = (M_{\\eta'}-M_{\\pi})/\\rho_{\\eta'}$ where $M_{\\eta'}, M_{\\pi}$ are the screening masses of the anomalous and regular pions and $\\rho_{\\eta'}$ is the mass-scale that governs the low energy fluctuations of the anomalous symmetry. We estimate that $r \\sim 7 $ in our model. This suggests that a strong anomaly at the two-flavor QCD chiral phase transition is necessary to wash out the first order transition. ", "machine_abstract": "We study effects of the axial U(1) A anomaly in the two-flavor lattice QCD with Wilson fermions at finite temperature and density by using the Polyakov-loop extended Nambu-Jona-Lasinio (PNJL) model. We find that the PNJL results are consistent with those obtained by the standard NJL model, which does not include the anomaly effect explicitly. The critical chemical potential for the onset of diquark condensation is shifted to larger values due to the anomaly effect. This shift becomes smaller as we increase the current quark mass. In addition, the magnitude of the diquark condensate decreases when the anomaly term is included.     PACS numbers: 11.10.Wx, 12.38.Gc, 14.40.Nd     Introduction: It has been known that the axial U(1)         Here mq denotes the current quark mass.", "paraphrased_abstract": "Fig. 11 \u2013 PACS number: 11.10 \u2013 Wx, 12.38 \u2013 Gc, 14.40 \u2013 Nd Introduction: It has long been known that the axial U(1) has an inverse correlation with the axial asymmetry. Suppose we assume that the axial asymmetry is a biquark, and that the diquark has an inverse relationship with the kinetic mass of the electrons. This is the basis of the inverse relationship between the electrons and the gases. As we increase the current mass, the potential for condensation is increased by the inverse relationship, and this is smaller as we increase the current mass. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1944, "title": "On the nature of optical nuclei in FR I radio-galaxies from ACS/HST imaging polarimetry", "abstract": "  We obtained optical imaging polarimetry with the ACS/HRC aboard the HST of the 9 closest radio-galaxies in the 3C catalogue with an FR I morphology. The nuclear sources seen in direct HST images in these galaxies are found to be highly polarized with levels in the range ~2-11 % with a median value of 7 %. We discuss the different mechanisms that produce polarized emission and conclude that the only viable interpretation is a synchrotron origin for the optical nuclei. This idea is strengthened by the analogy with the polarization properties of BL Lac objects, providing also further support to the FRI/BL Lac unified model. This confirms previous suggestions that the dominant emission mechanism in low luminosity radio-loud AGN is related to non-thermal radiation produced by the base of their jets. In addition to the nuclear polarization (and to the large scale optical jets), polarization is detected co-spatially with the dusty circumnuclear disks, likely due to dichroic transmission; the polarization vectors are tangential to the disks as expected when the magnetic field responsible for the grains alignment is stretched by differential rotation. We explored the possibility to detect the polarimetric signature of a misaligned radiation beam in FR I, expected in our sources in the frame of the FR I/ BL Lac unification. We did not find this effect in any of the galaxies, but our the results are not conclusive on whether a misaligned beam is indeed present in FR I. ", "machine_abstract": "We present new HST/ACS observations of four nearby Fanaroff-Riley type-I (FR I) radio galaxies, 3C 31, 3C 264, 4C+55.17 and PKS 0745-191 . We use these data to study their nuclear properties by means of aperture photometry and image decomposition techniques. The results show that all sources have an unresolved core component with typical luminosities between 1040-1050 erg s-1 Hz-1 at rest-frame 5100 \u00c5 , which is consistent with previous studies based on ground-based near-infrared spectroscopy. In addition we find evidence for extended emission around the nucleus in two out of four objects studied here. This extended emission has been previously detected only in one other FR I galaxy, namely M 87. Our analysis shows that this extended emission can be explained as scattered light from the central source. Finally, our results suggest that the observed polarization fraction increases towards shorter wavelengths, indicating that dust extinction plays an important role in determining the intrinsic polarization properties of these systems.", "paraphrased_abstract": "The study has shown that the core of the objects is not resolved, with a typical brightness of tens of tens of tens of tens, of a magnitude of about 4010 eg s-1 Hz, at a fixed position of approximately 500  in the distance, and it is in agreement with previous observations by the ground. Our findings suggest that the polarization fraction increases with the wavelength, indicating that dust extinction is a significant factor in the polarization of these systems. In addition, we see a considerable amount of emission around the nucleus of two objects, namely M 87. We show that this extended emission can be explained by the scattering light of the central source. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0626, "title": "Probing dark energy with steerable wavelets through correlation of WMAP and NVSS local morphological measures", "abstract": "  Using local morphological measures on the sphere defined through a steerable wavelet analysis, we examine the three-year WMAP and the NVSS data for correlation induced by the integrated Sachs-Wolfe (ISW) effect. The steerable wavelet constructed from the second derivative of a Gaussian allows one to define three local morphological measures, namely the signed-intensity, orientation and elongation of local features. Detections of correlation between the WMAP and NVSS data are made with each of these morphological measures. The most significant detection is obtained in the correlation of the signed-intensity of local features at a significance of 99.9%. By inspecting signed-intensity sky maps, it is possible for the first time to see the correlation between the WMAP and NVSS data by eye. Foreground contamination and instrumental systematics in the WMAP data are ruled out as the source of all significant detections of correlation. Our results provide new insight on the ISW effect by probing the morphological nature of the correlation induced between the cosmic microwave background and large scale structure of the Universe. Given the current constraints on the flatness of the Universe, our detection of the ISW effect again provides direct and independent evidence for dark energy. Moreover, this new morphological analysis may be used in future to help us to better understand the nature of dark energy. ", "machine_abstract": "We propose to probe the nature of dark energy by correlating the morphological properties of galaxies in the nearby universe (z < 0.1) as measured by the Sloan Digital Sky Survey (SDSS), with those at higher redshifts, z ~ 1, obtained from the Wilkinson Microwave Anisotropy Probe (WMAP). We use steerable wavelet transforms on galaxy images to obtain morphological parameters such as concentration index, asymmetry index, Gini coefficient etc., which are then correlated with their counterparts derived from WMAP data using the same technique. The results show that these correlations can be used to discriminate between different models for dark energy. This is an extension of our earlier work where we have shown how one can correlate the morphology of galaxies in SDSS with CMB fluctuations observed by WMAP. In this work, we present detailed analysis of various statistical tests performed on simulated datasets generated under different cosmological scenarios.", "paraphrased_abstract": "As an extension of our earlier work, we have shown how to correlate the morphology of galaxies in the Slope of Sky survey with the morphology of the Mobius in the Mobius, compared with the morphology of the Mobius in the WMAP. We apply the same methods to a range of cosmological scenarios. The results of these methods are shown to be valid in different cosmological situations. We present a detailed analysis of the statistical results of experiments on simulated data, of which various statistical tests were conducted in the different cosmological conditions. We present the results of the Slope of Sky Survey with the Mobius at z=0.16 of the Dmitry VSS, a survey of the nearby universe. We have used the Sloan Digital Sky Survey to compare the morphological properties of galaxies at the higher redshifts, which are z=0.5, which have been derived from the Wilkinson Microwave Anisotropy Probe. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0024, "title": "Socioeconomic Networks with Long-Range Interactions", "abstract": "  We study a modified version of a model previously proposed by Jackson and Wolinsky to account for communicating information and allocating goods in socioeconomic networks. In the model, the utility function of each node is given by a weighted sum of contributions from all accessible nodes. The weights, parameterized by the variable $\\delta$, decrease with distance. We introduce a growth mechanism where new nodes attach to the existing network preferentially by utility. By increasing $\\delta$, the network structure evolves from a power-law to an exponential degree distribution, passing through a regime characterised by shorter average path length, lower degree assortativity and higher central point dominance. In the second part of the paper we compare different network structures in terms of the average utility received by each node. We show that power-law networks provide higher average utility than Poisson random networks. This provides a possible justification for the ubiquitousness of scale-free networks in the real world. ", "machine_abstract": "We study the dynamics on networks where nodes are connected by both short-range and long-range interactions, which can be either attractive or repulsive. We show that in this case there is no phase transition between ordered and disordered phases as it was observed for purely local interaction models. Instead we find an intermediate regime characterized by strong fluctuations of order parameters. The critical behavior at the boundary to the ordered state depends crucially on whether the long-range interactions are attractive or repulsive. In particular, if they are repulsive then the system exhibits mean-field-like criticality while for attractive long-range interactions one finds non-trivial exponents similar to those found previously for purely local systems.  I. INTRODUCTIO N The concept of self-organized criticality (SOC) has been introduced recently [1] . It describes a class of dynamical systems whose evolution leads them into states close to equilibrium without any fine tuning of control parameters [2] . In SOC systems, the appearance of power-law distributions of avalanche sizes [3] , relaxation times [4] , etc., is explained by assuming that these systems evolve towards their critical point [5] . This idea has been applied successfully to many different physical phenomena [6] including earthquakes [7] , forest fires [8] , traffic jams [9] , sandpiles [10] , neural networks [11] , epidemics [12] , population dynamics [13] , etc.. Recently, several authors have studied the effect of nonlocal interactions on the properties of SOC systems [14 -17] . They showed that nonlocal interactions lead to new interesting effects such as the absence of phase transitions [17] , the existence of multiple absorbing states [14] , and the possibility of having spatially localized solutions [15] . However, all previous studies were restricted to the case when only nearest-neighbor interactions exist. Here we consider more general cases when the network contains also long-range interactions.", "paraphrased_abstract": "The idea of self-organization has been recently introduced, and it describes a class of dynamical systems which, on their development, converge to equilibrium in the absence of any adjustment of their governing conditions. We have already found this in many different physical phenomena, such as earthquakes, forest fires, traffic jams, neural networks, epidemics, population dynamics, etc.. We have seen this concept of self-organization successfully applied to many different physical phenomena, such as earthquakes, forest fires, traffic jams, neural networks, epidemics, population dynamics, etc. In this class of dynamic systems, there is no transition between ordered and disordered phases, as in other systems, and no change in order parameters, but only in a very weak phase, which has a high level of criticality. In this case, the equilibrium of the system is not disturbed, but influenced by the strong fluctuations of the order parameter. In our example, the system is characterized by short-range and long-range interactions, which are both attractive and repellent. In this case, the system is described as a swarming, symmetrical system, whose trajectories are the product of the law of avalanches and the time of relaxation. The criticality at the boundary of the ordered state depends on whether the long-range interactions are attractive or repellent, in particular, they exhibit ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1248, "title": "L'Univers en expansion et probl\\`emes d'\\'energie", "abstract": "  In this paper we first construct a mathematical model for the Universe expansion that started up with the original Big Bang. Next, we discuss the problematic of the mechanical and physical laws invariance regarding the spatial frame exchanges. We then prove the (theoretical) existence of a variable metric $g_t$, depending on time and satisfying to a simplified Einstein equation, so that all free ordinary trajectories are geodesics. This is done by considering the classical Galileo-Newtonian space and time relation, by using generalized Newtonian principles and adding the approved physical new ones (as covariance principle, Mach principle, the Einstein equivalence principle ...) in order to establish a new cosmological model of the dynamical Universe as being $(U(t))_{t\\geq 0}=(B_e(O,t),g_t)_{t\\geq 0}$, where $B_e(O,t)$ is the Euclidean ball of radius $t$ in $\\mathbb R^3$. The cosmological metric $g_t$ is totally determined, at time $t$, by the mass energy distribution $E_t(X)$ on $B_e(O,t)$. We study also the black holes phenomenon and we prove that the total and global cosmological energy distribution $E_t(X)$ satisfies to the wave equation whose solutions are characterized by pseudo-frequencies depending on time and related to the spectrum of the Dirichlet problem on the unit ball $B_e(O,1)$ for the Laplace-Beltrami operator -$\\Delta$. Our model is consistent in the sense that all Newtonian and classical physical laws are valid as particular cases in classical situations. Then, we show that all basic results of Modern Physics are still valid without using the second postulate of special relativity nor the uncertainty principle. ", "machine_abstract": "L'expansion de l'univers est un ph\\u00e9nom\\u00e8ne qui se produit depuis le Big Bang, il y a environ 13 milliards d'ann\\u00e9es. Elle continue aujourd'hui \\u00e0 une vitesse d'environ 70 km/seconde par m\\u00e8tre carr\\u00e9 (ou 1 m/seconde par kilom\\u00e8tre). Cette vitesse d'expansion augmente avec la distance entre les objets observ\\u00e9s dans l'univers. La th\\u00eorie du Big Bang pr\\u00e9dit que l'expansion devrait continuer \\u00e0 s'accro\\u00eetre au cours du temps. Cependant, si l'on observe l'univers sur des distances plus petites, on constate qu'il ne semble pas s'agrandir aussi vite qu'on pourrait s'y attendre. Il existe donc quelque chose qui contrarie cette tendance \\u00e0 l'accroissement de l'expansion. On appelle cela \"l'\u00e9nergie noire\" ou \"dark energy\". L'\u00e9nergie noire repr\u00e9sente environ 70% de l'\u00e9nergie totale pr\u00e9sente dans l'univers.", "paraphrased_abstract": "Depuis le Big Bang, il y a environ 13 milliards d\u2019annu00es. Elle continue aujourd\u2019hui \u00e0 une vitesse d\u2019environ 70 km/seconde par mu00e8tre carru00e8tre (ou 1 m/seconde par kilomotre). Elle continue aujourd\u2019hui \u00e0 une vitesse d\u2019environ 70 km/seconde par mu00e8tre carru00e8tre (ou 1 m/seconde par kilomotre). Cependant, si l\u2019on observe l\u2019univers sur des distances plus petites, on constate qu\u2019il ne semble pas s\u2019agrandir aussi vite qu\u2019on pourrait s\u2019y attendre. L\u2019expansion de l\u2019univers est un ph9nom9e9ne qui se produit depuis le Big Bang, il y a environ 13 milliards d\u2019ann9es. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1505, "title": "Very High Energy Gamma-ray Radiation from the Stellar-mass Black Hole Cygnus X-1", "abstract": "  We report on the results from the observations in very high energy band (VHE, E_gamma > 100 GeV) of the black hole X-ray binary (BHXB) Cygnus X-1. The observations were performed with the MAGIC telescope, for a total of 40 hours during 26 nights, spanning the period between June and November 2006. Searches for steady gamma-ray signals yielded no positive result and upper limits to the integral flux ranging between 1 and 2% of the Crab nebula flux, depending on the energy, have been established. We also analyzed each observation night independently, obtaining evidence of gamma-ray signals at the 4.0 standard deviations (sigma) significance level (3.2 sigma after trial correction) for 154 minutes effective on-time (EOT) on September 24 between 20h58 and 23h41 UTC, coinciding with an X-ray flare seen by RXTE, Swift and INTEGRAL. A search for faster-varying signals within a night resulted in an excess with a significance of 4.9 sigma (4.1 sigma after trial correction) for 79 minutes EOT between 22h17 and 23h41 UTC. The measured excess is compatible with a point-like source at the position of Cygnus X-1, and excludes the nearby radio nebula powered by its relativistic jet. The differential energy spectrum is well fitted by an unbroken power-law described by dN/(dA dt dE) = (2.3+- 0.6) x 10^{-12} (E/1 TeV)^{-3.2 +- 0.6}. This is the first experimental evidence of VHE emission from a stellar-mass black hole, and therefore from a confirmed accreting X-ray binary. ", "machine_abstract": "The Very Energetic Radiation Imaging Telescope Array System (VERITAS) is an array of four 12m diameter imaging atmospheric Cherenkov telescopes located at the Fred Lawrence Whipple Observatory in southern Arizona, USA. The instrument was designed to detect gamma rays with energies between 100 GeV and 30 TeV. In this work we report on observations made by VERITAS during 2005-2007 that reveal very high energy emission from the black hole binary system Cygnus X-1. We find evidence for two distinct spectral components; one peaking near 1 TeV and another extending up to 20 TeV. These results are consistent with previous measurements made using other instruments such as MAGIC and HEGRA. This source has been observed previously but never before detected above 10 TeV. Our data also show significant variability over timescales ranging from minutes to months. Finally, we present preliminary results from our analysis of archival data taken prior to 2005 which suggest that there may be additional periods when Cygnus X-1 emits VHE radiation.", "paraphrased_abstract": "The data we have analyzed are based on historical records and the preliminary results of our study. The Very Energetic Radiation Array (VERITAS) is an array of four twelve-meter diameter, correlating Cherenkov-class telescopes located at the Fred Lawrence Observatory in the southern part of Arizona, a spectrometer designed to detect gamma rays at the energies of 100 and 30 eV. The source is known previously, but has not been observed at the energies of 10 eV., it has been observed at 10 eV and not before, it has been discovered at 20 eV. We have here a preliminary study of the archival data obtained in the last five years, which indicates that Cygnus X-1 might be active in future. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0993, "title": "A map of the day-night contrast of the extrasolar planet HD 189733b", "abstract": "  \"Hot Jupiter\" extrasolar planets are expected to be tidally locked because they are close (<0.05 astronomical units, where 1 AU is the average Sun-Earth distance) to their parent stars, resulting in permanent daysides and nightsides. By observing systems where the planet and star periodically eclipse each other, several groups have been able to estimate the temperatures of the daysides of these planets. A key question is whether the atmosphere is able to transport the energy incident upon the dayside to the nightside, which will determine the temperature at different points on the planet's surface. Here we report observations of HD 189733, the closest of these eclipsing planetary systems, over half an orbital period, from which we can construct a 'map' of the distribution of temperatures. We detected the increase in brightness as the dayside of the planet rotated into view. We estimate a minimum brightness temperature of 973 +/- 33 K and a maximum brightness temperature of 1212 +/- 11 K at a wavelength of 8 microns, indicating that energy from the irradiated dayside is efficiently redistributed throughout the atmosphere, in contrast to a recent claim for another hot Jupiter. Our data indicate that the peak hemisphere-integrated brightness occurs 16$\\pm$6 degrees before opposition, corresponding to a hot spot shifted east of the substellar point. The secondary eclipse (when the planet moves behind the star) occurs 120 +/- 24 s later than predicted, which may indicate a slightly eccentric orbit. ", "machine_abstract": "We present an analysis of high-resolution near-infrared images obtained with NACO/VLT and SPHERE/ZIMPOL/ESO to study the photometric variability of the exoplanet HD 189733b in its transit across the stellar disk. We find that the planet's brightness varies by more than one magnitude between phases 0.8-0.9, which is consistent with previous results based on lower resolution data. The observed phase curve shows two maxima at phases 0.85 and 0.95, indicating that the planet has a strong east-west temperature gradient. This result can be explained if we assume that the planet orbits close to edge-on (i = 85\u00b0) and that it receives most of its energy from the star through a single hemisphere. In this case, the planet would have a large day/night temperature difference of about 1000 K. Our observations also show evidence for a small hot spot located near the substellar point.", "paraphrased_abstract": "It was discovered that the brightness of the exoplanet X. 189733b was of the most striking magnitudes between 0.8 and 0.9, and matched that of the earlier luminous results. We have calculated the mean temperature difference between the two phases at 0.9 and 0.9. This is in keeping with previous results based on the more coarse ones. This result is a consequence of the fact that the planet is orbiting at an angle of i. 89 and the star receiving most of its energy through a single hemisphere. The planet is at a temperature of more than a thousand degrees. This means that the temperature difference between the two phases is a very large one. We have found that the brightness of the planet is variable by more than one tenth of a degree between the 0.9 and 0.9, which is in keeping with the results obtained by previous studies. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0402, "title": "On the orbital periods of the AM CVn stars HP Librae and V803 Centauri", "abstract": "  We analyse high-time-resolution spectroscopy of the AM CVn stars HP Librae and V803 Centauri, taken with the New Technology Telescope (NTT) and the Very Large Telescope (VLT) of the European Southern Observatory, Chile.   We present evidence that the literature value for V803 Cen's orbital period is incorrect, based on an observed `S-wave' in the binary's spectrogram. We measure a spectroscopic period P=1596.4+/-1.2s of the S-wave feature, which is significantly shorter than the 1611-second periods found in previous photometric studies. We conclude that the latter period likely represents a `superhump'. If one assumes that our S-wave period is the orbital period, V803 Cen's mass ratio can be expected to be much less extreme than previously thought, at q~0.07 rather than q~0.016. This relaxes the constraints on the masses of the components considerably: the donor star does then not need to be fully degenerate, and the mass of the accreting white dwarf no longer has to be very close to the Chandrasekhar limit.   For HP Lib, we similarly measure a spectroscopic period P=1102.8+/-0.2s. This supports the identification of HP Lib's photometric periods found in the literature, and the constraints upon the masses derived from them. ", "machine_abstract": "We have analyzed new photometric data for two AM CVn-type systems, HP Librae (=PG 1336-018) and V803 Cen, obtained with the Kepler space telescope in order to determine their orbital periods more accurately than before.  We find that both systems are eclipsing binaries with very short orbital periods of only P orb = 81.6 min for HP Librae and P orb = 80.3 min for V803 Cen. The light curves show deep primary minima which we interpret as being caused by total eclipses of the accretion disks around the white dwarfs. In addition, there is evidence for an additional eclipse feature at phase 0.5-0.7 in the case of HP Librae. This could be due either to partial eclipses or occultations of one component by another. For comparison purposes, we also present results on the previously known system CR Boo.", "paraphrased_abstract": "The light curves we see, on the left, are quite deep and suggest that it is a fusion of two planetary systems, the GP 1336-018 and the GP 80.3, and the GP 80.3 is not a partial eclipse, and it may be caused by a partial eclipse or a partial eclipse of a part. As for CR Boo, we have previously tested it. We have a new measurement of the distance of two planetary systems, the PG Librae (P 1336-018) and the PG 803 Cen (P 803 Cen). Both are at very short distances, P orb = 81.6 min for HP Librae, P orb = 80.3 min for V 803 Cen. We have obtained new data on the orbital periods of both systems and find that they are very short, P orb = 80.3 min for V 803 Cen. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4149, "title": "What happened to the Cosmological QCD Phase Transition?", "abstract": "  The scenario that some first-order phase transitions may have taken place in the early Universe offers us one of the most intriguing and fascinating questions in cosmology. Indeed, the role played by the latent \"heat\" or energy released in the phase transition is highly nontrivial and may lead to some surprising, important results. In this paper, we take the wisdom that the cosmological QCD phase transition, which happened at a time between 10^(-5) sec and 10^(-4) sec or at the temperature of about 150 MeV and accounts for confinement of quarks and gluons to within hadrons, would be of first order. To get the essence out of the scenario, it is sufficient to approximate the true QCD vacuum as one of degenerate theta-vacua and when necessary we try to model it effectively via a complex scalar field with spontaneous symmetry breaking. We examine how and when \"pasted\" or \"patched\" domain walls are formed, how long such walls evolve in the long run, and we believe that the significant portion of dark matter could be accounted for in terms of such domain-wall structure and its remnants. Of course, the cosmological QCD phase transition happened in the way such that the false vacua associated with baryons and many other color-singlet objects did not disappear (that is, using the bag-model language, there are bags of radius 1.0 fermi for the baryons) - but the amount of the energy remained in the false vacua is negligible. The latent energy released due to the conversion of the false vacua to the true vacua, in the form of \"pasted\" or \"patched\" domain walls in the short run and their numerous evolved objects, should make the concept of the \"radiation-dominated\" epoch, or of the \"matter-dominated\" epoch to be re-examined. ", "machine_abstract": "The cosmological electroweak phase transition is now known to be first order, and it has been suggested that this may have important consequences for baryogenesis in the early universe.  In contrast, there are no compelling theoretical reasons why the analogous quantum chromodynamics (QCD) phase transition should also be first-order.   Here we argue that if the QCD phase transition were indeed second-order then it would lead to an unacceptably large number density of topological defects such as monopoles or cosmic strings at late times.  We show how these constraints can be satisfied by introducing new physics into the Standard Model which leads to a strongly first-order phase transition with a critical temperature T_c ~ 100 GeV. The cosmological electroweak phase tran-sition is now known to be of first order [1] , and it has been suggested [2] that this could have important consequences for baryogensis in the early universe. In contrast, there are few compelling theoretical reasons [3] why the analogous quantum chromo-dynamics (QCD) [4] phase transition should also be of first order. If the QCD phase transition was instead second-order [5] , then it would lead to unacceptable numbers [6] of topological defects [7, 8] such as monopoles [9] or cosmic strings [10] . These constraints can be satisfied [11] by introducing new physics [12] into the Standard Model [13] which leads to a strongly-first-order [14] phase transition [15] with a critical temperature Tc~100GeV [16] .", "paraphrased_abstract": "We argue that, if the QCD transition was in fact second-order, it would cause unacceptable numbers of topological defects, such as monopoles or cosmic strings to occur at late times. We prove that by introducing new physics into the standard model, we will produce a strongly first-order transition, Tc  100 GeV. We show that the QCD transition is now known to be first-order, and this could have important consequences for the early universe. But, however, there are few strong theoretical reasons for the analogous QCD transition. The cosmological electroweak phase transition is now known to be first-order, and this has been proposed to have important consequences for the early universe, in contrast to the opposite, the analogous QCD transition. We show that, if the QCD transition is second-order, then a high number of topological defects, such as monopoles or cosmic strings, will be inexplicably high at late times. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2277, "title": "Constraints on the electroweak chiral Lagrangian from the precision data", "abstract": "  In the framework of the effective field theory method, we use the experimental data and the perturbative unitarity bounds to determine the values and uncertainty of all the 11 chiral coefficients ($\\al_i, i=0, ..., 10$) of the standard electroweak chiral Lagrangian. Up to linear terms in $\\al_i$, we provide the one-loop renormalization group equations of all the chiral coefficients, which are calculated in the Feynman-'t Hooft gauge using the modified minimal subtraction scheme. With the improved renormalization group equations to sum over the logarithmic corrections, we analyze the current experimental uncertainty of oblique correction parameters, $S(\\Lambda)$ and $T(\\Lambda)$. We find that, due to the large uncertainty in the triple gauge-boson coupling measurements, the parameter space of positive $S(\\Lambda)$ for $\\Lambda > 1$ TeV is still allowed by the current experimental data. $T(\\Lambda)$ tends to increase with $\\Lambda$ even in the presence of the operators that contribute to the triple and quartic gauge-boson couplings. ", "machine_abstract": "We present an update to our previous analysis [1] , where we used the most recent experimental results for the W and Z boson masses, M W = 80.385 \u00b1 0.025 GeV [2] and M Z = 91.1876 \u00b1 0.0021 GeV [3] . We also use the new value for the top quark mass m t = 172.7 \u00b1 1.1 GeV [4] . The main goal is to determine the parameters in the effective lagrangian describing the interactions between quarks and leptons at low energies. The relevant terms are given by  where L SM denotes the Standard Model (SM) part of the lagrangian, O i are dimension-six operators with coefficients c i which can be determined experimentally or calculated using perturbation theory. In this work we will only consider operators that contribute to fermion-boson vertices. For more details see Ref. [5] . To extract information about these parameters one has to perform fits to various observables such as e + e \u2212 \u2192 ff cross sections [6] , forward-backward asymmetries [7, 8] , polarized deep-inelastic scattering [9] , etc..", "paraphrased_abstract": "\u201cThe first thing to do is to compute the parameters of the lagrangian model that describes the interactions between quarks and leptons at low energies. This is best performed by a number of experiments, such as the following: the LSM\u2014the standard model of the lagrangian, the O\u2014the operator-elements that have coefficients of c, which may be calculated from experimental observations or from the perturbation theory. In this study we will consider the parameters of the sphere of fermions of the lower order. We will use the new value of the highest mass of the fermions, which is 172.7  1.1 GeV. For more on this, see the Reference. For more details, see Ref. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1316, "title": "Physisorption of Nucleobases on Graphene", "abstract": "  We report the results of our first-principles investigation on the interaction of the nucleobases adenine (A), cytosine (C), guanine (G), thymine (T), and uracil (U) with graphene, carried out within the density functional theory framework, with additional calculations utilizing Hartree--Fock plus second-order Moeller-Plesset perturbation theory. The calculated binding energy of the nucleobases shows the following hierarchy: G > T ~ C ~ A > U, with the equilibrium configuration being very similar for all five of them. Our results clearly demonstrate that the nucleobases exhibit significantly different interaction strengths when physisorbed on graphene. The stabilizing factor in the interaction between the base molecule and graphene sheet is dominated by the molecular polarizability that allows a weakly attractive dispersion force to be induced between them. The present study represents a significant step towards a first-principles understanding of how the base sequence of DNA can affect its interaction with carbon nanotubes, as observed experimentally. ", "machine_abstract": "The physisorption of nucleobases (adenine, cytosine, guanine and thymine) onto graphene is investigated by density functional theory calculations at the B3LYP/6-31G(d) level in vacuum conditions. The results show that all four bases are adsorbed on the surface with different binding energies ranging between -0.27 eV for adenine to -1.10 eV for cytosine. In addition, it was found that the adsorption energy decreases as the number of nitrogen atoms increases. This indicates that the interaction strength depends strongly on the electronegativity of the base molecules. It has been shown that the most stable configuration corresponds to an end-on orientation where the carbonyl oxygen atom interacts directly with one of the C-C bonds of the graphene sheet.     Keywords: Physisorption; Graphene; Nucleobase; Density Functional Theory Calculations. Introduction     Graphene is a two-dimensional material consisting of sp2-hybridized carbon atoms arranged into a honeycomb lattice structure [1] . Due to its unique electronic properties such as high carrier mobility [2] , large specific surface area [3] , thermal conductivity [4] , mechanical flexibility [5] , chemical stability [6] and biocompatibility [7, 8] , this material has attracted considerable attention over recent years [9] . However, despite these advantages, there have been some challenges associated with the use of pristine graphene sheets due to their hydrophobic nature [10] which limits their applications [11] . Therefore, many efforts have been made towards modifying the physical and chemical characteristics of graphene through various approaches including covalent [12] or non-covalent [13] functionalization [14] .   In particular, non-covalent functionalization can be achieved via \u03c0-\u03c0 interactions [15] , hydrogen bonding [16] , electrostatic [17] , van der Waals [18] and ionic [19] forces [20] . Among them, \u03c0-\u03c0 stacking is considered to be the strongest noncovalent force [21] . For example, several studies have reported that aromatic compounds [22] , fullerenes [23] , porphyrins [24] , metal complexes [25] and biomolecules [26] could interact with graphene surfaces via \u03c0-", "paraphrased_abstract": "And in this way, in view of the high physical properties, a high number of the atoms, the large specific surface area, the thermal conductivity, the mechanical flexibility, the chemical stability, and the biocompatibility have been studied for some time. Graphene is a two-dimensional material made of sp2 carbon atoms arranged in a honeycomb structure. This material has been studied extensively in recent years, and the characteristics of the material are: high density, high specific surface area, high thermal conductivity, thermal conductivity, mechanical flexibility, chemical stability, biocompatibility, and biocompatibility. Among them, a stacked -glue is the strongest -glue, which has the strongest non-covalent force. The physisorption of nucleobases (adenine, cytosine, guanine, and thymine) in graphene has been examined in the vacuum system of the D.C.C. in which the oxygen atom interacts with the carbon atom. It has been shown that the most stable configuration is formed by the insertion of carbonyl oxygen in a cyclic form, where the oxygen atom is directly in contact with the carbon atom of the carbon. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4328, "title": "Mixed-up trees: the structure of phylogenetic mixtures", "abstract": "  In this paper we apply new geometric and combinatorial methods to the study of phylogenetic mixtures. The focus of the geometric approach is to describe the geometry of phylogenetic mixture distributions for the two state random cluster model, which is a generalization of the two state symmetric (CFN) model. In particular, we show that the set of mixture distributions forms a convex polytope and we calculate its dimension; corollaries include a simple criterion for when a mixture of branch lengths on the star tree can mimic the site pattern frequency vector of a resolved quartet tree. Furthermore, by computing volumes of polytopes we can clarify how ``common'' non-identifiable mixtures are under the CFN model. We also present a new combinatorial result which extends any identifiability result for a specific pair of trees of size six to arbitrary pairs of trees. Next we present a positive result showing identifiability of rates-across-sites models. Finally, we answer a question raised in a previous paper concerning ``mixed branch repulsion'' on trees larger than quartet trees under the CFN model. ", "machine_abstract": "We study the problem of reconstructing evolutionary histories that contain both reticulation events and vertical transmission, where reticulations are represented by hybridization or horizontal gene transfer (HGT). We show how to use maximum parsimony to find an optimal solution for this problem in polynomial time when all input trees have bounded degree. This is achieved through a novel dynamic programming algorithm which computes the minimum number of reticulation events needed to explain each set of taxa on every tree in the input collection. Our approach can be used as part of a larger method for inferring species networks using collections of rooted binary phylogenies inferred from different types of data such as DNA sequences, morphological characters, fossil records etc., and it may also provide insights into the evolution of complex organisms like viruses and bacteria.  Introduction  The reconstruction of evolutionary relationships among biological entities has been one of the central problems in biology since Darwin's On the Origin of Species was published 150 years ago [1] . The most widely accepted model of evolution assumes that new species evolve over time via vertical transmission -that is, they pass their genetic material down directly from parent to offspring [2] . However, there are many cases where we cannot assume that evolution proceeds strictly vertically; instead, some form of horizontal gene transfer (HG T) occurs between two lineages [3] , resulting in what is known as reticulate evolution [4] . For example, if a virus infects a bacterium, then its genes will be passed horizontally across the host cell membrane [5] ; similarly, if a plant hybridizes with another plant [6] , then the offspring inherits genetic material from both parents [7, 8] . In these examples, the process of HGT results in a network-like relationship between the descendants of the original ancestor(s), rather than a strict hierarchy [9] .", "paraphrased_abstract": "There are many cases when a new species cannot be born vertically; instead, the transfer of genes between two species is referred to as a reticulation; this is called a reticulation. A reticulation, on the other hand, is a process of horizontal propagation, and a process of horizontal propagation \u2013 we call it reticulation. Our study shows how to compute a maximum number of reticulations for a given species on each tree. The method can be used to predict the evolution of complex organisms, such as viruses and bacteria. The most widely accepted model of evolution assumes that a new species has evolved in time, that is, they have passed on from one parent to the next. The model assumes that new species have a vertical propagation, that is, from one parent to another. However, there are many cases when this is not possible, and the process of reticulation, or reticulation, is not followed by a linear sequence. The method is known as reticulation, which involves the simultaneous transgression of two or more species, which is known as reticulation. We show how to achieve this in a unified system, when the input tree has a bounded degree. We show how to use a new dynamic algorithm to find the minimum number of reticulation events required to", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3133, "title": "Transverse Polarization of Lambda and Lambda-bar Hyperons in Quasireal Photoproduction", "abstract": "  The HERMES experiment has measured the transverse polarization of Lambda and Lambda-bar hyperons produced inclusively in quasireal photoproduction at a positron beam energy of 27.6 GeV. The transverse polarization of the Lambda hyperon is found to be positive while the observed Lambda-bar polarization is compatible with zero. The transverse polarization values averaged over the kinematic acceptance of HERMES are 0.078 +/- 0.006(stat) +/- 0.012(syst) and -0.025 +/- 0.015(stat) +/- 0.018(syst) for Lambda and Lambda-bar hyperons, respectively. The dependences of Lambda and Lambda-bar polarizations on the fraction $\\zeta$ of the beam's light-cone momentum carried by the hyperon and on the hyperon's transverse momentum $p_T$ were investigated. The measured Lambda polarization rises linearly with $p_T$ and exhibits a different behavior for low and high values of $\\zeta$, which approximately correspond to the backward and forward regions in the center-of-mass frame of the $\\gamma^*N$ reaction. ", "machine_abstract": "We report on the measurement of transverse polarization for lambda ($\\Lambda$) hyperon production in quasireal photoproduction at Jefferson Lab, using data collected with the CLAS detector during Hall B running periods E94-110 and E97-107.  The results are presented as functions of the photon virtuality $Q^2$, the invariant mass $W$ of the final state system, and the angle $\\theta_{K}$ between the normal to the scattering plane and the direction of the kaon produced in association with the hyperon.  We find that the magnitude of the transverse polarization decreases rapidly with increasing $Q^2$ but is relatively insensitive to changes in $W$ or $\\theta_{K}$.  These observations are consistent with theoretical predictions based on perturbative QCD calculations performed within the framework of collinear factorization. DOI: 10.1063/1.3189000. PACS numbers: 13.60.Le, 12.38.Qk", "paraphrased_abstract": "In this study, we report the polarization of lambda (Lambda) hyperons in quasireal photosynthesis at Jefferson Lab, using the CLAS detector. We calculate the degree of polarization with increasing Q, the mass of the final state system, the angle of the angle of the kaon derived from it, and the polarization is shown as a function of the taphon (T) of the light-shielding plane, the orientation of the kaon, the kaon produced by the hyperon., and the taphon. The taphon is a function of the taphon, a function of the taphon (T), the time between the normal and the kaon produced by the hyperon. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0652, "title": "Nanostructure and velocity of field-driven solid-on-solid interfaces moving under a phonon-assisted dynamic", "abstract": "  The nanoscopic structure and the stationary propagation velocity of (1+1)-dimensional solid-on-solid interfaces in an Ising lattice-gas model, which are driven far from equilibrium by an applied force, such as a magnetic field or a difference in (electro)chemical potential, are studied by an analytic nonlinear-response approximation together with kinetic Monte Carlo simulations. Here we consider the case that the system is coupled to a two-dimensional phonon bath. In the resulting dynamic, transitions that conserve the system energy are forbidden, and the effects of the applied force and the interaction energies do not factorize (a so-called hard dynamic). In full agreement with previous general theoretical results we find that the local interface width changes dramatically with the applied force. However, in contrast with other hard dynamics, this change is nonmonotonic in the driving force. However, significant differences between theory and simulation are found near two special values of the driving force, where certain transitions allowed by the solid-on-solid model become forbidden by the phonon-assisted dynamic. Our results represent a significant step toward providing a solid physical foundation for kinetic Monte Carlo simulations. ", "machine_abstract": "We study the dynamics of an interface driven by a periodic external force in one dimension, using molecular dynamics simulations with Lennard-Jones particles at low temperatures (T = 0.1 \u2212 1). We find that the interface moves as a whole when it is driven slowly enough; otherwise, it breaks into several segments which move independently. The number of segments increases with increasing driving frequency or decreasing temperature. In addition to these two regimes, we observe another regime where the interface exhibits stick-slip motion. This third regime occurs for intermediate values of the driving amplitude A and driving frequency f . For this regime, we show that there exists a scaling relation between the average velocity V , the driving frequency f , and the driving amplitude A: V \u221d Af 2 . Finally, we propose a simple model based on the concept of phonon-enhanced diffusion to explain our results. DOI: 10.1103/PhysRevE.77.020101", "paraphrased_abstract": "For this condition, we study the movement of an interface in a purely physical plane, by a particle-simulation method, using the Lenard-Jones particles at a low temperature of a few hundred kcal. At this level, we demonstrate the existence of a scaling relation between the average velocity V and the driving frequency A: V  A 2 ; for the other, we demonstrate that there exists a scale relation between the average velocity V and the driving frequency A, namely, V  A 2 ; and, finally, we propose a simple model based on the theory of phonon-enhanced diffusion. The results obtained are: a constant movement of the interface in one dimension when driven slowly; otherwise, it breaks into several segments, which each have its own momentum. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1251, "title": "Coupling between magnetic ordering and structural instabilities in perovskite biferroics: A first-principles study", "abstract": "  We use first-principles density functional theory-based calculations to investigate structural instabilities in the high symmetry cubic perovskite structure of rare-earth (R $=$ La, Y, Lu) and Bi-based biferroic chromites, focusing on $\\Gamma$ and $R$ point phonons of states with para-, ferro-, and antiferromagnetic ordering. We find that (a) the structure with G-type antiferromagnetic ordering is most stable, (b) the most dominant structural instabilities in these oxides are the ones associated with rotations of oxygen octahedra, and (c) structural instabilities involving changes in Cr-O-Cr bond angle depend sensitively on the changes in magnetic ordering. The dependence of structural instabilities on magnetic ordering can be understood in terms of how super-exchange interactions depend on the Cr-O-Cr bond angles and Cr-O bond lengths. We demonstrate how adequate buckling of Cr-O-Cr chains can favour ferromagnetism. Born effective charges (BEC) calculated using the Berry phase expression are found to be anomalously large for the A-cations, indicating their chemical relevance to ferroelectric distortions. ", "machine_abstract": "We have investigated the coupling between magnetic ordering and structural instability in BiFeO3 by using density functional theory (DFT). We find that ferroelectricity is induced by spin-current mechanism, which can be understood as follows. The Fe-3d orbitals hybridize with O-2p orbitals to form bonding and antibonding states near Fermi level. When spins are parallel along z-direction, the antibonding state lies above Fermi energy while the bonding state below it. This leads to an increase of electron charge at Bi-6s orbital and decrease at O-2p orbital on one side of unit cell, resulting in polarization pointing towards this side. On the other hand, when spins are antiparallel along z-direction, both antibonding and bonding states lie below Fermi energy leading to no net polarization. In addition, we also found that there exists strong spin-phonon interaction due to large magnetostriction effect.", "paraphrased_abstract": "As we have been shown, the magnetostrain is due to the large magnetostrain, and we have measured the effect of the spin-phonon effect on the magnetic order of the atoms. We have measured the magnetic order of the atoms and the stability of the atoms by the density functional theory. We have studied the entanglement between magnetic order and structural instability in BiFeO3. We have investigated the relationship between magnetic order and phonon asymmetry by the method of density functional theory. We have found that the magnetic order of the atoms is induced by the action of the spin-current. The atoms at the bi-d orbits and at the O-2 orbits form the antibonding and the antibonding states below the Fermi energy. The antibonding and the antibonding are merged in the lower half of the cell, and this resulted in polarization towards the other side. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0125, "title": "Effect of Electron Interaction on Statistics of Conductance Oscillations in Open Quantum Dots: Does the Dephasing Time Saturate?", "abstract": "  We perform self-consistent quantum transport calculations in open quantum dots taking into account the effect of electron interaction. We demonstrative that in the regime of the ultralow temperatures $2\\pi k_BT\\lesssim\\Delta$ ($\\Delta $ being the mean level spacing), the electron interaction strongly affects the conductance oscillations and their statistics leading to a drastic deviation from the corresponding predictions for noninteracting electrons. In particular, it causes smearing of conductance oscillations, which is similar to the effect of temperature or inelastic scattering. For $2\\pi k_BT\\gtrsim\\Delta$ the influence of electron interaction on the conductance becomes strongly diminished. Our calculations (that are free from phenomenological parameters of the theory) are in good quantitative agreement with the observed ultralow temperature statistics (Huibers \\textit{et al.}, Phys. Rev. Lett. \\textbf{81}, 1917 (1998)). Our findings question a conventional interpretation of the ultralow temperature saturation of the coherence time in open dots which is based on the noninteracting theories where the electron interaction is neglected and the agrement with the experiment is achieved by introducing additional phenomenological channels of dephasing. ", "machine_abstract": "We study statistics of conductance oscillations in open quantum dots with electron-phonon interaction and dephasing time saturation at high temperatures. We show that this effect leads to appearance of new peaks in the distribution function of conductance fluctuations, which are absent for noninteracting electrons or when the dephasing time is not saturated. The positions of these peaks depend on temperature and dot size. This dependence can be used as an experimental tool for studying phonons in open quantum dots.     Introduction     In recent years there has been growing interest in transport through mesoscopic systems such as semiconductor nanowires [1] , carbon nanotubes [2] , graphene [3] . These structures have unique properties due to their small dimensions (of order 10 nm) [4] . For example, they exhibit ballistic [5] and coherent [6] transport regimes [7, 8] .   In particular, it was shown experimentally [9] that the amplitude of conductance fluctuations in open quantum dots depends strongly on temperature T and dot size L. It decreases rapidly with increasing T and decreasing L [10] . At low temperatures, the main contribution to conductance fluctuations comes from interference effects [11] . However, at higher temperatures, thermal averaging destroys phase coherence between different paths [12] . As a result, the amplitude of conductance fluctuation decreases exponentially with temperature [13] :", "paraphrased_abstract": "I have found in recent years that there is a growing interest in mesoscopic structures, i.e., nanowires, carbon-tubes, graphene, which are of a rather small size (as little as 10 nm in dimension) and in which the conductivity of electrons is a finite quantity and the dephasing time is insufficient. Therefore, the magnitude of conductivity is gradually decreasing with temperature. We study the statistical properties of the phonon in open quantum dots and their dephasing time. At low temperatures the main factor of conductivity is a varying interference, and at higher temperatures thermal averaging destroys the continuity of the phases between different paths. In consequence, the amplitude of conductivity is reduced with the temperature. We demonstrate that this effect produces new peaks in the distribution of the conductance, which are absent for non-interacting electrons and for which the dephasing time is not a saturated one. This dependence is useful for detecting phonons in open quantum dots. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0525, "title": "Nonstationary random acoustic and electromagnetic fields as wave diffusion processes", "abstract": "  We investigate the effects of relatively rapid variations of the boundaries of an overmoded cavity on the stochastic properties of its interior acoustic or electromagnetic field. For quasi-static variations, this field can be represented as an ideal incoherent and statistically homogeneous isotropic random scalar or vector field, respectively. A physical model is constructed showing that the field dynamics can be characterized as a generalized diffusion process. The Langevin--It\\^{o} and Fokker--Planck equations are derived and their associated statistics and distributions for the complex analytic field, its magnitude and energy density are computed. The energy diffusion parameter is found to be proportional to the square of the ratio of the standard deviation of the source field to the characteristic time constant of the dynamic process, but is independent of the initial energy density, to first order. The energy drift vanishes in the asymptotic limit. The time-energy probability distribution is in general not separable, as a result of nonstationarity. A general solution of the Fokker--Planck equation is obtained in integral form, together with explicit closed-form solutions for several asymptotic cases. The findings extend known results on statistics and distributions of quasi-stationary ideal random fields (pure diffusions), which are retrieved as special cases. ", "machine_abstract": "We consider the statistical properties of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions, power spectra, and probability density functions (PDFs). We show that these quantities can be expressed by means of solutions to certain partial differential equations with time-dependent coefficients. The PDFs are obtained for both stationary and nonstationary cases using the method of characteristics. In particular, we derive an exact expression for the PDF of the amplitude fluctuations of a monochromatic plane wave propagating through a turbulent medium. This result is used to obtain expressions for the mean-square fluctuation levels of the electric field strength and intensity at any point along the propagation path. Finally, we discuss some applications of our results to radiowave scintillation theory and radar detection problems. PACS: 42.65.Tg, 43.20 .Fx, 47.55.+q, 47.60.+j", "paraphrased_abstract": "Specifically, we have determined the PDF of the amplitude of a monochromatic plane wave moving through a turbulent medium. The result of this study is to give the equation of the power spectra of the wave in terms of its density, power-spectra, and density of the magnetic field at any given time in the propagation path. The PDFs are computed for the stationary and the non-stationary cases by means of the characteristics. We then show how to express the PDFs of the electric field and the strength and the intensity of the electric field at any time along its path. We apply our results to the theory of radiated radars and to the detection of radar. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3497, "title": "Analysis of thermally stimulated luminescence and conductivity without quasiequilibrium approximation", "abstract": "  Thermally stimulated luminescence (TSL) and conductivity (TSC) are considered using the classical insulator model that assumes one kind of the active trap, one kind of inactive deep trap, and one kind of the recombination center. Kinetic equations describing the model are solved numerically without and with the use of the quasiequilibrium (QE) approximation. The QE state parameter qI,, the relative recombination probability g, and a new parameter called quasi-stationary (QS) state parameter q*=qIg are used for the analysis of the TSL and TSC. The TSL and TSC curves and the temperature dependences of qI, q*, g, the recombination lifetime, and the occupancies of active traps and recombination centers are numerically calculated for five sets of kinetic parameters and different heating rates. These calculation results show that: (1) the upper limit of the heating rate for presence of the QS state appears at higher heating rate than that for the QE state when the retrapping process is present, and (2) the TSL (TSC) curves in the QS state have the properties similar to those for the TSL (TSC) curves in the QE state. Approximate formulas for calculation of the parameters qI and q* in the initial range of the TSL and TSC curves are derived and used in the heating-rate methods, proposed in this work, for determination of those parameters from the calculated TSL curves. ", "machine_abstract": "We present an analysis of the temperature dependence of thermally stimulated luminescent (TSL) glow curves in terms of the nonstationary electron-phonon relaxation theory, which does not assume that the system is close to equilibrium at any time during its evolution.  We show how this approach can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL data obtained on different types of materials. The results are compared with those obtained by other methods such as photoluminescence excitation spectroscopy or Raman scattering. In particular we demonstrate that our method allows one to determine the energy gap between the conduction band minimum and valence band maximum in semiconductors. This work was supported by Russian Science Foundation grant No. 14-50-00040. DOI: 10.1063/1.4935190  I. INTRODUCTORY REMARK The study of luminescence phenomena has been attracting considerable attention over many years because it provides valuable information about electronic structure and optical properties of solids [1] . Thermal stimulation luminescence (TSL), also known as optically stimulated luminescence (OSL), is particularly useful since it enables us to probe the distribution function of electrons excited into the conduction band [2] . In recent decades there have been numerous attempts to develop theoretical models describing various aspects of luminescence processes [3] , including thermal stimulation luminescence [4] - [8] . However, most of these works were based on the assumption that the system under consideration is always close to equilibrium [9] . As a result they cannot describe correctly some important features observed experimentally [10] . For example, the shape of the TSL glow curve depends strongly on the type of material [11] : while in insulators it usually exhibits a single peak [12] , in metals it often consists of several peaks [13] . Moreover, even within the same class of materials, e.g., semiconductor crystals [14] , the number of peaks may vary depending on the doping level [15] . These observations cannot be explained using existing theories [16] .", "paraphrased_abstract": "The study of X-rays has a long history, because it can provide an extremely rich and valuable overview of the electronic structure and the optical properties of solids. The X-rays are very sensitive to the temperature of the source, and their intensity is very high. Moreover, in the same class, as in the semiconductor crystals, the number of peaks may be different depending on the level of ozone [20], which, however, is often different. It has been very much studied in recent years because it has been of great interest for the discovery of electronic structures and optical properties of solids. In addition, there has been a great deal of research on the subject of X-rays. Several theories have been developed for the study of X-rays, especially thermal phosphorescence, and the so-called phonon relaxation. This is not an equilibrium state. The most recent theories, however, were developed according to the assumption that the system must always be near the equilibrium, and that in such a case, they failed to detect certain important features. We present an analysis of the phonon spectrum and the density of charge carriers in the X-rays. The analysis consists in a new theory of phonons and the density of charge carriers in the X-rays. We show how this technique can be used to calculate the energy gap between the minimum and", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2697, "title": "Unconventional superconducting pairing by conventional phonons", "abstract": "  The common wisdom that the phonon mechanism of electron pairing in the weak-coupling Bardeen-Cooper-Schrieffer (BCS) superconductors leads to conventional s-wave Cooper pairs is revised. An inevitable anisotropy of sound velocity in crystals makes the phonon-mediated attraction of electrons non-local in space providing unconventional Cooper pairs with a nonzero orbital momentum in a wide range of electron densities. As a result of this anisotropy quasi-two dimensional charge carriers undergo a quantum phase transition from an unconventional d-wave superconducting state to a conventional s-wave superconductor with more carriers per unit cell. In the opposite strong-coupling regime rotational symmetry breaking appears as a result of a reduced Coulomb repulsion between unconventional bipolarons dismissing thereby some constraints on unconventional pairing in the Bose-Einstein condensation (BEC) limit. The conventional phonons, and not superexchange, are shown to be responsible for the d-wave symmetry of cuprate superconductors, where the on-site Coulomb repulsion is large. ", "machine_abstract": "We show that the electron-phonon interaction can lead to unconventional Cooper pairs in materials with strong spin-orbit coupling and broken inversion symmetry, even if only conventional phonon modes are present.  We consider two different models for such materials: (i) Rashba semiconductors where the conduction band is split into spin-up and -down subbands due to an electric field perpendicular to the plane of the crystal; and (ii) topological insulators which have a single Dirac cone at their surface but exhibit time-reversal-symmetry breaking on the surface or in proximity to magnetic impurities. In both cases we find that the effective low-energy theory contains a term proportional to $(\\vec{\\psi}^\\dagger \\vec{\\psi})^2$, where $\\psi$ denotes the fermionic annihilation operator corresponding to electrons near the Fermi level. This leads to unconventional s-wave pairing between electrons belonging to opposite spins and momenta.", "paraphrased_abstract": "vecpsidagger vecpsi22, where psi is the annihilation operator of electrons at Fermi level. In these cases, we observe that the effective low-energy theory is constituted by a term proportional to the effective vecpsidagger vecpsi2, in which vecpsidagger vecpsi2, where vecpsidagger vecpsi is the term used for electrons of opposite spins and moments. In both cases, we observe that the phonon-phonon irradiation can be used to produce a peculiar s-wave coupling in a material with strong spin orbital and broken inversion symmetry, even if the phonon modes are not present. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0005, "title": "Gravitational Stability of Circumnuclear Disks in Elliptical Galaxies", "abstract": "  A significant fraction of nearby elliptical galaxies are known to have high density gas disks in their circumnuclear (CN) region (0.1 to a few kpc). Yet, ellipticals, especially luminous ones, show little signs of recent star formation (SF). To investigate the possible cause of the dearth of SF in these systems, we study the gravitational stability of CN gas disks embedded within the potentials of both the stellar bulge and the central massive black hole (BH) in ellipticals. We find that CN disks in higher mass galaxies are generally more stable than those in lower mass galaxies, because higher mass galaxies tend to have more massive BHs and more centrally concentrated stellar density profiles. We also consider the case in which the central stellar density profile has a core, which is often observed for ellipticals whose total stellar mass is higher than about 10^11 Msun. Such a cored stellar density profile leads to more unstable CN disks than the power-law density profile characteristic of less massive galaxies. However, the more massive BHs in high-mass galaxies act to stabilize the CN disk. Our results demonstrate that the gravitational potentials of both the central BH and the stellar component should be taken into account when studying the properties of CN disks, as their stability is sensitive to both the BH mass and the stellar density profile. Our results could explain the observed trend that less luminous ellipticals have a greater tendency to exhibit ongoing SF than giant ellipticals. ", "machine_abstract": "We study the stability properties of circumnuclear disks (CNDs) embedded within elliptical galaxies, using N-body simulations with live dark matter halos and stellar components. We find that CNDs are generally stable against bar formation for most reasonable disk parameters. However, we also show that if the central black hole is massive enough to dominate the gravitational potential at small radii, then it can induce strong bars or even destroy the entire disk. This result suggests that the presence of a supermassive black hole may be responsible for some observed nuclear bars in nearby elliptical galaxies.     Keywords: Gravitational instability; Black holes; Bars; Nuclear activity; Galaxy evolution; Disk galaxies; Dark matter halos; Stellar dynamics; Cosmology     1 Introduction     The existence of nuclear bars has been inferred observationally by several authors based on photometric data (e.g., Laine et al. 2002; Erwin 2004) . In particular, Erwin & Sparke (2003) found that about half of their sample of early-type galaxies have nuclear bars. These results suggest that nuclear bars play an important role in galaxy evolution. For example, they could provide fuel for active galactic nuclei through gas inflow into the center of the host galaxy (Shlosman et al. 1990 ). On the other hand, there are only few observational studies which directly detect nuclear bars via high-resolution imaging techniques such as HST observations (Erwin 2004; Sheth et al. 2005) , mainly due to technical difficulties associated with resolving very compact structures near the centers of distant galaxies. Therefore, theoretical investigations of the dynamical behavior of nuclear bars will help us understand how these objects evolve over time.     2 Previous Work     Several previous works studied the stability of nuclear bars in elliptical galaxies. Athanassoula et al. (2005a) performed numerical experiments where they added a rigidly rotating spherical component representing a bulge to a model consisting of a live halo and a rigidly rotating disk. They showed that this system becomes unstable when the mass ratio between the bulge and the disk exceeds a critical value", "paraphrased_abstract": "Athenian et al. (2005b) performed numerical experiments on the stability of the equilateral bulge and the active halo, in which a rigid halo was added to a halo and a disc, and they showed that this structure becomes unstable at the mass ratio of the halo and the disk, and the total mass of the halo increases. We also find that the central black hole, which dominates the gravitational potential, may also produce some strong bars, or even destroy the whole disk. This result suggests that a supermassive black hole could have contributed to some observed nuclear bars in nearby elliptic galaxies. Some authors have subsequently observed that a large number of early elliptic galaxies have a nuclear bar. We study the dynamics of a halo in elliptic galaxies and analyze their stability in N-body models of elliptic galaxies. We also study the stability of elliptic galaxies with simulated and real halo and stellar components. Several authors have studied the stability of haloes in elliptic galaxies. They include Erwin et al. (2005), but this work did not involve the direct observation of haloes by the HST technique. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1812, "title": "The LuckyCam Survey for Very Low Mass Binaries II: 13 new M4.5-M6.0 Binaries", "abstract": "  We present results from a high-angular-resolution survey of 78 very low mass (VLM) binary systems with 6.0 <= V-K colour <= 7.5 and proper motion >= 0.15 arcsec/yr. 21 VLM binaries were detected, 13 of them new discoveries. The new binary systems range in separation between 0.18 arcsec and 1.3 arcsec. The distance-corrected binary fraction is 13.5% (+6.5%/-4%), in agreement with previous results. 9 of the new binary systems have orbital radii > 10 AU, including a new wide VLM binary with 27 AU projected orbital separation. One of the new systems forms two components of a 2300 AU separation triple system. We find that the orbital radius distribution of the binaries with V-K < 6.5 in this survey appears to be different from that of redder (lower-mass) objects, suggesting a possible rapid change in the orbital radius distribution at around the M5 spectral type. The target sample was also selected to investigate X-ray activity among VLM binaries. There is no detectable correlation between excess X-Ray emission and the frequency and binary properties of the VLM systems. ", "machine_abstract": "We present the results of our second survey to find very low mass binaries using lucky imaging with the Palomar Observatory 1.2 m telescope and the PALM-3000 camera. We have discovered thirteen new systems, including ten brown dwarf companions (four previously unknown) in twelve nearby young open clusters ranging in age between 10 Myr and 300 Myr. The masses range from 0.03-0.10 M . These are among the lowest-mass objects ever found by direct imaging techniques. In addition we report on one system that is likely an unresolved binary consisting of two late-type stars. This work represents the largest sample of directly imaged brown dwarfs assembled so far. It will be used as input into population synthesis models aimed at understanding how these objects form and evolve over time.     Keywords: Brown Dwarf, Open Cluster, Direct Imaging, Lucky Imaging, Nearby Stars", "paraphrased_abstract": "The data presented here are the first of many to be collected in this manner, and the data presented herein are the most complete of the ones collected so far. We also present one of these systems, which may be an unresolved binary, consisting of two late stars. We report on thirteen new systems, including ten formerly unknown companions of brown dwarfs, in twelve young open clusters of a height of a hundred and fifty thousand years. The masses range from 0.025 to 0.010 M, and we are the first to report on the properties of these stars, which were discovered by direct observation. We report on a new system, probably an unresolved one, composed of two late stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.186, "title": "Galaxy Colours in the AKARI Deep SEP Survey", "abstract": "  We investigate the segregation of the extragalactic population via colour criteria to produce an efficient and inexpensive methodology to select specific source populations as a function of far-infrared flux. Combining galaxy evolution scenarios and a detailed spectral library of galaxies, we produce simulated catalogues incorporating segregation of the extragalactic population into component types (Normal, star-forming, AGN) via color cuts. As a practical application we apply our criteria to the deepest survey to be undertaken in the far-infrared with the AKARI (formerly ASTRO-F) satellite. Using the far-infrared wavebands of the Far-Infrared Surveyor (FIS, one of the focal-plane instruments on AKARI) we successfully segregate the normal, starburst and ULIRG populations. We also show that with additional MIR imaging from AKARI's Infrared Camera (IRC), significant contamination and/or degeneracy can be further decreased and show a particular example of the separation of cool normal galaxies and cold ULIRG sources. We conclude that our criteria provide an efficient means of selecting source populations (including rare luminous objects) and produce colour-segregated source counts without the requirement of time intensive ground-based follow up to differentiate between the general galaxy population. ", "machine_abstract": "We present galaxy colours derived from the AKARI/IRC deep survey data taken at 9 and 18 micron bands (hereafter, D9 & D18). The sample consists of galaxies with spectroscopic redshifts z < 0.7 selected by their optical counterparts detected in the Subaru/Suprime-Cam B J -band image. We compare our results to those obtained for Spitzer/MIPS 24 \u00b5m-selected galaxies at similar redshift ranges. Our main findings are as follows: (1) Galaxies show bimodal colour distributions both in D9/D18 and D9/D24 planes. (2) In the D9/D18 plane, we find that most of the galaxies have blue or greenish colours while only few galaxies exhibit reddish colours.  (3) In the D9-D18 vs. D9-D24 plane, there is no clear correlation between two colours except for very bright objects which tend to be bluer than fainter ones. (4) There exists a strong anti-correlation between D9/D18 and stellar mass M * .", "paraphrased_abstract": "The astrophysical data is taken from the deep survey of AKARI/IRC from nine and eight microns (now called D9 and D18). In the D9 and D18 planes, most of the galaxies have blue or greenish hues, while only few have red hues. This means that the D9 and D18 planes are not closely related, except for very bright stars, which are bluer than faint ones. Moreover, there is a strong anti-correlation between D9 and D24 with the sphere M. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0782, "title": "Testing outer boundary treatments for the Einstein equations", "abstract": "  Various methods of treating outer boundaries in numerical relativity are compared using a simple test problem: a Schwarzschild black hole with an outgoing gravitational wave perturbation. Numerical solutions computed using different boundary treatments are compared to a `reference' numerical solution obtained by placing the outer boundary at a very large radius. For each boundary treatment, the full solutions including constraint violations and extracted gravitational waves are compared to those of the reference solution, thereby assessing the reflections caused by the artificial boundary. These tests use a first-order generalized harmonic formulation of the Einstein equations. Constraint-preserving boundary conditions for this system are reviewed, and an improved boundary condition on the gauge degrees of freedom is presented. Alternate boundary conditions evaluated here include freezing the incoming characteristic fields, Sommerfeld boundary conditions, and the constraint-preserving boundary conditions of Kreiss and Winicour. Rather different approaches to boundary treatments, such as sponge layers and spatial compactification, are also tested. Overall the best treatment found here combines boundary conditions that preserve the constraints, freeze the Newman-Penrose scalar Psi_0, and control gauge reflections. ", "machine_abstract": "We present results on testing different outer boundary conditions in numerical relativity, using two black hole spacetimes as testbeds.  In particular we consider the case where one or both holes are spinning and use several coordinate systems to evolve these solutions numerically.   We find that the choice of coordinates can have significant effects on the accuracy with which the solution is recovered at large distances from the source region. The most accurate results were obtained by evolving the initial data sets in Kerr-Schild Cartesian coordinates (KSC). However, even when evolved in KSC it was found necessary to impose additional constraints near the outer boundaries in order to obtain stable evolutions over many dynamical timescales. These constraints effectively remove all gravitational radiation from the computational domain. Finally, we also considered an alternative approach based on excision techniques. This method involves removing the interior regions containing singularities from the computational grid and replacing them with suitable analytic expressions.", "paraphrased_abstract": "Moreover, we have a different approach, called entropy. We consider the case of black holes that spin, in which we consider a situation in which the holes are spinning and give some numerical solutions, and then present these solutions in a variety of coordinate systems. We have developed the results of our numerical tests by applying KSC to the first data, in order to obtain stable solutions over many dynamical times. KSC is the most accurate, but in order to evolve these solutions in the KSC, we have to apply additional constraints on the equilateral side of the boundary. This restriction effectively removes all gravitational radiation from the computational domain. The first results we obtained are obtained by extending the KSC system with Kerr-Schild's Cartesian coordinates. Then, as we extend the KSC, we have to impose additional constraints on the outer boundary, and in order to make the solutions resolvable over large distances, in many different timescales. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0404, "title": "Room temperature coexistence of large electric polarization and magnetic order in BiFeO3 single crystals", "abstract": "  From an experimental point of view, room temperature ferroelectricity in BiFeO3 is raising many questions. Electric measurements made a long time ago on solid-solutions of BiFeO3 with Pb(Ti,Zr)O3 indicate that a spontaneous electric polarization exists in BiFeO3 below the Curie temperature TC=1143K. Yet in most reported works, the synthesised samples are too conductive at room temperature to get a clear polarization loop in the bulk without any effects of extrinsic physical or chemical parameters. Surprisingly, up to now there has been no report of a P(E) (polarization versus electric field) loop at room temperature on single crystals of BiFeO3. We describe here our procedure to synthesize ceramics and to grow good quality sizeable single crystals by a flux method. We demonstrate that BiFeO3 is indeed ferroelectric at room-temperature through evidence by Piezoresponse Force Microscopy and P(E) loops. The polarization is found to be large, around 60 microC/cm2, a value that has only been reached in thin films. Magnetic measurements using a SQUID magnetometer and Mossbauer spectroscopy are also presented. The latter confirms the results of NMR measurements concerning the anisotropy of the hyperfine field attributed to the magnetic cycloidal structure. ", "machine_abstract": "BiFeO3 is an antiferromagnetic insulator with high Curie temperature (TC = 1103 K) that exhibits ferroelectricity at room temperature when doped or under pressure. Here we report the observation of spontaneous magnetization, M(T), and electrical polarization, P(E), simultaneously existing over wide ranges of temperatures up to 300 K in pure BiFeO3 single crystal grown by flux method. The results show that both M(T) and P(E) are nearly independent on each other below TC but they start to decrease rapidly above TC. We also found that the magnitude of M(T) decreases faster than that of P(E). These observations suggest that there exists strong coupling between spin and lattice degrees of freedom in this material.     Keywords: Room-temperature multiferroic materials, BiFeO3, Electric polarization, Magnetic properties, Flux growth technique.", "paraphrased_abstract": "There is a single crystalline form of BiFeO3, which is made of a single crystal by the process of electric polarisation. The results show that the magnetic properties of both M(T) and P(E) are almost independent, but at a high temperature the latter diminishes. These results suggest that there is a strong relation between the spin and the lattice degrees of freedom in this material. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2184, "title": "Calibration of the GLAST Burst Monitor detectors", "abstract": "  The GLAST Burst Monitor (GBM) will augment the capabilities of GLAST for the detection of cosmic gamma-ray bursts by extending the energy range (20 MeV to > 300 GeV) of the Large Area Telescope (LAT) towards lower energies by 2 BGO-detectors (150 keV to 30 MeV) and 12 NaI(Tl) detectors (10 keV to 1 MeV). The physical detector response of the GBM instrument for GRBs is determined with the help of Monte Carlo simulations, which are supported and verified by on-ground calibration measurements, performed extensively with the individual detectors at the MPE in 2005. All flight and spare detectors were irradiated with calibrated radioactive sources in the laboratory (from 14 keV to 4.43 MeV). The energy/channel-relations, the dependences of energy resolution and effective areas on the energy and the angular responses were measured. Due to the low number of emission lines of radioactive sources below 100 keV, calibration measurements in the energy range from 10 keV to 60 keV were performed with the X-ray radiometry working group of the Physikalisch-Technische Bundesanstalt (PTB) at the BESSY synchrotron radiation facility, Berlin. ", "machine_abstract": "The Large Area Telescope (LAT) on board Fermi Gamma-ray Space Telescope is scheduled to be launched in June 2008 and will perform high energy gamma ray astronomy for at least 3 years. The LAT consists of 4 identical towers, each with tracker and calorimeter subsystems. Each tower has an anti-coincidence detector that covers its entire surface area. In addition there are two instruments dedicated to detecting bursts of gamma rays -the GBM and the LAT itself. The GBM is composed of 12 sodium iodide scintillation crystals read out by photomultiplier tubes. It is designed to detect GRBs over a wide range of energies between 8 keV and 1 MeV. This work presents results obtained during ground calibration tests performed on all four GBM detectors prior to launch. We present data taken using radioactive sources as well as cosmic muons. These data were used to determine the response functions of the detectors which can then be used to reconstruct the incident photon fluxes.", "paraphrased_abstract": "This work presents the results of the ground calibration of all four GBMs before their launch, comparing them with the data of radioactive sources, and cosmic muons. The LAT consists of four identical telescopes, each with a tracker and a thermometer, and each of these contains an anti-coincidence detector, which covers the entire surface. The GBM is composed of 12 sodium iodide crystals, which are read by photomultipliers and measure gamma-rays. The gamma-ray detector has been designed to detect the gamma-rays of eight keV and one mev. The LAT consists of four identical telescopes, each with a calorimeter, each with an anti-coincidence detector, covering the whole surface. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0841, "title": "Difermion condensates in vacuum in 2-4D four-fermion interaction models", "abstract": "  Theoretical analysis of interplay between the condensates $<\\bar{q}q>$ and $<qq>$ in vacuum is generally made by relativistic effective potentials in the mean field approximation in 2D, 3D and 4D models with two flavor and $N_c$ color massless fermions. It is found that in ground states of these models, interplay between the two condensates mainly depend on the ratio $G_S/H_S$ for 2D and 4D case or $G_S/H_P$ for 3D case, where $G_S$, $H_S$ and $H_P$ are respectively the coupling constants in a scalar $(\\bar{q}q)$, a scalar $(qq)$ and a pseudoscalar $(qq)$ channel. In ground states of all the models, only pure $<\\bar{q}q>$ condensates could exist if $G_S/H_S$ or $G_S/H_P$ is bigger than the critical value $2/N_c$, the ratio of the color numbers of the fermions entering into the condensates $<qq>$ and $<\\bar{q}q>$. As $G_S/H_S$ or $G_S/H_P$ decreases to the region below $2/N_c$, differences of the models will manifest themselves. Depending on different models, and also on $N_c$ in 3D model, one will have or have no the coexistence phase of the two condensates, besides the pure $<qq>$ condensate phase. The $G_S-H_S$ (or $G_S-H_P$) phase diagrams in these models are given. The results also implicate a real constraint on two-flavor QCD-analogous NJL model. ", "machine_abstract": "We study the ground state properties of fermions interacting via attractive four-fermion interactions in two, three and four dimensions (2D, 3D and 4D). We show that for any dimension d > 1 there is always an energy gap between the lowest lying excitations and the ground state which implies that these systems are gapped insulators. In 2D we find that the system has a finite density of states at zero energy leading to a metallic phase with power law decaying correlations. The existence of this metallic phase can be understood by mapping our model onto a free fermionic theory using bosonization techniques. For 3D and 4D we argue that the system is either a Mott insulator or a charge density wave insulator depending on whether the Fermi surface encloses one or more than one electron per unit cell respectively. Finally we discuss how our results may be relevant to recent experiments studying strongly correlated materials such as high temperature superconductors.     Introduction     Strongly correlated electronic systems have been studied extensively over many years both theoretically and experimentally [1] . One of the most interesting phenomena observed in these systems is the formation of ordered phases where electrons localize into spatially separated regions known as \"Cooper pairs\" [2] , \"Mott insulating\" [3] or \"Wigner crystal\" [4] phases. These phases occur when the kinetic energy of the electrons cannot overcome their mutual Coulomb repulsion resulting in a suppression of the single particle spectral weight near the chemical potential [5] .   In order to understand the physics behind these exotic phases it is important to develop theoretical tools capable of describing them accurately [6] . A powerful technique used to describe these types of problems is the so-called \"bosonization method\" [7, 8] . This approach maps the original problem involving fermions to another problem involving bosons thereby allowing us to use standard methods developed for bosonic theories [9] . However, despite its successes, the bosonization method suffers from some drawbacks [10] . Firstly, it only works well if the number of fermions N is large compared to the correlation length \u03be [11] . Secondly, even though the bosonized description", "paraphrased_abstract": "In the physics of these exotic phases it is necessary to develop theories to account for their existence. The most common phenomenon observed in such systems is the formation of ordered phases where electrons localize in spatially separated zones called \u201ccooper pairs\u201d (i.e., \u201cMott insulating\u201d or \u201cWigner crystal\u201d\u2014a region where the kinetic energy of electrons is unable to overcome the affinity of Coulomb repulsion and, thus, the single spectral weight of the single particle is reduced to the spectral potential)\u2014this form of phase is known as \u201ccooper-insulating\u201d or \u201ccooper-insulating\u201d (i.e., \u201ccooper-insulating\u201d) states, in which the maximum number of atoms in the atoms is not less than one, and so the atoms are enclosed by the atoms, but the atoms are not in the atoms. Therefore, we compared the two and three dimensions of the system with that of a free fermionic theory, a technique that is incompatible with the Bosonic Theory. We show that at any dimension d > 1, there is always a energy gap between the lowest excitations and the ground, which indicates that these systems are insulators. The second and third dimensions are considered as Mottinsulators,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1312, "title": "Hitting probabilities for systems of non-linear stochastic heat equations with multiplicative noise", "abstract": "  We consider a system of d non-linear stochastic heat equations in spatial dimension 1 driven by d-dimensional space-time white noise. The non-linearities appear both as additive drift terms and as multipliers of the noise. Using techniques of Malliavin calculus, we establish upper and lower bounds on the one-point density of the solution u(t,x), and upper bounds of Gaussian-type on the two-point density of (u(s,y),u(t,x)). In particular, this estimate quantifies how this density degenerates as (s,y) converges to (t,x). From these results, we deduce upper and lower bounds on hitting probabilities of the process {u(t,x)}_{t \\in \\mathbb{R}_+, x \\in [0,1]}, in terms of respectively Hausdorff measure and Newtonian capacity. These estimates make it possible to show that points are polar when d >6 and are not polar when d<6. We also show that the Hausdorff dimension of the range of the process is 6 when d>6, and give analogous results for the processes t \\mapsto u(t,x) and x \\mapsto u(t,x). Finally, we obtain the values of the Hausdorff dimensions of the level sets of these processes. ", "machine_abstract": "We consider the following system of nonlinear stochastic partial differential equations (SPDE) in $\\mathbb{R}^d$, where $d\\in\\mathbb{N}$ is fixed and $(\\Omega,\\mathcal{F},P)$ denotes some probability space:  \\begin{align} u_t - \\Delta u + f(u) = \\xi_1(t), & t \\geq 0, x \\in \\Omega_0\\\\ v_t - \\Delta v + g(u) = \\eta_2(t), & t\\geq0,x\\in\\Omega_1, \\end{align} where $u$ and $v$ are unknown functions on $\\Omega_0$ resp. $\\Omega_1$, which we assume to be open subsets of $\\mathbb{R}^d$; $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ and $g:\\mathbb{R}\\rightarrow\\mathbb{R}_{\\geq0}$ are given smooth functions satisfying certain growth conditions at infinity, while $\\xi_1$ and $\\eta_2$ denote two independent Brownian motions on $(\\Omega,\\mathcal F, P)$.     We prove that there exists an explicit formula for the hitting probabilities $P_x(T_1<\\tau_{x_0})$ and $P_x(T_2<\\tau_{x_0}$) associated with this SPDE-system, where $x\\in\\Omega_0$, $T_{1,2}>0$ and $\\tau_{x_{0}}=\\inf\\{t>0:u(t,x)=v(t,x)\\}$. In particular, we show that these hitting probabilities can be expressed as solutions of suitable backward Kolmogorov PDEs.", "paraphrased_abstract": "\u2013 in a sense it\u2019s the exact formula for the probability of hitting an object. In this case we are assuming a closed set of \u201cOmega\u201d objects. We are to assume that a smooth function of \u2018U\u2019 and \u2018T\u2019 is given to smooth functions of \u2018Omega\u2019 and \u2018Omega\u2019. Our two unknown functions, u\u2019 and \u2018t\u2019, are unknown functions on \u2018Omega\u2019 and \u2018Omega\u2019, and we assume that they are open subsets of \u2018Omega\u2019; our f:mathbb R\u2013right Arrow\u2019 and \u2018G\u2019:mathbb R\u2013 Right Arrow\u2019 are smooth functions that satisfy certain growth conditions at infinity, while \u2018X\u2019 and \u2018T\u2019 are two independent Brownian motions on \u2018Omega\u2019. The following system of nonlinear stochastic partial differential equations (PDEs) is formulated in the following way: \u2018Begin\u2019align\u2019 u-t- \u2013 \u2013 Delta u + f\u2013g-u = \u2013 \u2013 \u2018Omega\u2019 \u2013 \u2013 \u2018P\u2019, where \u2013 \u2018T\u2019 is \u2018T\u2019 and \u2018T\u2019 is \u2018Omega\u2019 \u2013 \u2018P\u2019, which we assume to be open subsets of \u2018Omega\u2019; \u2018F", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.212, "title": "Near-IR Spectra of Red Supergiants and Giants. I- Models with Solar and with Mixing-Induced Surface Abundance Ratios", "abstract": "  We provide a grid of PHOENIX spectra of red giant and supergiant stars, that extend through optical and near-IR wavelengths. For the first time, models are also provided with modified surface abundances of C, N and O, as a step towards accounting for the changes known to occur due to convective dredge-up (and to be enhanced in the case of rotation). The aims are (i) to assess how well current models reproduce observed spectra, (ii) to quantify the effects of the abundance changes on the spectra, and (iii) to determine how these changes affect estimates of fundamental stellar parameters.   Observed giant star spectra can be fitted very well at solar metallicity down to about 3400K. Modified surface abundances are preferred in only a minority of cases for luminosity class II, possibly indicating mixing in excess of standard first dredge-up.   Supergiant stars show a larger variety of near-IR spectra, and good fits are currently obtained for about one third of the observations only. Modified surface abundances help reproducing strong CN bands, but do not suffice to resolve all the difficulties. The effect of the abundance changes on the estimated Teff depends on the wavelength range of observation and can amount several 100K. Reasons for the remaining discrepancies are discussed. ", "machine_abstract": "We present new near-infrared (NIR) spectra for red supergiants and giants, obtained at the European Southern Observatory in La Silla, Chile.  The sample includes stars with spectral types ranging from M5 to M2Iab, luminosity classes III-V, and surface gravities between log g = 1.0 and 3.8 dex. We compare these observations with synthetic NIR spectra computed using MARCS model atmospheres that include mixing-induced chemical abundance changes near the stellar surface. Our results show that models including such effects are able to reproduce most observed features better than standard solar-scaled models. In particular, we find that the inclusion of mixing leads to an improved fit of the CO bandheads around 2.3 microns as well as of several atomic lines. However, some discrepancies remain which may be due to shortcomings in our current understanding of convection or other physical processes occurring close to the stellar surface.", "paraphrased_abstract": "We demonstrate that our models rely on mixing to make all the visible features more accurate than the solar-scale models. However, we observe that the occurrence of mixing in the NIR can have a more remarkable effect on the conformity of a large number of the observed features than with the solar-scale models. Our results demonstrate that, for the stars with M5 spectral types, M2 spectral types, and the three-dimensional gratings, they are much more consistent with the astrophysical dimensions of the universe. We present new NIR spectral data for red supergiants and giants, obtained from the European Southern Observatory in La Silla, Chile. We show that this data is superior to that produced by the solar model, and the result is better than that of the solar models. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1871, "title": "Simulation study of the two-dimensional Burridge-Knopoff model of earthquakes", "abstract": "  Spatiotemporal correlations of the two-dimensional spring-block (Burridge-Knopoff) model of earthquakes are extensively studied by means of numerical computer simulations. The model is found to exhibit either ``subcritical'' or ``supercritical'' behavior, depending on the values of the model parameters. Transition between these regimes is either continuous or discontinuous. Seismic events in the ``subcritical'' regime and those in the ``supercritical'' regime at larger magnitudes exhibit universal scaling properties. In the ``supercritical'' regime, eminent spatiotemporal correlations, {\\it e.g.}, remarkable growth of seismic activity preceding the mainshock, arise in earthquake occurrence, whereas such spatiotemporal correlations are significantly suppressed in the ``subcritical'' regime. Seismic activity is generically suppressed just before the mainshock in a close vicinity of the epicenter of the upcoming event while it remains to be active in the surroundings (the Mogi doughnut). It is also observed that, before and after the mainshock, the apparent $B$-value of the magnitude distribution decreases or increases in the ``supercritical'' or ``subcritical'' regimes, respectively. Such distinct precursory phenomena may open a way to the prediction of the upcoming large event. ", "machine_abstract": "We present results on the statistical properties of earthquake sequences generated by the 2D Burridge-Knopff (BK) model with random initial conditions and periodic boundary conditions, using numerical simulations. We find that the BK model produces power-law distributions for both the inter-event time distribution and magnitude-frequency relation in agreement with observations. The exponent values are found to be dependent upon the system size N . In particular we show that the exponents decrease as 1/N , which is consistent with previous studies. Finally, we discuss possible reasons behind this dependence. Keywords: Earthquake statistics; Power laws; Random initial conditions; Periodic boundary conditions; Statistical mechanics; Numerical simulation; Burridge-Knopf model. 1 Introduction It has been known since Gutenberg's work [1] that there exists an empirical relationship between the frequency f of occurrence of earthquakes and their magnitudes M : log10(f ) = \u03b1 \u2212 \u03b2M . ( The constants \u03b1 and \u03b2 depend on the region under consideration [2] . This relationship can also be expressed in terms of the number n of events per unit area A within some range [Mmin, Mmax] of magnitudes as: dn/dA \u221d 10 \u03b3\u2212\u03b4M min where dn/da \u221d 10 \u03b3\u2212\u03b1M max [3] . For example, if one considers all earthquakes occurring over a period of time T in a given geographical area then it follows that:  where Ntot denotes the total number of earthquakes during the observation period T . If one instead counts only those earthquakes whose magnitude lies in the interval [Mmin, Mmax]:", "paraphrased_abstract": "There is an empirical relation between the frequency of earthquakes and the magnitude of M. Hence, the following expression is given: n/d = 10M min, where n/d = 10M max. If we consider all earthquakes in a certain region, then it is given that the number of earthquakes in that region corresponds to the number of the magnitudes of M. (Expressed as log10(f)m.) The exponents are therefore increased in proportion to N, and we show that this is true in the case of M. Then the exponents decrease as N increases in proportion, and that in the case of M., the exponents are decreasing. The following results are given: the statistical properties of the two-dimensional model BK., with random initial conditions and periodic boundary conditions; we find that the BK model, for the time and magnitudes, imposes on the earth a power-law relating the time distribution and the frequency of the earthquakes in accordance with the observations, and the relative scales of the exponents. This is what we call the inverse of the observed quantities, which are based on the assumption that the quantity  is greater than the number ; it is also the case that the number  is greater than the quantity. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3922, "title": "A Systematic Study of Departures from Chemical Equilibrium in the Atmospheres of Substellar Mass Objects", "abstract": "  We present a systematic study of the spectral consequences of departures from chemical equilibrium in the atmospheres of L and T dwarfs, and for even cooler dwarfs. The temperature/pressure profiles of the non-equilibrium models are fully consistent with the non-equilibrium chemistry. Our grid of non-equilibrium models includes spectra for effective temperatures from 200 K to 1800 K, three surface gravities, four possible values of the coefficient of eddy diffusion in the radiative zone, and three different CO/CH$_4$ chemical reaction prescriptions. We find that the non-equilibrium overabundance of CO translates into flux suppressions in the M ($\\sim4-$5 $\\mu$m) band of at most $\\sim$40% between effective temperatures of 600 and 1800 K. The effect is largest around $T_{\\rm eff} \\approx 1100$ K. The underabundance of ammonia due to non-equilibrium chemistry translates into flux enhancements of no more than $\\sim$20% for the $T_{\\rm eff}$ range from 300 to 1800 K, with the largest effects at the lowest values of $T_{\\rm eff}$. The magnitude of the departure from chemical equilibrium increases with decreasing gravity, with increasing eddy diffusion coefficient, and with decreasing speed of the CO/CH$_4$ reaction. Though these effects are modest, they lead to better fits with the measured T dwarf spectra. Furthermore, the suppression in the M band due to non-equilibrium enhancements in the CO abundance disappears below $\\sim$500 K, and is only partial above $\\sim$500 K, preserving the M band flux as a useful diagnostic of cool atmospheres and maintaining its importance for searches for brown dwarfs cooler than T dwarfs. ", "machine_abstract": "We present an analysis of chemical equilibrium and disequilibrium processes occurring in the atmospheres of substellar mass objects (SMBOs). We have developed a new method for calculating departures from chemical equilibrium, which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any given point within the atmosphere. This approach allows us to calculate the abundances of individual molecular species as functions of altitude above the photosphere. The results show that there can be significant deviations from chemical equilibrium even under conditions where the gas temperature is much higher than the dust temperature. In particular, we find that carbon monoxide may become depleted by several orders of magnitude relative to its abundance predicted by chemical equilibrium models. These findings suggest that SMBO observations should take into account possible non-equilibrium effects when interpreting their spectra.     Keywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium", "paraphrased_abstract": "In addition, we found that carbon monoxide, for example, was depleted by several orders of magnitude relative to the predicted chemical equilibrium. The spectral spectrum of SMBOs is investigated. We present a new approach for the calculation of the chemical equilibrium of individual species, and we have derived from this an estimate of the relative abundance of each molecular species as a function of the height of the photosphere. We have devised a new method for the calculation of the chemical equilibrium in the atmosphere. The method relies on the assumption that all species are in the local thermodynamic equilibrium at a given time in the atmosphere. We show that the concentrations of various molecular species in the atmosphere can be significant, even at a temperature that is much higher than that of the photosphere. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3348, "title": "Out-of-equilibrium relaxation of the Edwards-Wilkinson elastic line", "abstract": "  We study the non-equilibrium relaxation of an elastic line described by the Edwards-Wilkinson equation. Although this model is the simplest representation of interface dynamics, we highlight that many (not though all) important aspects of the non-equilibrium relaxation of elastic manifolds are already present in such quadratic and clean systems. We analyze in detail the aging behaviour of several two-times averaged and fluctuating observables taking into account finite-size effects and the crossover to the stationary and equilibrium regimes. We start by investigating the structure factor and extracting from its decay a growing correlation length. We present the full two-times and size dependence of the interface roughness and we generalize the Family-Vicsek scaling form to non-equilibrium situations. We compute the incoherent cattering function and we compare it to the one measured in other glassy systems. We analyse the response functions, the violation of the fluctuation-dissipation theorem in the aging regime, and its crossover to the equilibrium relation in the stationary regime. Finally, we study the out-of-equilibrium fluctuations of the previously studied two-times functions and we characterize the scaling properties of their probability distribution functions. Our results allow us to obtain new insights into other glassy problems such as the aging behavior in colloidal glasses and vortex glasses. ", "machine_abstract": "We study numerically and analytically the dynamics of an elastic interface in two dimensions driven by thermal fluctuations, starting far away from equilibrium. We find that the system relaxes to its steady state via coarsening with power law growth of characteristic length scales. The exponents are determined both for the case where the initial condition is random noise as well as for the case when it has a regular pattern. In particular we show how the exponent depends on the strength of disorder present in the initial conditions. This work was supported by NSF grant DMR-0704520 (M.S.) and by DFG grant SFB-TR6 (A.K.).  I. INTRODUCTORY REMARkS  The motion of interfaces between different phases or states plays an important role in many physical systems ranging from crystal growth [1] , fluid flow [2] , magnetic domain wall motion [3] , fracture [4] , wetting [5] , etc.. A common feature shared by all these phenomena is that they involve some kind of competition between surface tension which tries to smooth out any roughness at the interface and other driving forces such as gravity [6] , electric field [7] , chemical potential [8] , etc., which tend to make the interface roughen. It turns out that this competition leads to interesting nonequilibrium behavior [9] . For example, if one starts with flat surfaces then the presence of quenched disorder can lead to the formation of fractal structures [10] . In recent years there have been several studies [11] - [16] devoted to understanding the statistical properties of growing interfaces near their critical dimension d c = 2 [17] . These investigations were motivated primarily by experiments [18] - [20] performed on various types of thin films grown under controlled experimental conditions [21] . One of the main goals of these studies is to understand whether the scaling laws observed experimentally [22] - [24] are universal [25] or depend crucially on microscopic details [26] . Another motivation comes from theoretical interest in studying the interplay between nonlinearity and disorder [27] - [29] . Finally, another reason for investigating the problem theoretically is due to possible applications [30] - [32] in data storage devices [33] and optical", "paraphrased_abstract": "The dynamics of elastic interfaces of different phases and phases are of particular importance in many physical systems, including crystal growth, fluid flow, magnetic domain walls, fracture, wetting, etc., a common feature of all these phenomena is the competition between the tension at the surface, which attempts to smooth out the roughness, and other forces, such as gravity, electric field, chemical potential, etc., which tend to make the surface rough. One of the main goals of these experiments is to understand whether the scaling laws observed experimentally are universal, or if the details of these processes are based on microscopic details. In this respect, it is interesting to examine the interplay between disorder and nonlinearity. This research was supported by the NSF grant DMR-0704520 (M.S.), and by the DFG grant SFB-TR6 (A.K.). We calculate the dynamics of a elastic interface, surrounded by a thermally induced disturbance, and begin to study the general dynamics of the interface. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3097, "title": "Spatially resolved kinematics and stellar populations of brightest cluster and group galaxies", "abstract": "  We present an examination of the kinematics and stellar populations of a sample of 3 Brightest Group Galaxies (BGGs) and 3 Brightest Cluster Galaxies (BCGs) in X-ray groups and clusters. We have obtained high signal-to-noise Gemini/GMOS (Gemini South Multi-Object Spectrograph) long-slit spectra of these galaxies and use Lick indices to determine ages, metallicities and alpha-element abundance ratios out to at least their effective radii. We find that the BGGs and BCGs have very uniform masses, central ages and central metallicities. Examining the radial dependence of their stellar populations, we find no significant velocity dispersion, age, or alpha-enhancement gradients. However, we find a wide range of metallicity gradients, suggesting a variety of formation mechanisms. The range of metallicity gradients observed is surprising given the homogeneous environment these galaxies probe and their uniform central stellar populations. However, our results are inconsistent with any single model of galaxy formation and emphasize the need for more theoretical understanding of both the origins of metallicity gradients and galaxy formation itself. We postulate two possible physical causes for the different formation mechanisms. ", "machine_abstract": "We present spatially-resolved spectroscopic observations for the central regions (r < 1 kpc) of six nearby, massive early-type galaxies in clusters or groups with Mvir > 1013M\u2299. The data were obtained using the Gemini Multi-Object Spectrograph on Gemini North telescope as part of our ongoing program to study the formation histories of these systems. We use the pPXF code to fit the observed spectra with single-single component models consisting of an old passively-evolving population plus a younger burst superimposed at different ages and metallicities. Our main results are summarized below:  - All objects show evidence for multiple components in their line-of-sight velocity distributions.  - In all cases we find that the best-fit model consists of two distinct components: one is dominated by older stars (age>8 Gyr), while the other has intermediate age (1-8 Gyr). - For four out of six targets, the second component shows higher metallicity than the first one.", "paraphrased_abstract": "We have calculated the distances between the central regions (r  1 kpc) of six nearby, massive early-type galaxies, in clusters or groups, with Mvir > 1013. All objects have evidence of multiple components in the radiance of the line of sight. All objects have evidence of two separate components, the older ones, the older ones, the younger ones, the younger ones, the higher they are. We describe the results below. All objects show evidence of multiple components in the velocity of their line of sight. We present the results of a single-component model, in which an old, passive population of stars is combined with a young one, of different ages and metals. The main results are summarized in the following: ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1503, "title": "A Diagrammatic Category for the Representation Theory of U_q(sl_n)", "abstract": "  This thesis provides a partial answer to a question posed by Greg Kuperberg in q-alg/9712003 and again by Justin Roberts as problem 12.18 in \"Problems on invariants of knots and 3-manifolds\", math.GT/0406190, essentially: \"Can one describe the category of representations of the quantum group U_q(sl_n) (thought of as a spherical category) via generators and relations?\"   For each n \\geq 0, I define a certain tensor category of trivalent graphs, modulo isotopy, and construct a functor from this category onto (a full subcategory of) the category of representations of the quantum group U_q(sl_n). One would like to describe completely the kernel of this functor, by providing generators. The resulting quotient of the diagrammatic category would then be a category equivalent to the representation category of U_q(sl_n).   I make significant progress towards this, describing certain generators of the kernel, and some obstructions to further elements. It remains a conjecture that these relations generate the kernel. My results extend those of q-alg/9712003, MR1659228, math.QA/0310143 and math.GT/0506403.   The argument is essentially by constructing a diagrammatic version of the forgetful functor coming from the inclusion of U_q(sl_{n-1}) in U_q(sl_n}. We know this functor is faithful, so a diagram is in the kernel for n exactly if its image under the diagrammatic forgetful functor is in the kernel for n-1. This allows us to perform inductive calculations, both establishing families of elements of the kernel, and finding obstructions. ", "machine_abstract": "We introduce a diagrammatic category $\\mathsf{D}_n$, which is equivalent to the representation theory of the quantum group $U_q(\\mathfrak{sl}_n)$ at $q = 1$.  The objects in this category are finite sets with an action of the symmetric group $S_n$ and morphisms between two such objects are given by diagrams consisting of $n$ dots arranged into rows of length $m$ and $n$ respectively, where each row has exactly one dot on top of another (and no other dots above or below).  We show that these categories have many nice properties including being monoidal, braided, spherical, ribbon, and Koszul.  In particular we prove that they satisfy the conditions necessary to be used as input data for the Fomin-Kirillov algebraization theorem, giving rise to new families of cluster algebras. This work was supported by NSF grant DMS-0805040.", "paraphrased_abstract": "The work was supported by the NSF grant DMS-0805040. We have a mathematical classification of n, which is equivalent to the representation of the symmetric group n, of n. The objects of this classification are finite sets, whose action is symmetric, and which corresponds to the morphisms between two such objects, in the form of diagrams composed of n dots, each of which contains a single n-dot on top of the other, and no more dots on the other. We show that these classifications have many useful properties, such as being monoidal, braided, spherical, ribbon, and Koszul. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2954, "title": "Search for Single-Top Production at CDF", "abstract": "  This article reports on recent searches for single-top-quark production by the CDF collaboration at the Tevatron using a data set that corresponds to an integrated luminosity of 955 pb^-1. Three different analyses techniques are employed, one using likelihood discriminants, one neural networks and one matrix elements. The sensitivity to single-top production at the rate predicted by the standard model ranges from 2.1 to 2.6 sigma. While the first two analyses observe a deficit of single-top like events compared to the expectation, the matrix element method observes an excess corresponding to a background fluctuation of 2.3 sigma. The null results of the likelihood and neural network analyses translate in upper limits on the cross section of 2.6\\ pb for the t-channel production mode and 3.7 pb for the s-channel mode at the 95% C.L. The matrix element result corresponds to a measurement of 2.7^+1.5_-1.3 pb for the combined t- and s-channel single-top cross section. In addition, CDF has searched for non-standard model production of single-top-quarks via the s-channel exchange of a heavy W^\\prime boson. No signal of this process is found resulting in lower mass limits of 760 GeV/c^2 in case the mass of the right-handed neutrino is smaller than the mass of the right-handed W^\\prime or 790 GeV/c^2 in the opposite case. ", "machine_abstract": "The D0 and CDF experiments have searched for single-top production in the t-channel, s-channel, and associated Wt channel using data corresponding to an integrated luminosity of 5.4 fb-1 collected by the Fermilab Tevatron Collider between 1992 and 1996.  The results are presented as 95% confidence level upper limits on the cross sections times branching ratios into leptons (electrons or muons) plus jets.  In addition, we present measurements of the top quark mass made with these events. We find no evidence for new physics beyond standard model expectations. These results supersede those previously reported by both collaborations.  We thank our colleagues in the CERN accelerator departments for their vital contributions to this work. This research is supported by the U.S. Department of Energy under contract No. DE-AC02-76SF00515. The D0 experiment has also performed searches for single-top quarks produced via the t-channel, s-channnel, and associated Wt channels using 4.3 fb-1 of data taken during Run II of the Fermilab TeVatron collider [1] .", "paraphrased_abstract": "These results exceed the results previously reported. The D0 and CDF experiments have examined the top quarks in the t-channel, the s-channel, and the associated Wt channel. These measurements have a  80% confidence level for the cross sections of electrons and muons. This work is supported by the National Science Foundation under contract No. DEAC02-76SF00515. The D0 experiment has also analyzed the top quarks in the t-channel, the s-channel, and the associated Wt channel, from 4.4 fb-1 of data collected during the second run of the Fermilab Tevatron. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1804, "title": "Heat Capacity of PbS: Isotope Effects", "abstract": "  In recent years, the availability of highly pure stable isotopes has made possible the investigation of the dependence of the physical properties of crystals, in particular semiconductors, on their isotopic composition. Following the investigation of the specific heat ($C_p$, $C_v$) of monatomic crystals such as diamond, silicon, and germanium, similar investigations have been undertaken for the tetrahedral diatomic systems ZnO and GaN (wurtzite structure), for which the effect of the mass of the cation differs from that of the anion. In this article we present measurements for a semiconductor with rock salt structure, namely lead sulfide. Because of the large difference in the atomic mass of both constituents ($M_{\\rm Pb}$= 207.21 and ($M_{\\rm S}$=32.06 a.m.u., for the natural isotopic abundance) the effects of varying the cation and that of the anion mass are very different for this canonical semiconductor. We compare the measured temperature dependence of $C_p \\approx C_v$, and the corresponding derivatives with respect to ($M_{\\rm Pb}$ and $M_{\\rm S}$), with \\textit{\\textit{ab initio}} calculations based on the lattice dynamics obtained from the local density approximation (LDA) electronic band structure. Quantitative deviations between theory and experiment are attributed to the absence of spin-orbit interaction in the ABINIT program used for the electronic band structure calculations. ", "machine_abstract": "The heat capacity of lead sulfide (PbS) has been measured in the temperature range 1.8 to 300 K using adiabatic calorimetry and compared with that for lead selenide (PbSe). The results show that, at low temperatures, the heat capacities are dominated by phonon contributions which can be described within Debye theory. At higher temperatures, anharmonic effects become important leading to deviations between experiment and theory. In addition, we find evidence for isotopic effects on both lattice and electronic properties.  These findings suggest that the use of lead chalcogenides as thermoelectric materials may require careful consideration of their thermal stability. Lead chalcogenides have attracted considerable interest recently due to their potential applications in thermoelectrics [1-3]. However, there is still much debate about how these compounds behave under different conditions [4] . This is partly because it is difficult to measure accurately the physical properties such as electrical conductivity or Seebeck coefficient over large ranges of temperature and doping concentration [5] . In this work, we report measurements of the specific heat capacity of two samples of lead sulfide (one natural abundance sample and one enriched in the heavier isotopes 206Pb and 207Pb), together with theoretical calculations based on density functional theory [6] , in order to investigate the effect of isotopic composition on the thermodynamic properties of lead chalcogenide semiconductors [7-9]. We also compare our experimental data with those obtained previously for lead selenide [10] .  Our results demonstrate that the isotopic composition affects not only the lattice but also the electronic contribution to the total heat capacity.", "paraphrased_abstract": "And, in our study, we take the physical properties of two samples of lead sulfide: one from natural abundance and one from the richer isotopes 206 and 207; and, with the aid of density functional theory, we investigate the effect of isotopic composition on the thermodynamic properties of lead sulfide, particularly on the electronic properties. In the present work, we consider the specific heat capacity of two samples of lead sulfide, one enriched in 206Pb and 207Pb, and by the addition of the results of density functional theory, we examine the effects of isotopic composition on the thermodynamic properties of lead sulfides, and we compared them with the results of the previous work. In general, the characterization of the physical properties of the two samples is still quite uncertain. Generally speaking, the physical properties of the materials, such as electrical conductivity or the Seebeck coefficient, are uncertain at higher temperatures. The results show that, below certain temperatures, phonons are predominant, which is analyzed in the theory of Debye. At higher temperatures, anharmonic effects are more important, leading to unexpected results. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2459, "title": "Witnessing the formation of a galaxy cluster at z=0.485: optical and X-ray properties of RX J1117.4+0743 ([VMF 98] 097)", "abstract": "  We present a multiwavelength study of the poor cluster RX J1117.4+0743 ([VMF 98] 097) at z=0.485, based on GMOS/Gemini South g', r' photometry and spectroscopy, and XMM-Newton observations. We examine its nature and surroundings by analyzing the projected galaxy distribution, the galaxy velocity distribution, the weak-lensing mass reconstruction, and the X-ray spectroscopy and imaging. The cluster shows a complex morphology. It is composed by at least two structures along the line-of-sight, with velocity dispersions of 592+-82 km s^-1 and 391+-85 km s^-1 respectively. Other structures are also detected in X-ray, in the galaxy projected number density map, and by weak-lensing. One of these clumps, could be gravitationally bound and associated to the main cluster. The derived temperature and bolometric X-ray luminosity reveal that [VMF 98] 097 behave like a normal cluster, in agreement with Lx-Tx correlation found for both local (z=0) and moderate redshift (z~0.4) clusters. We find that the mass determination inferred from weak-lensing is in average 3 to 4.8 times higher (depending on the model assumed) than the X-ray mass. We have two possible explanations for this discrepancy: i) the cluster is in non-equilibrium, then the deviation of the X-ray estimated mass from the true value can be as high as a factor of two; ii) the intervening mass along the line-of-sight of the cluster is producing an over-estimation of the weak-lensing mass. Based on the analysis presented, we conclude that [VMF 98] 097 is a perturbed cluster with at least two substructures in velocity space and with other nearby structures at projected distances of about 1 h^-1 Mpc. This cluster is an example of a poor cluster caught in the process of accreting sub-structures to become a rich cluster. ", "machine_abstract": "We present new observations of the distant galaxy cluster RX J1117.4+07431, which was discovered in the ROSAT All-Sky Survey data by Voges et al. (1999) . The cluster is located at redshift z = 0.485 \u00b1 0.001 with an estimated mass M500 = 1.7 \u00d7 1013 h-1M\u2299 within r500 = 2.1h-1Mpc . We have obtained deep optical images using Suprime-Cam on Subaru telescope to study its member galaxies. In addition we observed this cluster with Chandra ACIS-I for about 50 ks. Our results are as follows:  -The color-magnitude diagram shows that there exists a red sequence of early-type galaxies down to our limiting magnitude RAB=25 mag.  -From the photometric redshift analysis, we find that the number density profile of the member galaxies follows well the NFW model prediction up to 3 virial radii. -The temperature map derived from the Chandra observation reveals two hot spots near the center of the cluster. These features may be associated with shock heating due to merging activity between sub-clusters or groups.", "paraphrased_abstract": "\u201cIn the color scheme we have shown that the red bands are red, ranging from the very limiting magnitude of our planetary system to the range of three radii. The colour diagram of the clusters shows that there is a red band of early stars, ranging from the limiting magnitude of 25 to the level of 25. The number density is as expected by the NFW, for the entire range of sizes, to 3 virial radii. The new observations of the distant cluster RX J1117.4 +07431 have been made by the Subaru telescope and were used for the analysis of its members. The results are as follows: - The color of the chart shows that a red band of early galaxies, which we call a red, is a red, and it is not a red one, but an early, red, red ring, which is, in fact, a red ring of atoms with a mass of M500 = 1.7  1013 h-1 M, and r500 = 2.1 h-1 Mpc. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0544, "title": "Modeling Repulsive Gravity with Creation", "abstract": "  There is a growing interest in the cosmologists for theories with negative energy scalar fields and creation, in order to model a repulsive gravity. The classical steady state cosmology proposed by Bondi, Gold and Hoyle in 1948, was the first such theory which used a negative kinetic energy creation field to invoke creation of matter. We emphasize that creation plays very crucial role in cosmology and provides a natural explanation to the various explosive phenomena occurring in local (z<0.1) and extra galactic universe. We exemplify this point of view by considering the resurrected version of this theory - the quasi-steady state theory, which tries to relate creation events directly to the large scale dynamics of the universe and supplies more natural explanations of the observed phenomena. Although the theory predicts a decelerating universe at the present era, it explains successfully the recent SNe Ia observations (which require an accelerating universe in the standard cosmology), as we show in this paper by performing a Bayesian analysis of the data. ", "machine_abstract": "The gravitational force between two objects is proportional to the product of their masses, and inversely proportional to the square of the distance separating them.  The same relationship holds true for repulsive gravity as well.   In this article we will examine how such an effect can be modeled using creation in place of mass.    Creation is defined by Einstein's field equations as:  G = 8\u03c0G/c4T00 where T00 represents the energy density of space-time.  We will show that if one object has more creation than another then it exerts a greater repulsive force on its neighbor.   This effect may explain why some stars are surrounded by dark matter while others do not. It also provides insight into the nature of black holes which have been observed to emit radiation but no light. Gravitational attraction occurs when two bodies exert mutual forces upon each other due to their respective masses.  If body A has twice the mass of body B then it exerts double the force on body B compared to what body B exerts on itself.  Similarly, if body C has three times the mass of body D then it exerts triple the force on body D compared to what body D exerts on itself.  Force is measured in newtons (N) or dynes (dyne).", "paraphrased_abstract": "There are two forces that cause two objects to exert mutually alternating force on each other. This force is measured in dynes or in dynes. A force is measured in newtons or dynes. In the following article, we will use the equation of creation in place of mass, as follows: G = 8  G/c4 T00 where T00 is the energy density of space-time. The gravitational force between two objects is proportional to the product of their masses, and the opposite is the square of their distances. This is how black holes emitted no light. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0174, "title": "Entropy signature of the running cosmological constant", "abstract": "  Renormalization group (RG) improved cosmologies based upon a RG trajectory of Quantum Einstein Gravity (QEG) with realistic parameter values are investigated using a system of cosmological evolution equations which allows for an unrestricted energy exchange between the vacuum and the matter sector. It is demonstrated that the scale dependence of the gravitational parameters, the cosmological constant in particular, leads to an entropy production in the matter system. The picture emerges that the Universe started out from a state of vanishing entropy, and that the radiation entropy observed today is essentially due to the coarse graining (RG flow) in the quantum gravity sector which is related to the expansion of the Universe. Furthermore, the RG improved field equations are shown to possess solutions with an epoch of power law inflation immediately after the initial singularity. The inflation is driven by the cosmological constant and ends automatically once the RG running has reduced the vacuum energy to the level of the matter energy density. ", "machine_abstract": "We show that entropy production in quantum field theory is related to the vacuum energy density and its time evolution, which can be used as an indicator for the presence of dark energy. We consider two different models with varying degrees of complexity: (i) a scalar field model where we calculate the entropy production using the Schwinger-Keldysh formalism; (ii) a more realistic model based on QED including fermions and photons. In both cases we find that the entropy production increases monotonically when the vacuum energy decreases over time. This behavior is consistent with our expectation that the universe becomes less ordered if it expands faster than usual due to the presence of dark energy.     Introduction     The accelerated expansion of the universe has been confirmed by many observations [1] . It is generally believed that this acceleration is driven by some form of \"dark energy\" [2] , whose nature remains unknown. One possible explanation for such a phenomenon could be provided by the so-called \"quintessence\" fields [3] - [4] . These are hypothetical scalar fields with negative pressure, which evolve slowly down their potential wells leading to a decrease in the vacuum energy density over time. Another possibility would be that the observed acceleration results from modifications of gravity at large distances [5] - [6] . However, there exist other explanations [7] - [8] .     In order to distinguish between these possibilities one needs to develop new methods allowing us to probe the properties of dark energy directly [9] . Here we propose a novel approach based on the concept of entropy production [10] - [11] . Entropy measures how much disorder or randomness exists in a system [12] . If the universe accelerates because of dark energy then the rate of entropy production should increase accordingly. On the contrary, if the universe decelerates then the entropy production will slow down. Therefore, measuring the amount of entropy produced during the cosmic evolution may provide important information about the underlying physics responsible for the current state of the universe.     In what follows we demonstrate that entropy production in quantum electrodynamics (QED), i.e., the interaction of electrons, positrons and photons, depends crucially on the value", "paraphrased_abstract": "In this article we present an experimental framework for the measurement of the entropy in quantum electrodynamics (QED), that is, the interaction of electrons, positrons and photons, which are fundamental to the physics of the universe. The entropy of a system is the measure of the amount of disorder or randomness of its constituent elements. In the case of the accelerated expansion of the universe, a number of observations have been made, and we have generally accepted that the acceleration is attributed to a mysterious force, whose nature is unknown. One theory of this is the \u201cQuintessence Field\u201d, a so-called scalar field, which gradually sinks down to the eddy and slows down the potential energy, leading to a decrease in the vacuum energy density over time. We have then taken a novel approach to the study of the effects of dark energy in the quantum field, i.e., the chemical reactions of electrons, positrons, and photons. However, there are other possibilities. In our approach, we derived the notion of entropy production, and this is used to measure the amount of dark energy in the quantum field. The quantum field of entropy is characterized by the entropy content of the vacuum and the time it takes to eject the dark energy. We show", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2529, "title": "An experimental test of non-local realism", "abstract": "  Most working scientists hold fast to the concept of 'realism' - a viewpoint according to which an external reality exists independent of observation. But quantum physics has shattered some of our cornerstone beliefs. According to Bell's theorem, any theory that is based on the joint assumption of realism and locality (meaning that local events cannot be affected by actions in space-like separated regions) is at variance with certain quantum predictions. Experiments with entangled pairs of particles have amply confirmed these quantum predictions, thus rendering local realistic theories untenable. Maintaining realism as a fundamental concept would therefore necessitate the introduction of 'spooky' actions that defy locality. Here we show by both theory and experiment that a broad and rather reasonable class of such non-local realistic theories is incompatible with experimentally observable quantum correlations. In the experiment, we measure previously untested correlations between two entangled photons, and show that these correlations violate an inequality proposed by Leggett for non-local realistic theories. Our result suggests that giving up the concept of locality is not sufficient to be consistent with quantum experiments, unless certain intuitive features of realism are abandoned. ", "machine_abstract": "We report on an experiment that tests the predictions of quantum mechanics and local hidden variable theories for two entangled photons in a Bell state, using polarization-entanglement swapping to create a loophole-free Bell inequality violation.  The experiment is performed with a source of polarization-entangled photon pairs produced by spontaneous parametric down-conversion (SPDC) in a nonlinear crystal pumped by a continuous-wave laser at 405 nm.   A half wave plate rotates one of the photons' polarizations by 45 degrees before it enters a beam splitter which separates the pair into two spatially separated beams.   One of these beams passes through a quarter wave plate oriented such that its fast axis makes an angle of 22.5 degrees relative to horizontal; this transforms the horizontally polarized component of the beam's electric field vector into vertically polarized light.   After passing through another quarter wave plate whose fast axis is aligned with vertical, both components are transformed back into horizontally polarized light.   This transformation can be represented as follows:  H \u2192 V \u2192 H = HVH.   The other beam travels directly to Alice's measurement station where she performs measurements along three different axes corresponding to projections onto the basis states |V>, |H>, and |D>, where D denotes diagonal.   Bob measures his photon along four different axes corresponding to projections", "paraphrased_abstract": "The other beam goes directly to Alice's measuring station, where she takes measurements on three axes, corresponding to the projections on the base states, V, H, and D, where D indicates diagonal. The other beam goes straight to Alice's measurement station, where it takes measurements on three axes, corresponding to the projections of the polarizations of the two objects. The polarizations of the two objects are transformed, and a fourth one is split into two beams. A half-wave plate rotates one of the photos by 45 degrees, and then enters a beam-splitting machine that separates them into two beams. The experiments are performed with a polarization-entangled pair produced by spontaneous parametric down-conversion (SPD) in a nonlinear crystal, pumped by a continuous-wave laser of 405 nm. The third beam passes through a quarter-wave plate oriented in the direction of 22.5 degrees and is transformed by a vertically polarized component of the electric field, which is then converted back into vertically polarized light. This transformation is called H, V, H. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0037, "title": "The discrete dipole approximation for simulation of light scattering by particles much larger than the wavelength", "abstract": "  In this manuscript we investigate the capabilities of the Discrete Dipole Approximation (DDA) to simulate scattering from particles that are much larger than the wavelength of the incident light, and describe an optimized publicly available DDA computer program that processes the large number of dipoles required for such simulations. Numerical simulations of light scattering by spheres with size parameters x up to 160 and 40 for refractive index m=1.05 and 2 respectively are presented and compared with exact results of the Mie theory. Errors of both integral and angle-resolved scattering quantities generally increase with m and show no systematic dependence on x. Computational times increase steeply with both x and m, reaching values of more than 2 weeks on a cluster of 64 processors. The main distinctive feature of the computer program is the ability to parallelize a single DDA simulation over a cluster of computers, which allows it to simulate light scattering by very large particles, like the ones that are considered in this manuscript. Current limitations and possible ways for improvement are discussed. ", "machine_abstract": "The Discrete Dipole Approximation (DDA) is used to simulate light scattering by large spherical particles in this study. The DDA was first developed as an efficient method for calculating the interaction between electromagnetic waves and small particles, but it has been extended recently to calculate the interactions with large particles. In order to validate our numerical results obtained using the DDA code, we compare them against those calculated using Mie theory. We find that both methods agree well when the size parameter x = 2\u03c0a/\u03bb < 100 where \u03bb is the incident wavelength and a is the radius of the particle. However, there are some discrepancies observed at higher values of x due to the fact that the DDA assumes that the scatterers have sharp boundaries while the actual particles do not. Finally, we show how the angular distribution of scattered intensity changes as the size of the particle increases. This work will be useful for studying aerosol optical properties such as extinction efficiency and single-scattering albedo.", "paraphrased_abstract": "The result was that, when the size of the particle was x, it was found that it was much more precise, since it assumed that the scatterers were sharply defined, while the actual particles did not have sharp edges. The DDA was developed for the calculation of the interaction between electromagnetic waves and small particles, and recently has been extended to the interaction of large particles. We present numerical results, and a Mie theory of optical properties. We use the DDA to simulate the light scattering by large particles. The DDA was developed to calculate the interaction of small particles with small particles. However, there were some blunders at higher values, because the DDA assumed that the particles have sharp edges, while the particles do not have them. We present numerical results, and compare them with those obtained by the Mie theory. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1914, "title": "Approche variationnelle pour le calcul bay\\'esien dans les probl\\`emes inverses en imagerie", "abstract": "  In a non supervised Bayesian estimation approach for inverse problems in imaging systems, one tries to estimate jointly the unknown image pixels $\\fb$ and the hyperparameters $\\thetab$. This is, in general, done through the joint posterior law $p(\\fb,\\thetab|\\gb)$. The expression of this joint law is often very complex and its exploration through sampling and computation of the point estimators such as MAP and posterior means need either optimization of non convex criteria or int\\'egration of non Gaussian and multi variate probability laws. In any of these cases, we need to do approximations. We had explored before the possibilities of Laplace approximation and sampling by MCMC. In this paper, we explore the possibility of approximating this joint law by a separable one in $\\fb$ and in $\\thetab$. This gives the possibility of developing iterative algorithms with more reasonable computational cost, in particular, if the approximating laws are choosed in the exponential conjugate families. The main objective of this paper is to give details of different algorithms we obtain with different choices of these families. ", "machine_abstract": "Nous proposons une approche variationnelle de la m{\\'e}thode du maximum d'entropie (MaxEnt) pour r{\\'e}soudre des probl\\`emes inverses; nous montrons que cette approche permet d'obtenir un algorithme num{\\'e}rique efficace et robuste, qui peut s'appliquer aux mod\\`eles non-lin{\\'e}aires avec incertitude sur les donn{\\'e}es d'entr{\\'e}e ou sur les param\\`etres inconnus. Nous appliquons notre m{\\'e}thodologie au cas d'un probl\\`eme inverse magn{\\'e}tique, o\u9709 l'on cherche {\\`a} estimer l'aimantation d'une source inconnue plong{\\'e}e dans un champ magn{\\'e}tique connu. Les exp{\\'e}riences num{\\'e}riques montrent que l'algorithme MaxEnt est capable d'estimer correctement la distribution de probabilit{\\'e} recherch{\\'ee}, tout en restant stable face \\u00e0 l'incertitude sur les donn{\\'e", "paraphrased_abstract": ", l'on cherche l'aimantation d'une source inconnue plongee dans un champ magn\u00e9tique connu. Nous appliquons notre mthodologie au cas d'un problme inverse magn\u00e9tique, l'on cherche l'on cherche l'aimantation d'une source inconnue plongee dans un champ magn\u00e9tique connu. We utilise une approche variationnelle de la mthode du maximum d'entropy (MaxEnt) pour rsoudre des problme inverses; we gu\u00e8re que cette approche permet d'obtenir un algorithme numrique efficace et robuste, qui peut s'appliquer aux modeles non-linaires avec incertitude sur les donnes d'entres ou sur les parametres inconnus. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0332, "title": "Nontrivial Geometries: Bounds on the Curvature of the Universe", "abstract": "  Probing the geometry of the universe is one of the most important endevours in cosmology. Current observational data from the Cosmic Microwave Background anisotropy (CMB), galaxy surveys and type Ia supernovae (SNe Ia) strongly constrain the curvature of the universe to be close to zero for a universe dominated by a cosmological constant or dark energy with a constant equation of state. Here we investigate the role of cosmic priors on deriving these tight bounds on geometry, by considering a landscape motivated scenario with an oscillating curvature term. We perform a likelihood analysis of current data under such a model of non-trivial geometry and find that the uncertainties on curvature, and correspondingly on parameters of the matter and dark energy sectors, are larger. Future dark energy experiments together with CMB data from experiments like Planck could dramatically improve our ability to constrain cosmic curvature under such models enabling us to probe possible imprints of quantum gravity. ", "machine_abstract": "The curvature of space-time is one of the most important quantities in general relativity, and it has been studied extensively for many years.  In this work we present new bounds on the curvature of the universe that are derived by using only information about its large-scale structure.   We show how these bounds can be used to constrain cosmological models with nontrivial geometries such as those containing extra dimensions or time-varying fundamental constants.    The results presented here have implications not just for theoretical physics but also for observational astronomy and cosmology. General relativity (GR) describes gravity at all scales ranging from microscopic black holes to supermassive galaxies. It predicts that matter curves spacetime around itself, which causes light rays emitted from distant sources to bend towards each other. This effect was first observed over 100 years ago when astronomers noticed that the positions of stars near the edge of our galaxy were systematically shifted away from their expected locations based on Newton's theory of gravitation [1] . Since then, gravitational lensing has become an extremely powerful tool for studying dark matter [2] , testing GR [3] , constraining modified theories of gravity [4] , and measuring distances [5] . In addition to predicting the bending of light, GR also makes precise predictions about the geometry of spacetime [6] . For example, if you travel along a straight line through empty space, your path will remain straight even though there may be massive objects nearby [7, 8] . Similarly, if you look out into space far enough so that no matter exists, you should see nothing but flat, featureless space [9] . These predictions depend crucially on the fact that the metric describing the geometry of spacetime must satisfy Einstein's field equations [10] . However, despite decades of research, it remains unclear whether the universe actually obeys these equations [11] .", "paraphrased_abstract": "In general relativity, gravity can be understood at any scale, from the microscopic black hole to the supermassive galaxies. Then it is said that matter in general, on the other hand, curves around itself, which causes light rays from distant sources to bend towards one another. Then it is determined by Einstein's field equations, and this equation is based on a new metric, which in reality fulfils the equation of Einstein's field. However, in the midst of this research, it remains unclear whether the universe obeys them. It is well known that GR is the only general theory that accounts for gravity, and GR is the only one of its kind that has been studied for a long time. This metric of the bending of the spacetime can be obtained by a number of different ways, such as that which is derived from the smallest dimensions, or of time. This is the most important metric in general relativity, and it has been studied for a long time. Since its beginning, GR has been very influential in the study of dark matter, the formation of dark matter, the verification of GR, the validation of modified theories of gravity, and the determination of distances. The curvature of the spacetime is also the most important factor in general relativity, and it has been studied for a long time.", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.4146, "title": "Dynamical 3-Space: A Review", "abstract": "  For some 100 years physics has modelled space and time via the spacetime concept, with space being merely an observer dependent perspective effect of that spacetime - space itself had no observer independent existence - it had no ontological status, and it certainly had no dynamical description. In recent years this has all changed. In 2002 it was discovered that a dynamical 3-space had been detected many times, including the Michelson-Morley 1887 light-speed anisotropy experiment. Here we review the dynamics of this 3-space, tracing its evolution from that of an emergent phenomena in the information-theoretic Process Physics to the phenomenological description in terms of a velocity field describing the relative internal motion of the structured 3-space. The new physics of the dynamical 3-space is extensively tested against experimental and astronomical observations, including the necessary generalisation of the Maxwell, Schrodinger and Dirac equations, leading to a derivation and explanation of gravity as a refraction effect of quantum matter waves. The flat and curved spacetime formalisms are derived from the new physics, so explaining their apparent many successes. ", "machine_abstract": "The concept of dynamical 3-space is introduced in this review article as an alternative to the standard space-time picture of relativistic physics. The main idea behind it is that, instead of considering time and space separately, one should consider them together as a single entity called \"dynamical 3-space\". This new approach has several advantages over the traditional viewpoint; for example, it provides a natural explanation for why we experience time flow only forward (and not backward), while at the same time allowing us to preserve causality. In addition, it also allows us to explain how particles can travel faster than light without violating any physical laws. Finally, by introducing the concept of \"quantum potential energy density\" into our description of matter fields, we are able to provide a simple mathematical framework within which all known fundamental interactions between elementary particles may be described. We conclude with some remarks on possible future research directions based upon this novel theoretical perspective.", "paraphrased_abstract": "As for the \"quantum potential energy density\", it is possible to add the basic elements of a unified mathematical model to our model of matter. The concept of dynamical 3-space is introduced in this review as a more logical solution to the relativistic concept of time. Its main idea is that instead of considering time and space separately, it is divided into three parts, and they are called dynamical 3-space. This new theoretical approach has many advantages, such as it gives us a natural explanation for the fact that we perceive time only forward and not backward, and at the same time, it retains causality. Then, by introducing the idea of the \"quantum potential energy density\" to our model of the fundamental interactions between elementary particles, we have provided a simple mathematical framework for all known interactions between elementary particles. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1523, "title": "The orbit, mass, size, albedo, and density of (65489) Ceto/Phorcys: A tidally-evolved binary Centaur", "abstract": "  Hubble Space Telescope observations of Uranus- and Neptune-crossing object (65489) Ceto/Phorcys (provisionally designated 2003 FX128) reveal it to be a close binary system. The mutual orbit has a period of 9.554 +/- 0.011 days and a semimajor axis of 1840 +/- 48 km. These values enable computation of a system mass of (5.41 +/- 0.42) 10^18 kg. Spitzer Space Telescope observations of thermal emission at 24 and 70 microns are combined with visible photometry to constrain the system's effective radius (109 +10/-11 km) and geometric albedo (0.084 +0.021/-0.014). We estimate the average bulk density to be 1.37 +0.66/-0.32 g cm^-3, consistent with ice plus rocky and/or carbonaceous materials. This density contrasts with lower densities recently measured with the same technique for three other comparably-sized outer Solar System binaries (617) Patroclus, (26308) 1998 SM165, and (47171) 1999 TC36, and is closer to the density of the saturnian irregular satellite Phoebe. The mutual orbit of Ceto and Phorcys is nearly circular, with an eccentricity <= 0.015. This observation is consistent with calculations suggesting that the system should tidally evolve on a timescale shorter than the age of the solar system. ", "machine_abstract": "We report the discovery of an unusual object in the outer solar system that is likely to be composed primarily of ice particles. The new body, (65489) Ceto/Phorcys, was discovered by NASA's Wide-field Infrared Survey Explorer mission on September 24th 2010 at heliocentric distance of ~5 AU. It has been observed for more than two years with ground-based telescopes as well as space-borne observatories such as Spitzer Space Telescope and Herschel Space Observatory. We present our analysis of these data sets which reveal that this object is a binary Centaur whose components are locked into mutual tidal evolution. This unique combination of properties makes it one of only three known objects in the Solar System where both components have diameters larger than 100 km. Our results show that Phorcys is a very elongated ellipsoidal shape with dimensions of about 400 x 300 x 250 kilometers. Its surface temperature is estimated to be around -100 degrees Celsius.", "paraphrased_abstract": "And we have a new body, which is supposed to be a Centaur, of which there are only three of them, and they are all ice particles. Our results show that Phorcys is a very long, ellipsoidal shape, with dimensions of about 400 x 300 x 250 kilometer, with an inclination of a few hundred kilometres. The temperature is estimated to be around 100 degrees. This is the first time in the solar system that it is composed of ice particles, and we report that this object is found in the far reaches of the solar system. We report that it was discovered by NASA\u2019s Wide-Field Imaging Explorer, September 24, 2010 at a heliocentric distance of 5 AU. This new object, which we believe to be made of ice, was discovered on September 24, 2010 by the Wide-Field Imaging Explorer, at a distance of about 5 AU. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2426, "title": "Testing General Metric Theories of Gravity with Bursting Neutron Stars", "abstract": "  I show that several observable properties of bursting neutron stars in metric theories of gravity can be calculated using only conservation laws, Killing symmetries, and the Einstein equivalence principle, without requiring the validity of the general relativistic field equations. I calculate, in particular, the gravitational redshift of a surface atomic line, the touchdown luminosity of a radius-expansion burst, which is believed to be equal to the Eddington critical luminosity, and the apparent surface area of a neutron star as measured during the cooling tails of bursts. I show that, for a general metric theory of gravity, the apparent surface area of a neutron star depends on the coordinate radius of the stellar surface and on its gravitational redshift in the exact same way as in general relativity. On the other hand, the Eddington critical luminosity depends also on an additional parameter that measures the degree to which the general relativistic field equations are satisfied. These results can be used in conjunction with current and future high-energy observations of bursting neutron stars to test general relativity in the strong-field regime. ", "machine_abstract": "We present the results of an analysis of gravitational wave data collected by the LIGO and Virgo detectors during the first observing run (O1) in 2015, which includes two candidate events for binary neutron star mergers. We use these observations to test general relativity against alternative theories of gravity that predict deviations from GR at high curvature regimes such as those encountered near black holes or neutron stars. In particular we consider scalar-tensor theories where the coupling between matter fields and the metric is mediated by a light scalar field. These theories are motivated by string theory and have been studied extensively over many decades.     For each event, we perform Bayesian model selection using simulated signals generated from both GR and several representative scalartensor theories. Our results show no evidence for deviations from GR within current uncertainties. However, this does not rule out all possible deviations from GR; it only rules out certain classes of deviations predicted by specific models.", "paraphrased_abstract": "But it does not preclude that GR may be violated by any one of the possibilities, nor does it preclude the possibility of certain deviations from GR, as has been argued in the past. We present an analysis of the gravitational wave data collected by the LIGO and Virgo explorers during the first observance of the year 2015 at the O-1 time, involving two candidates for the merger of two neutron stars. We examine these observations to test the generality of gravity against the theories of gravity that predict deviations from GR in very high curvature, such as that encountered near black holes or neutron stars. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.293, "title": "Probing Inward Motions in Starless Cores Using The HCN J = 1-0 Hyperfine Transitions : A Pointing Survey Toward Central Regions", "abstract": "  We have carried out a survey toward the central regions of 85 starless cores in HCN J = 1-0 to study inward motions in the cores. Sixty-four cores were detected with HCN lines. The infall asymmetry in the HCN spectra is found to be more prevalent, and more prominent than in any other previously used infall tracers such as CS J = 2-1, DCO+ J = 2-1, and N2H+ J = 1-0. We found close relation between the intensities of the HCN and N2H+ lines. This implies that the HCN is not much depleted in the central regions of the cores. In some cores, the HCN spectra show different sign of asymmetry from other molecular lines. A few cores show various signs of asymmetry in individual HCN hyperfine lines. The distribution of the velocity shift dV of the HCN profiles with respect to the systemic velocity of the optically thin tracer is found to be more shifted toward bluer side than those of other infall tracers, indicating that the HCN traces inward motions more frequently. The dV distribution of each HCN hyperfine line for all sources is similar. Moreover the dV values obtained from different HCN hyperfine lines for each source are nearly similar. These may mean that most of starless cores are in similar kinematic states across the layers of the cores. We identify 17 infall candidates using all available indicators such as the velocity shift dV and the blue to red peak intensity ratio of double peaked profiles for HCN J = 1-0, CS J = 2-1, J = 3-2, DCO+ J = 2-1, and N2H+ J = 1-0. Four of them, L63, L492, L694-2, and L1197 are found to show higher blue to red ratio in the HCN hyperfine line along the lower opacity, suggesting that infall speed becomes higher toward the center. ", "machine_abstract": "We present the results of an unbiased survey toward starless cores using the HCN hyperfine transitions at 1.42 GHz and 2.12 GHz with the Nobeyama 45 m telescope to probe inward motions within these objects. We detected emission lines from all targets, except for L1521F (Taurus), which is known as one of the coldest prestellar cores. From our observations we found that the line widths are narrower than those observed by single-dish telescopes. This suggests that there may be unresolved substructure or turbulence on small scales inside the cores. For most sources, the peak intensities of both frequencies agree well with each other; however, for some cases they show significant differences between them. These discrepancies can be explained if the core has a temperature gradient along its radius and/or if it contains multiple velocity components. Our analysis shows that the kinetic temperatures derived from the two different frequency data sets range from 10 K to 30 K.", "paraphrased_abstract": "We show that the temperature of the two-dimensional data is 10 K. We have analyzed the kinetic temperatures of the two-dimensional data, and found that they are between 25 and 30 K. The peak intensities of the two sources are very similar, but for some they are quite different. There may be a temperature gradient varying along its radius, and a change in the velocity. We find that the lines on all targets are narrower than those seen with a single dish telescope, which suggests that there is some instability of the structure or turbulence in the core. The kinetic temperatures of the two observations are approximately 40 K. The kinetic temperature of the two observations is between 10 and 30 K. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.3874, "title": "Enhanced Kondo Effect in an Electron System Dynamically Coupled with Local Optical Phonon", "abstract": "  We discuss Kondo behavior of a conduction electron system coupled with local optical phonon by analyzing the Anderson-Holstein model with the use of a numerical renormalization group (NRG) method. There appear three typical regions due to the balance between Coulomb interaction $U_{\\rm ee}$ and phonon-mediated attraction $U_{\\rm ph}$. For $U_{\\rm ee}>U_{\\rm ph}$, we observe the standard Kondo effect concerning spin degree of freedom. Since the Coulomb interaction is effectively reduced as $U_{\\rm ee}-U_{\\rm ph}$, the Kondo temperature $T_{\\rm K}$ is increased when $U_{\\rm ph}$ is increased. On the other hand, for $U_{\\rm ee}<U_{\\rm ph}$, there occurs the Kondo effect concerning charge degree of freedom, since vacant and double occupied states play roles of pseudo-spins. Note that in this case, $T_{\\rm K}$ is decreased with the increase of $U_{\\rm ph}$. Namely, $T_{\\rm K}$ should be maximized for $U_{\\rm ee} \\approx U_{\\rm ph}$. Then, we analyze in detail the Kondo behavior at $U_{\\rm ee}=U_{\\rm ph}$, which is found to be explained by the polaron Anderson model with reduced hybridization of polaron and residual repulsive interaction among polarons. By comparing the NRG results of the polaron Anderson model with those of the original Anderson-Holstein model, we clarify the Kondo behavior in the competing region of $U_{\\rm ee} \\approx U_{\\rm ph}$. ", "machine_abstract": "We study the effect of local optical phonons on the electronic transport properties of a quantum dot system by using the nonequilibrium Green's function method combined with the density functional theory (DFT). We find that the electron-phonon interaction can induce a strong enhancement to the Kondo resonance peak and lead to a significant reduction of the Kondo temperature TK, which is determined as the energy scale at which the conductance reaches its maximum value Gmax. The results show that the Kondo temperature decreases rapidly when increasing the strength of the electron-phonon coupling constant \u03bb. In addition, we also investigate how the Kondo temperature depends on the size of the quantum dots for different values of \u03bb. Our findings may be useful for understanding the physical mechanism behind some recent experiments. Introduction:-The Kondo effect has been studied extensively both theoretically [1]-[3]and experimentally [4] - [6] . It occurs due to the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level [7, 8] , leading to a sharp zero-bias anomaly in the differential conductance [9] . Recently, it was found that this phenomenon could occur even without any magnetic impurities [10] - [12] . In fact, the Kondo effect has attracted much attention recently because of its potential applications in spintronics devices [13] - [16] . For example, the Kondo effect can be used to design novel spin transistors [17] or single-spin qubits [18] . However, there are still several open questions about the Kondo effect such as: How does the Kondo temperature depend on the size of the nanostructures? What happens if one introduces other degrees of freedom into the system? To answer these questions, various theoretical methods have been developed [19] - [22] . Among them, the nonequilibrium Green functions technique [23] - [25] provides us with powerful tools to calculate the current through the systems under consideration [26] - [28] . This approach allows us not only to obtain the steady-state current but also to explore the time evolution of the current after switching on/off external fields [29] - [31] . Moreover, combining the nonequilibrium Green", "paraphrased_abstract": "The effect of Kondo has been studied in depth, both theoretically and experimentally, and is now a topic of great interest, for example in the field of spintronics, in which it may be used to design spintronic devices of spintronics, in the form of single-crystal dielectrics... In the course of the study, we have gathered together various theoretical methods and methods, which allow us to calculate the voltage through the systems that are under consideration. Besides, this method also allows us to examine the time of the Kondo temperature with different levels of ... The Kondo temperature is reduced rapidly by increasing the strength of the electron-phonon coupling constant. The Kondo temperature is a very sensitive parameter of the temperature of the electron-phonon coupling constant. The results show that Kondo temperature is decreasing rapidly with increasing . Besides, the Kondo temperature is increasing with the  increase in . ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.073, "title": "Revisiting the Issues On Netflow Sample and Export Performance", "abstract": "  The high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. Sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. The sampling process in nearly all cases is a deterministic process of choosing 1 in every N packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. Even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. However, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. The main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. To date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. In this paper, we assess the performance of the sampling process as used in NetFlow in detail, and we discuss some techniques for the compensation of loss of monitoring detail. ", "machine_abstract": "In this work, we revisit some issues on netflow sample and export performance in terms of packet loss rate (PLR) and flow sampling accuracy (FSA). We first present an analytical model to study PLR under different traffic conditions with various sampling rates. Then, based on our analysis results, we propose a new method for estimating FSA by using only one parameter -the average number of flows per second. Finally, through extensive experiments conducted over both real-world traces and synthetic data sets, we show that our proposed estimation method is accurate enough to be used as a practical tool for evaluating network monitoring systems. The rest of this paper is organized as follows. In Section 2, we introduce related works about netflow sampling and exporting. In Section 3, we describe our experimental environment. In Section 4, we analyze the relationship between PLR and sampling rate. In Section 5, we estimate FSA according to the analysis result presented in Section 4. In Section 6, we evaluate the effectiveness of our proposed estimation method via extensive experiments. Finally, conclusions are drawn in Section 7.", "paraphrased_abstract": "\u201cIn the following sections, I have gathered together the most important related publications on netflow sampling and exporting, and in a general overview of the situation in which netflows and exports are conducted. In this paper I will describe the situation in which netflows and exports are carried out and discuss their performance in terms of packet loss and FSA. In the following sections I will discuss our experimental environment, in which I will introduce the PLR and the FSA, and in which I will explain how this new method is derived from a single parameter, the average number of flows per second. In this paper we will examine the accuracy of the method, by means of numerous experiments. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.1037, "title": "Controlled collisions of a single atom and ion guided by movable trapping potentials", "abstract": "  We consider a system composed of a trapped atom and a trapped ion. The ion charge induces in the atom an electric dipole moment, which attracts it with an r^{-4} dependence at large distances. In the regime considered here, the characteristic range of the atom-ion interaction is comparable or larger than the characteristic size of the trapping potential, which excludes the application of the contact pseudopotential. The short-range part of the interaction is described in the framework of quantum-defect theory, by introducing some short-range parameters, which can be related to the s-wave scattering length. When the separation between traps is changed we observe trap-induced shape resonances between molecular bound states and vibrational states of the external trapping potential. Our analysis is extended to quasi-one-dimensional geometries, when the scattering exhibit confinement-induced resonances, similar to the ones studied before for short-range interactions. For quasi-one-dimensional systems we investigate the effects of coupling between the center of mass and relative motion, which occurs for different trapping frequencies of atom and ion traps. Finally, we show how the two types of resonances can be employed for quantum state control and spectroscopy of atom-ion molecules. ", "machine_abstract": "We report on the controlled collision between an individual trapped atom and a singly charged ion in a Paul trap, where both particles are confined to different regions of space separated by several micrometers. The ions can be moved along the axis connecting them with the atoms using electrostatic fields generated by electrodes outside the vacuum chamber. We demonstrate that this allows us to control the relative velocity at which they collide as well as their impact parameter. This opens up new possibilities for studying fundamental processes such as elastic scattering or charge exchange reactions. In addition we show how it is possible to use these techniques to prepare entanglement between two neutral atoms via a quantum gate operation mediated by one common ion. Quantum information processing requires scalable systems based on many qubits [1] . One promising approach towards realizing such devices relies on neutral atoms stored in optical lattices [2] , but suffers from limited coherence times due to spontaneous emission [3] . An alternative route involves storing atomic qubits in ensembles of trapped ions [4] . However, here too there exist severe limitations arising from decoherence caused by heating [5] . In order to overcome these difficulties, hybrid approaches have been proposed [6] combining advantages of both schemes [7, 8] . Here, the storage of quantum states takes place in a small number of highly coherent ions while large numbers of neutral atoms serve as flying qubits [9] . A crucial requirement for implementing such schemes is the ability to perform high-fidelity operations involving both types of qubit [10] . For example, it has recently been shown experimentally [11] that it is possible to entangle two neutral atoms via a shared ion [12] . To achieve this goal, however, the atoms need to interact with each other before being released into free flight [13] .", "paraphrased_abstract": "In our study, we demonstrate that a refractory collision can be controlled between a trapped atom and a charged atom in a Paul trap, where both atoms are confined to different regions of space separated by several micrometers, where they are connected by electric fields. The ions, which are entangled in the trap, are arranged in a symmetrical manner, namely, based on a tunable axis. These atoms are entangled in a symmetrical manner, but they need to be coupled in order to become free. The atoms need to be linked in order to become free. In this case, however, there are some ill-defined limitations due to the heat, so a hybrid approach is necessary. For instance, it has recently been demonstrated experimentally that a mutual ion can be formed between two neutral atoms. However, in order to achieve this, the atoms must be interacting with each other in order to reach a free fusion. In the course of this study, we will show the controlled collision between an individual trapped atom and a single charged ion in a Paul trap, in which the two particles are separated by a few micrometers. The ions are moved to the axis of the trap, which is arranged in a vacuum chamber, and", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0868, "title": "Effect of electron-electron interaction on the phonon-mediated spin relaxation in quantum dots", "abstract": "  We estimate the spin relaxation rate due to spin-orbit coupling and acoustic phonon scattering in weakly-confined quantum dots with up to five interacting electrons. The Full Configuration Interaction approach is used to account for the inter-electron repulsion, and Rashba and Dresselhaus spin-orbit couplings are exactly diagonalized. We show that electron-electron interaction strongly affects spin-orbit admixture in the sample. Consequently, relaxation rates strongly depend on the number of carriers confined in the dot. We identify the mechanisms which may lead to improved spin stability in few electron (>2) quantum dots as compared to the usual one and two electron devices. Finally, we discuss recent experiments on triplet-singlet transitions in GaAs dots subject to external magnetic fields. Our simulations are in good agreement with the experimental findings, and support the interpretation of the observed spin relaxation as being due to spin-orbit coupling assisted by acoustic phonon emission. ", "machine_abstract": "We study the effect of electron-electron interactions (EEI) on the spin-flip rate due to phonons in semiconductor quantum dots. We show that EEI leads to an enhancement of this rate by several orders of magnitude, which is explained by the formation of collective excitations known as plasmons. The latter are responsible for the strong coupling between electrons and phonons leading to the observed increase in the spin-flip rate. Our results demonstrate that EEI can be used to control the spin dynamics in quantum dot systems.  PACS numbers: 73.20.Dd, 74.78.Na, 78.67.Hc  The ability to manipulate spins in semiconductors has attracted considerable attention over recent years because it could lead to novel applications such as spin-based transistors or logic devices [1] . In particular, semiconductor quantum dots have been proposed as promising candidates for these purposes [2] , since they allow one to confine charge carriers within small volumes with high precision [3] . In order to use quantum dots for spin manipulation, however, we need to understand how their spin states evolve under different conditions. This requires knowledge about the mechanisms governing spin relaxation processes [4] . One important mechanism is provided by phonons [5] : when confined charges move through the lattice potential of the crystal, they generate strain fields [6] . These strains induce local deformations of the lattice structure [7, 8] , resulting in the emission of phonons [9] . Since phonons carry angular momentum [10] , they may flip the spin state of the confined carrier [11] . However, the strength of this process depends strongly on the details of the confinement potential [12] . For example, if the confining potential is parabolic [13] , then the spin-flip rate induced by phonons scales linearly with temperature [14] . On the other hand, if the confining", "paraphrased_abstract": "There is, however, one problem that must be addressed in the light of the fact that, in the light of the fact, we must understand how the spin states are produced in various states, in different temperatures. For instance, if the bounding potential is parallel, then the flipping rate of phonons is linear, and the speed of the phonons is linear. In order to produce spins in the semiconductors, it is necessary to know the mechanisms underlying the spin relaxation of the material, which in turn depends on the conditions in which they are produced. The ability to manipulate spins in the semiconductors has attracted great interest during the last few years, since it is possible to confine charged particles in small volumes with high precision. The electron-electron interaction with phonons increases the rate of the spins in the quantum dots, and we have shown that phonons increase the rate of the spins in the quantum dots. In the quantum dots, the phonons are induced by a local deformation, and the phonons are released from the crystal, and the spin of the phonon is changed, and a phonon is released. We are studying the effect of EEI on the spin speed of phonons, in particular the phonons, and we show that EEI increases the rate of the spin speed by several orders of magnitude,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1341, "title": "A Study in Depth of f0(1370)", "abstract": "  Claims have been made that f0(1370) does not exist. The five primary sets of data requiring its existence are refitted. Major dispersive effects due to the opening of the 4pi threshold are included for the first time; the sigma -> 4pi amplitude plays a strong role. Crystal Barrel data on pbar-p -> 3pizero at rest require f0(1370) signals of at least 32 and 33 standard deviations in 1S0 and 3P1 annihilation respectively. Furthermore, they agree within 5 MeV for mass and width. Data on pbar-p -> eta-eta-pizero agree and require at least a 19 standard deviation contribution. This alone is sufficient to demonstrate the existence of f0(1370). BES II data for J/Psi -> phi-pi-pi contain a visible f0(1370) signal > 8 standard devations. In all cases, a resonant phase variation is required. The possibility of a second pole in the sigma amplitude due to the opening of the 4pi channel is excluded. Cern-Munich data for pi-pi elastic scattering are fitted well with the inclusion of some mixing between sigma, f0(1370) and f0(1500). The pi-pi widths for f2(1565), rho3(1690), rho3(1990) and f4(2040) are determined. ", "machine_abstract": "The mass spectrum and decay properties of the scalar mesons are studied by using the QCD sum rule method with the light-cone distribution amplitudes (LCDAs). The masses, pole residues and coupling constants for the scalar mesons below 2 GeV are calculated systematically. In particular, we study the f0(1370), which is usually considered as an exotic state. We find that it can be naturally explained as a mixture of two conventional states, i.e., the lowest lying scalar glueball and the scalar quarkonium. Our results show that its mixing angle \u03b8 = \u221220\u00b0 \u00b1 5\u00b0 , where the first error comes from the uncertainty of the LQCD data used to determine the parameters of LCDAs, while the second one arises from the uncertainties of the input parameters such as Borel parameter M2B and threshold s0B .  I. INTRODUCTIO N In recent years, there has been great interest in studying the low energy hadronic physics due to both theoretical and experimental reasons [1] - [4] . On the theory side, lattice quantum chromodynamics (LQCD) provides us with valuable information on the nonperturbative aspects of strong interactions [5] . However, at present most calculations have only focused on the ground-state hadrons [6] . On the other hand, the experimental observations of many new excited states beyond the naive quark model predictions [7] - [9] provide further motivation to explore their underlying structures [10] - [12] . For example, the newly observed scalars around 1.4-1.7 GeV [13] - [16] may contain important information about the nature of confinement [17] - [20] . It should also be noted that some of these newly discovered resonances cannot be easily accommodated into the traditional qq picture [21] - [23] . Therefore, it becomes necessary to investigate them more carefully [24] - [26] . In this work, we will use the QCD sum rules [27] - [29] to calculate the masses, pole residues and couplings of various scalar mesons below 2GeV systematically [30] . In particular, we focus our attention on the f 0 (1370), whose existence", "paraphrased_abstract": "It is true that the new states have not been found for a long time, but only in the subterranean phase, but in the crystalline, amorphous phase, amorphous, and amorphous; on the other hand, it is a fact that new resonances are found that are not well known in the conventional qq, which is very difficult to interpret, and that it must be examined carefully. I. The study of the symmetry and scalarity of scalar peaks is particularly useful in estimating the masses, the poles, the coupling of various scalar peaks below 2 GeV. Then we will calculate the mass, the poles and the coupling of various scalar peaks below 2 GeV. The mass spectrum and decay of scalar peaks and the radiance of the corresponding amplitudes are studied in detail, with the aid of the LCDA. In particular, we will consider f0, which is considered an exotic state, and which is naturally a mixture of two conventional states, i.e. the scalar gluon ball and the scalar quark. In addition, the observations of some new excited states, beyond the predicted quark models, have been of interest to us, we will not be able to exclude them from our", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1889, "title": "Super stellar clusters with a bimodal hydrodynamic solution: an Approximate Analytic Approach", "abstract": "  We look for a simple analytic model to distinguish between stellar clusters undergoing a bimodal hydrodynamic solution from those able to drive only a stationary wind. Clusters in the bimodal regime undergo strong radiative cooling within their densest inner regions, which results in the accumulation of the matter injected by supernovae and stellar winds and eventually in the formation of further stellar generations, while their outer regions sustain a stationary wind. The analytic formulae are derived from the basic hydrodynamic equations. Our main assumption, that the density at the star cluster surface scales almost linearly with that at the stagnation radius, is based on results from semi-analytic and full numerical calculations. The analytic formulation allows for the determination of the threshold mechanical luminosity that separates clusters evolving in either of the two solutions. It is possible to fix the stagnation radius by simple analytic expressions and thus to determine the fractions of the deposited matter that clusters evolving in the bimodal regime blow out as a wind or recycle into further stellar generations. ", "machine_abstract": "We present the results of our study on super star clusters (SSCs) in which we have found that SSCs can be divided into two categories, namely, those having a single mode and those having a double-mode solution for their density profiles. We show how these solutions are related to each other by using approximate analytic methods. The main aim is to understand why some SSCs appear as point sources while others do not. In this work, we also discuss the possibility of formation of such objects through mergers between smaller clusters or stars. Super massive star clusters (SMCs), known as young globular clusters (YGCs), open clusters (OCs), compact elliptical galaxies (CEGs), etc., are observed in many galactic systems ranging from dwarf irregular galaxies to giant ellipticals. These objects are believed to form during violent events like galaxy mergers, tidal interactions, and/or gas-rich major mergers. However, it has been shown recently that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses (10^6-10^7 Msun). This type of cluster is referred to as \"Intermediate Massive Clusters\"(IMCs; Portegies Zwart et al. (2010)). It appears that IMCs may represent a transition phase between open clusters and YGCs.", "paraphrased_abstract": "I will now describe the results of my research on superstars, the formation of super-giants (SMCs) into two categories: one with one mode, and the other with two modes, whose density can be calculated by a metric method. Besides, the SMCs are referred to as \"intermediate megacities\"; the word 'intermediate' is used by Portegies Zwart and the author. In our study, the SMCs were divided into two categories, namely, those having a single mode and those having a double mode, based on the metric method. The results were discussed in detail, and they referred to some specialized supercities of supercities: those of the young globular YGCs, the open OCs, the compact elliptical galaxies, the elliptical galaxies, and so on. We hypothesize that some of these objects appear as points, and others are not. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0334, "title": "A Multiphilic Descriptor for Chemical Reactivity and Selectivity", "abstract": "  In line with the local philicity concept proposed by Chattaraj et al. (Chattaraj, P. K.; Maiti, B.; Sarkar, U. J. Phys. Chem. A. 2003, 107, 4973) and a dual descriptor derived by Toro-Labbe and coworkers (Morell, C.; Grand, A.; Toro-Labbe, A. J. Phys. Chem. A. 2005, 109, 205), we propose a multiphilic descriptor. It is defined as the difference between nucleophilic (Wk+) and electrophilic (Wk-) condensed philicity functions. This descriptor is capable of simultaneously explaining the nucleophilicity and electrophilicity of the given atomic sites in the molecule. Variation of these quantities along the path of a soft reaction is also analyzed. Predictive ability of this descriptor has been successfully tested on the selected systems and reactions. Corresponding force profiles are also analyzed in some representative cases. Also, to study the intra- and intermolecular reactivities another related descriptor namely, the nucleophilicity excess (DelW-+) for a nucleophile, over the electrophilicity in it has been defined and tested on all-metal aromatic compounds. ", "machine_abstract": "The concept of molecular descriptors is central to the development of quantitative structure-activity relationships (QSARs). In this work, we introduce a new descriptor that can be used in QSAR studies as well as other applications where chemical reactivity or selectivity are important factors.  The proposed descriptor is based on an extension of the concept of multipolarity developed by Mulliken. It has been shown previously that the multipole moments calculated using atomic orbital basis sets provide useful information about molecular properties such as polarizability, electronegativity, hardness, softness, etc.. We show here how these quantities may also be related to the electronic energy differences between different states of charge within a molecule. This relationship allows us to define a quantity called the \"multiphase index\" which provides a measure of the relative stability of molecules with respect to changes in their oxidation state. The multiphase index was applied successfully to several test cases including the prediction of the oxidation potentials of some organic compounds.", "paraphrased_abstract": "The concept of the atomic number of an atom gives a useful insight into the nature of the properties of atoms, polarization, electronegativity, hardness, softness, etc. This metric explains the equilibrium of molecules with respect to changes in their oxidation state. We develop a new descriptor for QSARs and other scientific applications where the chemistry or selectivity of the molecules are to be taken into account. This descriptor has been developed from the theory of Mulliken, who showed in his previous work that the moment of the multipoles is useful in measuring molecular properties, polarity, electronegativity, hardness, softness, etc., but we show that this dimension may also be correlated with the electronic energy differences between different states of charge. The multipole indices were shown to be derived from atomic orbits and have shown in a number of experiments that the electronic energy of the atoms was a function of the electronic energy of the molecules. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1726, "title": "A Census of Baryons in Galaxy Clusters and Groups", "abstract": "  We determine the contribution of stars in galaxies, intracluster stars, and the intracluster medium to the total baryon budget in nearby galaxy clusters and groups. We find that the baryon mass fraction (f_b) within r500 is constant for systems with M500 between 6e13 and 1e15 Msun. Although f_b is lower than the WMAP value, the shortfall is on the order of both the observational systematic uncertainties and the depletion of baryons within r500 that is predicted by simulations. The data therefore provide no compelling evidence for undetected baryonic components, particularly any that vary in importance with cluster mass. A unique feature of the current analysis is direct inclusion of the contribution of intracluster light (ICL) in the baryon budget. The increase in X-ray gas mass fraction with increasing total mass is entirely accounted for by a decrease in the total stellar mass fraction, supporting the argument that the behavior of both the stellar and X-ray gas components is dominated by a decrease in star formation efficiency in more massive environments. Within just the stellar component, the fraction of the total stellar luminosity in the BCG and ICL decreases as velocity dispersion increases, suggesting that the BCG+ICL component, and in particular the dominant ICL component, grows less efficiently in higher mass environments. The degree to which this behavior arises from our sample selection, which favored systems with central, giant elliptical galaxies, remains unclear. A more robust result is the identification of low mass groups with large BCG+ICL components, demonstrating that the creation of intracluster stars does not require a massive cluster environment. Within r500 and r200, the BCG+ICL contributes on average 40% and 33% of the total stellar light, respectively. ", "machine_abstract": "We present the results of an all-sky survey for galaxy clusters using data obtained with the Sunyaev-Zel'dovich effect (SZE) by the Planck satellite, complemented at low redshifts by X-ray observations made with XMM-Newton and Chandra satellites. We use this sample to study the evolution of baryon content in massive halos over cosmic time. The total mass is estimated through gravitational lensing measurements performed on Hubble Space Telescope images. Our main findings are as follows:  1. We find that the fraction of gas mass decreases strongly towards higher redshift.  2. At z < 0.5 we measure fgas = Mgas/Mtot = 0.11 \u00b1 0.01(stat.) \u00b10.02(sys.), where Mtot is the total gravitating mass within R500c, which corresponds to about half the virial radius. This value agrees well with previous estimates based on X-ray observations alone.  3. For our full cluster sample spanning the range 0.1 < z < 1.3, we obtain fgas = 0.09 \u00b1 0.01(\u00b1stat.).", "paraphrased_abstract": "We compute the fraction of gas with the Hubble Space Telescope, and we estimate it by means of gravitational lensing, on the Hubble Space Telescope, to be 0.027 (stat.). This is the result of a study of the evolution of the baryon fraction over the cosmic time. This result is consistent with previous measurements by X-rays alone. We show that the fraction of gas increases with the increase of the z-axis, and at z-axis 0.11  0.01  0.02  0.02  0.02. We calculate the enumeration rate with the Hubble Space Telescope.  0.012  0.5 is calculated: M-gas/Mtot = 0.11  0.01 (stat.) 0.02 (sys). This figure is compatible with our previous estimates, and is very close to our previous estimates. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0776, "title": "Nanomechanics of single keratin fibres: A Raman study of the alpha helix -> beta sheet transition and water effect", "abstract": "  The use of micro-Raman spectroscopy, through chemical bond nano-scale probes, allows the changes in conformations (alpha helix -> beta sheet), chain orientation, disconnection of disulfide bonds (-20%) and the increase of intra and inter-chain distances during the strain to be distinguished. The combination of micro-Raman spectroscopy and a allows a quantitative measure of the extension of chemical bonds in the peptidic chain during loading. The nano-structural transformations of keratin during the strain of human hair in a dry environment (40-60 % relative humidity) and saturated with water have been studied. The water permits the sliding of the chains and decreases the bond energy hair. Spectral analyses and 2D correlation are two coherent and independent methods to follow change the Raman probes which are sensitive to structural . The between nano-mechanical (Raman) and micro-mechanical (strain/stress) analyses confirms the validity of the experimental results, tools and principles used, as well as the agreement with the structural model of keratin fibres described by Chapman & Hearle. ", "machine_abstract": "The mechanical properties of hair are determined by its molecular structure, which is mainly composed of proteins called keratins. The main structural unit in keratins is an alpha-helical coiled coil that forms microfibrils with other similar units to form intermediate filaments (IFs). These IFs then aggregate into bundles forming macrofibrils, which are further bundled together to form the basic building blocks of hair -the cortical cells-. In this work we have studied how the mechanical properties change as these structures evolve during the process known as maturation or ageing. We used confocal Raman spectroscopy to measure the changes in the secondary structure of individual keratin fibres extracted from human scalp hairs at different stages of their growth cycle. Our results show that the elastic modulus increases significantly when the fibre goes through the transformation from the alpha helix conformation to the beta-sheet one. This increase can be explained by the formation of hydrogen bonds between neighbouring strands of the protein backbone. Moreover, our measurements reveal that the presence of water molecules has a significant influence on the mechanical behaviour of the fibres.", "paraphrased_abstract": "Moreover, our analysis showed that water is a major influence on the mechanical properties of the fibres. We have been investigating the mechanical properties of hair at various stages of its development, called the maturation or the ageing process. We have studied the secondary structure of individual keratin fibers extracted from human scalps during different stages of the growth process. The main structure of keratin is the alpha-helix, a chain of amino acids which forms microfibrils, with other filaments, forming macrofibrils, forming the basic building blocks of the hair. The modulus increases significantly with the resumption of alpha-helix to the beta-helix, because the hydrogen bonds are formed between the neighbouring strands of the protein. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.1787, "title": "Energy-Efficient Resource Allocation in Wireless Networks: An Overview of Game-Theoretic Approaches", "abstract": "  An overview of game-theoretic approaches to energy-efficient resource allocation in wireless networks is presented. Focusing on multiple-access networks, it is demonstrated that game theory can be used as an effective tool to study resource allocation in wireless networks with quality-of-service (QoS) constraints. A family of non-cooperative (distributed) games is presented in which each user seeks to choose a strategy that maximizes its own utility while satisfying its QoS requirements. The utility function considered here measures the number of reliable bits that are transmitted per joule of energy consumed and, hence, is particulary suitable for energy-constrained networks. The actions available to each user in trying to maximize its own utility are at least the choice of the transmit power and, depending on the situation, the user may also be able to choose its transmission rate, modulation, packet size, multiuser receiver, multi-antenna processing algorithm, or carrier allocation strategy. The best-response strategy and Nash equilibrium for each game is presented. Using this game-theoretic framework, the effects of power control, rate control, modulation, temporal and spatial signal processing, carrier allocation strategy and delay QoS constraints on energy efficiency and network capacity are quantified. ", "machine_abstract": "In this article, we provide an overview on the recent advances and challenges in energy-efficient resource allocation for wireless networks using game-theoretical approaches. We first introduce some basic concepts related to games and then present several representative works that have been done recently by researchers worldwide. Finally, we conclude with open research issues and future directions. In this article, we provide a survey on the recent advances and remaining challenges in energy-efficient radio resource management (RRM) for wireless networks using game theoretical approaches. The main goal is to show how these techniques can be used to improve network performance while reducing power consumption at each node. To achieve this objective, we first give a brief introduction about the concept of games as well as their applications in communication systems. Then, we review existing work on RRM problems formulated as non-cooperative or cooperative games. Finally, we discuss open research issues and possible future directions. This article is organized into four sections. Section II introduces some background information on games and their application in communications. Section III presents different types of games applied to solve RRM problems. Section IV discusses open research issues and concludes the article.", "paraphrased_abstract": "In this paper, we will discuss the current and upcoming advances and problems concerning the use of energy-efficient resources in wireless networks, using games. The first section introduces the concept of games, and then presents a few representative studies which have been published so far. Then we shall conclude with a few open-source problems and open-source directions. We will introduce the basic concepts of games and their application in wireless communications, and then describe some of the recent studies that have been published in this field. During the course of the chapter, we will introduce some preliminary work on energy-efficient networks, focusing on the most recent advances and challenges. In order to do this, we shall first introduce the concept of games, and then present several exemplary studies on games in reducing power consumption. In this paper, we shall discuss the advances and challenges in the area of wireless resource management, and then discuss the current developments and issues concerning the use of these techniques in wireless networks. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2454, "title": "Modeling transcription factor binding events to DNA using a random walker/jumper representation on a 1D/2D lattice with different affinity sites", "abstract": "  Surviving in a diverse environment requires corresponding organism responses. At the cellular level, such adjustment relies on the transcription factors (TFs) which must rapidly find their target sequences amidst a vast amount of non-relevant sequences on DNA molecules. Whether these transcription factors locate their target sites through a 1D or 3D pathway is still a matter of speculation. It has been suggested that the optimum search time is when the protein equally shares its search time between 1D and 3D diffusions. In this paper, we study the above problem using a Monte Carlo simulation by considering a very simple physical model. A 1D strip, representing a DNA, with a number of low affinity sites, corresponding to non-target sites, and high affinity sites, corresponding to target sites, is considered and later extended to a 2D strip. We study the 1D and 3D exploration pathways, and combinations of the two modes by considering three different types of molecules: a walker that randomly walks along the strip with no dissociation; a jumper that represents dissociation and then re-association of a TF with the strip at later time at a distant site; and a hopper that is similar to the jumper but it dissociates and then re-associates at a faster rate than the jumper. We analyze the final probability distribution of molecules for each case and find that TFs can locate their targets fast enough even if they spend 15% of their search time diffusing freely in the solution. This indeed agrees with recent experimental results obtained by Elf et al. 2007 and is in contrast with theoretical expectation. ", "machine_abstract": "Transcription factors (TFs) are proteins that bind specific sequences in the genome and regulate gene expression by recruiting other regulatory molecules or directly modulating RNA polymerase activity. The identification of TF binding sites is an important step towards understanding how genes are regulated, but it remains challenging due to their short length and degenerate sequence patterns. In this work we present a novel computational method for predicting TF binding sites based on a random walk model on a one-dimensional (1D) or two-dimensional (2D) lattice representing the DNA molecule. We show that our approach outperforms state-of-the-art methods when applied to several datasets containing experimentally validated binding sites. Our results suggest that the proposed algorithm can be used as part of a pipeline for identifying putative binding sites in large genomic regions. Transcription factors (TFs), which include zinc finger proteins, homeodomain proteins, basic helix-loop-helix proteins, and others [1] , play key roles in regulating gene expression [2] . They bind specific sequences in the promoter region of target genes [3] and recruit other regulatory molecules such as coactivators [4] or corepressors [5] , or they may directly affect the rate at which RNA polymerases synthesize messenger RNAs [6] . The identification of TF binding sites has been shown to be useful for studying gene regulation [7, 8] . However, it remains difficult because these sites have very short lengths [9] and exhibit highly degenerate sequence patterns [10] . Several algorithms have been developed to predict TF binding sites [11] ; however, most existing approaches suffer from high false positive rates [12] . For example, the widely-used position weight matrix (PWM)-based motif finding algorithms [13] cannot accurately identify TF binding sites [14] . This problem arises mainly because PWM models assume independence between positions within motifs [15] , while real TF binding sites often contain dependencies among adjacent bases [16] . To address this issue, some researchers have attempted to incorporate higher-order interactions into PWMs [17] . Other studies have focused on developing probabilistic graphical models [18] , hidden Markov models [19] , support vector machines [20] , and neural networks [21] .", "paraphrased_abstract": "Several models have been developed for the analysis of TF binding sites. Some have been based on the PWM model, but they cannot be analyzed accurately. They are derived from the PWM models, but they cannot be compared with the real binding sites. Thus, it is necessary to improve the accuracy of PWM models, i.e., by introducing higher-order interactions. In the past, it was decided to use a probabilistic approach, hidden Markov models, support vector machines, and neural networks. The TFs, which are zinc finger proteins, homeodomain proteins, helix-loop proteins, and others, are essential to the regulation of gene expression. They are able to bind specific sequences in the promoter region of a gene, recruit ligands, or regulate the production of messenger RNAs directly. Several methods have been devised to identify TF binding sites, but their accuracy is not yet certain, as they are too short and have a low sequence pattern. It is also difficult to find binding sites with short lengths and poorly sequenced motifs. Our method has been successfully applied to several datasets containing ligand binding sites and experimentally validated. This is an important step in the study of gene regulation, but it is very difficult because of its short length and the short sequence pattern. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.274, "title": "Study of Direct Photon plus Jet production in CMS Experiment at \\sqrt{s}=14 TeV", "abstract": "  We present simulation results of $\\gamma$ + Jet analysis using CMS (Compact Muon Solenoid) Object-Oriented software at the Large Hadron Collider (LHC) center of mass energy $\\sqrt{s}$=14 TeV. The study of direct photon production helps in validating the perturbative Quantum Chromodynamics (pQCD) and providing information on the gluon distribution in the nucleons. Direct photon processes also constitute a major background to several other Standard Model (SM) processes and signals of new physics. Thus these processes need to be understood precisely in the new energy regime. In this work, we have done a detailed study of the GEANT4 simulated $\\gamma$ + jet events generated with Pythia, and the related background processes. Isolation cuts have been optimized for direct photon which improves the signal over background ratio by $\\sim25%$ as compared to previous studies done in CMS. The inclusion of a large $\\Delta\\phi$ cut between the photon and the leading jet at $40^0$ in the analysis leads to a further increase of $\\sim15%$ in S/B, thus giving an overall gain of $\\sim42%$ in S/B ratio. ", "machine_abstract": "The measurement is performed using the data collected by the Compact Muon Solenoid (CMS) experiment during Run 2, corresponding to an integrated luminosity of 35.9 fb$^{-1}$ . The analysis uses events with one photon and two jets where both photons are reconstructed within the barrel calorimeter acceptance, |\u03b7| < 1.44. Events are selected requiring that each jet has transverse momentum pT > 30 GeV and pseudorapidity |\u03b7| < 4.7. A multivariate discriminant based on boosted decision trees is used for signal selection. The measured cross section as a function of the leading jet pT is compared to theoretical predictions obtained at next-to-leading order accuracy including resummation of soft gluons up to next-to-next-to-leading-logarithmic accuracy. Good agreement between theory and measurements is observed over most of the kinematic range probed.     Keywords: direct photon, dijet", "paraphrased_abstract": "The measurement is performed by the CMOS instrument, which had an integrated luminosity of 35.9 fb.   , the measured cross-section is compared with the theoretical predictions of the first order, the return of soft gluons up to the first order of logarithmic accuracy, and the good agreement of the theories and the measurements is observed. The data is based on a tetrad of one photon and two jets, where the two photons are taken as a pair, and are used in the barrel calorimeter acceptance,      ., and a multivariate decision tree is used to choose the signals. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.1564, "title": "The First Survey of X-ray Flares from Gamma Ray Bursts Observed by Swift: Spectral Properties and Energetics", "abstract": "  Observations of gamma ray bursts (GRBs) with Swift produced the initially surprising result that many bursts have large X-ray flares superimposed on the underlying afterglow. The flares were sometimes intense, had rapid rise and decay phases, and occurred late relative to the ``prompt'' phase. Some remarkable flares are observed with fluence comparable to the prompt GRB fluence. Many GRBs have several flares, which are sometimes overlapping. Short, intense, repetitive, and late flaring can be most easily understood within the context of the standard fireball model with the internal engine that powers the prompt GRB emission in an active state at late times. However, other models for flares have been proposed. Flare origin can be investigated by comparing the flare spectra to that of the afterglow and the initial prompt emission. In this work, we have analyzed all significant X-ray flares from the first 110 GRBs observed by Swift. From this sample 33 GRBs were found to have significant X-ray flares, with 77 flares that were detected above the 3$\\sigma$ level. In addition to temporal analysis presented in a companion paper, a variety of spectral models have been fit to each flare. In some cases, we find that the spectral fits favor a Band function model, which is more akin to the prompt emission than to that of an afterglow. We find that the average fluence of the flares is 2.4e-7 erg/cm^2/s in the 0.2-10 keV energy band, which is approximately a factor of ten below the average prompt GRB fluence. These results, when combined with those presented in the companion paper on temporal properties of flares, supports the hypothesis that most X-ray flares are late-time activity of the internal engine that spawned the initial GRB; not an afterglow related effect. ", "machine_abstract": "We present the first systematic survey of X-ray flares observed in gamma-ray burst (GRB) afterglows with the Swift satellite, which has been operating since 2004. We find that most GRBs show at least one X-ray flare during their prompt or early afterglow phase; we also find that some GRBs have multiple flaring episodes. The majority of these flares are soft, but there is evidence for both hard and very-hard flares as well. In addition to studying individual flares, we examine the spectral properties of all flares combined using time-resolved spectroscopy techniques. Our results indicate that the average spectrum can be described by an absorbed power law model with photon index \u0393 = 1.6 \u00b1 0.1 and hydrogen column density NH = 2 \u00d7 10^22 cm^{-2}.     Using this best-fit model, we calculate the total energy emitted in each flare over its duration. For the entire sample of flares studied here, we find that the mean fluence of the flares is ~10^-7 ergs/cm^2, while the mean peak luminosity is ~10^45 ergs/s. These values correspond to typical energies released in solar flares.", "paraphrased_abstract": "Among these, the energy of a flare is 107 ergs/cm2, the energy of the peak at a tenth of its normal range. We present a systematic study of the X-ray spectroscopy obtained with the Swift satellite, which has been operating since 2004. We report here the results of the spectroscopic investigation of a variety of flares, grouped into one category, with the radiance of a photon of 1.6  0.1, with a hydrogen column density of a single 1022 cm2; utilizing this spectroscopic model, we calculate the total radiance of each flare in time. We conclude that, for the whole class of flares studied here, the mean radiance of the flares is 107 ergs/cm2; the maximum radiance is 1045 ergs/s; and these values are in accordance with typical values in solar flares. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.19, "title": "Astrometry with \"Carte du Ciel\" plates, San Fernando zone. I. Digitization and measurement using a flatbed scanner", "abstract": "  We present an original method of digitizing and astrometrically reducing \"Carte du Ciel\" plate material using an inexpensive flatbed scanner, to demonstrate that for this material there is an alternative to more specialized measuring machines that are very few in number and thus not readily available. The sample of plates chosen to develop this method are original \"Carte du Ciel\" plates of the San Fernando zone, photographic material with a mean epoch 1903.6, and a limiting photographic magnitude ~14.5, covering the declination range of -10 < dec < -2. Digitization has been made using a commercial flatbed scanner, demonstrating the internal precision that can be attained with such a device. A variety of post-scan corrections are shown to be necessary. In particular, the large distortion introduced by the non-uniform action of the scanner is modelled using multiple scans of each plate. We also tackle the specific problems associated with the triple-exposure images on some plates and the grid lines present on all. The final measures are reduced to celestial coordinates using the Tycho-2 Catalogue. The internal precision obtained over a single plate, 3microns ~ 0.18\" in each axis, is comparable to what is realized with similar plate material using slower, less affordable, and less widely available conventional measuring machines, such as a PDS microdensitometer. The accuracy attained over large multi-plate areas, employing an overlapping plate technique, is estimated at 0.2\". ", "machine_abstract": "We present the results obtained in digitizing and measuring astrometrically about 1000 photographic plates taken by the Carte du Ciel project at the Observatorio Astron\u00f3mico Nacional (OAN), San Pedro M\u00e1rtir Observatory (SPMO) between 1950 and 1960. The OAN/SPMO is located on Mount Wilson, California, USA. We used a high resolution digital camera to scan these plates and then measured their positions relative to reference stars observed simultaneously on each plate. This allowed us to obtain accurate absolute proper motions for more than 100 000 stars down to V = 16 mag over an area of about 1 square degree centered around RA = 10 h 30 m , Dec = +60 deg . These data are available online through our web site http://obswww.unige.ch/~fuchs/CDCCatastro/Movies/movie1.html . The accuracy achieved in this work is comparable or better than that reported previously for similar projects based on scanning film material.    This research was supported by NSF grant AST-0307481.", "paraphrased_abstract": "\u201cThis work is compatible with other studies in detecting photographic materials, and it is comparable or better to that of those in similar studies of measuring film. This research is supported by a grant from the National Science Foundation (NSF) for the grant AST-030748. We have scanned and compared with a digital camera over the thousands of photographic plates taken by the Carte du Ciel project at the Observatory of the San Pedro Mars Observatory, at the SPMO on Mount Wilson. This work was carried out by an astronomy research project of the year 1950, at the Observatory of the San Pedro Mars Observatory in the state of California. We have measured, in detail, over the whole number of the objects and stars of the Carte du Ciel project, whose field of view is approximately one square degree centered on RA = 10 h 30 m, Dec = 60 deg. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 706.0832, "title": "INTEGRAL high energy monitoring of the X-ray burster KS 1741-293", "abstract": "  KS 1741-293, discovered in 1989 by the X-ray camera TTM in the Kvant module of the Mir space station and identified as an X-ray burster, has not been detected in the hard X band until the advent of the INTEGRAL observatory. Moreover this source has been recently object of scientific discussion, being also associated to a nearby extended radio source that in principle could be the supernova remnant produced by the accretion induced collapse in the binary system. Our long term monitoring with INTEGRAL, covering the period from February 2003 to May 2005, confirms that KS 1741-293 is transient in soft and hard X band. When the source is active, from a simultaneous JEM-X and IBIS data analysis, we provide a wide band spectrum from 5 to 100 keV, that can be fit by a two component model, a multiple blackbody for the soft emission and a Comptonized or a cut-off power law model for the hard component. Finally, by the detection of two X-ray bursters with JEM-X, we confirm the bursting nature of KS 1741-293, including this source in the class of the hard tailed X-ray bursters. ", "machine_abstract": "We present results on INTEGRAL observations of the X-ray burster (XRB) KS 1741-293, which is located in the globular cluster NGC 6388 at a distance of about 12 kpc. The source was discovered by BeppoSAX and has been observed several times since then with different satellites. We report here on data obtained during an observation performed between February 24th and March 1st 2004 using the IBIS/ISGRI detector on-board INTEGRAL. During this time we detected two bursts from the source. In addition to these bursts, we found that there are significant variations in the hard Xray flux over short periods of time. These short-term variabilities can be explained as being due to rapid changes in the accretion rate onto the compact object or possibly due to precession of the neutron star's magnetic field axis. This work shows how important it is for future missions like XEUS and Constellation-X to have good timing capabilities in order to study such sources.", "paraphrased_abstract": "The X-ray source, a black hole (inner astro) numbered 1741-293, was found by BeppoSAX and has been observed many times with different satellites. We report here the results of an observation made between the 24th and the 1st of March 2004 by the IBIS-ISGRI instrument aboard the X-ray observatory, and during this observation two bursts of X-ray light were observed. Besides the bursts, we found that for short periods there were considerable variations in the hard X-ray fluxes. This difference could be explained by the rapid movement of the compact object, or by the axis of the magnetic field of the neutron star. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0995, "title": "Decoherence of a driven multilevel quantum system interacting with a multi-bath reservoir", "abstract": "  A general theory is presented for the treatment of decoherence of a multilevel quantum system (with many degrees of freedom) interacting with multi-bath reservoir and driven by ac fields. In this approach, the system is described by a reduced density operator and the multi-bath reservoir is characterized by a number of spectral densities. The reduced density operator is governed by the master equation in which the effect of ac driving fields and the leakage to non-computational states are included. The theory is applied to the study of decoherence of a two-dimensional (2D) SQUID flux qubit coupled to control and readout circuits. The predicted results are in very good agreement with available experimental results in the absence of driving fields and with the analytic results of a dissipative two-level system in the presence of weak driving fields. The relaxation and decoherence times versus the parameters and temperature of the control and readout circuits are also explored in details to facilitate the optimization of the 2D SQUID qubit. ", "machine_abstract": "We study the decoherence dynamics of an open quantum system consisting of two-level atoms coupled to a single-mode cavity field in presence of dissipation and driving fields. The atom-cavity coupling is treated within the framework of Jaynes-Cummings model, while the dissipative effects are described by using the master equation for the reduced density matrix of the system. We show that the steady state solution of this problem can be obtained analytically when the decay rates of all atomic levels are equal. In particular we find that the steady-state entanglement between the atom-field subsystem depends on both the strength of the external driving field as well as the number of excited states involved in the process. Finally, we discuss how our results could be used to explain recent experimental observations concerning the generation of nonclassical light via spontaneous emission processes. PACS numbers: 03.67.Mn, 42.50.Vk", "paraphrased_abstract": "I.D.; 11.19.Mn; 11.19.Vk. Then I.D., a , is used for a , the  ; a , is used for the ; a , a   ; a    ; a   , is used for the , a, a, a , a, a, a, and a, a  and a ; the, a, and, and the., a, a, a, a, a, a , a, a, a, a, a, a , a, a, a, a, a, a, a, a, and a., a, a, a,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0893, "title": "Electrostatic Interactions of Asymmetrically Charged Membranes", "abstract": "  We predict the nature (attractive or repulsive) and range (exponentially screened or long-range power law) of the electrostatic interactions of oppositely charged and planar plates as a function of the salt concentration and surface charge densities (whose absolute magnitudes are not necessarily equal). An analytical expression for the crossover between attractive and repulsive pressure is obtained as a function of the salt concentration. This condition reduces to the high-salt limit of Parsegian and Gingell where the interaction is exponentially screened and to the zero salt limit of Lau and Pincus in which the important length scales are the inter-plate separation and the Gouy-Chapman length. In the regime of low salt and high surface charges we predict - for any ratio of the charges on the surfaces - that the attractive pressure is long-ranged as a function of the spacing. The attractive pressure is related to the decrease in counter-ion concentration as the inter-plate distance is decreased. Our theory predicts several scaling regimes with different scaling expressions for the pressure as function of salinity and surface charge densities. The pressure predictions can be related to surface force experiments of oppositely charged surfaces that are prepared by coating one of the mica surfaces with an oppositely charged polyelectrolyte. ", "machine_abstract": "The electrostatic interactions between two asymmetrically charged membranes are studied by using the mean-field theory and Monte Carlo simulations. The results show that there is an attractive interaction between these membranes, which can be explained as follows. When one membrane approaches another with opposite charges on their surfaces, it will induce a dipole moment in its neighbor due to charge redistribution at the interface. This induced dipole moment causes an additional attraction between them. In addition, we find that this effect becomes more pronounced when the dielectric constant of water decreases. Finally, our study shows that the magnitude of the electrostatic force depends strongly on the surface charge density difference between the two membranes. We also discuss how the electrostatic forces affect the phase behavior of lipid bilayers. DOI: 10.1063/1.3189000 I. INTRODUCTIO N In recent years, many studies have been carried out on the properties of biomembranes [1] . It has been found that the physical characteristics of biological systems such as cell adhesion [2] , vesicle fusion [3] , protein folding [4] , etc., depend crucially on the structure and composition of the underlying lipid bilayer [5] . Biological membranes consist mainly of phospholipids [6] . These lipids contain hydrophobic tails and hydrophilic heads [7, 8] . Due to the amphiphilicity of phospholipids, they tend to self-assemble into bilayers [9] . A typical example for such a system is shown schematically in Fig.  1(a) . Each layer consists of a monolayer of phospholipids arranged in a fluid-like state [10] . The thickness of each layer is about 5 nm [11] . The head groups point towards the aqueous solution while the tail groups face away from it [12] . Because of the presence of water molecules inside the layers, the effective dielectric constant of the medium is high (about 80) [13] . However, outside the layers, where only air exists, the dielectric constant is low (about 1). Therefore, the electric field lines penetrate easily through the interior region but not so much through the exterior region [14] .", "paraphrased_abstract": "A typical example of such a system is shown in Figure 1 (A). It consists of a monolayer of phospholipids, which are arranged in a fluid state. The two layers are of a fluid form, and their thickness is about five nm. The head groups point at the aqueous solution, the tail groups point at the air. This is because the water is there, the effective dielectric constant is high (over 80), while outside the layers, where there is no air, the dielectric constant is very low. We have studied the behavior of phospholipid bilayers, in which the phospholipids are arranged in a fluid-like state. The properties of phospholipids, in particular, depend upon the structure and composition of the lipid bilayer. In fact, it has been found that the properties of cells, vesicle fusion, protein folding, etc., depend on the structure of the lipid bilayer. This is why the bilayers of two asymmetrically charged phospholipids are so attractive, that they can be explained in turn by the following: if one membrane approaches another with the opposite charges on its surface, it causes a dipole moment in its neighbour, because the dipole moment increases the attraction between the two membranes, and if the water dielectric is lower, the dielectric constant will increase,", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.0355, "title": "Trigonometric parallaxes of high velocity halo white dwarf candidates", "abstract": "  The status of 38 halo white dwarf candidates identified by Oppenheimer et al. (2001) has been intensively discussed by various authors. In analyses undertaken to date, trigonometric parallaxes are crucial missing data. Distance measurements are mandatory to kinematically segregate halo object from disk objects and hence enable a more reliable estimate of the local density of halo dark matter residing in such objects.   We present trigonometric parallax measurements for 15 candidate halo white dwarfs (WDs) selected from the Oppenheimer et al. (2001) list. We observed the stars using the ESO 1.56-m Danish Telescope and ESO 2.2-m telescope from August 2001 to July 2004. Parallaxes with accuracies of 1--2 mas were determined yielding relative errors on distances of $\\sim5$% for 6 objects, $\\sim12$% for 3 objects, and $\\sim20$% for two more objects. Four stars appear to be too distant (probably farther than 100 pc) to have measurable parallaxes in our observations. Distances, absolute magnitudes and revised space velocities were derived for the 15 halo WDs from the Oppenheimer et al. (2001) list. Halo membership is confirmed unambiguously for 6 objects while 5 objects may be thick disk members and 4 objects are too distant to draw any conclusion based solely on kinematics. Comparing our trigonometric parallaxes with photometric parallaxes used in previous work reveals an overestimation of distance as derived from photometric techniques. This new data set can be used to revise the halo white dwarf space density, and that analysis will be presented in a subsequent publication. ", "machine_abstract": "We report trigonometric parallax measurements for four high proper motion (HPM) white dwarfs with tangential velocities greater than 400 km/sec, and distances between 50 pc to 100 pc. The HPMs are all members of the Galactic halo population based on their kinematics. We find that three out of these four stars have masses less than 0.5 Msun which is consistent with theoretical predictions for white dwarfs in this mass range. However one star has an unexpectedly large mass of 1.1 Msun. This may be due to unresolved binarity or it could represent a new class of massive white dwarfs. These results will help us understand how the Galactic halo formed.     Keywords: High Proper Motion Stars, White Dwarf, Parallax Measurement, Halo Population         INTRODUCTION     In recent years there has been considerable interest in studying the properties of high proper motion (HPM) white dwarfs because they can provide important information about the formation history of our Galaxy. For example, by measuring the space motions of individual objects we can determine whether they belong to the thin disk, thick disk, or halo populations. Furthermore, if we know the distance to each object then we can estimate its age using cooling models. Finally, accurate photometry combined with spectroscopy allows us to measure atmospheric parameters such as effective temperature, surface gravity, and chemical composition. All of these quantities together give us valuable insight into the evolution of the Milky Way.     There are several ongoing surveys dedicated to finding HPM white dwarfs including the Palomar Observatory Sky Survey (POSS), SuperCOSMOS Sky Survey (SSS), Sloan Digital Sky Survey (SDSS), UK Schmidt Telescope Photometric H-Alpha Survey (UKSTPHAS), and Panoramic Survey Telescope & Rapid Response System (Pan-STARRS). Most recently, the Catalina Real-Time Transient Survey (CRTS) has discovered more than 1000 HPM white dwarves over the past decade. Many of them were found serendipitously during searches", "paraphrased_abstract": "In recent years, interest has been expressed in studying the properties of the white dwarfs of high motion, particularly those of the low disk, the thick disk, and the halo. This is done by measuring the space motion of objects to determine whether they belong to the low disk, the high disk, or the halo. Moreover, the measurements of spectroscopic properties give an insight into the formation of the galaxy. This is done by detecting the distance to each object, by measuring the effective temperature, by the chemical composition of the air, by measuring the density of the atmosphere, by examining the climatology of the atmosphere, by estimating the time of their formation, and by detecting the time of their formation. Then we present an overview of the structure of the six high-pitch white dwarves and the white dwarfs, as well as the general characteristics of the four white dwarfs. The four white dwarfs are all members of the halo population, according to their kinematics. We show that three of these four stars have masses less than 0.5 Msun, which is consistent with our theoretical prediction of white dwarfs of this size, but one has an unexpected mass of 1.1 Msun, which is perhaps a new type of massive white dwarfs. We report the data from the tangential parallax measurements of four white dwarfs with a ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.3643, "title": "Bayesian analysis of the low-resolution polarized 3-year WMAP sky maps", "abstract": "  We apply a previously developed Gibbs sampling framework to the foreground corrected 3-yr WMAP polarization data and compute the power spectrum and residual foreground template amplitude posterior distributions. We first analyze the co-added Q- and V-band data, and compare our results to the likelihood code published by the WMAP team. We find good agreement, and thus verify the numerics and data processing steps of both approaches. However, we also analyze the Q- and V-bands separately, allowing for non-zero EB cross-correlations and including two individual foreground template amplitudes tracing synchrotron and dust emission. In these analyses, we find tentative evidence of systematics: The foreground tracers correlate with each of the Q- and V-band sky maps individually, although not with the co-added QV map; there is a noticeable negative EB cross-correlation at l <~ 16 in the V-band map; and finally, when relaxing the constraints on EB and BB, noticeable differences are observed between the marginalized band powers in the Q- and V-bands. Further studies of these features are imperative, given the importance of the low-l EE spectrum on the optical depth of reionization tau and the spectral index of scalar perturbations n_s. ", "machine_abstract": "We present an update on our previous work (Jarosik et al., 2006) where we used Bayesian methods to analyze the polarization data obtained by Wilkinson Microwave Anisotropy Probe (WMAP). We use the same likelihood function as in Jarosik et al. (2006), but now include also information about the noise covariance matrix and its uncertainty, which was not available at that time. The main goal is to test whether the observed large-scale temperature-polarization correlation can be explained within the standard cosmological model with adiabatic initial conditions and Gaussian primordial fluctuations or it requires some additional physics beyond this simple picture. In particular, we are interested in testing predictions made recently by several authors who suggested that such correlations could arise due to gravitational lensing effects induced by large scale structure along the line-of-sight. Our results show no significant evidence for any deviations from the simplest theoretical models considered here.", "paraphrased_abstract": "The main goal of the present investigation is to find out whether the observed large-scale temperature-polarization correlation can be explained within the standard cosmological model with the adiabatic initial conditions and the Gaussian primordial fluctuations, or it requires some additional physics beyond this simple model. We make a recent review of our previous work, namely Jarosik et al., and we consider the data of the Wilkinson Microwave Anisotropy Probe. We reappraise our results in accordance with the same likelihood function as Jarosik et al., but with additional information on the noise covariance matrix, which was not yet known at that time. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 704.2179, "title": "Spitzer spectral line mapping of supernova remnants: I. Basic data and principal component analysis", "abstract": "  We report the results of spectroscopic mapping observations carried out toward small (1 x 1 arcmin) regions within the supernova remnants W44, W28, IC443, and 3C391 using the Infrared Spectrograph of the Spitzer Space Telescope. These observations, covering the 5.2 - 37 micron spectral region, have led to the detection of a total of 15 fine structure transitions of Ne+, Ne++, Si+, P+, S, S++, Cl+, Fe+, and Fe++; the S(0) - S(7) pure rotational lines of molecular hydrogen; and the R(3) and R(4) transitions of hydrogen deuteride. In addition to these 25 spectral lines, the 6.2, 7.7, 8.6, 11.3 and 12.6 micron PAH emission bands were also observed. Most of the detected line transitions have proven strong enough to map in several sources, providing a comprehensive picture of the relative distribution of the various line emissions observable in the Spitzer/IRS bandpass. A principal component analysis of the spectral line maps reveals that the observed emission lines fall into five distinct groups, each of which may exhibit a distinct spatial distribution: (1) lines of S and H2 (J > 2); (2) the H2 S(0) line; (3) lines of ions with appearance potentials less than 13.6 eV; (4) lines of ions with appearance potentials greater than 13.6 eV, not including S++; (5) lines of S++. Lines of group (1) likely originate in molecular material subject to a slow, nondissociative shock that is driven by the overpressure within the supernova remnant, and lines in groups (3) - (5) are associated primarily with dissociative shock fronts with a range of (larger) shock velocities. The H2 S(0) line shows a low-density diffuse emission component, and - in some sources - a shock-excited component. ", "machine_abstract": "We present the first results on Spitzer infrared spectroscopy (IRS) observations of four Galactic supernova remnants, G11.2\u22120.3, Kes 17, RCW 103, and W44. The main goal is to study their physical conditions in detail by analyzing the observed emission lines with non-LTE radiative transfer models. We find that all these objects are dominated by dense molecular gas at temperatures between 100 K and 300 K. In addition we detect atomic hydrogen emission lines which indicate the presence of hotter plasma components. For two sources, Kes 17 and W44, we also observe strong [O III] emission indicating shock-heated ionized gas. Finally, for one source, Kes 17, we identify an additional cold dust component at T = 20 K. This work demonstrates the power of mid-infrared spectroscopy as a tool to investigate the physics of supernova remnants. It will be followed up by detailed studies of individual targets using more sophisticated modeling techniques.     Keywords: Supernova Remnant, Mid-Infrared Spectroscopy", "paraphrased_abstract": "\u201cThe first results of this study were obtained on the spectroscopic observations made by Spitzer\u2019s infrared spectroscopic method on four objects: G 11.2 0.3, Kes 17, RCW 103 and W44. All these objects were dominated by dense molecular gas, with temperatures between 100 K and 300 K. Two sources of gas were detected with the atomic hydrogen spectroscopic method, indicating a hotter plasma. For one object, Kes 17, we found an additional component of cold dust at a temperature of 20 K. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.2345, "title": "Uniqueness of polynomial canonical representations", "abstract": "  Let P(z) and Q(y) be polynomials of the same degree k>=1 in the complex variables z and y, respectively. In this extended abstract we study the non-linear functional equation P(z)=Q(y(z)), where y(z) is restricted to be analytic in a neighborhood of z=0. We provide sufficient conditions to ensure that all the roots of Q(y) are contained within the range of y(z) as well as to have y(z)=z as the unique analytic solution of the non-linear equation. Our results are motivated from uniqueness considerations of polynomial canonical representations of the phase or amplitude terms of oscillatory integrals encountered in the asymptotic analysis of the coefficients of mixed powers and multivariable generating functions via saddle-point methods. Uniqueness shall prove important for developing algorithms to determine the Taylor coefficients of the terms appearing in these representations. The uniqueness of Levinson's polynomial canonical representations of analytic functions in several variables follows as a corollary of our one-complex variables results. ", "machine_abstract": "We prove that the set of all polynomials in one variable with integer coefficients is uniquely representable as a sum of squares of rational functions, and we give an explicit algorithm for computing such representation.  We also show how to extend this result to multivariate polynomials over finite fields. The problem of representing a given polynomial as a sum of squares (SOS) has been studied extensively since Hilbert's 19th century work on quadratic forms. In particular, it was shown by Artin that every nonnegative integer can be represented as a sum of four squares of integers; see [1] . This theorem implies that any positive integer n can be written as a sum of at most 4 square roots of integers. However, there are infinitely many ways to write each number as a sum of two or three square roots of integers. For example, 1 = 2 + \u221a 3 , but also 1 = 2 \u2212 \u221a 3 . The uniqueness question becomes more interesting when restricted to certain classes of polynomials. It turns out that if f \u2208 Z[x] is a monic polynomial whose Galois group G(f ) is solvable then f is uniquely representable as SOS [2] . On the other hand, if G(f ) contains a nonsolvable subgroup then f may have several distinct SOS representations [3] .", "paraphrased_abstract": "It was shown by Artin that every non-negative integer was represented by four squares of integers, see... Then he showed that every positive integer could be written as a sum of four square roots. This resulted in a different set of representations for each integer, which he gave in a specific way. We show that all polynomials of a variable with integer coefficients are uniquely represented by a square, and we propose a general method for this. We prove that the whole group of polynomials in one variable is unique by a square, and we present an explicit method for this calculation. Moreover, we prove that the whole group of all polynomials in a single variable is unique by a square, and we present an explicit algorithm for the computation of this representation. For instance, if f = Zx, a monic polynomial whose Galois group G (f) is solvable, then f is uniquely represented by a square SOS. ", "model": "bigscience bloomz", "source": "arxiv"}
{"source_id": 705.0711, "title": "Off-center HII regions in power-law density distributions", "abstract": "  The expansion of ionization fronts in uniform and spherically symmetric power-law density distributions is a well-studied topic. However, in many situations, such as a star formed at the edge of a molecular cloud core, an offset power-law density distribution would be more appropriate. In this paper a few of the main issues of the formation and expansion of HII regions in such media are outlined and results are presented for the particular cases where the underlying power laws are 1/r^2 and 1/r^3. A simple criterion is developed for determining whether the initial photoionized region will be unbounded, which depends on the power-law exponent and the ratio of the equivalent Stroemgren radius produced by the star in a uniform medium to the stellar offset distance. In the expansion stage, the ionized volumes will eventually become unbounded unless pressure balance with the external medium is reached before the ionization front velocity becomes supersonic with respect to the ionized gas. ", "machine_abstract": "We present the results of numerical simulations aimed at studying the formation and evolution of off-center ionized hydrogen (HII) regions around massive stars embedded in an isothermal, self-gravitating gas distribution with a power-law density profile.  We find that the expansion velocity of the ionization front decreases as it moves away from the star's position towards larger radii. The decrease occurs because the pressure gradient becomes smaller when moving outward due to the decreasing density. As a result, the ionization front may become unstable against small perturbations which lead to its fragmentation into several blobs or fingers. These blobs are surrounded by dense shells of neutral material produced by photo-evaporation of the surrounding medium. In addition, we show that the mass loss rate increases significantly for large values of the index n of the power law density distribution. This increase can be explained by the fact that the gravitational force per unit volume acting on the shell decreases faster than the ram pressure of the expanding bubble.", "paraphrased_abstract": "A swarm of molecules in the tropics, surrounded by a dense, non-conductive shell, are formed. These molecules are surrounded by a thick layer of neutral matter, evaporating from the surrounding medium. We present the results of the numerical simulations of the formation and evolution of ionized hydrogen, which is ionized hydrogen, on massive stars in a self-gravitating, nongravitating gas. We show that the density of the ionized hydrogen decreases with the height of the star, and decreases as the density decreases. The decrease in density can be explained by the fact that the gravitational force per volume decreases faster than the ram pressure of the expanding bubble. The mass loss rate is a major factor of increasing the index n of the density, which increases by the number of atoms per unit volume. ", "model": "bigscience bloomz", "source": "arxiv"}
